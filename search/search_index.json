{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"On becoming an OBO Semantic Engineer","text":"<p>Welcome to the OBOOK and our OBO Semantic Engineering Training!</p>"},{"location":"#introduction-to-the-obook-open-biological-and-biomedical-ontologies-organized-knowledge","title":"Introduction to the OBOOK (Open Biological and Biomedical Ontologies Organized Knowledge)","text":"<p>Documentation in the OBOOK is organised into 4 distinct sections based on the Di\u00e1taxis framework of documentation:</p> <ol> <li>Tutorials: Learning-oriented documentation that contains exercises to help a beginner achieve basic competence in a specific area.</li> <li>How-to Guides: Task-oriented documentation that functions as directions to guide the reader through the steps to achieve a specific end.</li> <li>Reference Guides: Information-oriented documentation that describes a single topic in a succinct, technical and orderly way.</li> <li>Explanations: Understanding-oriented documentation that clarifies, deepens and broadens the reader\u2019s understanding of a subject.</li> </ol> <p>To accommodate for the various training use cases we support, we added the following categories:</p> <ol> <li>Courses: A convenience content type that assembles materials from OBOOK for a specific, taught unit, such as the yearly ICBO tutorials.</li> <li>Pathways: A kind of course pertaining to a specific role in the OBO-sphere, such as curator or software developer.</li> <li>Lessons: A collection of materials (tutorials, explanations and how-to guides) that together aim to teach a well defined concept.</li> </ol> <p>Note: We are in the process of transforming the documentation accordingly, so please be patient if some of the documentation is not yet in the correct place. Feel free to create an issue if you find something that you suspect isn't in place.</p>"},{"location":"#editorial-team","title":"Editorial Team","text":"<ul> <li>James Overton, Knocean Inc. James has been a developer of software to support ontology development in and around the OBO sphere for more than a decade and delivers services to the OBO community through his company, Knocean Inc.</li> <li>Becky Jackson, Bend Informatics. Becky has been a Semantic Software developer and Ontology Pipeline specialist since 2016, now working as an independent consultant.</li> <li>Nicole Vasilevsky, Critical Path Institute. Nicole is the Associate Director of Data Science and an Ontology Curator for the Mondo Disease Ontology, and contributes to several other ontologies including the Human Phenotype Ontology, Uberon Anatomy Ontology, and Cell Ontology.</li> <li>Nicolas Matentzoglu, Semanticly, Athens, Greece. Nico is an Ontology Engineer and Ontology Pipeline Specialist, being the Principal Ontology Pipeline Developer for the Monarch Initiative.</li> <li>Shawn Tan, Novo Nordisk</li> <li>David Osumi-Sutherland, EMBL-EBI</li> <li>Sabrina Toro, University of North Carolina at Chapel Hill</li> </ul> <p>If you would like to contribute to this training, please find out more here.</p>"},{"location":"#content","title":"Content","text":"<ul> <li>Getting started (read this first)</li> <li>Course overview and lessons</li> <li>How to contribute</li> <li>Getting started for OBOOK documentation writers</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":""},{"location":"#critical-path-institute","title":"Critical Path Institute","text":"<p>Critical Path Institute (CPI) is an independent, nonprofit organisation dedicated to bringing together experts from regulatory agencies, industry and academia to collaborate and improve the medical product development process.</p> <p>In April 2021, the CPI has commissioned the first version of this OBO course, contributing not only funding for the preparation and delivery of the materials, but also valuable feedback about the course contents and data for the practical exercises. We thank the CPI for contributing significantly to the OBO community and open science!</p> <p>https://c-path.org/</p>"},{"location":"contributing/","title":"Contributing to OBO Semantic Engineering Tutorials","text":"<p>We rely on our readers to correct our materials and add to them - the hope is to centralise all the usual teaching materials for OBO ontology professionals in one place. Feel free to:</p> <ul> <li>Request new lessons (video/how-to guides) on the issue tracker</li> <li>Make a pull request if you find errors or want to add some clarifying remarks. All files of the OBOAcademy website can be found in the docs directory. The Table of Contents is edited here.</li> <li>Join the OBO slack space (<code>#obo-training</code> channel) to ask any questions (you can request access on the issue tracker)</li> </ul>"},{"location":"getting-started-obook/","title":"Getting started for OBOOK editors","text":"<p>The OBOOK is trying to centralise all OBO documentation in one place. It is, and will be, a big construction site, for years to come. The goal is to iterate and make things better.</p> <p>We follow two philosophies:</p> <ul> <li>Diataxis for organising our documentation</li> <li>Flipped classroom for the organisation of training materials</li> </ul> <p>There are three main consequences to this:</p> <ol> <li>Our materials are organised in a certain way (according to the four-way split suggested by Diataxis).</li> <li>We superimpose three more categories to organise the content across all materials and facilitate self guided studying: Pathways, courses and lessons (see below).</li> <li>All training materials must be self-contained to ensure that they can be studied without any further guidance by a teacher.</li> </ol>"},{"location":"getting-started-obook/#preparation","title":"Preparation","text":"<ul> <li>Browse through this page: https://diataxis.fr/</li> <li>Watch the introduction to the Diataxis framework:</li> </ul>"},{"location":"getting-started-obook/#beyond-diataxis-the-obook-categories","title":"Beyond Diataxis: the OBOOK categories:","text":"<p>We just introduced a new concept to OBOOK called <code>pathways</code>. The idea is that we provide a linear guide for all 6 roles mentioned on the getting started page through the materials. This will help us also complete the materials and provide a good path to reviewing them regularly.</p>"},{"location":"getting-started-obook/#tutorial","title":"Tutorial","text":"<p>A step-by-step guide to complete a well-defined mini-project. Examples: ROBOT template tutorial. DOSDP template tutorial. Protege tutorial on using the reasoner.</p>"},{"location":"getting-started-obook/#lesson","title":"Lesson","text":"<p>A collection of materials (tutorials, explanations and how-to-guides) that together seek to teach a well defined concept. Examples: Contributing to OBO ontologies; An Introduction to templates in OBO; An Introduction to OBO Application development. While the distinction to \"tutorial\" is often fuzzy, the main distinguishing feature should be that a lesson conveys a general concept independent of some concrete technology stack. While we use concrete examples in lessons, we do always seek to generalise to problem space.</p>"},{"location":"getting-started-obook/#course","title":"Course","text":"<p>A convenience content type that allows us to assemble materials from obook for a specific taught unit, such as the yearly ICBO tutorials, or the ongoing Monarch Ontology Tutorials and others. Course pages serve as go-to-pages for course participants and link to all the relevant materials in the documentation. Course usually comprise lessons, tutorials and how-to guides.</p>"},{"location":"getting-started-obook/#pathways","title":"Pathways","text":"<p>A pathway is a kind of course, but without the expectation that it is ever taught in a concrete setting. A pathways pertains to a single concrete role (Ontology Curator, Pipeline Developer etc). It is a collection of materials (lessons, tutorials, how-to-guides) that is ordered in a linear fashion for the convenience of the student. For example, we are developing a pathway for ontology pipeline developers that start by teaching common concepts such as how to make term requests etc, and then go into depth on ROBOT pipelines, ODK and Make.</p>"},{"location":"getting-started-obook/#best-practices","title":"Best practices:","text":"<ul> <li>Items in the explanation section should conceptually start with a Why or a How question.</li> <li>For ordered lists, only use 1. 1. 1., ever 1. 2. 3. This makes it easier to remove and shuffle items during edits</li> </ul>"},{"location":"getting-started/","title":"How to start with the lessons","text":"<p>Before you start with the lessons of this course, keep the following in mind:</p> <ol> <li>The materials in this course are all intended to be used for self-study. We sometimes offer flipped-classroom sessions for new members on our teams - this means that we expect them to work through the entire course themselves and then come to us with questions and requests for clarifications.</li> <li>There is no need to reinvent the wheel: there are a lot of great materials out there already. Providing references to these external resources is an essential part of the course - some lessons primarily comprise external tutorials, blog articles and more - please make sure you take advantage of them.</li> <li>Some of the materials developed by us are a bit rough around the edges, and we need your help to fix and improve them. To that end, we appreciate anything from suggestions for improvement to pull requests.</li> <li>Depending on your specific role and interest, you can choose which lessons are relevant to you. There is no specific order, but if you want to start somewhere, we recommend Contributing to OBO ontologies: Protege and Github and/or Using Ontologies and Ontology Terms. If you have no prior knowledge of ontology and GitHub experience, and prefer a simplifed guide, we recommend The Basics of GitHub: Organizations, Repositories, &amp; Issues.</li> </ol> <p></p>"},{"location":"getting-started/#the-different-roles-of-obo-semantic-engineering","title":"The different roles of OBO Semantic Engineering","text":"<p>There are a wide variety of entry points into the OBO world, for example:</p>"},{"location":"getting-started/#database-curator-you-are","title":"Database Curator: You are","text":"<ul> <li>using ontologies for annotating datasets, experiments and publications</li> <li>requesting new terms from ontologies</li> <li>suggesting corrections to existing ontologies, such as wrong or missing synonyms, typos and definitions</li> </ul>"},{"location":"getting-started/#ontology-curator-you-are","title":"Ontology Curator: You are","text":"<ul> <li>developing and maintaining ontologies</li> <li>adding terms to ontologies</li> <li>performing changes to ontologies, like adding or correcting synonyms</li> <li>responsible for ontology releases</li> </ul>"},{"location":"getting-started/#ontology-engineerdeveloper-you-are","title":"Ontology Engineer/Developer: You are","text":"<ul> <li>developing design patterns for ontologies, specifying the logical structure of terms</li> <li>responsible for ensuring the specification and consistent application of metadata in your ontologies (which annotation properties to use, minimal metadata standards)</li> <li>defining quality control checks</li> </ul>"},{"location":"getting-started/#ontology-pipeline-specialist-you-are","title":"Ontology Pipeline Specialist: You are","text":"<ul> <li>developing ontology pipelines with <code>make</code> and <code>ROBOT</code></li> <li>building the release and quality control architecture that Engineers and Curators need to do their work.</li> <li>building infrastructure for application ontologies, implementing dynamic imports modules, transformations of and mappings to other ontologies.</li> </ul>"},{"location":"getting-started/#semantic-etl-engineer-you-are","title":"Semantic ETL Engineer: You are","text":"<ul> <li>building ingests from public life science resources such as Bgee, Panther, UniProt and many more</li> <li>using ontologies to glue together data from different sources</li> <li>using ontologies to augment the information in your data sources through inference</li> </ul>"},{"location":"getting-started/#semantic-software-engineer-you-are","title":"(Semantic) Software Engineer: You are","text":"<ul> <li>using ontologies to generate value to end-user applications (user interfaces, semantic faceted search)</li> <li>building widgets that exploit the logical and graph structure of ontologies, for example phenotypic profile matching</li> <li>building ontology term browsers such as OLS.</li> </ul> <p>Of course, many of you will occupy more than one of the above \"hats\" or roles. While they all require specialised training, many shared skill requirements exist. This course is being developed to:</p> <ul> <li>Provide basic training for OBO Semantic Engineers of any of the above flavours</li> <li>Provide an entry point for people new to the field, for example as part of onboarding activities for projects working with ontologies</li> <li>Capture some of the typical pitfalls and how-to's guides to address common problems across the OBO-sphere</li> </ul>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#lessons","title":"Lessons","text":""},{"location":"overview/#using-ontologies-and-ontology-terms","title":"Using Ontologies and Ontology Terms","text":"<ul> <li>Target roles: Database Curators</li> <li>Skills:</li> <li>know what ontologies are good for</li> <li>find good ontologies: ontology repositories, OBO</li> <li>find good terms: ontology browsers</li> <li>assess for use: license, quality</li> <li>map local terms to ontology terms</li> <li>identify missing terms</li> <li>use IRIs, prefixes, CURIEs, labels</li> <li>use Protege?</li> </ul>"},{"location":"overview/#contributing-to-obo-ontologies-1-protege-and-github","title":"Contributing to OBO ontologies 1: Protege and Github","text":"<ul> <li>Target roles: Database Curators, Ontology Curator, Ontology Engineer/Developer</li> <li>Builds on:</li> <li>Ontology Term Use</li> <li>Skills:</li> <li>use GitHub: issues, Pull Requests</li> <li>understand basic Open Source etiquette<ul> <li>reading READMEs</li> </ul> </li> <li>understand basics of ontology development workflows</li> <li>understand ontology design patterns</li> <li>use templates: ROBOT, DOS-DP</li> <li>basics of OWL</li> </ul>"},{"location":"overview/#ontology-fundamentals","title":"Ontology Fundamentals","text":"<ul> <li>Target roles: Ontology Curators, Ontology Engineer/Developer</li> <li>Builds on:</li> <li>Ontology Term Use</li> <li>Skills:</li> <li>RDF</li> <li>RDFS</li> <li>OWL</li> <li>Reasoners</li> <li>basic SPARQL</li> <li>Turtle, JSON-LD</li> </ul>"},{"location":"overview/#linked-data-analysis","title":"Linked Data Analysis","text":"<ul> <li>Target roles: Ontology Curators, (Semantic) Software Engineer</li> <li>Builds on:</li> <li>Ontology Fundamentals</li> <li>Skills:</li> <li>Advanced SPARQL</li> <li>Term enrichment</li> <li>Semantic similarity</li> <li>Named Entity Recognition</li> <li>more...</li> </ul>"},{"location":"overview/#ontology-development","title":"Ontology Development","text":"<ul> <li>Builds on:</li> <li>Ontology Fundamentals</li> <li>Contributing to OBO ontologies</li> <li>Skills:</li> <li>Manage GitHub</li> <li>Manage ontology imports</li> <li>Use ROBOT extract: MIREOT, SLME</li> <li>Use ROBOT report</li> <li>Pruning trees</li> </ul>"},{"location":"overview/#semantic-databases","title":"Semantic Databases","text":"<ul> <li>Builds on:</li> <li>Ontology Development</li> <li>Skills:</li> <li>advanced term mapping</li> <li>ontology terms in SQL</li> <li>terminology table JOINs, constraints</li> <li>convert tables to triples</li> <li>triplestores</li> <li>knowledge graphs</li> </ul>"},{"location":"overview/#automating-ontology-development-workflows","title":"Automating Ontology Development Workflows","text":"<ul> <li>Builds on:</li> <li>Ontology Development</li> <li>Ontology Pipelines</li> <li>Skills:</li> <li>Unix shell</li> <li><code>make</code></li> <li>Advanced git, GitHub</li> <li>ROBOT</li> <li>ODK</li> </ul>"},{"location":"overview/#developing-an-obo-reference-ontology","title":"Developing an OBO Reference Ontology","text":"<ul> <li>Builds on:</li> <li>Ontology Development Automation</li> <li>Skills:</li> <li>Detailed knowledge of OBO principles and best practises</li> <li>Use OBO Dashboard</li> <li>Use OBO Registry</li> <li>Use PURL system</li> </ul>"},{"location":"overview/#tutorials","title":"Tutorials","text":"<ul> <li>ROBOT Tutorial 1: Convert, Extract and Template</li> <li>ROBOT Tutorial 2: Annotate, Merge, Reason and Diff</li> <li>Introduction to GitHub</li> <li>Intro to managing and tracking issues in GitHub</li> </ul>"},{"location":"overview/#how-to-guides","title":"How-to guides","text":"<ul> <li>Install Elk 0.5 in Protege</li> <li>Getting set up with Docker and the Ontology Development Kit</li> </ul>"},{"location":"config/template/","title":"Course title","text":""},{"location":"config/template/#prerequisites","title":"Prerequisites","text":"<ul> <li>TBD</li> </ul>"},{"location":"config/template/#preparation","title":"Preparation","text":"<ul> <li>TBD</li> </ul>"},{"location":"config/template/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: add here</p>"},{"location":"config/template/#learning-objectives","title":"Learning objectives","text":"<ul> <li>add learning objectives here</li> </ul>"},{"location":"config/template/#tutorials","title":"Tutorials","text":"<ul> <li>in person or video (link videos here as they become available)</li> </ul>"},{"location":"config/template/#additional-materials-and-resources","title":"Additional materials and resources","text":"<ul> <li>TBD</li> </ul>"},{"location":"config/template/#contributors","title":"Contributors","text":"<ul> <li>Add contributors with ORCID here</li> </ul>"},{"location":"courses/icbo2021/","title":"ICBO Tutorial 2021","text":"<p>Wednesday, September 15, 2021</p> <ul> <li>4 PM to 7 PM CEST</li> <li>10 AM to 1 PM EDT</li> <li>7 AM to 10 AM PDT</li> </ul>"},{"location":"courses/icbo2021/#goal","title":"Goal","text":"<p>The goal of this tutorial is to provide a flavor of the OBO landscape, from the OBO Foundry organization to the ontology curators and OBO engineers that are doing the daily ontology development.</p>"},{"location":"courses/icbo2021/#organizers","title":"Organizers","text":"<ul> <li>James A. Overton, Knocean Inc.</li> <li>Chris Mungall, Lawrence Berkeley National Laboratory</li> <li>Nicole Vasilevsky, University of Colorado Anschutz Medical Campus</li> <li>Nico Matentzoglu, semanticly Ltd</li> <li>Randi Vita, La Jolla Institute for Allergy &amp; Immunology</li> </ul>"},{"location":"courses/icbo2021/#agenda","title":"Agenda","text":"Time CEST Presenter Topic 4:00 - 4:10pm James Overton Workshop overview 4:10 - 4:20pm James Overton OBO Foundry Overview 4:20 - 4:30pm Nicole Vasilevsky Controlled Vocabularies and Ontologies 4:30 - 4:50pm Nicole Vasilevsky Using and Reusing Ontology Terms 4:50 - 5:25pm Nicole Vasilevsky A day in the life of an Ontology Curator 5:25 - 5:30pm Break 5:30pm - 5:40pm Nico Matentzoglu Ontology 201 Overview 5:40 - 6:15 pm James Overton ROBOT Tutorial 6:15 - 6:35 pm Nico Matentzoglu ODK presentation 6:35 - 6:55 pm Nico Matentzoglu A brief introduction into ontology QC using the OBO dashboard 6:55 - 7:00 pm James Overton Wrap up"},{"location":"courses/icbo2022/","title":"ICBO OBO Tutorial 2022: Using and Reusing Ontologies","text":"<p>September 26, 2022, 9:00 am \u2013 12:30 pm ET</p> <ul> <li>Overview</li> <li>Organizers</li> <li>Agenda</li> </ul> <p>We'd love any feedback on this tutorial via this short survey.</p> <p></p>"},{"location":"courses/icbo2022/#overview","title":"Overview","text":"<p>The Open Biological and Biomedical Ontologies (OBO) community includes hundreds of open source scientific ontology projects, committed to shared principles and practices for interoperability and FAIR data. An OBO tutorial has been a regular feature of ICBO for a decade, introducing new and experienced ontology users and developers to ontologies in general, and to current OBO tools and techniques specifically. While ICBO attracts many ontology experts, it also includes an audience of ontology beginners, and of ontology users looking to become ontology developers or to further refine their skills. Our OBO tutorial will help beginner and intermediate ontology users with a combination of theory and hands-on practice.</p> <p>For ICBO 2022 we will host a half-day OBO tutorial consisting of two parts, with a unifying theme of ontology term reuse.</p> <p>The first part of our tutorial will be introductory, aimed at an audience that is new to ontologies and to the OBO Foundry. We will introduce OBO, its community, principles, resources, and best practices. We will finish the first part with a hands-on lesson in basic tools: ontology browsers, how to contribute to ontologies via GitHub (creating issues and making Pull Requests), and the Protege ontology editor.</p> <p>The second part will build on the first, addressing an audience that is familiar with ontologies and OBO, and wants to make better use of OBO workflows and tools in their own projects. The focus will be on making best use of OBO community open source software. We will introduce ROBOT, the command-line tool and library for automating ontology development tasks. We will show how the Ontology Development Kit (ODK) is used to standardize ontology projects with a wide range of best practices. The special emphasis of this year's tutorial will be ontology reuse, and specifically on how ROBOT and ODK can be used to manage imports from other ontologies and overcome a number of challenges to term reuse.</p> <p>This material for this year's OBO Tutorial will build on the content here in the OBO Academy. The OBO Academy offers free, open, online resources with self paced learning materials covering various aspects of ontology development and curation and OBO. Participants are encouraged to continue their learning using this OBO Academy website, and contribute to improving the OBO documentation.</p> <p>As an outcome of this workshop, we expect that new ontologists will have a clearer understanding of why we need and use ontologies, how to find ontology terms and contribute to ontologies and make basic edits using Protege. Our more advanced participants should be able to apply OBO tools and workflows to their own ontology development practices.</p> <p></p>"},{"location":"courses/icbo2022/#organizers","title":"Organizers","text":"<ul> <li>James A. Overton, Knocean Inc.</li> <li>Becky Jackson, Bend Informatics</li> <li>Chris Mungall, Lawrence Berkeley National Laboratory</li> <li>Nicole Vasilevsky, University of Colorado Anschutz Medical Campus</li> <li>Nico Matentzoglu, Semanticly, Athens, Greece</li> <li>Randi Vita, La Jolla Institute for Allergy &amp; Immunology</li> </ul>"},{"location":"courses/icbo2022/#agenda","title":"Agenda","text":"Time Topic Presenter 09:00 am ET Introduction to OBO, its community, principles, resources, and best practices James Overton 09:20 am ET Hands-on lesson in basic tools: see details below Nicole Vasilevsky 10:15 am ET Coffee break 10:30 am ET Introduction to ROBOT Becky Jackson 11:30 pm ET Introduction to the Ontology Development Kit (ODK) and Core Workflows Nico Matentzoglu 12:15 pm ET How to be an open science ontologist  (Slides are here) Nico Matentzoglu"},{"location":"courses/icbo2022/#hands-on-lesson-in-basic-tools","title":"Hands on lesson in basic tools","text":"<p>Instructor: Nicole Vasilevsky</p>"},{"location":"courses/icbo2022/#outline","title":"Outline","text":"<ol> <li>Protege ontology editor<ul> <li>Protege basic functionality</li> <li>Plugins. See guide on installing ELK reasoner.</li> </ul> </li> <li>How to contribute to ontologies via GitHub<ul> <li>ontology maintenance and workflows</li> <li>practice with reasoners</li> <li>making Pull Requests (PRs)</li> </ul> </li> </ol> <p>Example: We will work on this ticket.  </p>"},{"location":"courses/icbo2023/","title":"ICBO OBO Tutorial 2023: Using and Reusing Ontologies","text":"<p>Conference website: https://icbo-conference.github.io/icbo2023/ </p> <p>ICBO Workshops details: https://www.icbo2023.ncor-brasil.org/program.html#workshops </p> <p>Date: August 28, 2023 13:30-15:00   (Part 1) and 15:30-17:15 (Part 2) </p> <p>Tutorial recordings: </p> <ul> <li>part 1: https://www.youtube.com/watch?v=83lI3u7KX0g</li> <li>part 2: https://www.youtube.com/watch?v=m2khZcJVKU0)</li> </ul> <p>Table of content: </p> <ul> <li>Overview</li> <li>Organizers</li> <li>Preparation</li> <li>Agenda</li> </ul> <p></p>"},{"location":"courses/icbo2023/#overview","title":"Overview","text":"<p>The Open Biological and Biomedical Ontologies (OBO) community includes hundreds of open source scientific ontology projects,  committed to shared principles and practices for interoperability and FAIR data. An OBO tutorial has been a regular feature of ICBO for a decade, introducing new and experienced ontology users and developers to ontologies in general, and to current OBO tools and techniques specifically. While ICBO attracts many ontology experts, it also includes an audience of ontology beginners, and of ontology users looking to become ontology developers or to further refine their skills. Our OBO tutorial will help beginner and intermediate ontology users with a combination of theory and hands-on practice.</p> <p>For ICBO 2023 we will host a half-day OBO tutorial consisting of two parts.</p> <p>The first part of our tutorial will be introductory, aimed at an audience that is new to ontologies and to the OBO Foundry.  We will introduce OBO, its community, principles, resources, and best practices.  We will finish the first part with a hands-on lesson in basic tools: ontology browsers, how to contribute to ontologies via  GitHub (creating issues and making Pull Requests), and the Protege ontology editor.</p> <p>The second part will build on the first, addressing an audience that is familiar with ontologies and OBO, and wants to make better use of  OBO workflows and tools in their own projects. </p> <p>This material for this year's OBO Tutorial will build on the content here in the OBO Academy.  The OBO Academy offers free, open, online resources with self paced learning materials covering various aspects of ontology development and  curation and OBO. Participants are encouraged to continue their learning using this OBO Academy website, and contribute to improving the  OBO documentation.</p> <p></p>"},{"location":"courses/icbo2023/#organizers","title":"Organizers","text":"<ul> <li>Nicole Vasilevsky, Critical Path Institute, Tucson, AZ, USA (Based in Portland, OR, USA)</li> <li>Nico Matentzoglu, Semanticly, Athens, Greece</li> <li>Tiago Lubiana, Universidade de S\u00e3o Paulo, S\u00e3o Paulo, SP, Brazil</li> </ul>"},{"location":"courses/icbo2023/#advance-preparation","title":"Advance preparation","text":"<p>The tutorial is designed to be 'show and tell' format, but you are welcome to install the following software on your machine in advance, if you'd like to follow along in real time:  </p> <ul> <li>GitHub Desktop</li> <li>Protege</li> <li>ROBOT</li> </ul> <p></p>"},{"location":"courses/icbo2023/#agenda","title":"Agenda","text":"<p>Time are in UTC - 3:00, Brasilia, Brazil</p> <p>THE REAL SCHEDULE (updated LIVE) can be found here.</p> Time Topic Presenter Duration 13:30 pm Welcome Tiago Lubiana 10 min 13:40 pm Introduction to OBO, its community, principles, resources, and best practices Darren Natale 20 min 14:00 pm Hands-on lesson in basic tools Sabrina Toro 40 min 14:40 pm Protege updates and new features Damien Goutte-Gattat 20 min 15:00 pm Break 15 min 15:15 pm Overview of OBO Dashboard Anita Caron 20 min 15:35 pm Introduction to ROBOT Becky Jackson 45 min 16:20 pm Role of ChatGPT in OBO Ontology Development Sierra Moxon 20 min 16:40 pm On Becoming An Open Science Engineer Nico Matentzoglu 20 min 17:00 pm Extended Question and Answers Tiago Lubiana 30 min 17:30 pm Adjourn"},{"location":"courses/icbo2024/","title":"ICBO Tutorial 2024","text":"<p>Conference website: https://icbo-conference.github.io/icbo2024/ </p> <p>Date: July 10, 2024, 8-10:30am PT/11-1:30pm ET/5-7:30pm Europe</p>"},{"location":"courses/icbo2024/#overview","title":"Overview","text":"<p>The Open Biological and Biomedical Ontology (OBO) Foundry is an international community of ontology developers dedicated to constructing ontologies based on a unified set of principles. With hundreds of ontologies already integrated into the OBO Foundry, and more continually being added, the community continues to expand and grow. The annual OBO Foundry Tutorial at ICBO welcomes both newcomers and experienced curators and developers in the field of ontology development. Our goal is to provide a virtual workshop where participants can gain insights into the latest updates, exchange knowledge, and explore current best practices in the field, with a focus on collaborative workflows.</p> <p>In this tutorial, we will build upon the freely available online training resources in the OBO Academy. We will explore strategies for effectively maintaining and advancing the development of community-based ontologies within a distributed and decentralized environment. Many OBO ontologies are built and managed by volunteers or dispersed development teams, all of which are openly accessible for community contributions and utilization. Through real-life use cases and introducing interactive challenges, we'll delve into best practices for leveraging GitHub to facilitate issue tracking, documentation, discussions, and automation pipelines, and discuss social workflows, and adherence to open science principles.</p> <p>The OBO community eagerly embraces fresh perspectives and contributions, fostering collaboration and innovation. Join us in shaping the future of ontology development.</p>"},{"location":"courses/icbo2024/#organizers","title":"Organizers","text":"<ul> <li>Nico Matentzoglu, Semanticly</li> <li>Nicole Vasilevsky, Critical Path Institute</li> </ul>"},{"location":"courses/icbo2024/#advance-preparation","title":"Advance preparation","text":"<p>The tutorial is designed to be 'show and tell' format, but you are welcome to install the following software on your machine in advance, if you'd like to follow along in real time:  </p> <ul> <li>GitHub Desktop</li> <li>Protege</li> <li>ROBOT</li> </ul>"},{"location":"courses/icbo2024/#agenda","title":"Agenda","text":"Topic Presenter Duration Welcome Nico Matentzoglu + Nicole Vasilevsky 5 min Introduction to OBO, its community, principles, resources, and best practices James Overton 10 min Introduction to GitHub and version control Sabrina Toro 15 min Collaborative workflows and open science Nico Matentzoglu 15 min Introduction to issues and pull requests (PRs) Nicole Vasilevsky 20 min Break 10 min How to write a good review Nicole Vasilevsky 25 min Enhancing collaborative workflows with GitHub actions: QCs, Diffs, Automatic updates of files Nico Matentzoglu 15 min Wrap up Nico Matentzoglu + Nicole Vasilevsky 5 min"},{"location":"courses/icbo2024/#slides","title":"Slides","text":"<p>Slides: OBO Foundry Tutorial:  Collaborative Editing Workflows in Biomedical Ontologies </p>"},{"location":"courses/icbo2024/#recording","title":"Recording","text":""},{"location":"courses/monarch-obo-training/","title":"OBO Academy - A Monarch Training Series","text":""},{"location":"courses/monarch-obo-training/#goal","title":"Goal","text":"<p>The goal of this course is to provide ongoing training for the OBO community. As with previous tutorials, we follow the flipped classroom concept: as organisers, we provide you with materials to look at, and you will work through the materials on your own or will guidance during an OBO Academy session. During our monthly meeting, we will answer your questions, provide you with additional demonstrations where needed and go into depth wherever you as a student are curious to learn more. This means that this course can only work if you are actually putting in the time to preparing the materials. That said, we nevertheless welcome anyone to just lurk or ask related questions.</p>"},{"location":"courses/monarch-obo-training/#you-students","title":"You (Students)","text":"<ul> <li>Join the Monarch Tutorials Mailing List to receive reminders about upcoming tutorials</li> <li>Join the OBO Slack Community and find us on the #OBO-Academy channel</li> <li>Read the \"Getting started\" guide</li> <li>Check which lessons interest you in the upcoming schedule - mark your calendar and just participate in the ones you care about</li> <li>Make detailed issues on our issue tracker when materials are broken, hard to follow or need more details - that is really important as we rely on you to help to improve our materials continuously.</li> <li>Find recordings of previous session on our YouTube channel</li> <li>Request new course units from us using the issue tracker. If you are intersted in leading a session, reach out to the organisers listed below! </li> </ul>"},{"location":"courses/monarch-obo-training/#we-tutors-and-organisers","title":"We (Tutors and Organisers)","text":"<ul> <li>Nico Matentzoglu, Semanticly, Athens, Greece</li> <li>Sabrina Toro, University of North Carolina at Chapel Hill</li> <li>Nicole Vasilevsky, Critical Path Institute</li> <li>Sarah Gehrke, University of North Carolina at Chapel Hill</li> <li>Prepare the materials for each lesson and provide schedules</li> <li>Build new training materials where needed</li> <li>Organise the course and rooms</li> </ul>"},{"location":"courses/monarch-obo-training/#schedule","title":"Schedule","text":"<p>Most session take place on the listed date at 8AM PT / 3PM UTC. If the tutorial time is different, those on the mailing list or slack will receive a message with the information. Note: this schedule is tentative and subject to change </p> Date Lesson Presenter / Notes Recordings 2026/03/31 Coming soon! 2026/03/03 Coming soon! 2026/02/03 Efficient Biocuration and Bioinformatics with Claude Code Part 2 Chris Tabone 2025/11/11 Generating SPARQL queries with ChatGPT Trish Whetzel Here 2025/10/14 Efficient Biocuration and Bioinformatics with Claude Code Part 1 Chris Tabone here July &amp; August &amp; Sept 2025 No Meeting Enjoy Summer break! 2025/06/24 From Fuzzy to Clear: Writing Ontology Definitions That Work  This is a tutorial for curators at any level. Sabrina Toro here 2025/06/09 Using AI coding apps for ontology developers. Tutorial material Chris Mungall Here 2025/04/08 Introduction to developing agentic workflows for semantic engineers. Tutorial material Chris Mungall Here 2025/02/04 More! Hands On Workshop: OntoGPT Harry Caufield 2024/10/15 Hands On Workshop: OntoGPT Harry Caufield, LBNL Here 2024/06/25 Structures and Instructors: Applying the Strengths of Large Language Models to Biomedical Informatics Harry Caufield Here 2024/05/14 Introduction to the legendary Uberon Anatomy ontology Damien Goutte-Gattat and Nico Matentzoglu Here 2024/04/30 An Introduction to Synonyms in OBO Ontologies Nicole Vasilevsky Here 2024/04/16 AI-assisted ontology editing workflows 2: Validation Chris Mungall, LBNL Here 2024/04/02 Phenotype data and the role of ontologies James McLaughlin and Nico Matentzoglu Here 2024/03/05 AI-assisted ontology editing workflows 1: generating and augmenting Chris Mungall Here 2024/02/20 Ontology Metadata Standardisation Anita Caron Here 2024/02/06 Introduction to Open Source licenses Seth Carbon 2024/01/23 Bridge2AI data standards: a practical introduction Harry Caufield Here 2023/12/12 Introduction to DOSDP pattern design and workflows Ray Stefancsik (Slides) Here 2023/11/14 Enhancing curation workflows with CurateGPT Chris Mungall Here 2023/10/17 Using ontologies for data annotation and consequences for ontology development Tutorial by Sabrina Toro Here 2023/10/03 Units modeling in and around OBO James Overton Here 2023/09/19 Improving ontology interoperability with Biomappings Charlie Hoyt Here 2023/09/05 Modern prefix management with Bioregistry and <code>curies</code> Charlie Hoyt Here 2023/08/22 How to determine if two entities are the same? Nico Matentzoglu Here 2023/06/13 Modelling with Subclass and Equivalent class statements Henriette Harmse slides 2023/05/30 First steps with ChatGPT for semantic engineers and curators Sierra Moxon and Nico Matentzoglu N/A 2023/04/18 Overview of Protege 5.6 - the latest features Damien Goutte-Gattat (slides) Here 2023/04/04 Introduction to Exomiser Valentina Cipriani, Yasemin Bridges and Carlo Kroll Here 2023/03/21 Introduction to Wikidata Andra Waagmeester and Tiago Lubiana Here 2023/03/07 OAK for the Ontology Engineering community Chris Mungall Here 2023/02/21 OBO Academy Clinic Bring your ontology issues and questions to discuss with Sabrina and Nico! Attend the Ontology Summit Seminars instead! 2023/02/07 Querying the Monarch KG using Neo4J Kevin Schaper Here 2023/01/24 OBO Academy Clinic Bring your ontology issues and questions to discuss with Sabrina and Nico! 2023/01/10 Modeling with taxon constraints Jim Balhoff Here 2022/12/13 Introduction to Semantic Entity Matching Slides Here 2022/11/29 OBO Academy hackathon Work on open tickets together. 2022/11/15 Contributing to OBO ontologies - Part 2 Here 2022/11/01 Contributing to OBO ontologies - Part 1 Here 2022/10/18 Introduction to Medical Action Ontology (MAxO) Here 2022/10/04 No Meeting ISB virtual conference: register here 2022/09/20 How to be an open science ontologist Here 2022/09/06 Pull Requests: Part 2 Here 2022/07/26 Pull Requests: Part 1 Here 2022/07/12 Basic introduction to the CLI: Part 2 Due to intermitent connection issues, the first few minutes of this recording are not included. Refer to the Tutorial link for the initial directions. Here 2022/06/28 Basic introduction to the CLI: Part 1 Here 2022/06/14 Application/project ontologies Here 2022/05/31 Contributing to ontologies: annotation properties Here 2022/05/17 Introduction to managing mappings with SSSOM Here 2022/04/19 Disjointness and Unsatisfiability Here 2022/03/22 Creating an ontology from scratch Here 2022/03/08 Obsoletions in OBO ontologies Review Obsoleting an Existing Ontology Term and Merging Ontology Terms. Slides are here. Here 2022/02/22 SPARQL for OBO ontology development Here 2022/02/07 ODK/DOSDPs Here 2022/01/25 Contributing to OBO ontologies This is not new content but we'll start at the beginning again with our previous lessons. Here 2022/01/11 Office hours with Nicole and Sabrina - no formal lesson Bring any open questions. 2021/12/14 Lessons learned from troubleshooting ROBOT Open discussion, no advance preparation is needed. 2021/11/30 Semantics of object properties (including Relations Ontology) 2021/11/16 SPARQL for OBO ontology development Here 2021/11/02 Templating: DOSDPs and ROBOT 2021/10/19 Ontology Design 2021/10/05 Cancelled due to overlap with ISB conference 2021/09/21 Ontology Pipelines with ROBOT 2 2021/09/08 Migrating legacy ontology systems to ODK 2021/09/07 Ontology Pipelines with ROBOT 2021/09/01 Manage dynamic imports the ODK 2021/08/25 Ontology Release Management with the ODK Here 2021/08/24 Contributing to OBO ontologies 2 Here 2021/08/17 Contributing to OBO ontologies"},{"location":"courses/monarch-obo-training/#notes","title":"Notes","text":"<p>Most of materials used by this course were developed by James Overton, Becky Jackson, Nicole Vasilevsky and Nico Matentzoglu as part of a project with the Critical Path Institute (see here). The materials are continually improved as part of an internal training program (onboarding and CPD) for the Phenomics First project (NIH / NHGRI 1RM1HG010860) and Monarch Initiative (NIH OD 5R24OD011883).</p> <p>Thanks to Sarah Gehrke for her help with project management.</p>"},{"location":"courses/ontology-summit-2023/","title":"Ontology Summit 2023","text":"<p>This course unit only covers the OBO part of the Ontology Summit 2023, for a full overview see https://ontologforum.org/index.php/OntologySummit2023.</p>"},{"location":"courses/ontology-summit-2023/#goal","title":"Goal","text":"<p>Giving a broad overview of the key OBO methodologies and tools to the general ontology community.</p>"},{"location":"courses/ontology-summit-2023/#tutors","title":"Tutors","text":"<ul> <li>Nico Matentzoglu</li> </ul>"},{"location":"courses/ontology-summit-2023/#schedule","title":"Schedule","text":"Date Lesson Tutors Notes 2023/01/25 Introduction to COB Chris Mungall Slides 2023/02/01 Introduction to ROBOT and OAK James Overton and Chris Mungall <ul><li>ROBOT Slides</li><li>OAK Slides 2023/02/08 Managing the Ontology Life Cycle with the Ontology Development Kit Anita Caron, Damien Goutte-Gattat, Philip Stroemert, Nicolas Matentzoglu <ul><li>09:00am: Introduction and use cases and workflows</li><li>09:15am: Overview of the ODK toolbox</li><li>09:30am: Setting up your own repo</li><li>10:00am: Advanced customisation, adding a custom SPARQL check</li><ul> 2023/02/15 Using Dashboards to monitor OBO ontologies Charlie Hoyt, Nicolas Matentzoglu, Anita Caron <ul><li>09:05am: The problem of encouraging and monitoring ontology standardisation (@matentzn, @cthoyt)</li><li>09:15am: Overview of the OBO Dashboard: Standardising metadata across OBO Foundry ontologies (@matentzn)</li><li>09:30am: Other dashboards for standardising metadata (oquat, versions) (@cthoyt)</li><li>09:37am: Dashboards for measuring social workflows (OBO health) (@cthoyt)</li><li>09:45am: Using the OBO Dashboard Kit to create your own, customised Dashboard (@anitacaron)</li></ul> 2023/02/02 Using OBO Ontologies: Ubergraph and other applications Jim Balhoff <ul><li>09:00am: Ubergraph: integrating OBO ontologies into a unified semantic graph</li><li>09:35am: SSSOM - Open, FAIR and standardised mappings for ontologies, controlled vocabularies and database entities</li></ul>"},{"location":"explanation/annotation-properties/","title":"Contributing to ontologies: annotation properties","text":"<p>Editors: Sabrina Toro (@sabrinatoro), Nicolas Matentzoglu (@matentzn) Examples with images can be found here.</p>"},{"location":"explanation/annotation-properties/#what-are-annotation-properties","title":"What are annotation properties?","text":"<p>An entity such as an individual, a class, or a property can have annotations, such as labels, synonyms and definitions. An annotation property is used to link the entity to a value, which in turn can be anything from a literal (a string, number, date etc) to another entity (such as, another class).</p> <p>Here are some examples of frequently used annotation properties: (every element in bold is an annotation property)</p> <p>http://purl.obolibrary.org/obo/MONDO_0004975</p> <ul> <li>rdfs:label \u2013&gt; \u2018Alzheimer disease\u2019</li> <li>oboInOwl:hasExactSynonym \u2013&gt; Alzheimer dementia</li> <li>oboInOwl:hasDbXref -&gt; <code>NCIT:C2866</code></li> <li>skos:exactMatch -&gt; http://www.orpha.net/ORDO/Orphanet_238616</li> </ul>"},{"location":"explanation/annotation-properties/#some-useful-things-to-know-about-annotation-properties","title":"Some useful things to know about annotation properties","text":"<p>Annotation properties have their own IRIs, just like classes and individuals. For example, the IRI of the RDFS built in label property is http://www.w3.org/2000/01/rdf-schema#label. Other examples:</p> <ul> <li>oboInOwl:hasExactSynonym : http://www.geneontology.org/formats/oboInOwl#hasExactSynonym</li> <li>oboInOwl:hasDbXref : http://www.geneontology.org/formats/oboInOwl#hasDbXref</li> </ul> <p>Annotation properties are just like other entities (classes, individuals) and can have their own annotations. For example, the annotation propert http://purl.obolibrary.org/obo/IAO_0000232 has an rdfs:label ('curator note') and a human readable definition (IAO:0000115): 'An administrative note of use for a curator but of no use for a user'.</p> <p>Annotation properties can be organised in a hierarchical structure.</p> <p>For example, the annotation property 'synonym_type_property' (http://www.geneontology.org/formats/oboInOwl#SynonymTypeProperty) is the parent property of other, more specific ones (such as \"abbreviation\").</p> <p>Annotation properties are (usually) used with specific type of annotation values.</p> <ul> <li>Literal: (one can see [type: xsd:string] in the annotation)</li> <li>xsd:string<ul> <li>e.g. 'definition' (http://purl.obolibrary.org/obo/IAO_0000115)</li> </ul> </li> <li>xds:boolean<ul> <li>e.g. 'owl:deprecated' (http://www.w3.org/2002/07/owl#deprecated)</li> </ul> </li> <li>Entity IRI :</li> <li>Classes or individuals: e.g. 'has curation status' (http://purl.obolibrary.org/obo/IAO_0000114)</li> <li>Arbitray URIs, e.g. links to website with the 'term tracker item' (type xsd:anyURI) (http://purl.obolibrary.org/obo/IAO_0000233) property</li> <li>Or even other annotation properties <code>*</code><ul> <li>e.g. 'has_synonym_type' (http://www.geneontology.org/formats/oboInOwl#hasSynonymType)</li> <li>e.g. 'in_subset' (http://purl.obolibrary.org/obo/IAO_0000112)</li> </ul> </li> </ul> <p>Note: the type of annotation required for an annotation property can be defined by adding a Range + \"select datatype\" in the Annotation Property's Description e.g. : 'scheduled for obsoletion on or after' (http://purl.obolibrary.org/obo/IAO_0006012)</p> <ul> <li>Annotations do not affect reasoning. No matter what values you connect with your annotation properties, the reasoner will ignore it - even if it is nonsensical.</li> </ul>"},{"location":"explanation/annotation-properties/#annotation-property-vs-data-and-object-properties","title":"Annotation Property vs Data and Object Properties","text":"<p>Some annotation properties look like data properties (connecting an entity to a literal value) and others look like object properties (connecting an entity to another entity). Other than the fact that statements involving data and object properties look very different in RDF, the key difference from a user perspective is that OWL Reasoners entirely ignore triples involving annotation properties. Data and Object Properties are taken into account by the reasoner.</p> <p>Object properties are different to annotation properties in that they:</p> <ul> <li>connect pairs of individuals in way that affects reasoning</li> <li>represent relationship between classes in way that affects reasoning</li> <li>Example property: 'has part' (http://purl.obolibrary.org/obo/BFO_0000051)</li> <li>Object Properties can have the following property characteristics: Inverse, Symmetric, Asymmetric, Reflexive, Irreflexive, Functional, Inverse Functional, and Transitive which effect reasoning. Annotation properties cannot have such properties (or if they had, reasoners would ignore them).</li> </ul> <p>Data properties are different to annotation properties in that they:</p> <ul> <li>connect individuals with literals in way that affects reasoning</li> <li>represent relation between a class and literal in way that affects reasoning</li> <li>You can use data properties to logically define OWL classes with data ranges. For example, you can define the class of <code>Boomer</code> as all people born between 1946 and 1964. If an individual would be asserted to be a Boomer, but is born earlier than 1946, the reasoner would file a complaint.</li> <li>Example Data Property: 'hasName', 'hasPrice', 'hasCalories', 'hasSugarContent',...</li> <li>More details on how to use Data Properties here</li> </ul>"},{"location":"explanation/annotation-properties/#creating-new-annotation-properties","title":"Creating new Annotation Properties","text":"<p>Note: before creating a new annotation property, it is always a good idea to check for an existing annotation property first.</p> <ul> <li>For example: OBO Metadata Ontology (https://www.ebi.ac.uk/ols/ontologies/omo), which could be imported</li> </ul> <p>Detailed explanations for adding a new annotation property can be found here</p>"},{"location":"explanation/annotation-properties/#the-term-annotation-in-ontologies-and-data-curation-means-different-things","title":"The term \"Annotation\" in Ontologies and Data Curation means different things.","text":"<p>The word \"annotation\" is used in different contexts to mean different things. For instance, \"annotation in owl\" (ie annotations to an ontology term) is different from \"annotation in the biocuration sense\" (ie gene-to-disease, gene-to-phenotype, gene-to-function annotations). It is therefore crucial to give context when using the word \"annotation\".</p>"},{"location":"explanation/existential-restrictions/","title":"Existential restrictions","text":""},{"location":"explanation/existential-restrictions/#prerequesites","title":"Prerequesites","text":"<ul> <li><code>SubClassOf</code> vs <code>EquivalentTo</code></li> </ul>"},{"location":"explanation/existential-restrictions/#qualified-existential-restrictions","title":"Qualified existential restrictions","text":""},{"location":"explanation/existential-restrictions/#semantics","title":"Semantics","text":"<p>Given </p> <pre><code>ObjectProperty: r\nClass: D\n    EquivalentTo: r some C\nClass: C\n</code></pre> <p>the semantics of  <code>r some C</code> is the set of individuals such that for each individual <code>x</code> there is at least 1 individual <code>y</code> of type <code>C</code> that is linked to <code>x</code> via the object property <code>r</code>.</p> <p>Based on this semantics, a possible world adhering to our initial equivalence axiom may be:</p> <p></p> <p>In this Venn diagram we assume individuals are black dots.  Thus, our world consists of 7 individuals, with only 2 classes, namely <code>C</code> and <code>D</code>, as well 2 object properties, namely <code>r</code> and <code>q</code>.  In this world, <code>D</code> and thus the class <code>r some C</code>, consist of only 2 individuals. <code>D</code> and <code>r some C</code> consist of only 2 individuals because these are the only individuals linked  via object property <code>r</code> to at least 1 individual respectively in <code>C</code>.</p>"},{"location":"explanation/existential-restrictions/#examples","title":"Examples","text":"<p>In the following we define a pet owner as someone that owns at least 1 pet.</p> <pre><code>ObjectProperty: owns\nClass: PetOwner\n    EquivalentTo: owns some Pet\nClass: Pet\n</code></pre> <p>If we want to introduce the class <code>DogOwner</code>, assuming we can only use the class <code>Pet</code> and the object property <code>owns</code> (assuming we have not defined <code>PetOwner</code>), we could say  that a dog owner is a subset of pet owners:</p> <pre><code>ObjectProperty: owns\nClass: DogOwner\n    SubClassOf: owns some Pet\nClass: Pet\n</code></pre> <p>In this case we use <code>SubClassOf</code> instead of <code>EquivalentTo</code> because not every pet owner necessarily owns a dog. This is equivalent to stating:</p> <pre><code>ObjectProperty: owns\nClass: PetOwner\n    EquivalentTo: owns some Pet\nClass: Pet\nClass: DogOwner \n    SubClassOf: PetOwner\n</code></pre>"},{"location":"explanation/existential-restrictions/#variations-on-existential-restrictions","title":"Variations on existential restrictions","text":""},{"location":"explanation/existential-restrictions/#unqualified-existential-restrictions","title":"Unqualified existential restrictions","text":"<p>In the previous section we modeled a <code>PetOwner</code> as <code>owns some Pet</code>. In the expression  <code>owns some Pet</code> <code>Pet</code> is referred to as the filler of <code>owns</code> and more specifically  we say <code>Pet</code> is the <code>owns</code>-filler. </p> <p>The <code>PetOwner EquivalentTo: owns some Pet</code> state that pet owners are those individuals  that own a pet and ignore all other <code>owns</code>-fillers that are not pets. How can we define arbitrary ownership? </p> <pre><code>ObjectProperty: owns\nClass: Owner\n    EquivalentTo: owns some owl:Thing\n</code></pre>"},{"location":"explanation/existential-restrictions/#value-restrictions","title":"Value restrictions","text":"<p>We can base restrictions on having a relation to a specific named individual, i.e.: </p> <pre><code>Individual: UK\nObjectProperty: citizenOf\nClass: UKCitizen\n    EquivalentTo: citizenOf hasValue UK\n</code></pre>"},{"location":"explanation/existential-restrictions/#existential-restrictions-on-data-properties","title":"Existential restrictions on data properties","text":"<p>This far we have only considered existential restrictions based on object properties, but it is possible to define existential restrictions based on data properties. As an example, we all expect that persons have at least 1 name. This could be expressed as follows:</p> <pre><code>DataProperty: name\nClass: Person\n    SubClassOf: name some xsd:string\n</code></pre>"},{"location":"explanation/existential-restrictions/#when-to-use-subclassof-vs-equivalentto-with-existential-restrictions","title":"When to use SubClassOf vs EquivalentTo with existential restrictions","text":"<p>In our example of <code>Person SubClassOf: name some xsd:string</code>, why did we use <code>SubClassOf</code> rather than <code>EquivalentTo</code>? That is, why did we not use <code>Person EquivalentTo: name some xsd:string</code>? With using the <code>EquivalentTo</code> axiom, any  individual that has a name, will be inferred to be an instance of <code>Person</code>. However,  there are many things in the world that have names that are not persons. Some examples are pets, places, regions, etc:</p> <p></p> <p>Compare this with, for example, <code>DogOwner</code>:</p> <pre><code>ObjectProperty: owns\nClass: Dog\nClass: DogOwner\n    EquivalentTo: owns some Dog\n</code></pre>"},{"location":"explanation/intro-to-ontologies/","title":"Introduction to ontologies","text":"<p>Based on CL editors training by David Osumi-Sutherland</p>"},{"location":"explanation/intro-to-ontologies/#why-do-we-need-ontologies","title":"Why do we need ontologies?","text":"<p>We face an ever-increasing deluge of biological data analysis. Ensuring that this data and analysis are Findable, Accessible, Interoperable, and Re-usable (FAIR) is a major challenge. Findability, Interoperabiltiy, and Resuability can all be enhanced by standardising metadata. Well-standardised metadata can make it easy to find data and analyses despite variations in terminology ('Clara cell' vs 'nonciliated bronchiolar secretory cell' vs 'club cell') and precision ('bronchial epithelial cell' vs 'club cell'). Understanding which entities are referred to in metadata and how they relate to the annotated material can help users work out if the data or analysis they have found is of interest to them and can aid in its re-use and interoperability with other data and analyses. For example, does an annotation of sample data with a term for breast cancer refer to the health status of the patient from which the sample was derived or that the sample itself comes from a breast cancer tumor?</p>"},{"location":"explanation/intro-to-ontologies/#we-cant-find-what-were-looking-for","title":"We can't find what we're looking for","text":"<p>Given variation in terminology and precision, annotation with free text alone is not sufficient for findability. One very lightweight solution to this problem is to rely on user-generated keyword systems, combined with some method of allowing users to choose from previously used keywords. This can produce some degree of annotation alignment but also results in fragmented annotation and varying levels of precision with no clear way to relate annotations.</p> <p>For example, trying to refer to feces, in NCBI BioSample:</p> Query Records Feces 22,592 Faeces 1,750 Ordure 2 Dung 19 Manure 154 Excreta 153 Stool 22,756 Stool NOT faeces 21,798 Stool NOT feces 18,314"},{"location":"explanation/intro-to-ontologies/#we-dont-know-what-were-talking-about","title":"We don't know what we're talking about","text":"<p>Terminology alone can be ambiguous. The same term may be used for completely unrelated or vaguely analogous structures. An insect femur and an mammalian femur are neither evolutionarily related nor structurally similar. Biologists often like to use abbreviations to annotate data, but these can be extremely ambiguous. Drosophila biologists use DA1 to refer to structures in the tracheal system, musculature and nervous system. Outside of Drosophila biology it is used to refer to many other things including a rare disease, and a a neuron type in C.elegans.</p> <p>Some extreme examples of this ambiguity come from terminological drift in fields with a long history. For example in the male genitalia of a gasteruptiid wasp, these 5 different structures here have each been labeled \"paramere\" by different people, each studying different hymenopteran lineages. How do we know what \"paramere\" means when it is referred to?</p> <p></p> <p>This striking example shows that even precise context is not always sufficient for disambiguation.</p>"},{"location":"explanation/intro-to-ontologies/#controlled-vocabulary-cv","title":"Controlled vocabulary (CV)","text":"<p>Rather than rely on users to generate lists of re-usable keywords, we can instead pre-specify a set of terms to use in annotation. This is usually refered to a controlled vocabulary or CV.</p>"},{"location":"explanation/intro-to-ontologies/#key-features","title":"Key features","text":"<ul> <li>Terms are usually not defined</li> <li>Relationships between the terms are usually not defined</li> <li>Simplest form is a list</li> </ul>"},{"location":"explanation/intro-to-ontologies/#example-using-wines","title":"Example using wines","text":"<ul> <li>Pinot noir</li> <li>Red</li> <li>Chardonnay</li> <li>Chianti</li> <li>Bordeaux</li> <li>Riesling</li> </ul>"},{"location":"explanation/intro-to-ontologies/#hierarchical-controlled-vocabulary","title":"Hierarchical controlled vocabulary","text":""},{"location":"explanation/intro-to-ontologies/#definition","title":"Definition","text":"<p>Any controlled vocabulary that is arranged in a hierarchy.</p>"},{"location":"explanation/intro-to-ontologies/#key-features_1","title":"Key features","text":"<ul> <li>Terms are arranged in a hierarchy, typically from general (top) to specific (bottom) with each term having only one parent.</li> <li>Terms are usually not defined.</li> <li>Relationships between the terms are usually not named or defined</li> </ul>"},{"location":"explanation/intro-to-ontologies/#example-using-wines-taxonomy-of-wine","title":"Example using wines (Taxonomy of wine)","text":"<ul> <li>Red<ul> <li>Merlot</li> <li>Zinfandel</li> <li>Cabernet</li> <li>Pinot Noir</li> </ul> </li> <li>White<ul> <li>Chardonnay</li> <li>Pinot Gris</li> <li>Riesling</li> </ul> </li> </ul> <p>Taxonomy describes a hierarchical CV in which hierarchy equals classification. E.g., 'Merlot' is classified as a 'Red' (wine). Not all hierchical CVs are classifications. For example, anatomical atlases often have hierarchical CVs representing \"parthood\". The femur is a part of the leg, but it is not 'a leg'.</p>"},{"location":"explanation/intro-to-ontologies/#support-for-grouping-and-varying-levels-of-precision","title":"Support for grouping and varying levels of precision","text":"<p>The use of a hierachical CV in which general terms group more specific terms allows for varying precision (glial cell vs some specific subtype) and simple grouping of annotated content.</p> <p>For example:</p> <p></p>"},{"location":"explanation/intro-to-ontologies/#from-hierarchical-cvs-to-ontologies","title":"From hierarchical CVs to ontologies","text":"<p>Hierarchical CVs tend to increase in complexity in particular ways:</p>"},{"location":"explanation/intro-to-ontologies/#synonyms","title":"Synonyms","text":"<p>To support findability, terms in hierarchical CVs often need to be associated with synonyms, or cross-referenced to closely related terms inside the CV.</p>"},{"location":"explanation/intro-to-ontologies/#polyhierarchy","title":"Polyhierarchy","text":"<p>CV content is often driven by requests from annotators and so expansion is not driven by any unified vision of scheme. This often leads to pressure for hierarchies to support terms having multiple parents, either reflecting multiple relationship types, or multiple types of classification. For example, an anatomical CV could reasonably put 'retinal bipolar cell' under 'retina' based on location and, at the same time, under 'bipolar neuron' and 'glutamatergic neuron' based on cell type classification.</p>"},{"location":"explanation/intro-to-ontologies/#named-relationships","title":"Named relationships","text":"<p>Developers of hierarchical CVs often come to realise that multiple relationship types are represented in the hierarchy and that it can be useful to name these relationship for better distinction. For example, a heart glial cell is a 'type of' glial cell, but is 'part of' the heart.</p> <p></p>"},{"location":"explanation/intro-to-ontologies/#what-is-an-ontology","title":"What is an ontology?","text":""},{"location":"explanation/intro-to-ontologies/#definition_1","title":"Definition","text":"<p>Definitions of ontologies can be controversial. Rather than attempting a comprehensive definition, this tutorial will emphasise ontologies as:</p> <ul> <li>Classifications</li> <li>Queryable stores of knowledge</li> </ul>"},{"location":"explanation/intro-to-ontologies/#key-features-of-well-structured-ontologies","title":"Key features of well-structured ontologies:","text":"<ul> <li> <p>Terms are arranged in a classification hierarchy</p> </li> <li> <p>Terms are defined</p> </li> <li> <p>Terms are richly annotated:</p> <ul> <li>Textual definitions</li> <li>References</li> <li>Synonyms</li> <li>Links</li> <li>Cross-references</li> </ul> </li> <li> <p>Relationships between terms are defined, allowing logical inference and sophisticated queries as well as graph representations.</p> </li> <li> <p>Expressed in a knowledge representation language such as RDFS, OBO, or OWL</p> </li> </ul>"},{"location":"explanation/intro-to-ontologies/#examples","title":"Examples","text":"<ul> <li>Gene Ontology, Uberon, Cell Ontology, EFO, SNOMED</li> </ul>"},{"location":"explanation/intro-to-ontologies/#non-logical-parts-of-ontologies","title":"Non-logical parts of ontologies","text":"<p>Terminology can be ambiguous, so text definitions, references, synonyms and images are key to helping users understand the intended meaning of a term.</p> <p></p>"},{"location":"explanation/intro-to-ontologies/#identifiers","title":"Identifiers","text":""},{"location":"explanation/intro-to-ontologies/#using-identifiers-devoid-of-intrinsic-meaning","title":"Using identifiers devoid of intrinsic meaning","text":"<p>Identifiers that do not hold any inherent meaning are important to ontologies. If you ever need to change the names of your terms, you're going to need identifiers that stay the same when the term name changes.</p> <p>For example:</p> <p>A microglial cell is also known as: hortega cell, microglia, microgliocyte and brain resident macrophage. In the cell ontology, it is however referred to by a unique identifier: <code>CL:0000129</code> These identifiers are short ways of referring to IRIs (e.g., CL:000129 = http://purl.obolibrary.org/obo/CL_0000129) This IRI is a unique, resolvable identifier on the web. A group of ontologies - loosely co-ordinated through the OBO Foundry, have standardised their IRIs (e.g. http://purl.obolibrary.org/obo/CL_0000129 - A term in the cell ontology; http://purl.oblibrary.org/obo/cl.owl - The cell ontology)</p>"},{"location":"explanation/intro-to-ontologies/#iris-uris-urls","title":"IRIs? URIs? URLs?","text":"<ul> <li>URI: Unique Resource Identifier - a string of characters, following a standard specification, that unambiguously identifies a particular (web) resource.</li> <li>IRI: Internationalised Resource Identifier - a URI that can use characters in multiple languages.</li> <li>URL: Uniform Resource Locator - a web-resolvable URI.</li> </ul>"},{"location":"explanation/intro-to-ontologies/#building-scalable-ontologies","title":"Building scalable ontologies","text":""},{"location":"explanation/intro-to-ontologies/#format","title":"Format","text":"<p>OBO ontologies are mostly written in OWL2 or OBO syntax. The latter is a legacy format that maps completely to OWL.</p> <p>For a more in-depth explanation of formats (OWL, OBO, RDF etc.) refer to explainer on OWL format variants. In the examples below we will use OWL Manchester syntax, which allows us to express formal logic in English-like sentences.</p>"},{"location":"explanation/intro-to-ontologies/#an-ontology-as-a-classification","title":"An ontology as a classification","text":"<p>Ontology terms refer to classes of things in the world. For example, the class of all wings.</p> <p>Below you will see a classification of parts of the insect and how it is represented in a simple ontology.</p> <p></p> <p>We use a SubClassOf (or is_a in obo format) to represent that one class fully subsumes another. For example: OWL: hindwing SubClassOf wing OBO: hindwing is_a wing</p> <p>In English we might say: \"a hindwing is a type of wing\" or more specifically, \"all instances of hindwing are instances of wing.\" 'Instance' here refers to a single wing of an individual fly.</p> <p></p> <p>In the previous section, we talked about different types of relationships. In OWL we can define specific relations (known as object properties). One of the commonest is 'part of' which you can see used below.</p> <p></p> <p>English: all (insect) legs are part of a thoracic segment OWL: 'leg' SubClassOf part_of some thoracic segment OBO: 'leg'; relationship: part_of thoracic segment</p> <p>It might seem odd at first that OWL uses subClassOf here too. The key to understanding this is the concept of an anonymous class - in OWL, we can refer to classes without giving them names. In this case, the anonymous class is the class of all things that are 'part of' (some) 'thoracic segment' (in insects). A vast array of different anatomical structures are subclasses of this anonymous class, some of which, such as wings, legs, and spiracles, are visible in the diagram.</p> <p>Note the existential quantifier <code>some</code> in OWL format -- it is interpreted as \"there exists\", \"there is at least one\", or \"some\".</p> <p></p> <p>The quantifier is important to the direction of relations.</p> <p>subClassOf: <code>'wing' SubClassOf part_of some 'thoracic segment'</code> is correct <code>'thoracic segment' SubClassOf has_part some 'wing'</code> is incorrect as it implies all thoracic segment have wings as a part.</p> <p>Similarly: <code>'claw' SubClassOf connected_to some 'tarsal segment'</code> is correct <code>'tarsal segment' SubClassOf connected_to some 'claw'</code> is incorrect as it implies all tarsal segments are connected to claws (for example, some tarsal segments are connected to other tarsal segments)</p> <p></p> <p>These relationships store knowledge in a queryable format. For more information about querying, please refer to guide on DL queries and SPARQL queries.</p>"},{"location":"explanation/intro-to-ontologies/#scaling-ontologies","title":"Scaling Ontologies","text":"<p>There are many ways to classify things. For example, a neuron can be classified by structure, electrophysiology, neurotransmitter, lineage, etc. Manually maintaining these multiple inheritances (that occur through multiple classifications) does not scale.</p> <p></p> <p>Problems with maintaining multiple inheritance classifications by hand</p> <ul> <li> <p>Doesn\u2019t scale</p> <ul> <li> <p>When adding a new class, how are human editors to know</p> <ul> <li> <p>all of the relevant classifications to add?</p> </li> <li> <p>how to rearrange the existing class hierarchy?</p> </li> </ul> </li> </ul> </li> <li> <p>It is bad for consistency</p> <ul> <li> <p>Reasons for existing classifications often opaque</p> </li> <li> <p>Hard to check for consistency with distant superclasses</p> </li> </ul> </li> <li> <p>Doesn\u2019t allow for querying</p> <ul> <li>A formalized ontology can be queried for classes with arbitrary sets of properties. A manual classification can not.</li> </ul> </li> </ul>"},{"location":"explanation/intro-to-ontologies/#automated-classifications","title":"Automated Classifications","text":"<p>The knowledge an ontology contains can be used to automate classification. For example:</p> <p>English: Any sensory organ that functions in the detection of smell is an olfactory sensory organ OWL:</p> <pre><code>'olfactory sensory organ'\n EquivalentTo \u2018sensory organ\u2019\nthat\ncapable_of some \u2018detection of smell\u2019\n</code></pre> <p>If we then have an entity <code>nose</code> that is subClassOf <code>sensory organ</code> and <code>capable_of some detection of smell</code>, it will be automatically classified as an olfactory sensory organ.</p> <p></p>"},{"location":"explanation/intro-to-ontologies/#how-to-interpret-an-and-or-an-or","title":"How to interpret an 'and' or an 'or'","text":"<p>Many classes, especially in the domains of disease and phenotype, describe combinations of multiple classes - but it is very important to carefully distinguish whether this combination follows \"disjunctive\" logic (\"or\") or \"conjunctive\" logic (\"and\"). Both mean something entirely different. Usually where a class has 'and' in the label, such as 'neonatal inflammatory skin and bowel disease' (MONDO:0017411), the class follows a conjunctive logic (as expected), and should be interpreted in a way that someone that presents with this disease has both neonatal inflammatory skin disease and bowel disease at once. This class should be classified as a child of 'bowel disease' and 'neonatal inflammatory skin disease'.  Note, however, that naming in many ontologies is not consistent with this logic, and you need to be careful to distinguish wether the interpretation is supposed to be conjunctive or disjunctive (i.e. \"and\" could actually mean \"or\", which is especially often the case for clinical terminologies).</p> <p>Having asserted multiple SubClassOf axioms means that an instance of the class is a combination of all the SubClass Of statements (conjunctive interpretation, see above). For example, if 'neonatal inflammatory skin and bowel disease' is a subclass of both 'bowel disease' and 'neonatal inflammatory skin disease', then an individual with this disease has 'bowel disease' and 'neonatal inflammatory skin disease'.</p> <p></p> <p>If there were a class 'neonatal inflammatory skin or bowel disease', the intention is usually that this class follows disjunctive logic. A class following this logic would be interpreted in a way that an individual with this disease has either bowel disease or neonatal inflammatory skin disease or both. It would not be accurate to classify this class as a child of bowel disease and neonatal inflammatory skin disease. This type of class is often called a \"grouping class\", and is used to aggregate related diseases in a way useful to users, like \"disease\" and \"sequelae of disease\".</p> <p></p>"},{"location":"explanation/intro-to-ontologies/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>David Osumi-Sutherland (original creator of slides)</li> <li>Nicole Vasilevsky (Critical Path Institute)</li> <li>Alex Diehl (Buffalo)</li> <li>Nico Matentzoglu</li> <li>Matt Brush</li> <li>Matt Yoder </li> <li>Carlo Toriniai</li> <li>Simon Jupp</li> <li>Chris Mungall (LNBL)</li> <li>Melissa Haendal (University of Colorado Anschutz Medical Campus)</li> <li>Jim Balhoff (RENCI)</li> <li>James Overton - slides, ideas &amp; discussions</li> <li>Terry Meehan - who edited CL more than anyone</li> <li>Helen Parkinson (EBI)</li> <li>Michael Ashburner</li> </ul>"},{"location":"explanation/logical-axiomatization/","title":"Logical axiomatization of classes &amp; use of reasoning","text":"<p>This explainer requires understanding of ontology classifications. Please see \"an ontology as a classification\" section of the introduction to ontologies documentation if you are unfamiliar with these concepts.</p> <p>You can watch this video about an introduction to Logical Description.</p>"},{"location":"explanation/logical-axiomatization/#what-are-logical-axioms","title":"What are logical axioms","text":"<p>Logical axioms are relational information about classes that are primarily aimed at machines. This is opposed to annotations like textual definitions which are primarily aimed at humans. These logical axioms allow reasoners to assist in and verify classification, lessening the development burden and enabling expressive queries.</p>"},{"location":"explanation/logical-axiomatization/#what-should-you-axiomatize","title":"What should you axiomatize?","text":"<p>Ideally, everything in the definition should be axiomatized when possible. For example, if we consider the cell type <code>oxytocin receptor sst GABAergic cortical interneuron</code>, which has the textual definition:</p> <p>\"An interneuron located in the cerebral cortex that expresses the oxytocin receptor. These interneurons also express somatostatin.\"</p> <p>The logical axioms should then follow accordingly:</p> <p>SubClassOf:</p> <ul> <li>interneuron</li> <li>'has soma location' some 'cerebral cortex'</li> <li>expresses some 'oxytocin receptor'</li> <li>expresses some somatostatin</li> <li>'capable of' some 'gamma-aminobutyric acid secretion, neurotransmission'</li> </ul> <p>These logical axioms allow a reasoner to automatically classify the term. For example, through the logical axioms, we can infer that <code>oxytocin receptor sst GABAergic cortical interneuron</code> is a <code>cerebral cortex GABAergic interneuron</code>.</p> <p></p> <p>Axiomatizing definitions well will also allow for accurate querying. For example, if I wanted to find a neuron that expresses oxytocin receptor, having the SubClassOf axioms of <code>interneuron</code> and <code>expresses some 'oxytocin receptor'</code> will allow me to do so on DL query (see tutorial on DL query for more information about DL queries).</p> <p></p>"},{"location":"explanation/logical-axiomatization/#what-should-you-not-axiomatize","title":"What should you NOT axiomatize?","text":"<p>Everything in the logical axioms must be true, (do not axiomatize things that are true to only part of the entity) For example, the cell type <code>chandelier pvalb GABAergic cortical interneuron</code> is found in upper L2/3 and deep L5 of the cerebral cortex. We do not make logical axioms for <code>has soma location</code> some layer 2/3 and layer 5. Axioms with both layers would mean that a cell of that type must be in both layer 2/3 and layer 5, which is an impossibility (a cell cannot be in two seperate locations at once!). Instead we axiomatize a more general location: 'has soma location' some 'cerebral cortex'</p>"},{"location":"explanation/logical-axiomatization/#equivalent-class-logical-definitions","title":"Equivalent class logical definitions","text":"<p>An equivalent class axiom is an axiom that defines the class; it is a necessary and sufficient logical axiom that defines the cell type. It means that if a class B fulfils all the criteria/restrictions in the equivalent axiom of class A, class B is by definition a subclass of class A. Equivalent classes allow the reasoner to automatically classify entities.</p> <p>For example:</p> <ul> <li><code>chandelier cell</code> has the equivalent class axiom <code>interneuron and ('has characteristic' some 'chandelier cell morphology')</code></li> <li><code>chandelier pvalb GABAergic cortical interneuron</code> has the subclass axioms <code>'has characteristic' some 'chandelier cell morphology'</code> and <code>interneuron</code></li> <li><code>chandelier pvalb GABAergic cortical interneuron</code> is therefore a subclass of <code>chandelier cell</code></li> </ul> <p>Equivalent class axioms classification can be very powerful as it takes into consideration complex layers of axioms.</p> <p>For example:</p> <ul> <li><code>primary motor cortex pyramidal cell</code> has the equivalent class axiom <code>'pyramidal neuron' and ('has soma location' some 'primary motor cortex')</code>.</li> <li><code>Betz cell</code> has the axioms <code>'has characteristic' some 'standard pyramidal morphology'</code> and <code>'has soma location' some 'primary motor cortex layer 5'</code></li> <li><code>Betz cell</code> are inferred to be <code>primary motor cortex pyramidal cell</code> through the following chain (you can see this in Prot\u00e9g\u00e9 by pressing the ? button on inferred class):</li> </ul> <p></p> <p>The ability of the reasoner to infer complex classes helps identify classifications that might have been missed if done manually. However, when creating an equivalent class axiom, you must be sure that it is not overly constrictive (in which case, classes that should be classified under it gets missed) nor too loose (in which case, classes will get wrongly classified under it).</p> <p>Example of both overly constrictive and overly loose equivalent class axiom:</p> <p><code>neuron equivalent to cell and (part_of some 'central nervous system')</code></p> <ul> <li>This is overly constrictive as there are neurons outside the central nervous system (e.g. peripheral neurons).</li> <li>This is also too loose as there are cells in the central nervous system that are not neurons (e.g. glial cells).</li> </ul> <p>In such cases, sometimes not having an equivalent class axioms is better (like in the case of neuron), and asserting is the best way to classify a child.</p>"},{"location":"explanation/logical-axiomatization/#style-guide","title":"Style guide","text":"<p>Each ontology has certain styles and conventions in how they axiomatize. This style guide is specific to OBO ontologies. We will also give reasons as to why we choose to axiomatize in the way we do. However, be aware of your local ontology's practices.</p>"},{"location":"explanation/logical-axiomatization/#respect-the-ontology-style","title":"Respect the ontology style","text":"<p>It is important to note that ontologies have specific axiomatization styles and may apply to, for example, selecting a preferred relation. This usually reflects their use cases. For example, the Cell Ontology has a guide for what relations to use. An example of an agreement in the community is that while anatomical locations of cells are recorded using <code>part of</code>, neurons should be recorded with <code>has soma location</code>. This is to accommodate for the fact that many neurons have long reaching axons that cover multiple anatomical locations making them difficult to axiomatize using <code>part of</code>.</p> <p>For example, <code>Betz cell</code>, a well known cell type which defines layer V of the primary motor cortex, synapses lower motor neurons or spinal interneurons (cell types that reside outside the brain). Having the axiom <code>'Betz cell' part_of 'cortical layer V'</code> is wrong. In this case <code>has soma location</code> is used. Because of cases like these that are common in neurons, all neurons in CL should use <code>has soma location</code>.</p>"},{"location":"explanation/logical-axiomatization/#avoid-redundant-axioms","title":"Avoid redundant axioms","text":"<p>Do not add axioms that are not required. If a parent class already has the axiom, it should not be added to the child class too. For example:</p> <ul> <li><code>retinal bipolar neuron</code> is a child of <code>bipolar neuron</code></li> <li><code>bipolar neuron</code> has the axiom <code>'has characteristic' some 'cortical bipolar morphology'</code></li> <li>Therefore we do not add <code>'has characteristic' some 'cortical bipolar morphology'</code> to <code>retinal bipolar neuron</code></li> </ul> <p>Axioms add lines to the ontology, resulting in larger ontologies that are harder to use. They also add redundancy, making the ontology hard to maintain as a single change in classification might require multiple edits.</p>"},{"location":"explanation/logical-axiomatization/#let-the-reasoner-do-the-work","title":"Let the reasoner do the work","text":"<p>Asserted is_a parents do not need to be retained as entries in the 'SubClass of' section of the Description window in Prot\u00e9g\u00e9 if the logical definition for a term results in their inference.</p> <p>For example, <code>cerebral cortex GABAergic interneuron</code> has the following logical axioms:</p> <pre><code>Equivalent_To\n  'GABAergic interneuron' and\n  ('has soma location' some 'cerebral cortex')\n</code></pre> <p>We do not need to assert that it is a <code>cerebral cortex neuron</code>, <code>CNS interneuron</code>, or <code>neuron of the forebrain</code> as the reasoner automatically does that.</p> <p>We avoid having asserted subclass axioms as these are redundant lines in the ontology which can result in a larger ontology, making them harder to use.</p> <p>Good practice to let the reasoner do the work:</p> <pre><code>1) If you create a logical definition for your term, you should delete all redundant, asserted is_a parent relations by clicking on the X to the right of the term.\n2) If an existing term contains a logical definition and still shows an asserted is_a parent in the 'SubClass of' section, you may delete that asserted parent. Just make sure to run the Reasoner to check that the asserted parent is now replaced with the correct reasoned parent(s).\n3) Once you synchronize the Reasoner, you will see the reasoned classification of your new term, including the inferred is_a parent(s).\n4) If the inferred classification does not contain the correct parentage, or doesn't make sense, then you will need to modify the logical definition.\n</code></pre>"},{"location":"explanation/ontology-matching/","title":"Ontology Matching","text":""},{"location":"explanation/ontology-matching/#ontology-matching-basic-techniques","title":"Ontology Matching: Basic Techniques","text":"<p>10 min overview of J\u00e9r\u00f4me Euzenat and Pavel Shvaiko ground-breaking Ontology Matching.</p>"},{"location":"explanation/owl-building-blocks/","title":"The logical building blocks of OWL","text":"<p>Here we briefly list the building blocks that are used in OWL that enable reasoning.</p> OWL Semantics Example instance or individual A member of a set. A person called <code>Mary</code> or a dog called <code>Fido</code>. class A set of individuals. The <code>Person</code> class consisting of persons or the <code>Dog</code> class consisting of dogs. object property A set of pairs of individuals. The <code>owns</code> object property can link a pet and its <code>owner: Mary owns Fido</code>. data property A set of pairs where each pair consists of an individual linked to a data value. The data property <code>hasAge</code> can link a number representing an age to an individual: <code>hasAge(Mary, 10)</code>."},{"location":"explanation/owl-format-variants/","title":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why?","text":"<p>For reference of the more technical aspects of release artefacts, please see documentation on Release Artefacts</p> <p>Ontologies come in different serialisations, formalisms, and variants For example, their are a full 9 (!) different release files associated with an ontology released using the default settings of the Ontology Development Kit, which causes a lot of confusion for current and prospective users.</p> <p>Note: In the OBO Foundry pages, \"variant\" is currently referred to as \"products\", i.e. the way we use \"variant\" here is synonymous with with notion of \"product\".</p>"},{"location":"explanation/owl-format-variants/#overview-of-the-relevant-concepts","title":"Overview of the relevant concepts","text":"<ul> <li>A formalism or formal language can be used to describe entities and their relationships in an ontology. The most important formalisms we have are:</li> <li>Web Ontology Language (OWL): OWL is by far the dominant formalism in the biomedical domain due to its inference capabilities.</li> <li>RDF(S): Is a generally weaker language than OWL, but widely used by triple stores and other SPARQL engines. RDF(S) is lacking some of the strong logical guarantees that come with OWL and should only be used in scenarios where scalability (computation time) is the primary concern.</li> <li>OBO: OBO used to be the dominant language in the biomedical domain before the advent of OWL. It also used to have its own specific semantics associated with it. OBO semantics have since been mapped into OWL semantics, so that for all practical purposes, we consider \"OBO\" now a dialect of OWL, which means that when you hear 'OBO format' today, we are generally referring to the serialisation (see below), NOT the formalism. Note that when we say OBO ontologies we mean literally Open Biomedical and Biological Ontologies, and NOT Ontology in OBO format.</li> </ul> <p>Some people like to also list SHACL and Shex as ontology languages and formalisms. Formalisms define syntax (e.g. grammar rules) and semantics (what does what expression mean?). The analogue in the real world would be natural languages, like English or Greek.</p> <ul> <li>A format, or serialisation of a language is used to write down statements of a formal language in some way. Formats are not formalisms - they simply enable statements in a formalism to be expressed in some (usually textual) way. The most common formats in our domains are:</li> <li>RDF/XML. This is the default serialisation language of the OWL flavours of OBO ontologies. It is a pretty ugly format, really hard to understand by most users but it has one advantage - it can be understood widely by RDF-focused tools like rdflib, OWL-focused tools like those based on the OWL API</li> <li>OWL Functional Syntax: This is very common syntax for editing ontologies in OWL, because they look nice in diff tools such as <code>git diff</code>, i.e changes to ontologies in functional syntax are much easier to review. RDF/XML is not suitable for manual review, due to its verbosity and complexity.</li> <li>OWL Manchester Syntax: This is the default language for OWL tutorials and for writing class expressions in editors such as Protege</li> <li>OBO Format: The most easy to read of all the serialisations. In many ontologies such as Mondo and Uberon, we still use OBO as the editors' format (as opposed to OWL Functional Syntax, which is more wide-spread). OBO format looks clear and beautiful in diffs such as git diffs, and therefore still continues to be widespread. OBO Format does not cover all of owl, and should only be used in conjunction with ontologies that stay within the limits of the OBO format specification.</li> <li>OBO Graphs JSON: A simple JSON serialisation of ontologies. This format roughly reflects the capabilities of the OBO format, but is intended for consumption by tools. Again, it does not cover all of OWL, but it does cover the parts that are relevant in 99% of the use cases.</li> </ul> <p>The real-world analogue of serialisation or format is a script, i.e. Latin or Cyrillic script (not quite clean analogue).</p> <ul> <li>A variant is a version of the ontology exported for a specific purpose. The most important variants are:</li> <li>Edit: The variant of the ontology that is edited by ontology curators. Its sole purpose is to be used by ontology editors, and should not be used by any other application. In a ODK-style repository, the edit file is typically located hidden from view, e.g. <code>src/ontology/cl-edit.owl</code>.</li> <li>Full: The ontology with all its imports merged in, and classified using a reasoner, see docs. The Full variant should be used by users that require the use of reasoners and a guarantee that all the inferences work as intended by the ontology developers. This is the default variant of most OBO ontologies.</li> <li>Base: The axioms belonging to the ontology, excluding any axioms from imported ontologies, see docs. Base variants are used by ontology repository developers to combine the latest versions of all ontologies in a way that avoids problems due to conflicting versions. Base files should not be used by users that want to use the ontology in downstream tools, such as annotation tools or scientific databases, as they are incomplete, i.e. not fully classified.</li> <li>Simple: A version of the ontology that only contains only a subset of the ontology (only the direct relations, see docs). The simple variant should be used by most users that build tools that use the ontology, especially when serialised as OBO graphs json. This variant should probably be avoided by power-users working with reasoners, as many of the axioms that drive reasoning are missing.</li> <li>Basic: A variant of Simple, in that it is reduced to only a specific set of relations, such as <code>subClassOf</code> and <code>partOf</code>. Some users that require the ontology to correspond to acyclic graphs, or deliberately want to focus only on a set of core relations, will want to use this variant, see docs). The formal definition of the basic variant can be found here.</li> <li>Other variants: Some variants are still used, like \"non-classified\", see docs), but should be avoided. Others like base-plus, a variant that corresponds to base + the inferred axioms, are still under development, and will be explained here when they are fully developed.</li> </ul>"},{"location":"explanation/owl-format-variants/#best-practices","title":"Best practices","text":"<ul> <li>Tool developers developing tools that use the ontology (and do not need reasoners), such as database curation tools, web-browsers and similar, should typically use OBO graphs JSON and avoid using OBO format or any of the OWL focussed serialisations (Functional, Manchester or RDF/XML). OWL-focussed serialisations contain a huge deal of axiomatic content that make no sense to most users, and can lead to a variety of mistakes. We have seen it many times that software developers try to interpret OWL axioms somehow to extract relations. Do not do that! Work with the ontologies to ensure they provide the relationships you need in the appropriate form.</li> <li>Tool developers building tools to work with ontologies should typically ensure that they can read and write RDF/XML - this is the most widely understood serialisation. Work with ontologies means here 'enable operations that change the content of the ontology'.</li> <li>Tool developers building infrastructure to query across ontologies should consider using base variants - these ensure that you can always use the latest version of each ontology and avoid most of the common version clashes. It is important that such users are keenly aware of the role of OWL reasoning in such a process.</li> <li>Many users of ontologies think they need the reasoner actually don't. Make sure you consult with an expert before building a system that relies on OWL reasoners to deliver user facing services.</li> <li>As an ontology developer, it is great practice to provide the above variants in the common serialisations. The Ontology Development Kit provides defaults for all of these.</li> <li>As an ontology developer, you should avoid publishing your ontology with <code>owl:imports</code> statements - these are easily ignored by your users and make the intended \"content\" of the ontology quite opaque.</li> </ul>"},{"location":"explanation/subClassOf-vs-equivalentTo/","title":"<code>SubClassOf</code> vs <code>EquivalentTo</code>","text":""},{"location":"explanation/subClassOf-vs-equivalentTo/#prerequisites","title":"Prerequisites","text":"<p>This lesson assumes you have basic knowledge wrt ontologies and OWL as explained in:</p> <ul> <li>Introduction to ontologies</li> <li>Logical building blocks of OWL</li> </ul>"},{"location":"explanation/subClassOf-vs-equivalentTo/#subclassof","title":"<code>SubClassOf</code>","text":"<p>In this section we explain the semantics of <code>SubClassOf</code>, give an example of using <code>SubClassOf</code> and provide guidance for when not to use <code>SubClassOf</code>.</p>"},{"location":"explanation/subClassOf-vs-equivalentTo/#semantics","title":"Semantics","text":"<p>If we have </p> <pre><code>Class: C\n  SubClassOf: D\nClass: D\n</code></pre> <p>the semantics of it is given by the following Venn diagram:</p> <p></p> <p>Thus, the semantics is given by the subset relationship, stating the <code>C</code> is a subset of <code>D</code>. This means every individual  of <code>C</code> is necessarily an individual of <code>D</code>, but not every individual of <code>D</code> is necessarily an individual of <code>C</code>. </p>"},{"location":"explanation/subClassOf-vs-equivalentTo/#a-concrete-example","title":"A concrete example","text":"<pre><code>Class: Dog\n  SubClassOf: Pet\nClass: Pet\n</code></pre> <p>which as a Venn diagram will look as follows:</p> <p></p>"},{"location":"explanation/subClassOf-vs-equivalentTo/#guidance","title":"Guidance","text":"<p>There are at least 2 scenarios which at first glance may seem like <code>C SubClassOf D</code> holds, but it does not hold, or  using <code>C EquivalentTo D</code> may be a better option.</p> <ol> <li>This is typically where <code>C</code> has many individuals that are in <code>D</code>, but there is at least 1 individual of <code>C</code> that is  not in <code>D</code>. The following Venn diagram is an example. Thus, to check whether you may be dealing with this scenario, you  can ask the following question: Is there any individual in <code>C</code> that is not in <code>D</code>? If 'yes', you are dealing with this  scanario and you should not be using <code>C SubClassOf D</code>. </li> </ol> <p></p> <ol> <li>When you have determined that (1) does not hold, you may deal with the scenario where not only is every individual of <code>C</code> in <code>D</code>, but also every individual in <code>D</code> is in <code>C</code>. This means <code>C</code> and <code>D</code> are equivalent. In the case you rather want  to make use of <code>EquivalentTo</code>.</li> </ol> <p></p>"},{"location":"explanation/subClassOf-vs-equivalentTo/#equivalentto","title":"<code>EquivalentTo</code>","text":""},{"location":"explanation/subClassOf-vs-equivalentTo/#semantics_1","title":"Semantics","text":"<p>If we have</p> <pre><code>Class: C\n    EquivalentTo: D\nClass: D\n</code></pre> <p>this means the sets <code>C</code> and <code>D</code> fit perfectly on each other, as shown in the next Venn diagram:</p> <p></p> <p>Note that <code>C EquivalentTo D</code> is shorthand for </p> <pre><code>Class: C\n    SubClassOf: D\nClass: D\n    SubClassOf: C\n</code></pre> <p>though, in general it is better to use <code>EquivalentTo</code> rather than the 2 <code>SubClassOf</code> axioms when <code>C</code> and <code>D</code> are equivalent.</p>"},{"location":"explanation/subClassOf-vs-equivalentTo/#a-concrete-example_1","title":"A concrete example","text":"<p>We all probably think of humans and persons as the exact same set of individuals.</p> <pre><code>Class: Person\n    EquivalentTo: Human\nClass: Human\n</code></pre> <p>and as a Venn diagram:</p> <p></p>"},{"location":"explanation/subClassOf-vs-equivalentTo/#guidance_1","title":"Guidance","text":"<p>When do you not want to use <code>EquivalentTo</code>? </p> <ol> <li>When there is an individual of <code>C</code> that is not in <code>D</code>.</li> </ol> <p></p> <ol> <li>When there is an individual of <code>D</code> that is not in <code>C</code>.</li> </ol> <p></p>"},{"location":"explanation/taxon-constraints-explainer/","title":"Guide to Taxon Restrictions","text":""},{"location":"explanation/taxon-constraints-explainer/#tutorials","title":"Tutorials","text":""},{"location":"explanation/taxon-constraints-explainer/#monarch-obo-training-tutorial","title":"Monarch OBO training Tutorial","text":""},{"location":"explanation/taxon-constraints-explainer/#modeling-with-taxon-constraints","title":"Modeling with Taxon Constraints","text":""},{"location":"explanation/taxon-constraints-explainer/#what-are-taxon-restrictions","title":"What are taxon restrictions?","text":"<p>Taxon restrictions (or, \"taxon constraints\") are a formalised way to record what species a term applies to\u2014something crucial in multi-species ontologies.</p> <p>Even species neutral ontologies (e.g., GO) have classes that have implicit taxon restriction.</p> <pre><code>GO:0007595 ! Lactation - defined as \u201cThe secretion of milk by the mammary gland.\u201d\n</code></pre>"},{"location":"explanation/taxon-constraints-explainer/#uses-for-taxon-restrictions","title":"Uses for taxon restrictions","text":"<ol> <li> <p>Finding inconsistencies. Taxon restrictions use terms from the NCBI Taxonomy Ontology, which asserts pairwise disjointness between sibling taxa (e.g., nothing can be both an insect and a rodent). When terms have taxon restrictions, a reasoner can check for inconsistencies.</p> <p>When GO implemented taxon restrictions, they found 5874 errors!</p> </li> <li> <p>Defining taxon-specific subclasses. You can define a taxon-specific subclass of a broader concept, e.g., 'human clavicle'. This allows you, for example, to assert relationships for the new term that don't apply to all instances of the broader concept:</p> <pre><code>'human clavicle' EquivalentTo ('clavicle bone' and ('in taxon' some 'Homo sapiens'))\n'human clavicle' SubClassOf ('connected to' some sternum)\n</code></pre> </li> <li> <p>Creating SLIMs. Use a reasoner to generate ontology subsets containing only those terms that are logically allowed within a given taxon.</p> </li> <li> <p>Querying. Facet terms by taxon. E.g., in Brain Data Standards, in_taxon axioms allow faceting cell types by species. (note: there are limitations on this and may be incomplete).</p> </li> </ol>"},{"location":"explanation/taxon-constraints-explainer/#types-of-taxon-restrictions","title":"Types of Taxon Restrictions","text":"<p>There are, in essence, three categories of taxon-specific knowledge we use across OBO ontologies. Given a class <code>C</code>, which could be anything from an anatomical entity to a biological process, we have the following categories:</p> <ol> <li>The ALL-IN restriction: \"C in_taxon T\"<ul> <li>\"Hair is found only in Mammals\"</li> </ul> </li> <li>The NOT-IN restriction: \"C never_in_taxon T\"<ul> <li>\"Hair is never found in Birds\"</li> </ul> </li> <li>The SOME-IN restriction: \"C present_in_taxon T\"<ul> <li>\"Hair is found in Skunks\"</li> <li>\"Hair is found in Whales\"</li> </ul> </li> </ol>"},{"location":"explanation/taxon-constraints-explainer/#the-all-in-restriction-c-in_taxon-t","title":"The ALL-IN restriction: \"C in_taxon T\"","text":"<ul> <li>Meaning: \"All instances of <code>C</code> are in some instance of taxon <code>T</code>\"</li> <li>As this is a relation between instances, it may have been more correct to give this property a label such as \"in organism\".</li> <li>Canonical logical representation:   <pre><code>C SubClassOf (in_taxon some T)\n</code></pre></li> <li>Alternative representations: None</li> <li>Editor guidance: Editors use the canonical logical representation in a SubClassOf axiom to add a taxon restriction, or in a simple (non-nested) EquivalentClass axiom to define a taxon-specific subclass (which will also imply the taxon restriction). When used in a SubClassOf axiom, the taxon should be as specific as possible for the maximum utility, but may still need to be quite broad, as it applies to every instance of <code>C</code>.</li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#the-not-in-restriction-c-subclassof-not-in_taxon-some-t","title":"The NOT-IN restriction: \"C SubClassOf (not (in_taxon some T))\"","text":"<ul> <li>Meaning: \"No instances of <code>C</code> are in taxon <code>T</code>\"</li> <li>Canonical logical representation:     <pre><code>C SubClassOf (not (in_taxon some T))`\n</code></pre></li> <li>Alternative representations:<ul> <li>Alternative EL logical representation: <code>C DisjointWith (in_taxon some T)</code></li> <li>EL helper axiom: <code>C SubClassOf (in_taxon some (not T))</code></li> <li>Canonical shortcut: AnnotationAssertion: <code>C never_in_taxon T</code> # Editors use this</li> </ul> </li> <li>Editor guidance: Editors use the canonical shortcut (annotation axiom). For <code>never_in_taxon</code> annotations, the taxon should be as broad as possible for the maximum utility, but it must be the case that a <code>C</code> is never found in any subclass of that taxon.</li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#the-some-in-restriction-a-classassertion-c-and-in_taxon-some-t","title":"The SOME-IN restriction: \"a ClassAssertion: <code>C</code> and in_taxon some <code>T</code>\"","text":"<ul> <li>Meaning: \"At least one specific instance of <code>C</code> is in taxon <code>T</code>\".</li> <li>Canonical logical representation:     <pre><code>IND:a Type (C and (in_taxon some T))`\n</code></pre></li> <li>Alternative representations:<ul> <li>Generated subclass for QC purposes: <code>C_in_T SubClassOf (C and (in_taxon some T)</code> (<code>C_in_T</code> will be unsatisifiable if violates taxon constraints)</li> <li>Canonical shortcut: AnnotationAssertion: <code>C present_in_taxon T</code> # Editors use this</li> </ul> </li> <li>Editor guidance: Editors use the canonical shorcut (annotation axiom). The taxon should be as specific as possible, ideally a species.</li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#how-to-add-taxon-restrictions","title":"How to add taxon restrictions:","text":"<p>Please see how-to guide on adding taxon restrictions</p>"},{"location":"explanation/taxon-constraints-explainer/#using-taxon-restrictions-for-quality-control","title":"Using taxon restrictions for Quality Control","text":"<p>As stated above, one of the major applications for taxon restrictions in OBO is for quality control (QC), by finding logical inconsistencies. Many OBO ontologies consist of a complex web of term relationships, often crossing ontology boundaries (e.g., GO biological process terms referencing Uberon anatomical structures or CHEBI chemical entities). If particular terms are only defined to apply to certain taxa, it is critical to know that a chain of logic implies that the term must exist in some other taxon which should be impossible. Propagating taxon restrictions via logical relationships greatly expands their effectiveness (the GO term above may acquire a taxon restriction via the type of anatomical structure in which it occurs).</p> <p>It can be helpful to think informally about how taxon restrictions propagate over the class hierarchy. It's different for all three types:</p> <ul> <li>ALL-IN restrictions (<code>in_taxon</code>) include all superclasses of the taxon, and all subclasses of the subject term:      %% Future editors, note that link styles are applied according to the link index, so be careful if adding or removing links.    graph BT;      n1(hair) ;      n2(whisker) ;      n3(Mammalia) ;      n4(Tetrapoda) ;      n2--is_a--&gt;n1 ;      n3--is_a--&gt;n4 ;      n1==in_taxon==&gt;n3 ;      n1-.in_taxon.-&gt;n4 ;      n2-.in_taxon.-&gt;n3 ;      n2-.in_taxon.-&gt;n4 ;      linkStyle 0 stroke:#999 ;      linkStyle 1 stroke:#999 ;      style n1 stroke-width:4px ;      style n3 stroke-width:4px ;</li> <li>NOT-IN restrictions (<code>never_in_taxon</code>) include all subclasses of the taxon, and all subclasses of the subject term:      %% Future editors, note that link styles are applied according to the link index, so be careful if adding or removing links.    graph BT;      n1(facial whisker) ;      n2(whisker) ;      n3(Homo sapiens) ;      n4(Hominidae) ;      n1--is_a--&gt;n2 ;      n3--is_a--&gt;n4 ;      n2==never_in_taxon==&gt;n4 ;      n2-.never_in_taxon.-&gt;n3 ;      n1-.never_in_taxon.-&gt;n4 ;      n1-.never_in_taxon.-&gt;n3 ;      linkStyle 0 stroke:#999 ;      linkStyle 1 stroke:#999 ;      style n2 stroke-width:4px ;      style n4 stroke-width:4px ;</li> <li>SOME-IN restrictions (<code>present_in_taxon</code>) include all superclasses of the taxon, and all superclasses of the subject term:      %% Future editors, note that link styles are applied according to the link index, so be careful if adding or removing links.    graph BT;      n1(hair) ;      n2(whisker) ;      n3(Felis) ;      n4(Carnivora) ;      n2--is_a--&gt;n1 ;      n3--is_a--&gt;n4 ;      n2==present_in_taxon==&gt;n3 ;      n1-.present_in_taxon.-&gt;n3 ;      n2-.present_in_taxon.-&gt;n4 ;      n1-.present_in_taxon.-&gt;n4 ;      linkStyle 0 stroke:#999 ;      linkStyle 1 stroke:#999 ;      style n2 stroke-width:4px ;      style n3 stroke-width:4px ;</li> </ul> <p>The Relation Ontology defines number of property chains for the <code>in_taxon</code> property. This allows taxon restrictions to propagate over other relationships. For example, the <code>part_of o in_taxon -&gt; in_taxon</code> chain implies that if a muscle is part of a whisker, then the muscle must be in a mammal, but not in a human, since we know both of these things about whiskers:</p>      %% Future editors, note that link styles are applied according to the link index, so be careful if adding or removing links.      graph BT;        n1(hair) ;        n2(whisker) ;        n3(Mammalia) ;        n4(Homo sapiens) ;        n5(Hominidae) ;        n6(whisker muscle) ;        n2--is_a--&gt;n1 ;        n5--is_a--&gt;n3 ;        n4--is_a--&gt;n5 ;        n6--part_of--&gt;n2 ;        n1==in_taxon==&gt;n3 ;        n2==never_in_taxon==&gt;n5 ;        n2-.in_taxon.-&gt;n3 ;        n6-.in_taxon.-&gt;n3 ;        n6-.never_in_taxon.-&gt;n4 ;        n2-.never_in_taxon.-&gt;n4 ;        n6-.never_in_taxon.-&gt;n5 ;        linkStyle 0 stroke:#999 ;        linkStyle 1 stroke:#999 ;        linkStyle 2 stroke:#999 ;        linkStyle 3 stroke:#008080 ;        style n6 stroke-width:4px ; <p>Property chains are the most common way in which taxon restrictions propagate across ontology boundaries. For example, Gene Ontology uses various subproperties of results in developmental progression of to connect biological processes to Uberon anatomical entities. Any taxonomic restrictions which hold for the anatomical entity will propagate to the biological process via this property.</p> <p>The graph depictions in the preceding illustrations are informal; in practice <code>never_in_taxon</code> and <code>present_in_taxon</code> annotations are translated into more complex logical constructions using the <code>in_taxon</code> object property, described in the next section. These logical constructs allow the OWL reasoner to determine that a class is unsatisfiable when there are conflicts between taxon restriction inferences.</p>"},{"location":"explanation/taxon-constraints-explainer/#implementation-and-reasoning-with-taxon-restrictions","title":"Implementation and reasoning with taxon restrictions","text":"<p>The OWL axioms required to derive the desired entailments for taxon restrictions are somewhat more complicated than one might expect. Much of the complication is the result of workarounds to limitations dictated by the OWL EL profile. Because of the size and complexity of many of the ontologies in the OBO Library, particularly those heavily using taxon restrictions, we primarily rely on the ELK reasoner, which is fast and scalable since it implements OWL EL rather than the complete OWL language. In the following we discuss the particular kinds of axioms required in order for taxon restrictions to work with ELK, with some comments about how it could work with HermiT (which implements the complete OWL language but is much less scalable). We will focus on this example ontology:</p>      %% Future editors, note that link styles are applied according to the link index, so be careful if adding or removing links.      graph BT;        n1(hair) ;        n2(whisker) ;        n3(muscle) ;        n4(whisker muscle) ;        n5(whisker muscle in human) ;        n6(whisker in catfish) ;        n7(whisker in human) ;        n8(Vertebrata) ;        n9(Teleostei) ;        n10(Siluriformes) ;        n11(Tetrapoda) ;        n12(Mammalia) ;        n13(Hominidae) ;        n14(Homo sapiens) ;        n2--is_a--&gt;n1 ;        n4--is_a--&gt;n3 ;        n9--is_a--&gt;n8 ;        n10--is_a--&gt;n9 ;        n11--is_a--&gt;n8 ;        n12--is_a--&gt;n11 ;        n13--is_a--&gt;n12 ;        n14--is_a--&gt;n13 ;        n5--is_a--&gt;n4 ;        n6--is_a--&gt;n2 ;        n7--is_a--&gt;n2 ;        n4--part_of--&gt;n2 ;        n11 --disjoint_with--- n9 ;        n1==in_taxon==&gt;n12 ;        n2==never_in_taxon==&gt;n13 ;        n5==in_taxon==&gt;n14 ;        n7==in_taxon==&gt;n14 ;        n6==in_taxon==&gt;n10 ;        linkStyle 0 stroke:#999 ;        linkStyle 1 stroke:#999 ;        linkStyle 2 stroke:#999 ;        linkStyle 3 stroke:#999 ;        linkStyle 4 stroke:#999 ;        linkStyle 5 stroke:#999 ;        linkStyle 6 stroke:#999 ;        linkStyle 7 stroke:#999 ;        linkStyle 8 stroke:#999 ;        linkStyle 9 stroke:#999 ;        linkStyle 10 stroke:#999 ;        linkStyle 11 stroke:#008080 ;        linkStyle 12 stroke:red ;        style n5 stroke-width:4px,stroke:red ;        style n6 stroke-width:4px,stroke:red ;        style n7 stroke-width:4px,stroke:red ; <p>There are three classes outlined in red which were created mistakenly; the asserted taxon for each of these conflicts with taxon restrictions in the rest of the ontology:</p> <ul> <li>'whisker in human' \u2014 We expect this to be unsatisfiable since it is a subclass of 'whisker', which has a 'never in Hominidae' restriction. 'Whisker in human' is asserted to be in_taxon 'Homo sapiens', a subclass of 'Hominidae'.</li> <li>'whisker in catfish' \u2014 We expect this to be unsatisfiable since it is a subclass of 'whisker', and thus a subclass of 'hair'. 'Hair' has an 'only in Mammalia' restriction. 'Whisker in catfish' is asserted to be in_taxon 'Siluriformes' (catfish), which is a subclass of Teleostei and thus disjoint from 'Mammalia'.</li> <li>'whisker muscle in human' \u2014 We expect this to be unsatisfiable since it is a 'whisker muscle' and thus part of a 'whisker', and thus inherits the 'never in Hominidae' restriction from 'whisker' via the property chain <code>part_of o in_taxon -&gt; in_taxon</code>. This conflicts with its asserted in_taxon 'Homo sapiens', a subclass of 'Hominidae'.</li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#taxon-restriction-modeling","title":"Taxon restriction modeling","text":"<p>We can start by modeling the two taxon restrictions in the ontology like so:</p> <ul> <li>'hair' 'in_taxon' 'Mammalia': <code>'hair' SubClassOf (in_taxon some 'Mammalia')</code></li> <li>'whisker' 'never_in_taxon' 'Mammalia': <code>'whisker' SubClassOf (not (in_taxon some 'Hominidae'))</code></li> </ul> <p>Both HermiT and ELK can derive that 'whisker in human' is unsatisfiable. This is the explanation:</p> <ul> <li><code>'human whisker' EquivalentTo ('whisker' and (in_taxon some 'Homo sapiens'))</code></li> <li><code>'Homo sapiens' SubClassOf 'Hominidae'</code></li> <li><code>'whisker' SubClassOf (not ('in_taxon' some 'Hominidae'))</code></li> </ul> <p>Unfortunately, neither reasoner detects the other two problems. We'll address the 'whisker in catfish' first. The reasoner infers that this class is <code>in_taxon</code> both 'Mammalia' and 'Siluriformes'. While these are disjoint classes (all sibling taxa are asserted to be disjoint in the taxonomy ontology), there is nothing in the ontology stating that something can only be in one taxon at a time. The most intuitive solution to this problem would be to assert that <code>in_taxon</code> is a functional property. However, due to limitations of OWL, functional properties can't be used in combination with property chains. Furthermore, functional properties aren't part of OWL EL. There is one solution that works for HermiT, but not ELK. We could add an axiom like the following to every \"always in taxon\" restriction:</p> <ul> <li><code>'hair' SubClassOf (in_taxon only 'Mammalia')</code></li> </ul> <p>This would be sufficient for HermiT to detect the unsatisfiability of 'whisker in catfish' (assuming taxon sibling disjointness). Unfortunately, <code>only</code> restrictions are not part of OWL EL. Instead of adding the <code>only</code> restrictions, we can generate an extra disjointness axiom for every taxon disjointness in the taxonomy ontology, e.g.:</p> <ul> <li><code>(in_taxon some 'Tetrapoda') DisjointWith (in_taxon some 'Teleostei')</code></li> </ul> <p>The addition of axioms like that is sufficient to detect the unsatisfiability of 'whisker in catfish' in both HermiT and ELK. This is the explanation:</p> <ul> <li><code>'whisker in catfish' EquivalentTo ('whisker' and (in_taxon some 'Siluriformes'))</code></li> <li><code>'whisker' SubClassOf 'hair'</code></li> <li><code>'hair' SubClassOf (in_taxon some 'Mammalia')</code></li> <li><code>'Mammalia' SubClassOf 'Tetrapoda'</code></li> <li><code>'Siluriformes' SubClassOf 'Teleostei'</code></li> <li><code>(in_taxon some 'Teleostei') DisjointWith (in_taxon some 'Tetrapoda')</code></li> </ul> <p>While we can now detect two of the unsatisfiable classes, sadly neither HermiT nor ELK yet finds 'whisker muscle in human' to be unsatisfiable, which requires handling the interaction of a \"never\" assertion with a property chain. If we were able to make <code>in_taxon</code> a functional property, HermiT should be able to detect the problem; but as we said before, OWL doesn't allow us to combine functional properties with property chains. The solution is to add even more generated disjointness axioms, one for each taxon (in combination with the extra disjointness we added in the previous case), e.g.,:</p> <ul> <li><code>(in_taxon some Hominidae) DisjointWith (in_taxon some (not Hominidae))</code></li> </ul> <p>While that is sufficient for HermiT, for ELK we also need to add another axiom to the translation of each never_in_taxon assertion, e.g.,:</p> <ul> <li><code>'whisker' SubClassOf (in_taxon some (not 'Hominidae'))</code></li> </ul> <p>Now both HermiT and ELK can find 'whisker muscle in human' to be unsatisfiable. This is the explanation from ELK:</p> <ul> <li><code>'whisker muscle in human' EquivalentTo ('whisker muscle' and (in_taxon some 'Homo sapiens'))</code></li> <li><code>'Homo sapiens' SubClassOf 'Hominidae'</code></li> <li><code>'whisker muscle' SubClassOf (part_of some 'whisker')</code></li> <li><code>'whisker' SubClassOf (in_taxon some ('not 'Hominidae'))</code></li> <li><code>part_of o in_taxon SubPropertyOf in_taxon</code></li> <li><code>(in_taxon some 'Hominidae') DisjointWith (in_taxon some (not 'Hominidae'))</code></li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#modeling-present_in_taxon","title":"Modeling present_in_taxon","text":"<p>The above example didn't incorporate any present_in_taxon (SOME-IN) assertions. These work much the same as ALL-IN in_taxon assertions. However, instead of stating that all instances of a given class are in a taxon (<code>C SubClassOf (in_taxon some X)</code>), we either state that there exists an individual of that class in that taxon, or that there is some subclass of that class whose instances are in that taxon:</p> <ol> <li> <p><code>&lt;generated individual IRI&gt; Type (C and (in_taxon some X))</code> \u2014 violations involving this assertion will make the ontology logically inconsistent.</p> <p>or</p> </li> <li> <p><code>&lt;generated class IRI&gt; SubClassOf (C and (in_taxon some X))</code> \u2014 violations involving this assertion will make the ontology logically incoherent, i.e., a named class is unsatisfiable (here, <code>&lt;generated class IRI&gt;</code>).</p> </li> </ol> <p>Incoherency is easier to debug than inconsistency, so option 2 is the default expansion for <code>present_in_taxon</code>.</p> <p>In summary, the following constructs are all needed for QC using taxon restrictions:</p> <ul> <li>Relation Ontology<ul> <li><code>in_taxon</code> property chains for relations which should propagate <code>in_taxon</code> inferences</li> </ul> </li> <li>NCBI Taxonomy Ontology<ul> <li><code>X DisjointWith Y</code> for all sibling taxa <code>X</code> and <code>Y</code></li> <li><code>(in_taxon some X) DisjointWith (in_taxon some Y)</code> for all sibling taxa <code>X</code> and <code>Y</code></li> <li><code>(in_taxon some X) DisjointWith (in_taxon some (not X))</code> for every taxon <code>X</code></li> </ul> </li> <li>Each ALL-IN taxon restriction <code>C in_taxon X</code><ul> <li><code>C SubClassOf (in_taxon some X)</code></li> </ul> </li> <li>Each NOT-IN taxon restriction <code>C never_in_taxon X</code><ul> <li><code>C SubClassOf (not (in_taxon some X))</code></li> <li><code>C SubClassOf (in_taxon some (not X))</code></li> </ul> </li> <li>Each SOME-IN taxon restriction <code>C present_in_taxon X</code>)<ul> <li><code>&lt;generated class IRI&gt; SubClassOf (C and (in_taxon some X))</code></li> </ul> </li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#employing-taxon-restrictions-in-your-qc-pipeline","title":"Employing taxon restrictions in your QC pipeline","text":"<p>If you are checking an ontology for coherency in a QC pipeline (such as by running ROBOT within the ODK), you will need to have the required constructs from the previous section present in your import chain:</p> <ul> <li>Relation Ontology     \u2014 import as usual</li> <li>NCBI Taxonomy Ontology<ul> <li>import the main taxonomy (<code>http://purl.obolibrary.org/obo/ncbitaxon.owl</code>)</li> <li>import <code>http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim-disjoint-over-in-taxon.owl</code> (or implement a way to generate the needed disjointness axioms)<ul> <li>Note: that file only covers a subset of the taxonomy, and is missing <code>(in_taxon some X) DisjointWith (in_taxon some (not X))</code>. You may need to implement a way to generate the needed disjointness axioms until this is corrected.</li> </ul> </li> </ul> </li> <li>Your own taxon restrictions within your ontology:<ul> <li>ALL-IN taxon restrictions require no expansion. If you are using the <code>never_in_taxon</code> and <code>present_in_taxon</code> shortcut annotation properties, you can expand these into the logical forms using <code>robot expand</code>.</li> <li>Because <code>present_in_taxon</code> expansions add named classes to your ontology, you will probably want to organize your pipeline in such a way that this expansion only happens in a QC check, and the output is not included in your published ontology.</li> </ul> </li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#exploring-taxon-restrictions-in-protege","title":"Exploring taxon restrictions in Prot\u00e9g\u00e9","text":"<p>Using the DL Query panel and a running reasoner, it is straightforward to check whether a particular taxon restriction holds for a term (such as when someone has requested one be added to your ontology). Given some term of interest, e.g., 'whisker', submit a DL Query such as <code>'whisker' and (in_taxon some Mammalia)</code>. Check the query results:</p> <ul> <li>If <code>Equivalent classes</code> includes <code>owl:Nothing</code>, then a never_in_taxon is implied for that taxon.</li> <li>If <code>Equivalent classes</code> includes the term of interest itself (and not <code>owl:Nothing</code>), then an in_taxon is implied for that taxon.</li> <li>If <code>Superclasses</code> includes the term of interest (and the query isn't equivalent to <code>owl:Nothing</code>), then there is no particular taxon restriction involving that taxon.</li> </ul>"},{"location":"explanation/taxon-constraints-explainer/#obo-taxon-constraints-plugin-for-protege","title":"OBO taxon constraints plugin for Prot\u00e9g\u00e9","text":"<p>To quickly see exactly which taxon restrictions are in effect for a selected term, install the OBO taxon constraints plugin for Prot\u00e9g\u00e9. Once you have the plugin installed, you can add it to your Prot\u00e9g\u00e9 window by going to the menu <code>Window &gt; Views &gt; OBO views &gt; Taxon constraints</code>, and then clicking the location to place the panel. The plugin will show the taxon constraints in effect for the selected OWL class. When a reasoner is running, any inferred taxon constraints will be shown along with directly asserted ones. The plugin executes many reasoner queries behind the scenes, so there may be a delay before the user interface is updated.</p> <p></p>"},{"location":"explanation/term-comments/","title":"Term Comments","text":""},{"location":"explanation/term-comments/#what-are-comments","title":"What are comments?","text":"<p>Comments are annotations that may be added to ontology terms to further explain their intended usage, or include information that is useful but does not fit in areas like definition.</p> <p>Some examples of comments, and possible standard language for their usage, are:</p> <p>WARNING: THESE EXAMPLES ARE NOT UNIVERSALLY USED AND CAN BE CONTROVERSIAL IN SOME ONTOLOGIES! PLEASE CHECK WITH THE CONVENTIONS OF YOUR ONTOLOGY BEFORE DOING THIS!</p>"},{"location":"explanation/term-comments/#do-not-annotate","title":"Do Not Annotate","text":"<p>This term should not be used for direct annotation. It should be possible to make a more specific annotation to one of the children of this term.</p> <p>Example: GO:0006810 transport</p> <p>Note that this term should not be used for direct annotation. It should be possible to make a more specific annotation to one of the children of this term, for e.g. transmembrane transport, microtubule-based transport, vesicle-mediated transport, etc.</p>"},{"location":"explanation/term-comments/#do-not-manually-annotate","title":"Do Not Manually Annotate","text":"<p>This term should not be used for direct manual annotation. It should be possible to make a more specific manual annotation to one of the children of this term.</p> <p>Example: GO:0000910 cytokinesis</p> <p>Note that this term should not be used for direct annotation. When annotating eukaryotic species, mitotic or meiotic cytokinesis should always be specified for manual annotation and for prokaryotic species use 'FtsZ-dependent cytokinesis; GO:0043093' or 'Cdv-dependent cytokinesis; GO:0061639'. Also, note that cytokinesis does not necessarily result in physical separation and detachment of the two daughter cells from each other.</p>"},{"location":"explanation/term-comments/#additional-information","title":"Additional Information","text":"<p>Information about the term that do not belong belong in the definition or gloss, but are useful for users or editors. This might include information that is adjacent to the class but pertinent to its usage, extended information about the class (eg extended notes about a characteristic of a cell type) that might be useful but does not belong in the definition, important notes on why certain choices were made in the curation of this terms (eg why certain logical axioms were excluded/included in the way they are) (Note: dependent on ontology, some of these might belong in editors_notes, etc.).</p> <p>Standard language for these are not given as they vary dependent on usage.</p>"},{"location":"explanation/which-ontology-to-use/","title":"Which biomedical ontologies should we use?","text":"<p>As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \"10 simple rules for selecting a Bio-ontology\" by Malone et al, but I would further recommend to ask yourself the following:</p> <ul> <li>Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation).</li> <li>Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently poly-hierarchical (such as Mondo), while others strive to be single inheritance (DO, ICD).</li> <li>Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data.</li> </ul> <p>Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being paid every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS!) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them.</p>"},{"location":"explanation/writing-good-issues/","title":"Writing Good Issues","text":"<p>Based on Intro to GitHub (GO-Centric) with credit to Nomi Harris and Chris Mungall</p> <p>Writing a good ticket (or issue) is crucial to good management of a repo. In this explainer, we will discuss some good practices in writing a ticket and show examples of what not to do.</p>"},{"location":"explanation/writing-good-issues/#best-practices","title":"Best Practices","text":"<ul> <li>Search existing issues before creating a new one -- maybe someone else already reported your problem</li> <li>Give your issue a short but descriptive and actionable title</li> <li>Describe the problem and the context and include a repeatable example.</li> <li>Clearly state what needs to be done to close the ticket</li> <li>Tickets should ideally be actionable units that can be closed via a PR</li> <li>Flag relevant people with @ (e.g., @nlharris)</li> <li>Mention related issues with # (e.g., #123)</li> <li>use a complete URL to link to tickets in other repos</li> <li>Make issue titles actionable</li> <li>eg \"Ontology download page on GO website\" is non-actionable, whereas \"Fix URLs on ontology download page on GO website\" is actionable and hence better a title</li> </ul>"},{"location":"explanation/writing-good-issues/#example-of-a-good-ticket","title":"Example of a good ticket","text":""},{"location":"explanation/writing-good-issues/#example-of-a-bad-ticket","title":"Example of a bad ticket","text":""},{"location":"howto/add-coc/","title":"Add a Code of Conduct","text":""},{"location":"howto/add-coc/#adding-a-code-of-conduct-to-your-repository","title":"Adding a Code of Conduct to your repository","text":"<p>To include a Code of Conduct (CoC) in your ontology repository, first create a dedicated file named <code>CODE_OF_CONDUCT.md</code> in the root directory of your repository. This file should clearly articulate the community's expected behaviors, identify unacceptable conduct such as harassment or discrimination, and describe the mechanisms through which community members can confidentially report any violations. You may base your document on widely recognized templates such as the Contributor Covenant or established examples like the OBO Foundry's Code of Conduct. Once created, prominently link your CoC from the repository's <code>README.md</code>, contribution guidelines (<code>CONTRIBUTING.md</code>), and other prominent documentation pages to ensure it is easily discoverable.</p> <p>It is important to tailor your CoC to the specific project it is applied to. Various features of the project, including its size, its primary stakeholders and funding situation imply different aspects of the COC to apply, or in need to be specifically highlighted.</p> <p>For example, there would be no point in having a CoC that advises to report violations to a \u201cconflict resolution team\u201d if the project itself is a two-person show. Project features like conflict resolution teams are only suitable for projects whose community has reached a certain size.</p> <p>Furthermore, make sure that all aspects of the COC are actually actionable. For example, when the COC talks about a \"point of contact\" (POC) to reach out to, it is important to make sure that the means of contact are explicitly mentioned, and that the POC can reliably be reached this way.</p> <p>So, don\u2019t pick just any CoC without thinking. Pick a CoC that works for your project.</p>"},{"location":"howto/add-new-slim/","title":"Creating a new Slim (under construction)","text":""},{"location":"howto/add-new-slim/#adding-a-new-subset-also-known-as-a-slim","title":"Adding a new subset (also known as a \"slim\")","text":"<p>See Daily Curator Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions.</p> <ol> <li> <p>In the main Prot\u00e9g\u00e9 window, click on the \"Entities\" tab. Below that, click the \"Annotation properties\" tab.</p> </li> <li> <p>Select the <code>subset_property</code> annotation property.</p> </li> <li> <p>Click on the \"Add sub property\" button.</p> </li> <li> <p>In the pop-up window, add the name of the new slim. The IRI will automatically populate according to settings in the user's \"New entities\" settings. Click OK.</p> </li> <li> <p>With the newly created annotation property selected, click on \"Refactor &gt; Rename entity...\" in the menu.</p> </li> <li> <p>In the pop-up window, select the \"Show full IRI\" checkbox. The IRI will appear.  Edit the IRI to fit the following standard:</p> </li> </ol> <p>http://purl.obolibrary.org/obo/{ontology_abbreviation}#{label_of_subset}</p> <p>For example, in CL, the original IRI will appear as:</p> <p>http://purl.obolibrary.org/obo/CL_1234567</p> <p>If the subset was labeled \"kidney_slim\", the IRI should be updated to:</p> <p>http://purl.obolibrary.org/obo/cl#kidney_slim</p> <ol> <li> <p>In the 'Annotations\" window, click the <code>+</code> next to \"Annotations\".</p> </li> <li> <p>In the pop-up window, select the <code>rdfs:comment</code> annotation property. Under \"Value\" enter a brief descripton for the slim. Under \"Datatype\" select <code>xsd:string</code>. Click OK.</p> </li> </ol> <p>See Daily Curator Workflow section for commit, push and merge instructions.</p>"},{"location":"howto/add-new-slim/#adding-a-class-term-to-a-subset-slim","title":"Adding a class (term) to a subset (slim)","text":"<p>See Daily Curator Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions.</p> <ol> <li> <p>In the main Prot\u00e9g\u00e9 window, click on the \"Entities\" tab. Select the class that is to be added to a subset (slim).</p> </li> <li> <p>In the 'Annotations\" window, click the <code>+</code> next to \"Annotations\".</p> </li> <li> <p>In the pop-up window, select the <code>in_subset</code> annotation property.</p> </li> <li> <p>Click on the \u2018Entity IRI\u2019 tab.</p> </li> <li> <p>Search for the slim label under \"Entity IRI\". In the pop-up that appears, double-click on the desired slim. Ensure that a sub property of <code>subset_property</code> is selected. Click OK.</p> </li> </ol> <p>See Daily Curator Workflow section for commit, push and merge instructions.</p>"},{"location":"howto/add-taxon-restrictions/","title":"Adding taxon restrictions","text":"<p>Before adding taxon restrictions, please review the types of taxon restrictions documentation.</p> <p>See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions.</p> <ol> <li><code>in taxon</code> relations are added as <code>Subclasses</code>.</li> <li>Navigate to the term for which you want to add the only in taxon restriction.</li> <li>In the Description window click on the <code>+</code>.</li> <li>In the pop-up window type a new relationship (e.g. <code>'in taxon' some Viridiplantae</code>).</li> <li>The taxa available are imported ontology terms and can be browsed just like any other ontology term.</li> </ol> <p></p> <ol> <li><code>never in taxon</code> or <code>present in taxon</code> relations added as <code>Annotations</code>.</li> <li>Navigate to the taxon term you want to add a restriction on.</li> <li>Copy the IRI (you can use command U to display this on a Mac)</li> <li>Navigate to the term for which you want to add the never in taxon restriction.</li> <li>In the class annotations window, click on the <code>+</code>.</li> <li>In the left-hand panel, select <code>never_in_taxon</code> or <code>present_in_taxon</code> as appropriate.</li> <li>In the right-hand panel, in the IRI editor panel, paste in the IRI.</li> <li>Click OK to save your changes.</li> <li>(Note - you can achieve the same thing using the Entity IRI tab + navigating to the correct taxon but this is slow and not very practical)</li> </ol> <p></p> <p>See Daily Workflow section for commit, push and merge instructions.</p>"},{"location":"howto/are-two-entities-the-same/","title":"Are these two entities the same? A guide.","text":""},{"location":"howto/are-two-entities-the-same/#are-these-two-entities-the-same-a-guide","title":"Are these two entities the same? A guide.","text":"<p>Disclaimer: Some of the text in this guide has been generated or refined with the help of ChatGPT (GPT 4).</p> <p>Summary: Entity mapping is the process of identifying correspondences between entities across semantic spaces. A \u201csemantic space\u201d in this context can be anything from an ontology, terminology, database or controlled vocabulary to enumerations in a data model. Entities are strings that identify/represent a real-world concept or instance in that space. Many such entities refer to the exact same, or similar, real world concept or instance 1,2. To integrate data from disparate semantic spaces, we need to develop maps that connect entities. The Simple Standard for Sharing Ontological Mappings (SSSOM) has been developed to support that process.</p> <p>Most semantic spaces (such as scientific databases or clinical terminologies) do not commit to any formal semantics (such as, say, an OWL Ontology). This makes the curation of mappings a shaky, ambiguous afair. Concepts with the same labels can refer two different real world concepts. Concepts with entirely different labels and taxonomic context can refer to the same real world concept. Here, we explore conceptually how to think of same-ness, leading to a practical protocol for mapping authors and reviewers.</p>"},{"location":"howto/are-two-entities-the-same/#preliminary-reading","title":"Preliminary reading","text":"<ul> <li>A basic tutorial for the creation of SSSOM mapping files.</li> <li>A guide for how to think of mapping predicates (exactMatch, broadMatch).</li> </ul>"},{"location":"howto/are-two-entities-the-same/#a-practical-step-by-step-protocol-for-determining-a-mapping","title":"A practical step-by-step protocol for determining a mapping","text":"<p>The following steps are designed to give you a basic framework for designing your own, use-case specific, mapping protocol. The basic steps are:</p> <ol> <li>Understand the underlying conceptual models of the semantic spaces</li> <li>Document the use case for the mapping</li> <li>Document the basic curation rules for the mapping</li> <li>Gathering evidence for the mapping</li> </ol> <p></p>"},{"location":"howto/are-two-entities-the-same/#understand-the-underlying-conceptual-models-of-the-semantic-spaces","title":"Understand the underlying conceptual models of the semantic spaces","text":"<p>Before we can even start with the mapping process, we need to establish what the source is all about, and what the conceptual model underpins it. This may or may not be easy - but this step should not be ommitted.</p>"},{"location":"howto/are-two-entities-the-same/#checklist-for-determining-the-conceptual-model-of-a-semantic-space","title":"Checklist for determining the conceptual model of a semantic space","text":"<ol> <li>What is the domain of the semantic space? (e.g. Clinical, Medical Research, Environment)</li> <li>What is the organising semantic framework? Is it an OWL ontology (set semantics)? A SKOS taxonomy (broader/narrow categories)? A database without a specific semantic framework underpinning it?</li> <li>What are the primary organising relationships (POR)? Primary organising relationships connect terms within the semantic spaces, like diseases to diseases, chemicals to chemicals etc, usually into some form of a graph. For an ontology, this could be for example rdfs:subClassOf, or BFO:0000050 (part of), like in the case of Uberon. For a taxonomy, the primary organising relationship could be skos:broader / skos:narrower. For a database, there could be no PORs at all, or relationships like \"isomer of\", \"functional group of\" etc, which are usually columns in a database table. In some cases, the PORs serve a function more than they reflect a representation of knowledge. For example, hierarchical relationships in ICD10 have to be mono-hierarchical (single parents) to serve billing and statistical purposes, which means that a lot of biological ambiguities will not be reflected at all (missing bioligically meaningful hierarchical relationships).</li> <li>What are the primary categories represented by the semantic space, e.g. one or more of diseases, phenotypes, genes, chemical entities, biological processes?</li> <li>What is the conceptual model underpinning the primary categories? This is the hardest part, because we very rarely have a document or schema (ontology!) that explicitly defines the conceptual model. For example, one ontology could implement a biomedical conceptual model of disease, which views diseases as primarily resulting from biological factors. It posits that diseases are caused by specific, identifiable agents like viruses, bacteria, genetic mutations, or physiological imbalances. Or an ontology could implement a biopsychosocial model of disease which views disease as a result of a complex interplay between biological, psychological, and social factors. Chemical entities can be defined and organised using a Structural Hierarchical Model (hierarchically organised from  atomic to molecular to macromolecular levels), or a Behavioral/Functional Model: (hierarchically organised based on their behavior or function).</li> </ol>"},{"location":"howto/are-two-entities-the-same/#example-for-determining-the-conceptual-model-of-a-semantic-space","title":"Example for determining the conceptual model of a semantic space","text":"<p>In our example, we are mapping ICD10CM to Mondo.</p> <ol> <li>domain:<ul> <li>MONDO: MONDO is an ontology focused on modelling diseases, with a special focus on rare diseases.</li> <li>ICD10CM: ICD-10-CM is a comprehensive system that goes beyond just coding diseases. In addition to diseases, ICD-10-CM includes codes for a variety of other features, such as Symptoms, Signs, and Abnormal Clinical and Laboratory Findings, Injuries and External Causes and Factors Influencing Health Status and Contact with Health Services.</li> </ul> </li> <li>organising semantic framework:<ul> <li>MONDO: MONDO is an OWL ontology.</li> <li>ICD10CM: ICD10 can safely be classified as a taxonomy, but does not commit to a specfic standard. The \"primary structure\" of ICD-10-CM, which refers to its hierarchical organization of codes from broad categories to specific diseases or conditions, can largely be expressed as a SKOS vocabulary. The basic hierarchical relationships (broader/narrower) align well with SKOS's skos:broader and skos:narrower properties. Likewise, preferred labels for terms can be represented using skos:prefLabel, while alternative labels or synonyms can be captured using skos:altLabel. However, while the main hierarchical structure of ICD-10-CM can be represented in SKOS, there are nuances and additional information in ICD-10-CM that might be challenging to represent losslessly in a simple SKOS model, such as Inclusion and Exclusion Notes and Complex (non-hierarchical) Relationships.</li> </ul> </li> <li>primary organising relationships:<ul> <li>MONDO: rdfs:subClassOf</li> <li>ICD10CM: can be expressed as skos:narrower, skos:broader, see considerations above (\"organising semantic framework\").</li> </ul> </li> <li>primary categories:<ul> <li>MONDO: diseases, disease characteristics, injuries, susceptibilities</li> <li>ICD10CM: diseases, signs and symptoms, injuries, pregancy related codes, and more (we only really care about diseases here)</li> </ul> </li> <li>conceptual model: A disease model captures the essential, defining features of diseases, including etiology and phenotypic presentation. As these are rarely explicitly defined, the disease model underlying a resource must often be determined through elucidation, term analysis and literature review.<ul> <li>MONDO: A disease is a disposition to undergo pathological processes that exists in an organism because of one or more disorders in that organism.</li> <li>ICD10CM: The International Classification of Diseases (ICD) serves as a foundational tool for health statistics, clinical care, and research, but it doesn't provide a singular, concise definition of \"disease\" in its introduction or general guidelines. Instead, the ICD system broadly encompasses a range of health-related conditions, and its structure reflects various ways in which the human body and mind can deviate from a perceived norm or experience dysfunction.</li> </ul> </li> </ol> <p>A small part of a disease model could look like this:</p> <p></p> <p>While this is vastly incomplete, you can gain certain important pieces of information: </p> <ol> <li>susceptibilities, injuries and diseases are mutually disjoint (different branches). So a disease model that subsumes an \"injury\" or a \"susceptibility to disease\" under the concept of \"disease\" would, technically speaking, be incompatible with that disease model.</li> <li>A \"disease grouping\" is considered a \"disease\" - many (if not most) conceptual models clearly separate the two.</li> <li>\"hereditary diseases by site\" are classified both under \"grouping by site\" and \"hereditary disease\". This means the conceptual model is polyhierarchical. Many disease models do not permit polyhierarchies in their \"primary organising relationships\".</li> </ol> <p></p>"},{"location":"howto/are-two-entities-the-same/#document-the-use-case-of-the-mapping","title":"Document the use case of the mapping","text":"<p>Before proceeding, you should document how you want to use the mapping. The mapping use case will determine certain factors like:</p> <ul> <li>which mapping predicates to consider,</li> <li>whether or not to \"conflate\",</li> <li>how far \"uphill\" or \"broad\" to map and</li> <li>which level of evidence you consider acceptable.</li> </ul> <p>Note that one goal of SSSOM is to increase our ability for \"cross-purpose reuse\" of mappings, which means that no matter what the use case, the mappings should never be \"wrong\" - but \"conflation\", which we will hear more about in a bit, is a natural part of mapping which will always cause mappings to be to some degree imprecise (imperfect).</p>"},{"location":"howto/are-two-entities-the-same/#example-use-case","title":"Example use case","text":"<p>In our example, we are mapping ICD10CM to Mondo for the purpose of data integration, in particular knowledge graph merging.</p> <p>One of the features of this use case is that we wish the target KG to have a specific feature: for every disease in the data, we want one, and only one, node in the graph. Therefore, carefully curated exact matches are our primary focus. All nodes should be represented by MONDO ids. On the flipside, some diseases in ICD 10 are too granular so we wish to map them to the next best disease in Mondo as a broad match. We have no analytic use for narrow and close matches because we cannot clearly deal with them in the knowledge graph, so we do not curate them.</p> <p></p> <p>Closely related to this knowledge graph integration use case is the need for counting. In order to be able to count precisely across resources, we need precise mappings to not overcount. For example, given a set of resources that represent more than 10,000 rare diseases, only 333 were represented by all:</p> <p></p> <p></p>"},{"location":"howto/are-two-entities-the-same/#document-the-basic-curation-rules-for-the-mapping","title":"Document the basic curation rules for the mapping","text":"<p>Once you have determined the conceptual models of the subject and object source (or a good approximation of it), you will have to lay some ground (curation) rules of the mappings. These ground rules are going to be dictated by your target use case.</p>"},{"location":"howto/are-two-entities-the-same/#checklist-for-defining-the-basic-curation-rules-for-the-mapping","title":"Checklist for defining the basic curation rules for the mapping","text":"<ol> <li>Document the primary categories you seek to map to each other.</li> <li>If the primary categories are different across subject and object source, e.g. you want to map genes in one resources to protein-products of genes in another, you document a conflation decision. \"Conflation\" is the conscious act of pretending two things are the same even if they commit to a different conceptual model (see checklist above)</li> <li>If the primary categories are the same across subject and object source, determine if the conceptual model is the same. If it is, fine, if not, document, again, if you wish to \"conflate\" these for the sake of this mapping (document in this sense means: write it down and add it to the mapping set description or documentation later). For example, if you map (1) analogous entities across different species (e.g. human diseases to animal diseases), or (2) chemical entities from a structural hierarchical model to ones defined in terms of function, or (3) diseases from a phenomenologically driven perspective to ones that are defined from a etiological perspective, you write it down. (Note that for cross-species mappings, you do not necessarily have to conflate, as we developed a specialised cross-species mapping vocabulary).</li> <li>Determine the mapping predicates. If you decided to conflate, pick the set of standard mapping predicates you wish to curate. Usually, this should be mapping relations from the SKOS vocabulary. If you decided not to conflate, pick the appropriate mapping relationship from another vocabulary, such as SEMAPV, or even the Relation Ontology.</li> <li>Document the minimal levels of evidence required for each mapping. This step is discussed in detail, separately, in the next section.</li> </ol>"},{"location":"howto/are-two-entities-the-same/#example-for-documenting-the-basic-curation-rules-for-the-mapping","title":"Example for documenting the basic curation rules for the mapping","text":"<ol> <li>Which primary categories? In this case we only want to map diseases in ICD10CM to diseases in Mondo, and ignore everything else. However, we recognise that occassionally, a term classified in ICD10CM as a \"sign or symptom\" may appear in Mondo as a disease, so we keep our minds open about that possibility.</li> <li>Conflation accross different categories? We are open to the possibility (based on our experience with disease mappings) that some conditions are represented in ICD 10 as signs or symptoms, while Mondo represents them as diseases. This is a murky area of disease data integration, so we choose to conflate diseases (disorders), sign and symptoms (clinical findings). SNOMED views, for example, disorders as subclasses for findings that are always abnormal. To support our use case (integrating all disease knowledge in a knowledge graph), we decide it is better to conflate the two. Not conflating would mean missing out on a potentially huge number of interesting associations in the data.</li> <li>Conflation within the same primary category. ICD 10 does not explicitly commit to a specific definition or model of disease, and even through literature review it is hard to pin down what they mean by a disease (e.g. phenomenological vs etiological viewpoints). We accept that ambiguity and explicitly acknowledge that we conflate the disease models of ICD10 and Mondo for the sake of this mapping (same arguments as above).</li> <li>Mapping predicates. Due to our conflation decisions we determine that the SKOS vocabulary is appropriate to represent our mapping predicates. (Note: if we had decided not to conflated, it would not have automatically meant that we could not map at all: we would, however, have to resort to much less useful mapping relations like skos:relatedMatch and skos:closeMatch, which are considerably harder, but not impossible, to use by data scientists. These mapping predicates are entirely unsuited for our use case, as the primary goal is \"providing one node in the graph per medical condition\")</li> <li>Minimal levels of evidence? We decide that for our purposes, noise is acceptable and we trust the same label = same disease assumption (while this, as we will see later is not always quite right). If the the label is not the same, we also trust a combination of a synonym exact match combined with a threshold on lexical similarity of 90% (levenshtein). If neither can be achieved, we require either a combination of proxy mapping with a non-domain-expert review (biocurator) or a full expert review (see next section).</li> </ol>"},{"location":"howto/are-two-entities-the-same/#conflation-on-primary-category-phenotypic-abnormality-vs-clinical-finding","title":"Conflation on primary category, phenotypic abnormality vs clinical finding","text":"<p>SNOMED cleanly separates between \"clinical findings\" and \"disorder\". While the SNOMED defines findings as \"normal or abnormal observations, judgments, or assessments of patients\", and disorders as \"always and necessarily an abnormal clinical state\". Strictly speaking, the presence of a finding term like SCTID:300444006 (Large kidney (finding)) does not imply any kind of level of abnormality, while all the corresponding term in HPO, HP:0000105 (Enlarged kidney) does. Now this is clearly a consequence of the weird way \"abnormal\" is defined in the world. Sometimes it is intended to mean \"outside the normal range\", and sometimes it is taken to mean \"deviating from the mean\". These are clearly different. None-the-less, the fact SNOMED does not imply \"abnormality\" means that we are conflating when we map the two.</p>"},{"location":"howto/are-two-entities-the-same/#the-difficulty-of-deciding-what-level-of-confidence-is-enough","title":"The difficulty of deciding what level of confidence is \"enough\"","text":"<p>MONDO:0000022 (Nocturnal Enuresis) is currently defined as \"urination during sleep\", and classified under psychiatric disorders.</p> <p>ICD10CM:N39.44 (Nocturnal enuresis) is classified as an urological disease (organic, rather than psychiatric, disorder), where a urinary incontinence not due to a substance or known physiological condition is explicitly excluded.</p> <p>Regardless of whether we believe that Mondo is misclassifying the disease (it should also be a urinary disease), either we are interpreting here the exact same disease/syndrome differently between Mondo and ICD10 (assigning different etiologies), or two etiologically different diseases have been assigned the exact same name.</p> <p>Again, we have a few options here. (1) we decide we dont care about the difference. A rough mapping seems to be good enough, and most our applications (data aggregation, analysis) wont care if both concepts are merged into the same. If this is generally the case for diseases with the exact same or very similary names, we just decide that the confidence given to us by \"same name\" is 99 or even 100%. (2) we decide they are different. In this case, we must have every single mapping reviewed by a clinical specialist, unless we have access to all properties of the conceptual model (full etiology, phenotypic profile, etc).</p> <p></p>"},{"location":"howto/are-two-entities-the-same/#gathering-evidence-for-a-mapping","title":"Gathering evidence for a mapping","text":"<p>Only after you understood the conceptual model underlying the subject and object sources you seek to map, and defining the basic curation rules for the mapping, are you ready to gather evidence for and against the mapping. The goal of evidence gathering is to increase confidence in a mapping. A single piece of evidence is called a justification. The confidence gained by multiple justifications can add up or be mutually exclusive. For example: a lexical match and match on a shared mapping are cumulative pieces of evidence. The confidence provided by multiple manual curators does not add up (usually the maximum or mean confidence is used). Every mapping project defines its own confidence levels.</p> <p></p> <p>Remember: You can never determine the correctness of a mapping. This is a direct consequence of our inability to assign a semantic space with an explicit, fully defined semantic model. You can only gather evidence for or against a mapping under the premises defined by your curation rules, and then, depending on your particular use case, decide which level of evidence is sufficient. \"Correctness\" in this context means \"under the curation rules defined for the mapping (previous section), the subject and object of the mappings relate to the same \"conceptual entity\" (disease, chemical entity) in the way specified by the mapping predicate\" (e.g. we can use skos:exactMatch if both subject and entity correspond exactly to the same concept of \"atom\" under the curation rules we defined).</p> <p></p>"},{"location":"howto/are-two-entities-the-same/#checklist-levels-of-evidence","title":"Checklist: Levels of Evidence","text":"<p>This checklist assumes a specific mapping candidate <code>{s,p,o,c}</code>, with <code>s</code> the subject, <code>p</code> the mapping predicate, <code>o</code> the object and <code>c</code> the confidence, initially 0, as a starting point. The goal of the checklist is to increase the confidence of the mapping to an acceptable degree. The required level of confidence should be set by the mapping authors.</p> <p>Instead of giving some arbitrary numbers for confidence, we just distinguish between LOW, MODERATE and HIGH confidence. Average confidence gained is a rough metric, that means: \"based on our experience, this justification leads to a HIGH, MODERATE, or LOW increase in confidence. A HIGH level of confidence could be, depending on the context, something between 0.8 and 1.0.</p> <ol> <li>We start with a confidence of 0.</li> <li>Sameness of primary label<ul> <li>Rationale: the primary label of an entity is largely unique. If <code>s</code> and <code>o</code> belong to the same primary category (e.g. gene, disease, chemical entity), the risk of Homonomy is low.</li> <li>Level of confidence gained on average: HIGH</li> <li>Preprocessing: Preprocessing may or may not affect the level of confidence.</li> <li>Cost: VERY LOW (fast, standard algorithms)</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Sameness of synonym<ul> <li>Rationale: synonyms are less trustworthy than primary labels for driving mapping decision, but if the precision (exact, broad, narrow) is known, are still valuable indicators.</li> <li>Level of confidence gained on average: MEDIUM to HIGH (this depends on wether the source carefully distinguishes exact from non-exact synonyms)</li> <li>Preprocessing: Preprocessing may or may not affect the level of confidence.</li> <li>Cost: VERY LOW (fast, standard algorithms)</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Proxy mapping (<code>s</code> and <code>o</code> share a mapping to the same third entity <code>x</code>)<ul> <li>Rationale: if the subject and object sources have already been mapped by a trustworthy mapping provider to the same third resource, we can usually avoid redoing the mapping.</li> <li>Level of confidence gained on average: MEDIUM to HIGH (this depends on how much trust the mapping author has in the sources of the mappings)</li> <li>Cost: VERY LOW (fast, standard algorithms)</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Similarity of primary label: using a number of measures determine if the two primary labels are lexically similar, for example by having a low edit distance, or one label is contained in the other (broad/narrow match).<ul> <li>Level of confidence gained on average: LOW (depends on the degree of similarity)</li> <li>Preprocessing: Preprocessing may or may not affect the level of confidence.</li> <li>Cost: VERY LOW (fast, standard algorithms)</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Similarity of synonym<ul> <li>Level of confidence gained on average: LOW (this depends on wether the source carefully distinguishes exact from non-exact synonyms, and on the degree of the similarity)</li> <li>Preprocessing: Preprocessing may or may not affect the level of confidence.</li> <li>Cost: VERY LOW (fast, standard algorithms)</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Hierarchical comparison of the primary organising relationship structure:<ul> <li>Method:<ul> <li>Are <code>s</code> and <code>o</code> the same kind* of entity? (High level branch analysis)</li> <li>Do <code>s</code> and <code>o</code> have the same/similar set of ancestors? (requires all ancestors to be mapped)</li> <li>Do <code>s</code> and <code>o</code> have the same/similar set of decendants? (requires all descendants to be mapped)</li> <li>Advanced: <code>s</code> and <code>o</code> are linked to similar feature sets (phenotypes, average weight, number of carbon atoms, etc). Shared references to the same scientific publications could count here as well.</li> </ul> </li> <li>Level of confidence gained on average: MEDIUM</li> <li>Note: the primary organising relationship structures of two semantic spaces rarely exactly correspond to each other - a failing hierarchical comparison is not a strong evidence against a mapping.</li> <li>Cost: MEDIUM to HIGH (this check can be automated to some degree, but due to the cost of needing ancestors mapped already, is probably executed better by a human)</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Domain expert review of definitions (comparing only the textual definitions provided by both sources and determining whether they refer to the same concept under the mapping curation rules defined). Note that this is only possible if both resources in question provide \"formal definitions\" - definitions that fully define the concept, rather than merely \"describe\" it.<ul> <li>Level of confidence gained on overage: MEDIUM to HIGH (this depends on the domain expertise and the semantic engineering expertise of the mapping author)</li> <li>Preprocessing: Avoid (could affect interpretation).</li> <li>Cost: HIGH (Expert curation is the highest level of cost and should only be used if no other justification or combination of justifications can be employed to attain the target level of confidence).</li> <li>Is the acquired confidence sufficient at this stage? If <code>yes</code>, move to the next mapping canidate. If <code>no</code>, move on.</li> </ul> </li> <li>Full domain expert review (expert aggregates evidence from within the resource (hierarchial structures, logical axioms, definitions, etc) and outside to determine if the subject and the object entity refer to the same concept). This may involve reviewing papers, links to websites, databases with additional information about the entities, and other sources.<ul> <li>Level of confidence gained on overage: MEDIUM to HIGH (this depends on the domain expertise and the semantic engineering expertise of the mapping author)</li> <li>Preprocessing: Avoid (could affect interpretation).</li> <li>Cost: VERY HIGH (Expert curation is the highest level of cost and should only be used if no other justification or combination of justifications can be employed to attain the target level of confidence).</li> </ul> </li> </ol>"},{"location":"howto/are-two-entities-the-same/#examples-for-levels-of-evidence","title":"Examples for levels of evidence","text":"<ul> <li>To determine if two rare genetic diseases as defined by OMIM and Orphanet are same is difficult because we often do not have exactly the same name documented. To avoid the costly review of a rare disease expert (in the absence of proxy mappings), we can leverage disease to gene assocations to determine very similar diseases according to their genotypic profile.   We can furthermore bolster our evidence by comparing the phenotypic profiles. Techniques for doing so range from simple Jaccard (overlap of the associated phenotypic profile of both diseases) to complex methods such as phenodigm that take the ontology semantics into account, or even modern LLM embeddings.</li> </ul>"},{"location":"howto/are-two-entities-the-same/#summary","title":"Summary","text":"<p>You can never determine if two entities are truly the same. You can only collect evidence for and against a mapping under a specific set of curation rules. To do so, you first determine the conceptual model of the semantic spaces defining the entities you seek to map. Then you define, depending on your use case, your specific curation rules. These rules determine which mapping predicates you use to curate, and which level of evidence you require for asserting a specific entity mapping. While mappings are, therefore, inherently connected to a use case, the goal of SSSOM and other mapping standardisation efforts is to make mappings usable across use cases.</p>"},{"location":"howto/are-two-entities-the-same/#out-of-context-a-reminder-of-how-mapping-predicates-work","title":"Out of context: a reminder of how mapping predicates work","text":""},{"location":"howto/change-files-pull-request/","title":"How to change files in an existing pull request","text":""},{"location":"howto/change-files-pull-request/#using-github","title":"Using GitHub","text":"<p>Warning: You should only use this method if the files you are editing are reasonably small (less than 1 MB).</p> <p>This method only works if the file you want to edit has already been editing as part of the pull request.</p> <ul> <li>Go to the pull request on GitHub, and click on the \"Files Changed\" tab up top</li> <li>Find the file you want to edit in the pull request.</li> <li>On the right, click on on the three <code>...</code>, and then \"Edit file\".</li> </ul> <p></p> <p>If this option is greyed out, it means that - you don't have edit rights on the repository - the edit was made from a different fork, and the person that created the pull request did not activate the \"Allow maintainers to make edits\" option when submitting the PR - the pull request has already been merged</p> <ul> <li>Do the edits, and then commit changes, usually to the same branch   </li> </ul>"},{"location":"howto/change-files-pull-request/#using-github-desktop","title":"Using GitHub Desktop","text":"<ol> <li>On the pull request in GitHub, click the copy button next to the branch name (see example below)</li> </ol> <ol> <li> <p>In GitHub Desktop, click the branch switcher button and paste in branch name (or you can type it in). </p> </li> <li> <p>Now you are on the branch, you can open the files to be edited and make your intended changes and push via the usual workflow.</p> </li> </ol>"},{"location":"howto/change-files-pull-request/#if-the-branch-is-on-a-fork","title":"If the branch is on a fork","text":"<ol> <li> <p>If a user forked the repository and created a branch, you can find that branch by going to the branch switcher button in GitHub Desktop, click on Pull Requests (next to Branches) and looking for that pull request </p> </li> <li> <p>Select that pull request and edit the appropriate files as needed and push via the usual workflow.</p> </li> </ol>"},{"location":"howto/clone-mondo-repo/","title":"Clone a repository","text":""},{"location":"howto/clone-mondo-repo/#cloning-a-repo","title":"Cloning a repo","text":"<p>Prerequisite: Install Github Desktop Github Desktop can be downloaded here</p> <p>For the purpose of going through this how-to guide, we will use Mondo as an example. However, all obo onotlogies can be cloned in a similar way.</p> <ol> <li>Open the GitHub repository where the ontology you want to clone lives, in this case, Mondo GitHub repository</li> <li>Click Code</li> </ol> <p></p> <ol> <li>Click 'Open with GitHub Desktop'</li> </ol> <p></p> <ol> <li>You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos.</li> <li>This will open GitHub Desktop and the repo should start downloading. This could take some time depending on the size of the repository.</li> </ol>"},{"location":"howto/clone-mondo-repo/#open-the-ontology-in-protege","title":"Open the Ontology in Protege","text":"<ol> <li>Open Protege</li> <li>Go to: File -&gt; Open</li> <li>Navigate to mondo/src/ontology/mondo-edit.obo and open this file in Protege    Note: <code>mondo</code> can be replaced with any ontology that is setup using the ODK as their architecture should be the same.</li> </ol> <p>If this all works okay, you are all set to start editing!</p>"},{"location":"howto/create-new-term/","title":"Creating a New Ontology Term in Protege","text":"<ol> <li> <p>To create a new term, the 'Asserted view' must be active (not the 'Inferred view').</p> </li> <li> <p>In the Class hierarchy window, click on the 'Add subclass' button at the upper left of the window.</p> </li> </ol> <p></p> <ol> <li>A pop-up window will appear asking you to enter the Name of the new term. When you enter the term name, you will see your ID automatically populate the IRI box. Once you have entered the term, click 'OK' to save the new term. You will see it appear in the class hierarchy.</li> </ol> <p></p> <ol> <li>In the annotation window add:</li> <li>Definition<ol> <li>Click on the <code>+</code> next to Annotations</li> <li>Select defintion (if there are multiple, you should use IAO:0000115)</li> <li>Add the textual definition in the pop-up box.</li> <li>Click OK.</li> </ol> </li> </ol> <p></p> <pre><code> 2. Add Definition References\n    1. Click on the circle with the \u2018@\u2019 in it next to definition and in the resulting pop-up click on the ```+``` to add a new ref, making sure they are properly formatted with a database abbreviation followed by a colon, followed by the text string or ID. Examples: ```PMID:27450630```.\n    2. Click OK.\n    3. Add each definition reference separately by clicking on the ```+``` sign.\n</code></pre> <p> 3. Add synonyms and dbxrefs following the same procedure if they are required for the term.</p> <ol> <li>Add appropriate logical axioms in the Description by clicking the <code>+</code> sign in the appropriate section (usually SubClass Of) and typing it in, using <code>Tab</code> to autocomplete terms.</li> </ol> <p></p> <pre><code>Converting to Equivalent To axioms:\nIf you want to convert your SubClassOf axioms to EquivalentTo axioms, you can select the appropriate rows and right click, selecting \"Convert selected rows to defined class\"\n</code></pre> <ol> <li> <p>In some cases, logical axioms reuiqre external ontologies (eg in the above example, the newly added CL term has_soma_location in the cerebellar cortex which is an uberon term), it might be necessary to import the term in. For instructions on how to do this, please see the import managment section of your local ontology documentation (an example of that in CL can be found here: https://obophenotype.github.io/cell-ontology/odk-workflows/UpdateImports/)</p> </li> <li> <p>When you have finished adding the term, run the reasoner to ensure that nothing is problematic with the axioms you have added (if there is an issue, you will see it being asserted under owl:Nothing)</p> </li> <li> <p>Save the file on protege and review the changes you have made in your Github Desktop (or use <code>git diff</code> in your terminal if you do not use Github Desktop)</p> </li> <li> <p>See Daily Workflow section for commit, push and merge instructions.</p> </li> </ol>"},{"location":"howto/create-ontology-from-scratch/","title":"How to create an OBO ontology from scratch","text":"<p>Editors:</p> <ul> <li>Nicolas Matentzoglu (@matentzn)</li> <li>Sabrina Toro (@sabrinatoro)</li> </ul> <p>Summary:</p> <p>This is a guide to build an OBO ontology from scratch. We will focus on the kind of thought processes you want to go through, and providing the following:</p> <ul> <li>Reasons for NOT building an ontology</li> <li>A basic recipe for getting started</li> <li>An overview of different starting points on your journey to building an ontology</li> <li>A guide for deciding what kind of ontology you want to build</li> <li>An example walk-through of the process.</li> </ul> <p></p>"},{"location":"howto/create-ontology-from-scratch/#minimal-conditions-for-building-an-ontology","title":"Minimal conditions for building an ontology","text":"<p>Before reading on, there are three simple rules for when NOT to build an ontology everyone interested in ontologies should master, like a mantra:</p> <p>Do not build a new ontology if:</p> <ol> <li>one in scope already exists (none-in-scope condition).</li> <li>something simpler than a full-fledged OWL ontology can do the job (something-simpler-works condition).</li> <li>there is not at least one glass-clear use case written down which could be addressed by the existence of the ontology (killer-use-case condition).</li> </ol> <p></p>"},{"location":"howto/create-ontology-from-scratch/#none-in-scope-condition","title":"None-in-scope condition","text":"<p>Scope is one of the hardest and most debated subjects in the OBO Foundry operation calls. There are essentially two aspects to scope:</p> <ol> <li>The entities you intended to model belong to some specific biological categories. For example <code>phenotype</code>, <code>disease</code>, <code>anatomical entity</code>, <code>assay</code>, <code>environmental exposure</code>, <code>biological process</code>, <code>chemical entity</code>. Before setting out to build an ontology, you should get a rough sense of what kind of entities you need to describe your domain. However, this is an iterative process and more entities will be revealed later on.</li> <li>The subject domain you intend to model. For example, you may want to provide an ontology to describe the domain of <code>Alzheimer's Disease</code>, which will need many different kinds of biological entities (like <code>anatomical entity</code> and <code>disease</code> classes).</li> </ol> <p>As a rule of thumb, <code>you should NOT create a term if another OBO ontology has a branch of for entities of the same kind</code>. For example, if you have to add terms for assays, you should work with the Ontology for Biomedical Investigations to add these to their assay branch.</p> <p>Remember, the vision of OBO is to build a semantically coherent ontology for all of biology, and the individual ontologies in the OBO Foundry should be considered \"modules\" of this super ontology. You will find that while collaboration is always hard the only way for our community to be sustainable and compete with commercial solutions is to take that hard route and work together.</p>"},{"location":"howto/create-ontology-from-scratch/#something-simpler-works-condition","title":"Something-simpler-works condition","text":"<p>There are many kinds of semantic artefacts that can work for your use case:</p> <ol> <li>Controlled vocabularies: Creating identifiers for concepts in your domain and without too much concern for logical reasoning. Some examples can be are Linked Open Data Vocabularies (LOV) or schema.org vocabularies. Sometimes a table of identifiers in an SQL database is enough.</li> <li>Thesauri: Describe the synonyms used in your domain in a standardised fashion.</li> <li>Taxonomies: Create a hierarchical categorisations for concepts in your domain, without any specific regards for semantics. You just create a hierarchy that \"makes some sense\" for your use case. Examples: ICD10, United Nations Standard Products and Services Code (UNSPSC).</li> <li>Semantic data models: If you need to define how terms in your database should be constrained in a semantic way (similar to a database schema), then Shape languages like SHEX or SHACL may be much more suitable for your use case. See LinkML tutorials to get a sense of this: you will build a semantic data model in Yaml which can then be exported to SHACL, OWL or JSON Schema (great tutorial, useful to do no matter what).</li> <li>Ontologies: Sets of logical axioms. If you require formal reasoning (and only then!) does it make sense to jump in the deep pit of ontology engineering. This is, by far, the hardest to build of the bunch. You will have to wrestle with Logic, Open World Assumption and many more arcane subjects.</li> </ol> <p>Think of it in terms of cost. Building a simple vocabulary with minimal axiomatisation is 10x cheaper than building a full fledged domain model in OWL, and helps solving your use case just the same. Do not start building an ontology unless you have some understanding of these alternatives first.</p>"},{"location":"howto/create-ontology-from-scratch/#killer-use-case-condition","title":"Killer-use-case condition","text":"<p>Do not build an ontology because someone tells you to or because you \"think it might be useful\". Write out a proper use case description, for example in the form of an agile user story, convince two or three colleagues this is worthwhile and only then get to work. Many ontologies are created for very vague use cases, and not only do they cost you time to build, they also cost the rest of the community time - time it takes them to figure out that they do not want to use your ontology. Often, someone you trust tells you to build one and you believe they know what they are doing - do not do that. Question the use of building the ontology until you are convinced it is the right thing to do. If you do not care about reasoning (either for validation or for your application), do not build an ontology.</p> <p></p>"},{"location":"howto/create-ontology-from-scratch/#basic-recipe-to-start-building-an-ontology","title":"Basic recipe to start building an ontology","text":"<p>Depending on your specific starting points, the way you start will be slightly different, but some common principles apply.</p> <ol> <li>Write down the use cases for the ontology (see above). This will determine certain design decisions later on. These should be concrete, like: controlled vocabulary for named entity recognition, logical model of a domain, auto-classification of data.</li> <li>Make a table of all similar ontologies that exist, within and outside OBO (this requires research, and is an essential part of the process). Document exactly in what way they are different from your use case, and why you need to build a new one (see none-in-scope condition above).</li> <li>Determine whether you have something to start from. Often, you will have a database with entities you may wish to turn into classes in your ontology. See starting points below.</li> <li>Gather your tools. You need to think about tools for at least two kinds of workflows to start with:</li> <li>Curation workflows: How will you edit your ontology? Some simple ontologies are edited using tables that link to logical templates. Others are edited primarily with Protege.</li> <li>Continuous integration and release workflows: How will you import terms from other ontologies? How will you ensure the quality of you ontology moving forward?</li> <li>Decide on the Ontology ID (important, do not skip). Changing this later can be extremely costly. Refer to the OBO ID policy for details. An ID should be short and unique.</li> <li>Create a basic set-up for managing your workflows. This comprises (usually) three aspects (you may wish to try and use the Ontology Development Kit - it does exactly that):</li> <li>Make a GitHub repository.</li> <li>Add your editors files (owl, tsv, whatever you decided to use) to that repository.</li> <li>Implement some <code>workflow</code> system, i.e. some way to run commands like <code>release</code> or <code>test</code>, as you will run these repeatedly. A typical system to achieve this is make, and many projects choose to encode their workflows as <code>make</code> targets (ODK, OBI Makfile).</li> <li>Determine the metadata and logical patterns you wish to employ for your curation. Here it is important that you determine what kind of an ontology you want to build.</li> </ol> <p>Note: Later in the process, you also want to think about the following:</p> <ul> <li>Think about how to manage your ontology project: Which roles you need, and how you manage community contributions.</li> </ul> <p></p>"},{"location":"howto/create-ontology-from-scratch/#starting-points","title":"Starting points","text":"<p>There are many different starting points for building an ontology:</p> <ul> <li>We have a database or a dataset and want to build an ontology that covers entities in that database. As a variation, you have two or more databases that you need to integrate.</li> <li>We already have a basic ontology in our domain (a cell ontology, an anatomy ontology), but need to build an extension (e.g. a species specific extension to an existing cross-species ontology).</li> <li>We have controlled vocabulary or a list of standard or commonly used terms for a domain and want to formalise them in an ontology for interoperability and machine-readability, with versioning support to manage evolution. Sometimes, we may even wish to simply using ontology infrastructure (tools and best practices) to maintain a quite informal vocabulary structure.</li> <li>There are existing ontologies that are, however, not quite fit for purpose (even if they should be) and there's no way to make any of them right, so I have to create Yet Another Variant.</li> <li>We have a large, hierarchical enumeration in a datamodel that pulls terms from many ontologies.</li> <li>We need to build a completely new ontology for a domain that currently does not even have a controlled vocabulary. This case almost never happens nowadays. In this case, all domain knowledge (concepts and their relations) is somewhere in the heads of the experts.</li> </ul> <p></p>"},{"location":"howto/create-ontology-from-scratch/#what-kind-of-ontology-do-you-need","title":"What kind of ontology do you need?","text":"<p>There are two fundamentally different kinds of ontologies which need to be distinguished:</p> <ol> <li>Project ontologies (sometimes referred to as application ontologies) are ontologies that are developed to fulfil a specific use case, like:</li> <li>Grouping data in your project</li> <li>Indexing search engines or your organisation</li> <li>Informing Natural Language Processing applications</li> <li>Populating the biocuration interface your organisation provides to enable curators to annotate data</li> <li>Domain ontologies are ontologies which seek to model a domain of discourse. In particular they:</li> <li>Reflect scientific consensus and are therefore social and collaborative enterprises subject to change</li> <li>Are build with re-use in mind:<ul> <li>They re-use terms from other domain ontologies</li> <li>They provide terms intended for re-use by other ontologies</li> <li>They work with other ontologies on implementing consistent logical patterns that apply across all domain ontologies in the community.</li> </ul> </li> <li>Are logically consistent with all ontologies they depend on, refer to, import.</li> </ol> <p>Some things to consider:</p> <ul> <li>It is extremely hard to build domain ontologies. Do not try to do that without a proper sustainability plan (i.e. considerable resources over multiple years).</li> <li>Project ontologies are not bad domain ontologies. Project ontologies can be build according to the same standards as domain ontologies. While controversial, the OBO Foundry is currently (March 2022) debating whether project ontologies are admissible to the OBO ontology library.</li> <li>Project ontologies can have huge impact. One of the most impactful ontologies in the biomedical world is the Experimental Factor Ontology (EFO) - a massive project ontology used for many applications from knowledge graph integration to biocuration.</li> <li>Project ontologies are allowed to change the semantics of imported ontologies, for example by adding additional axioms or even removing some - anything necessary to achieve the use case!</li> <li>Domain ontologies (in the OBO world) are not allowed to change semantics of imported ontologies.</li> <li>Project ontologies can import terms from domain ontologies, and coin their own terms where necessary. This can be a good option if resources are scarce, and there is not enough time for consensus building with the community or the often lengthy contribution workflows. \"I just need some terms\" usually points to \"I need a project ontology\".</li> <li>Domain ontologies seek to model a domain exhaustively: any concept that \"belongs\" to that domain is a strong candidate for a term.</li> </ul> <p>It is imperative that it is clear which of the two you are building. Project ontologies sold as domain ontologies are a very common practice and they cause a lot of harm for open biomedical data integration.</p> <p></p>"},{"location":"howto/create-ontology-from-scratch/#example-building-vertebrate-breed-ontology","title":"Example: Building Vertebrate Breed Ontology","text":"<p>We will re-iterate some of the steps taken to develop the Vertebrate Breed Ontology. At the time of this writing, the VBO is still in early stages, but it nicely illustrates all the points above.</p>"},{"location":"howto/create-ontology-from-scratch/#use-case","title":"Use case","text":"<p>See here. Initial interactions with the OMIA team further determined more long term goals such as phenotypic similarity and reasoning.</p>"},{"location":"howto/create-ontology-from-scratch/#similar-ontologies","title":"Similar ontologies","text":"<p>Similar ontologies. While there is no ontology OBO ontology related to breeds, the Livestock Breed Ontology (LBO) served as an inspiration (much different scale). NCBI taxonomy is a more general ontology about existing taxa as they occur in the wild.</p>"},{"location":"howto/create-ontology-from-scratch/#starting-point","title":"Starting point","text":"<p>Our starting point was the raw OMIA data.</p> <ul> <li>We got a list of breeds from DAD-IS, which includes name of the breed, transboundary name, species, country, and more</li> <li>We first had to understand the data and how the different pieces of data relate to each other.</li> <li>Some breed names are the same, but refer to either different species and/or different countries</li> <li>Several breeds share a common \"transboundary name\", which represent the original breed from which they come from</li> <li>We needed to determine what a single concept / an indentifiable term would be</li> <li>In order to define a single breed, we needed to include the name of the breed, the transboundary name (when applicable), the species, and the country</li> <li>We needed to understand the metadata and how each concepts relate to each other</li> <li>'breed' is an instance of 'species', therefore 'species' should be the parent term of 'breeds' (using a is_a relation)</li> <li>when applicable, 'transboundary' should be the parent term of 'breeds'</li> <li>Note about the concept of \"species\": is \"species\" equivalent to the NCBI Taxon representing \"species\"?     Design decision: Since <code>species</code> represents the same concept as \u2018species\u2019 in NCBI, the ontology should be built \u2018on top of\u2019 NCBI terms to avoid confusion of concepts and to avoid conflation of terms with the same concept</li> </ul> <p>Warnings based on our experience:</p> <ul> <li>Always retain links to original source ids (encoding problems, update problems)</li> <li>Always add provenance to as much information as you can (where do labels come from?)</li> </ul>"},{"location":"howto/create-ontology-from-scratch/#gather-your-tools","title":"Gather your tools","text":"<p>For us this was using Google Sheets, ROBOT &amp; ODK.</p>"},{"location":"howto/create-ontology-from-scratch/#the-ontology-id","title":"The Ontology ID","text":"<p>At first, we chose to name the ontology \"Unified Breed Ontology\" (UBO). Which meant that for everything from ODK setup to creating identifiers for our terms, we used the <code>UBO</code> prefix. Later in the process, we decided to change the name to \"Vertebrate Breed Ontology\". Migrating all the terms and the ODK setup from <code>ubo</code> to <code>vbo</code> required some expert knowledge on the workings of the ODK, and created an unnecessary cost. We should have finalised the choice of name first.</p>"},{"location":"howto/create-ontology-from-scratch/#create-a-basic-set-up","title":"Create a basic set up","text":"<ol> <li>Making a Repo with ODK</li> <li>Develop a workflow that turns a Google Sheet into a component.</li> </ol>"},{"location":"howto/create-ontology-from-scratch/#determine-the-metadata-and-logical-patterns-you-wish-to-employ","title":"Determine the metadata and logical patterns you wish to employ.","text":"<ul> <li>We decided to build a domain ontology, for the representation of vertebrate breeds.</li> <li>As our initial data is relatively flat, we decided to use ROBOT templates and Google Sheets to manage them.</li> </ul>"},{"location":"howto/create-ontology-from-scratch/#some-notes-need-to-be-cleaned-up-ignore","title":"Some notes, need to be cleaned up (ignore)","text":"<ul> <li>Creation of components: for basic information: each \u201clayer\u201d is built in a google sheet for example:</li> <li>Transboundary: are children of species</li> <li>Breeds: are children of either species or transboundary (therefore we need transboundary and species in order to be able to add breeds)</li> <li>Addition of new information as we have them</li> <li>E.g. adding xref and synonym from OMIA</li> <li>Upcoming: xref and synonym form another database.</li> <li>Future: Continue to add to the original document and/or create new components</li> </ul>"},{"location":"howto/create-ontology-from-scratch/#acknowledgements","title":"Acknowledgements","text":"<p>Thank you to Melanie Courtot, Sierra Moxon, John Graybeal, Chris Stoeckert, Lars Vogt and Nomi Harris for their helpful comments on this how-to.</p>"},{"location":"howto/create-permid-with-w3id/","title":"How to create permanent identifiers for your ontology","text":"<p>Author: Tim Alamenciak</p> <p>Permanent identifiers are a core feature of ontologies. Each term should have its own unique URI that points to information about that term. In large domain ontologies, these identifiers are handled by OBO Foundry and resolve to a major ontology index service. However, when making project ontologies or other small ontologies not in the OBO ecosystem, you are on your own.</p> <p>Fortunately there's w3id.org - an open, secure, permanent redirect service. This service can give you a custom URI for your ontology and handle redirects to where your ontology lives. </p> <p>This tutorial will show you how to use w3id.org to create permanent URIs for your ontology. This tutorial does not cover how to add prefixes and URIs to your ontology - for that you should check out the tutorial How to get started with your own ODK-style repository.</p>"},{"location":"howto/create-permid-with-w3id/#step-1-pick-your-prefix","title":"Step 1: Pick your prefix","text":"<p>The convention in ontology world is to have a short prefix for your ontology. This is usually related to the domain of the ontology. For example, the environment ontology has the prefix ENVO. </p> <p>When selecting your prefix, you should ensure that it is not in use or similar to other ontologies. See the OBO Foundry ID Policy page for more best practices.</p> <p>In our example, we will look at how I set up URIs for the Ecolink Model Ontology (Prefix: ELMO). </p>"},{"location":"howto/create-permid-with-w3id/#step-2-create-a-folder-and-upload-your-htaccess-file","title":"Step 2: Create a folder and upload your .htaccess file","text":"<p>The records for w3id.org are managed using GitHub. Here is a link to the repository: https://github.com/perma-id/w3id.org</p> <p>You need to fork the repository, create a folder with two files and submit a pull request. This sounds like a complicated set of actions, but they will become second nature to you while working on ontologies. </p> <p>Here is the official GitHub documentation on how to Fork a repository and Creating a pull request from a fork</p> <p>The name of the folder you create will form the URI. For example, we created the <code>elmo</code> folder and our URI is https://w3id.org/elmo/. Think of the w3id.org repository as the root of the website.</p> <p>You need to create exactly two files in this folder: <code>.htaccess</code> and <code>README.md</code>. You can use your favourite text editor to create these two files. </p> <p>The folks at w3id.org have created some documentation and examples for these files which can be found at this link, but we will provide ontology-specific examples here.</p>"},{"location":"howto/create-permid-with-w3id/#readmemd","title":"README.md","text":"<p>This file must contain, at a minimum:  - The name of the project to which the permanent identifier applies; - A name, email and GitHub username responsible for the identifier; - A GitHub repo for the ontology.</p> <p>The syntax is straightforward markdown. Here is the exact file used for elmo:</p> <pre><code># Ecolink Model Ontology (elmo)\nThis is a permanent identifier for the Ecolink Model Ontology.\n\nThis space is administered by: Tim Alamenciak\ntim.alamenciak@gmail.com\nGitHub username: timalamenciak  \n\nPlease see the Github repo for more details: https://github.com/timalamenciak/elmo\n</code></pre>"},{"location":"howto/create-permid-with-w3id/#htaccess","title":".htaccess","text":"<p>This file is a little more complicated and may take some trial and error to get right. You have two options here:</p> <ol> <li>A generic redirect that always points to the latest version of the ontology:</li> </ol> <pre><code># Ecolink Model Ontology (elmo)\n# This is a project ontology for the Ecolink Model, which \n# describes ecological knowledge.\n# \n# https://github.com/timalamenciak/elmo\n# This space is administered by:\n#\n# Tim Alamenciak\n# tim.alamenciak@gmail.com\n# GitHub username: timalamenciak  \nOptions +FollowSymLinks\nRewriteEngine On\n\n# Handle /elmo without additional path (exact match)\nRewriteRule ^$ https://timalamenciak.github.io/elmo/elmo.html [R=301,L,QSA]\n</code></pre> <ol> <li>A more sophisticated redirect that can properly resolve term URIs - e.g. https://w3id.org/elmo/elmo.html#ELMO_3620021</li> </ol> <pre><code># Ecolink Model Ontology (elmo)\n# This is a project ontology for the Ecolink Model, which \n# describes ecological knowledge.\n# \n# https://github.com/timalamenciak/elmo\n# This space is administered by:\n#\n# Tim Alamenciak\n# tim.alamenciak@gmail.com\n# GitHub username: timalamenciak  \nOptions +FollowSymLinks\nRewriteEngine On\n\n# Handle /elmo with additional path segments\nRewriteRule ^(.+)$ https://timalamenciak.github.io/elmo/elmo.html#$1 [R=301,NE,L,QSA]\n\n# Handle /elmo without additional path (exact match)\nRewriteRule ^$ https://timalamenciak.github.io/elmo/elmo.html [R=301,L,QSA]\n</code></pre>"},{"location":"howto/create-permid-with-w3id/#breaking-down-htaccess","title":"Breaking down .htaccess","text":"<p>This file uses a fairly uncommon syntax and it can be difficult to get right. Here is a breakdown of our example.</p> <p>First, there is the frontmatter, which essentially restates what you wrote in the <code>README.md</code>:</p> <pre><code># Ecolink Model Ontology (elmo)\n# This is a project ontology for the Ecolink Model, which \n# describes ecological knowledge.\n# \n# https://github.com/timalamenciak/elmo\n# This space is administered by:\n#\n# Tim Alamenciak\n# tim.alamenciak@gmail.com\n# GitHub username: timalamenciak  \nOptions +FollowSymLinks\nRewriteEngine On\n</code></pre> <p>There are two lines here that configure the server so that redirects work.</p> <p>The rewrite rules are where the bulk of the work happens. Here's the simple one: <code>RewriteRule ^$ https://timalamenciak.github.io/elmo/elmo.html [R=301,L,QSA]</code></p> <p>This line contains a snippet of Regex (<code>^$</code>) that matches the base URI for your redirect. In this case, it will match requests to <code>https://w3id.org/elmo</code> and direct them to <code>https://timalamenciak.github.io/elmo/elmo.html</code>. The end of the line contains some essential flags to enable the redirect: <code>[R=301,L,QSA]</code>. These have specific meanings in the .htaccess file. If you are very curious, you can see the meanings in this StackOverflow post. All three are required.</p> <p>Including just this rule will always redirect your base URI to whatever you point it at. This means that even if someone clicks on a term URI (e.g. <code>https://w3id.org/elmo/elmo_3620021</code>), it redirects to the entire ontology file rather than to the specific term. That functionality requires an additional rewrite rule and some processing of your ontology.</p> <p>This is the second rewrite rule you need, which has to come first in your .htaccess file: <code>RewriteRule ^(.+)$ https://timalamenciak.github.io/elmo/elmo.html#$1 [R=301,NE,L,QSA]</code></p> <p>There are two differences with this file. The first is the matching Regex which now looks like this: <code>^(.+)$</code>. This tells the web server to look at the URI (<code>^</code>) and capture anything between the base URI and the end (e.g. <code>#elmo_3620022</code>) and append that to the end of the redirect URI (signified by <code>$1</code>). There is one additional flag (<code>NE</code>) which tells the web server not to escape characters. Without this flag, the web server will pass along <code>%23</code> instead of <code>#</code>.</p> <p>Of course, this won't do anything if you are redirecting to your raw OWL file. (See: https://raw.githubusercontent.com/timalamenciak/elmo/refs/heads/main/elmo.owl#elmo_3620021). </p> <p>For ELMO, I wrote a custom Python script that creates a <code>.html</code> file from an OWL file based on a template. If you are handy with Python, it would be fairly straightforward to adapt this script for your own repo. If you are really handy with Python, consider helping to make this a generic script that anyone can use in their ontology! Here is the link: https://github.com/timalamenciak/elmo/blob/main/src/scripts/owl2mkdocs.py</p>"},{"location":"howto/daily-curator-workflow/","title":"Daily Ontology Curator Workflow with GitHub","text":""},{"location":"howto/daily-curator-workflow/#updating-the-local-copy-of-the-ontology-with-git-pull","title":"Updating the local copy of the ontology with 'git pull'","text":"<ol> <li> <p>Navigate to the ontology directory of go-ontology: <code>cd repos/MY-ONTOLOGY/src/ontology</code>.</p> </li> <li> <p>If the terminal window is not configured to display the branch name, type: <code>git status</code>. You will see:</p> </li> </ol> <p>On branch [master] [or the name of the branch you are on]    Your branch is up-to-date with 'origin/master'.</p> <ol> <li> <p>If you\u2019re not in the master branch, type: <code>git checkout master</code>.</p> </li> <li> <p>From the master branch, type: <code>git pull</code>. This will update your master branch, and all working branches, with the files that are most current on GitHub, bringing in and merging any changes that were made since you last pulled the repository using the command <code>git pull</code>. You will see something like this:</p> </li> </ol> <pre><code>~/repos/MY-ONTOLOGY(master) $ git pull\nremote: Counting objects: 26, done.\nremote: Compressing objects: 100% (26/26), done.\nremote: Total 26 (delta 12), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (26/26), done.\nFrom https://github.com/geneontology/go-ontology\n   580c01d..7225e89  master     -&gt; origin/master\n * [new branch]     issue#13029 -&gt; origin/issue#13029\nUpdating 580c01d..7225e89\nFast-forward\n src/ontology/go-edit.obo | 39 ++++++++++++++++++++++++---------------\n 1 file changed, 24 insertions(+), 15 deletions(-)\n~/repos/MY-ONTOLOGY(master) $\n</code></pre>"},{"location":"howto/daily-curator-workflow/#creating-a-new-working-branch-with-git-checkout","title":"Creating a New Working Branch with 'git checkout'","text":"<ol> <li> <p>When starting to work on a ticket, you should create a new branch of the repository to edit the ontology file.</p> </li> <li> <p>Make sure you are on the master branch before creating a new branch. If the terminal window is not configured to display the branch name, type: <code>git status</code> to check which is the active branch. If necessary, go to master by typing <code>git checkout master</code>.</p> </li> <li> <p>To create a new branch, type: <code>git checkout -b issue-NNNNN</code> in the terminal window. For naming branches, we recommend using the string 'issue-' followed by the issue number. For instance, for this issue in the tracker: https://github.com/geneontology/go-ontology/issues/13390, you would create this branch: <code>git checkout -b issue-13390</code>. Typing this command will automatically put you in the new branch. You will see this message in your terminal window:</p> </li> </ol> <pre><code>~/repos/MY-ONTOLOGY/src/ontology(master) $ git checkout -b issue-13390\nSwitched to a new branch 'issue-13390'\n~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $\n</code></pre>"},{"location":"howto/daily-curator-workflow/#continuing-work-on-an-existing-working-branch","title":"Continuing work on an existing Working Branch","text":"<ol> <li> <p>If you are continuing to do work on an existing branch, in addition to updating master, go to your branch by typing <code>git checkout [branch name]</code>. Note that you can view the existing local branches by typing <code>git branch -l</code>.</p> </li> <li> <p>OPTIONAL: To update the working branch with respect to the current version of the ontology, type <code>git pull origin master</code>.    This step is optional because it is not necessary to work on the current version of the ontology; all changes will be synchronized when git merge is performed.</p> </li> </ol>"},{"location":"howto/daily-curator-workflow/#loading-navigating-and-saving-the-ontology-in-protege","title":"Loading, navigating and saving the Ontology in Prot\u00e9g\u00e9","text":"<ol> <li> <p>Before launching Prot\u00e9g\u00e9, make sure you are in the correct branch. To check the active branch, type <code>git status</code>.</p> </li> <li> <p>Click on the 'File' pulldown. Open the file: go-edit.obo. The first time, you will have to navigate to <code>repos/MY-ONTOLOGY/src/ontology</code>. Once you have worked on the file, it will show up in the menu under 'Open'/'Recent'.</p> </li> <li> <p>Click on the 'Classes' tab.</p> </li> <li> <p>Searching: Use the search box on the upper right to search for a term in the ontology. Wait for autocomplete to work in the pop-up window.</p> </li> <li> <p>Viewing a term: Double-click on the term. This will reveal the term in the 'Class hierarchy' window after a few seconds.</p> </li> <li> <p>Launching the reasoner: To see the term in the 'Class hierarchy' (inferred) window, you will need to run the 'ELK reasoner'. 'Reasoner' &gt; select ELK 0.4.3, then click 'Start reasoner'. Close the various pop-up warnings about the ELK reasoner. You will now see the terms in the inferred hierarchy.</p> </li> <li> <p>After modification of the ontology, synchronize the reasoner. Go to menu: 'Reasoner' &gt; ' Synchronize reasoner'.</p> </li> <li> <p>NOTE: The only changes that the reasoner will detect are those impacting the ontology structure: changes in equivalence axioms, subclasses, merges, obsoletions, new terms.</p> </li> <li> <p>TIP: When adding new relations/axioms, 'Synchronize' the reasoner. When deleting relations/axioms, it is more reliable to 'Stop' and 'Start' the reasoner again.</p> </li> <li> <p>Use File &gt; Save to save your changes.</p> </li> </ol> <p></p>"},{"location":"howto/daily-curator-workflow/#committing-pushing-and-merging-your-changes-to-the-repository","title":"Committing, pushing and merging your changes to the repository","text":"<ol> <li> <p>Review: Changes made to the ontology can be viewed by typing <code>git diff</code> in the terminal window. If there are changes that have already been committed, the changes in the active branch relative to master can be viewed by typing <code>git diff master</code>.</p> </li> <li> <p>Commit: Changes can be committed by typing: <code>git commit -m \u2018Meaningful message Fixes #ticketnumber\u2019 go-edit.obo</code>.</p> <p>For example:</p> <pre><code>   git commit -m \u2018hepatic stellate cell migration and contraction and regulation terms. Fixes #13390\u2019 go-edit.obo\n</code></pre> <p>This will save the changes to the go-edit.obo file. The terminal window will show something like:</p> <pre><code> ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $ git commit -m 'Added hepatic stellate cell migration and contraction and regulation terms. Fixes #13390' go-edit.obo\n [issue-13390 dec9df0] Added hepatic stellate cell migration and contraction and regulation terms. Fixes #13390\n 1 file changed, 79 insertions(+)\n ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $\n</code></pre> <ul> <li>NOTE: The word 'fixes' is a magic word in GitHub; when used in combination with the ticket number, it will automatically close the ticket. In the above example, when the file is merged in GitHub, it will close issue number 13390. Learn more on this GitHub Help Documentation page about 'Closing issues via commit messages'.</li> <li>'Fixes' is case-insensitive.</li> <li>If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes'. The commit will be associated with the correct ticket but the ticket will remain open.</li> <li>NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed.</li> <li>TIP: Git needs to know who is committing changes to the repository, so the first time you commit, you may see the following message:</li> </ul> <p>Committer: Kimberly Van Auken vanauken@kimberlukensmbp.dhcp.lbnl.us   Your name and email address were configured automatically based on your username and hostname. Please check that they are accurate.</p> <ul> <li>See Configuration instructions to specify your name and email address.</li> </ul> </li> <li> <p>Push: To incorporate the changes into the remote repository, type: <code>git push origin mynewbranch</code>.</p> <p>Example:</p> <pre><code> git push origin issue-13390\n</code></pre> <ul> <li>TIP: Once you have pushed your changes to the repository, they are available for everyone to see, so at this stage you can ask for feedback.</li> </ul> </li> <li> <p>Pull</p> <ol> <li>In your browser, return to the GO Ontology repository on GitHub.</li> <li>Navigate to the tab labeled as 'Code' <code>geneontology/go-ontology/code</code>. You will see your commit listed at the top of the page in a light yellow box. If you don\u2019t see it, click on the 'Branches' link to reveal it in the list, and click on it.</li> <li>Click the green button 'Compare &amp; pull request' on the right.</li> <li>You may now add comments and ask a colleague to review your pull request. If you want to have the ticket reviewed before closing it, you can select a reviewer for the ticket before you make the pull request by clicking on the 'Reviewers' list and entering a GitHub identifier (e.g. @superuser1). The reviewer will be notified when the pull request is submitted. Since the Pull Request is also a GitHub issue, the reviewer\u2019s comments will show up in the dialog tab of the pull request, similarly to any other issue filed on the tracker.</li> <li>The diff for your file is at the bottom of the page. Examine it as a sanity check.</li> <li>Click on the green box 'Pull request' to generate a pull request.</li> <li>Wait for the Travis checks to complete (this can take a few minutes). If the Travis checks failed, go back to your working copy and correct the reported errrors.</li> </ol> </li> <li> <p>Merge If the Travis checks are succesful and if you are done working on that branch, merge the pull request. Confirming the merge will close the ticket if you have used the word 'fixes' in your commit comment.     NOTE: Merge the branches only when the work is completed. If there is related work to be done as a follow up to the original request, create a new GitHub ticket and start the process from the beginning.</p> </li> <li> <p>Delete your branch on the repository using the button on the right of the successful merge message.</p> </li> <li> <p>You may also delete the working branch on your local copy. Note that this step is optional. However, if you wish to delete branches on your local machine, in your terminal window:</p> <ol> <li>Go back to the master branch by typing <code>git checkout master</code>.</li> <li>Update your local copy of the repository by typing <code>git pull origin master</code></li> <li>Delete the branch by typing <code>git branch -d workingbranchname</code>.    Example: <code>git branch -d issue-13390</code></li> </ol> </li> </ol>"},{"location":"howto/deal-with-large-ontologies/","title":"Dealing with huge ontologies in your import chain","text":"<p>Dealing with very large ontologies, such as the Protein Ontology (PR), NCBI Taxonomy (NCBITaxon), Gene Ontology (GO) and the CHEBI Ontology is a big challenge when developing ontologies, especially if we want to import and re-use terms from them. There are two major problems:</p> <ol> <li>It currently takes about 12\u201316 GB of memory to process PR and NCBITaxon \u2013 memory that many of us do not have available.</li> <li>The files are so large, pulling them over the internet can lead to failures, timeouts and other problems.</li> </ol> <p>There are a few strategies we can employ to deal with the problem of memory consumption:</p> <ol> <li>We try to reduce the memory footprint of the import as much as possible. In other words: we try to not do the fancy stuff ODK does by default when extracting a module, and keep it simple.</li> <li>We manage the import manually ourselves (no import)</li> </ol> <p>To deal with file size, we:</p> <ol> <li>Instead of importing the whole thing, we import curated subsets.</li> <li>If available, we use gzipped (compressed) versions.</li> </ol> <p>All four strategies will be discussed in the following. We will then look a bit</p>"},{"location":"howto/deal-with-large-ontologies/#overwrite-odk-default-less-fancy-custom-modules","title":"Overwrite ODK default: less fancy, custom modules","text":"<p>The default recipe for creating a module looks something like this:</p> <pre><code>imports/%_import.owl: mirror/%.owl imports/%_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) query  -i $&lt; --update ../sparql/preprocess-module.ru \\\n        extract -T imports/$*_terms_combined.txt --force true --copy-ontology-annotations true --individuals exclude --method BOT \\\n        query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n\n.PRECIOUS: imports/%_import.owl\n</code></pre> <p>(Note: This snippet was copied here on 10 February 2021 and may be out of date by the time you read this.)</p> <p>As you can see, a lot of stuff is going on here: first we run some preprocessing (which is really costly in ROBOT, as we need to load the ontology into Jena, and then back into the OWL API \u2013 so basically the ontology is loaded three times in total), then extract a module, then run more SPARQL queries etc, etc. Costly. For small ontologies, this is fine. All of these processes are important to mitigate some of the shortcomings of module extraction techniques, but even if they could be sorted in ROBOT, it may still not be enough.</p> <p>So what we can do now is this. In your <code>ont.Makefile</code> (for example, <code>go.Makefile</code>, NOT <code>Makefile</code>), located in <code>src/ontology</code>, you can add a snippet like this:</p> <pre><code>imports/pr_import.owl: mirror/pr.owl imports/pr_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) extract -i $&lt; -T imports/pr_terms_combined.txt --force true --method BOT \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n\n.PRECIOUS: imports/pr_import.owl\n</code></pre> <p>Note that all the <code>%</code> variables and uses of <code>$*</code> are replaced by the ontology ID in question. Adding this to your <code>ont.Makefile</code> will overwrite the default ODK behaviour in favour of this new recipe.</p> <p>The ODK supports this reduced module out of the box. To activate it, do this:</p> <pre><code>import_group:\n  products:\n    - id: pr\n      use_gzipped: TRUE\n      is_large: TRUE\n</code></pre> <p>This will (a) ensure that PR is pulled from a gzipped location (you have to check whether it exists though. It must correspond to the PURL, followed by the extension <code>.gz</code>, for example <code>http://purl.obolibrary.org/obo/pr.owl.gz</code>) and (b) that it is considered large, so the default handling of large imports is activated for <code>pr</code>, and you don't need to paste anything into <code>ont.Makefile</code>.</p> <p>If you prefer to do it yourself, in the following sections you can find a few snippets that work for three large ontologies. Just copy and paste them into <code>ont.Makefile</code>, and adjust them however you wish.</p>"},{"location":"howto/deal-with-large-ontologies/#protein-ontology-pr","title":"Protein Ontology (PR)","text":"<pre><code>imports/pr_import.owl: mirror/pr.owl imports/pr_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) extract -i $&lt; -T imports/pr_terms_combined.txt --force true --method BOT \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n\n.PRECIOUS: imports/pr_import.owl\n</code></pre>"},{"location":"howto/deal-with-large-ontologies/#ncbi-taxonomy-ncbitaxon","title":"NCBI Taxonomy (NCBITaxon)","text":"<pre><code>imports/ncbitaxon_import.owl: mirror/ncbitaxon.owl imports/ncbitaxon_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) extract -i $&lt; -T imports/ncbitaxon_terms_combined.txt --force true --method BOT \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n\n.PRECIOUS: imports/ncbitaxon_import.owl\n</code></pre>"},{"location":"howto/deal-with-large-ontologies/#chebi","title":"CHEBI","text":"<pre><code>imports/chebi_import.owl: mirror/chebi.owl imports/chebi_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) extract -i $&lt; -T imports/chebi_terms_combined.txt --force true --method BOT \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n\n.PRECIOUS: imports/chebi_import.owl\n</code></pre> <p>Feel free to use an even cheaper approach, even one that does not use ROBOT, as long as it produces the target of the goal (e.g. <code>imports/chebi_import.owl</code>).</p>"},{"location":"howto/deal-with-large-ontologies/#use-slims-when-they-are-available","title":"Use slims when they are available","text":"<p>For some ontologies, you can find slims that are much smaller than full ontology. For example, NCBITaxon maintains a slim for OBO here: http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.owl, which smaller than the 1 or 2 GB of the full version. Many ontologies maintain such slims, and if not, probably should. (I would really like to see an OBO slim for Protein Ontology!)</p> <p>(note the .obo file is even smaller but currently robot has issues getting obo files from the web)</p> <p>You can also add your favourite taxa to the NCBITaxon slim by simply making a pull request on here: https://github.com/obophenotype/ncbitaxon/blob/master/subsets/taxon-subset-ids.txt</p> <p>You can use those slims simply like this:</p> <pre><code>import_group:\n  products:\n    - id: ncbitaxon\n      mirror_from: http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.obo\n</code></pre>"},{"location":"howto/deal-with-large-ontologies/#manage-imports-manually","title":"Manage imports manually","text":"<p>This is a real hack \u2013 and we want to strongly discourage it \u2013 but sometimes, importing an ontology just to import a single term is total overkill. What we do in these cases is to maintain a simple template to \"import\" minimal information. I can't stress enough that we want to avoid this, as such information will necessarily go out of date, but here is a pattern you can use to handle it in a sensible way:</p> <p>Add this to your <code>src/ontology/ont-odk.yaml</code>:</p> <pre><code>import_group:\n  products:\n    - id: my_ncbitaxon\n</code></pre> <p>Then add this to <code>src/ontology/ont.Makefile</code>:</p> <pre><code>mirror/my_ncbitaxon.owl:\n    echo \"No mirror for $@\"\n\nimports/my_ncbitaxon_import.owl: imports/my_ncbitaxon_import.tsv\n    if [ $(IMP) = true ]; then $(ROBOT) template --template $&lt; \\\n  --ontology-iri \"$(ONTBASE)/$@\" --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n\n.PRECIOUS: imports/my_ncbitaxon_import.owl\n</code></pre> <p>Now you can manage your import manually in the template, and the ODK will not include your manually-curated import in your base release. But again, avoid this pattern for anything except the most trivial case (e.g. you need one term from a huge ontology).</p>"},{"location":"howto/deal-with-large-ontologies/#file-is-too-large-network-timeouts-and-long-runtimes","title":"File is too large: Network timeouts and long runtimes","text":"<p>Remember that ontologies are text files. While this makes them easy to read in your browser, it also makes them huge: from 500 MB (CHEBI) to 2 GB (NCBITaxon), which is an enormous amount.</p> <p>Thankfully, ROBOT can automatically read gzipped ontologies without the need of unpacking. To avoid long runtimes and network timeouts, we can do the following two things (with the new ODK 1.2.26):</p> <pre><code>import_group:\n  products:\n    - id: pr\n      use_gzipped: TRUE\n</code></pre> <p>This will try to append <code>.gz</code> to the default download location (http://purl.obolibrary.org/obo/pr.owl \u2192 http://purl.obolibrary.org/obo/pr.owl.gz). Note that you must make sure that this file actually exists. It does for CHEBI and the Protein Ontology, but not for many others.</p> <p>If the file exists, but is located elsewhere, you can do this:</p> <pre><code>import_group:\n  products:\n    - id: pr\n      mirror_from: http://purl.obolibrary.org/obo/pr.owl.gz\n</code></pre> <p>You can put any URL in <code>mirror_from</code> (including non-OBO ones!)</p>"},{"location":"howto/deploy-custom-obo-dashboard/","title":"How to deploy a custom OBO dashboard","text":""},{"location":"howto/deploy-custom-obo-dashboard/#updated-2022-workflow","title":"Updated 2022 Workflow","text":"<p>We developed a completely automated variant of the Custom OBO Dashboard Workflow, which does not require any local installation.</p> <p></p> <ol> <li>Create a repository using the Dashboard template repository. (How to create a repository from a template)</li> <li>Modify the <code>dashboard-config.yml</code> file, in particular the <code>ontologies</code> section:<ol> <li>Important: Add your ontology ID to the ID 'id' field</li> <li>Add the path to your ontology to the <code>mirror_from</code> field.</li> </ol> </li> <li>Optional: use the <code>profile</code> section to overwrite the custom robot report profile and add custom checks!</li> </ol> <p><code>yaml    profile:      baseprofile: \"https://raw.githubusercontent.com/ontodev/robot/master/robot-core/src/main/resources/report_profile.txt\"      custom:        - \"WARN\\tfile:./sparql/missing_xrefs.sparql\"</code></p> <ol> <li> <p>Click on <code>Settings</code> &gt; <code>Pages</code> to configure the <code>GitHub pages</code>. Set the <code>Source</code> to deploy from branch, and <code>Branch</code> to build from <code>main</code> (or <code>master</code> if you are still using the old default) and <code>/(root)</code> as directory. Hit <code>Save</code>.</p> <p></p> </li> <li> <p>Click on the <code>Actions</code> tab in your repo. On the left, select the <code>Run dashboard</code> workflow and click on the <code>Run workflow</code> button. This action will rebuild the dashboard and make a pull request with the changes.</p> </li> <li>Review and merge the pull request. Once it is merged, GitHub will automatically rebuild your dashboard website.</li> <li>After 5 minutes, click on \"Settings\" &gt; \"Pages\" again. You should now a new section with information where your site is deployed:       </li> <li>Click on <code>Visit site</code> and you should find your new shiny dashboard page!</li> </ol>"},{"location":"howto/deploy-custom-obo-dashboard/#troubloushooting","title":"Troubloushooting","text":""},{"location":"howto/deploy-custom-obo-dashboard/#error-on-github-action-run-dashboard-section","title":"Error on GitHub Action - Run dashboard section","text":"<p><code>Failed: make dashboard ROBOT_JAR=/tools/robot.jar ROBOT=robot  -B with return code 2</code></p> <p>There is a known bug at the moment requiring at least one ontology with a warning, error, info and pass, see https://github.com/OBOFoundry/OBO-Dashboard/issues/85. </p> <ol> <li>In your <code>dashboard-config.yml</code>, add a temporary ontology we created to make this work. This is already in the Dashboard template repository. </li> </ol> <pre><code>ontologies:\n  custom:\n    - id: tmp\n      mirror_from: \"https://raw.githubusercontent.com/monarch-ebi-dev/robot_tests/master/custom-dashboard.owl\"\n</code></pre>"},{"location":"howto/deploy-custom-obo-dashboard/#error-on-github-action-create-pull-request-section","title":"Error on GitHub Action - Create Pull Request section","text":"<p><code>remote: Permission to &lt;name of the user or organization&gt;/&lt;name of the repository&gt;.git denied to github-actions[bot].</code></p> <p>You need to update the workflow permission for the repository.</p> <ol> <li>Click on <code>Settings</code>, then <code>Actions</code> on the left menu, then <code>General</code>.</li> <li>Scroll down to the 'Workflow permissions section. Select the option 'Read and write permissions'. Save.</li> </ol> <p><code>Error: GitHub Actions is not permitted to create or approve pull requests.</code></p> <p>You need to enable GitHub Actions to create pull requests.</p> <ol> <li>Click on <code>Settings</code>, then <code>Actions</code> on the left menu, then <code>General</code>.</li> <li>Scroll down to the 'Workflow permissions section. Click on the 'Allow GitHub Actions to create and approve pull requests' checkbox.</li> </ol>"},{"location":"howto/deploy-custom-obo-dashboard/#2021-edition","title":"2021 Edition","text":"<p>Contributed by <code>@XinsongDu</code>, edited by <code>@matentzn</code></p> <ol> <li>Clone https://github.com/OBOFoundry/obo-nor.github.io and copy all its contents to a new GitHub repo under your account. Ensure that the <code>.gitignore</code> from the <code>obo-nor.github.io</code> repo is also copied to your new repo (it is frequently skipped or hidden from the user in <code>Finder</code> or when using the <code>cp</code> command) and push to everything to GitHub.</li> <li>Pull the Ontology Development Kit from Docker Hub (can take a while):</li> </ol> <pre><code>docker pull obolibrary/odkfull\n</code></pre> <ol> <li>Modify the <code>dashboard-config.yml</code> file, in particular the <code>ontologies</code> section:</li> <li>Important: Add your ontology ID to the ID 'id' field</li> <li>Add the path to your ontology to the <code>mirror_from</code> field.</li> <li>Get the \"base uri namespace\" of the ontology using the following steps:    a. Open the ontology in Prot\u00e9g\u00e9    b. Select a class and press \"command + u\" (MacOS), the stem of the path would be the base URI namespace (e.g., in EDAM ontology, the base uri namespace is http://edamontology.org/, for Uberon it would be http://purl.obolibrary.org/obo/UBERON_)</li> <li>Add the base uri namespace to 'base_ns' field of your ontology in the dashboard-config.yml</li> <li>(As of October 2021 make sure there are multiple ontologies in the dashboard-config.yml, otherwise errors would be reported while running the code. There are currently some bugs in the dashboard code that require at least 2 or 3 ontologies in the list).</li> <li>In the Makefile uncomment the <code>#</code> before <code>pip install networkx==2.6.2</code> to ensure the correct network x version is installed.</li> <li>Run <code>sh run-dash.sh</code> (make sure dashboard folder is empty before running, e.g. <code>rm -rf dashboard/*</code>).</li> <li>When run successfully, push all changes to GitHub.</li> <li>Go to GitHub repo you just created, and go to Settings, then Pages, and select your main/master branch as \"source\", and your root directory. You will see a website URL highlighted in green, where your OBO dashboard is deployed.</li> </ol>"},{"location":"howto/edit-in-protege/","title":"Editing a term in protege","text":"<p>Before you start:</p> <ul> <li>make sure you are working on a branch here.</li> <li>make sure you have the editor's file open in Protege (in ODK ontologies, located in: ./src/ontology/ONT-edit.owl) where ONT is the name of your ontology (eg mondo-edit.owl for MONDO)</li> <li>familiarise yourself with the user interface of protege</li> </ul>"},{"location":"howto/edit-in-protege/#adding-editing-annotating-and-deleting-axioms","title":"Adding, editing, annotating and deleting axioms","text":""},{"location":"howto/edit-in-protege/#adding-annotations","title":"Adding annotations","text":"<p>Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. OBO Foundry ontologies includes a pre-declared set of annotation properties. The most commonly used annotations are below.</p> <ul> <li>rdfs:label</li> <li>definition</li> <li>has_exact_synonym</li> <li>has_broad_synonym</li> <li>has_narrow_synonym</li> <li>has_related synonym</li> <li>database_cross_reference</li> <li>rdfs:comment</li> </ul> <p>Note: OBO ontologies allow only one rdfs:label, definition, and comment.</p> <p>Note, most of these are bold in the annotation property list:</p> <p></p> <p>Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK.</p> <p>Example (based on MONDO):</p> <p>Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns.</p> <p></p> <p></p> <p>Definitions in OBO ontologies should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space)</p> <p>To add a dbxref to the definition:</p> <ol> <li>Click the @ symbol next to the definition</li> <li>Click the + button next in the pop-up window</li> <li>Scroll up on the left hand side until you find 'database_cross_reference', and click it</li> <li>Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces)</li> <li>Click OK</li> <li>Add the additional dbxref (e.g., adding GARD:0013029)</li> <li>The dbxrefs should appear as below.</li> </ol> <p></p> <p></p>"},{"location":"howto/edit-in-protege/#add-synonyms-and-database-cross-reference","title":"Add Synonyms and Database cross reference","text":"<p>To add a synonym:</p> <ol> <li>Select the + button to add an annotation to the selected entity</li> <li>Add the synonyms as 'has_exact_synonym' (note: use appropriate synonym annotation)</li> <li>Synonyms should have a reference to it</li> <li>Click the @ symbol next to the synonym</li> <li>Click the + button</li> <li>Select <code>database_cross_reference</code> on the left panel and add your reference to the Literal tab on the right hand side</li> </ol> <p></p>"},{"location":"howto/edit-in-protege/#the-class-description-view","title":"The Class description view","text":"<p>We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class.</p> <p>Note the Anonymous Ancestors. These are superclasses that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the superclass from.</p> <p></p> <p>When you press the '+' button to add a <code>SubClass of</code> axiom, you will notice a few ways you can add a term. The easiest of this is to use the Class expression editor. This allows you to type in the expression utilizing autocomplete. As you start typing, you can press the 'TAB' or '-&gt;|' button on your keyboard, and protege will suggest terms. You will also note that the term you enter is not in the ontology, protege will not allow you add it, with the box being highlighted red, and the term underlined red.</p>"},{"location":"howto/edit-in-protege/#make-a-pull-request","title":"Make a Pull Request","text":"<ol> <li>Make a pull request as usual (instructions here)</li> </ol>"},{"location":"howto/embed-video/","title":"Embed YouTube video into a OBOOK page","text":"<p>This guide explains how to embed a YouTube video into a page in this OBO Academy material. Example, see the videos on the Contributing to OBO Ontologies page.</p>"},{"location":"howto/embed-video/#instructions","title":"Instructions","text":"<ol> <li>Go to the YouTube video you would like to embed.</li> <li>Click Share</li> <li>Click Embed</li> <li>Click copy</li> <li>Paste the content into the page where you would like to see it displayed.</li> </ol> <p>The content should look something like this: <code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_z8-KGDzZ6U\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;</code></p> <p>The embedded video should look like this:</p>"},{"location":"howto/filter-text-file/","title":"Command Line Trick: Filter text files based on a list of strings","text":"<p>Let's say you want to remove some lines from a large text file programmatically. For example, you want to remove every line that contains certain IDs, but you want to keep the rest of the lines intact. You can use the command line utility <code>grep</code> with option <code>-v</code> to find all the lines in the file that do NOT contain your search term(s). You can make a file with a list of several search terms and use that file with <code>grep</code> using the <code>-f</code> option as follows:</p> <pre><code>grep -v -f your_list.txt target_file.tsv | tee out_file.tsv\n</code></pre>"},{"location":"howto/filter-text-file/#explanation","title":"Explanation","text":"<ul> <li>The target file is your text file from which you wish to remove lines. The text file can be of type <code>csv</code>, <code>tsv</code>, <code>obo</code> etc. For example, you wish to filter a file with these lines:</li> </ul> <p>keep this 1   this line is undesired 2, so you do not wish to keep it   keep this 3   keep this 4   keep this 5   keep this 6   something undesired 2   this line is undesired 1   keep this 7</p> <ul> <li>The file <code>your_list.txt</code> is a text file with your list of search terms. Format: one search term per line. For example:</li> </ul> <p>undesired 1   undesired 2</p> <ul> <li> <p>The utility <code>tee</code> will redirect the standard output to both the terminal and write it out to a file.</p> </li> <li> <p>You expect the <code>out_file.tsv</code> to contain lines:</p> </li> </ul> <p>keep this 1   keep this 3   keep this 4   keep this 5   keep this 6   keep this 7</p>"},{"location":"howto/filter-text-file/#do-the-filtering-and-updating-of-your-target-file-in-one-step","title":"Do the filtering and updating of your target file in one step","text":"<p>You can also do a one-step filter-update when you are confident that your filtering works as expected, or if you have a backup copy of your <code>target_file.tsv</code>. Use <code>cat</code> and pipe the contents of your text file as the input for <code>grep</code>. Redirect the results to both your terminal and overwrite your original file so it will contain only the filtered lines.</p> <pre><code>cat target_file.tsv | grep -v -f your_list.txt | tee target_file.tsv\n</code></pre>"},{"location":"howto/fixing-conflicts/","title":"Fixing merge conflicts","text":"<p>This video illustrates an example of fixing a merge conflict in the Mondo Disease Ontology.</p> <p>Instructions:</p> <ol> <li> <p>If a merge conflict error appears in your Github.com pull request after committing a change, open GitHub Desktop and select the corresponding repository from the \"Current Repository\" button. If the conflict emerged after editing the ontology outside of Prot\u00e9g\u00e9 5.5.0, see Ad hoc Reserialisation below.</p> </li> <li> <p>With the repository selected, click the \"Fetch origin\" button to fetch the most up-to-date version of the repository.</p> </li> <li> <p>Click the \"Current Branch\" button and select the branch with the merge conflict.</p> </li> <li> <p>From the menu bar, select Branch &gt; \"Update from master\".</p> </li> <li> <p>A message indicating the file with a conflict should appear along with the option to open the file (owl or obo file) in a text/code editor, such as Sublime Text. Click the button to open the file.</p> </li> <li> <p>Search the file for conflict markings ( &lt;&lt;&lt;&lt;&lt;&lt;&lt; ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; ).</p> </li> <li> <p>Make edits to resolve the conflict, e.g., arrange terms in the correct order.</p> </li> <li> <p>Remove the conflict markings.</p> </li> <li> <p>Save the file.</p> </li> <li> <p>Open the file in Prot\u00e9g\u00e9. If prompted, do not reload any previously opened file. Open as a new file.</p> </li> <li> <p>Check that the terms involved in the conflict appear OK, i.e., have no obvious errors.</p> </li> <li> <p>Save the file in Prot\u00e9g\u00e9 using File &gt; 'Save as...' from the menu bar and replace the ontology edit file, e.g., mondo-edit.obo</p> </li> <li> <p>Return to GitHub Desktop and confirm the conflicts are now resolved. Click the \"Continue Merge\" button and then the \"Push origin\" button.</p> </li> <li> <p>Return to Github.com and allow the QC queries to rerun.</p> </li> <li> <p>The conflict should be resolved and the branch allowed to be merged.</p> </li> </ol> <p>Ad hoc Reserialisation</p> <p>If the owl or obo file involved in the merge conflict was edited using Prot\u00e9g\u00e9 5.5.0, the above instructions should be sufficient. If edited in any other way, such as fixing a conflict in a text editor, the serialisation order may need to be fixed. This can be done as follows:</p> <ol> <li> <p>Reserialise the master file using the Ontology Development Kit (ODK). This requires setting up Docker and ODK. If not already set up, follow the instructions here.</p> </li> <li> <p>Open Docker.</p> </li> <li> <p>At the line command (PC) or Terminal (Mac), use the cd (change directory) command to navigate to the repository's src/ontology/ directory.    For example,</p> </li> </ol> <p><code>cd PATH_TO_ONTOLOGY/src/ontology/</code></p> <p>Replace \"PATH_TO_ONTOLOGY\" with the actual file path to the ontology. If you need to orient yourself, use the <code>pwd</code> (present working directory) or <code>ls</code> (list) line commands.</p> <ol> <li>If you are resolving a conflict in an .owl file, run:</li> </ol> <p><code>sh run.sh make normalize_src</code></p> <p>If you are resolving a conflict in an .obo file, run:</p> <p><code>sh run.sh make normalize_obo_src</code></p> <ol> <li> <p>In some ontologies (such as the Cell ontology (CL)), edits may result in creating a large amount of unintended differences involving ^^xsd:string. If you see these differences after running the command above, they can be resolved by following the instructions here.</p> </li> <li> <p>Continue by going to step 1 under the main Instructions above.</p> </li> </ol>"},{"location":"howto/github-actions/","title":"Using Github actions to automate tasks","text":""},{"location":"howto/github-actions/#post-a-comment-with-ontology-differences-on-pull-request","title":"Post a comment with ontology differences on pull request","text":"<p>The command line tool Robot has a diff tool that compares two ontology files and can print the differences between them in multiple formats, among them markdown.</p> <p>We can use this tool and GitHub actions to automatically post a comment when a Pull Request to master is created, with the differences between the two ontologies.</p> <p>To create a new GitHub action, create a folder in your ontology project root folder called <code>.github</code>. Then create a yaml file in a subfolder called <code>workflows</code>, e.g. <code>.github/workflows/diff.yml</code>. This file contains code that will be executed in GitHub when certain conditions are meant, in this case, when a PR to master is submitted. The comments in this file from FYPO will help you write an action for your own repository.</p> <p>The comment will look something like this.</p>"},{"location":"howto/github-basics/","title":"The Basics of GitHub: Organizations, Repositories, &amp; Issues","text":"<p>This serves as a quick reference and step by step guide for users with no prior GitHub experience. For more complete information and detailed instructions, see GitHub Fundamentals for OBO Engineers and GitHub Issue for OBO Engineers.</p>"},{"location":"howto/github-basics/#signing-up-for-github","title":"Signing up for GitHub","text":"<p>GitHub is a cloud-based platform where you can store, share, and work together with others to write code. Collaborative working, one of GitHub\u2019s fundamental features, is made possible by the open-source software, Git, upon which GitHub is built. </p> <ol> <li>Sign up for GitHub at github.com.</li> <li>After signing up, your Username will be your GitHub handle. This serves as your unique identity on the platform and is used to log in, reference your profile, and mention you in discussions.</li> <li>Note: You can go to Settings to provide additional personal information, connect ORCID id, and customize your avatar.</li> </ol>"},{"location":"howto/github-basics/#understanding-organization-and-repository","title":"Understanding Organization and Repository","text":"<ol> <li>Organization: A shared account that allows groups to collaboratively manage multiple repositories with centralized access control, permissions, and team management. Organizations own repositories (instead of individual users) and enable admins to manage members, teams, security settings, and integrations across all projects.</li> <li> <p>Example: https://github.com/zappfish, https://github.com/monarch-initiative</p> </li> <li> <p>Repository (repo): A storage space on GitHub where your project\u2019s files, code, or documentation are kept. It tracks every change made to those files, allowing collaboration, version control, and issue tracking.</p> </li> <li>Example: https://github.com/zappfish/zapp-atlas</li> </ol>"},{"location":"howto/github-basics/#creating-an-issue-from-a-repository","title":"Creating an Issue from a Repository","text":"<p>Issues are easy to create, flexible, and useful for tracking ideas, feedback, tasks, or bugs, making them an excellent tool for collaborate. Note: See Writing Good Issues for best practices on creating effective issues.</p> <ol> <li> <p>On GitHub, navigate to the main page of the repository (e.g. https://github.com/zappfish/zapp-atlas).</p> </li> <li> <p>Under your repository name, click Issues.  </p> </li> <li> <p>Click New issue. </p> </li> <li> <p>In the Add a title field, type a concise title for your issue.The title should convey at a glance what the issue is about.</p> </li> <li> <p>Example: add phenotype abnormal intestinal cells </p> </li> <li> <p>In the Add a description field, type a description providing more information of your issue.</p> </li> <li> <p>Example:https://github.com/zappfish/zapp-atlas/issues/52</p> </li> <li> <p>Add any labels to specify the issue by clicking on Labels. Note: This is not required.</p> </li> <li>To draw someone's attention (with access to the repository) to your issue, mention them in your issue, click on or type @, followed by a GitHub handle.</li> <li>To link related issues in the same repository, you can type # followed by part of the issue title and then clicking the issue that you want to link.</li> <li>When you're finished, click Create.</li> </ol>"},{"location":"howto/github-basics/#commenting-on-issues","title":"Commenting on Issues","text":"<p>Commenting on issues is an important way to communicate and collaborate. Anyone with write access to a repository can comment on issues, and issue creator will be notified through email. 1. Under the repository name, click Issues.  </p> <ol> <li> <p>Select an issue by clicking its title.</p> </li> <li> <p>In the Add a comment field, type in your comment under the Write tab. </p> </li> <li> <p>Note: Github content is written using Markdown, a text-to-HTML conversion tool. More information on Markdown can be found here. If you aren't familiar with Markdown, you can simply use the format commands at the top of a comment section to format your text. </p> </li> <li> <p>To mention someone in your comment, click on or type @, followed by a GitHub handle.</p> </li> <li> <p>Preview your comment under the Preview tab.</p> </li> <li> <p>When you're finished, click Comment.</p> </li> </ol>"},{"location":"howto/github-create-fork/","title":"Fork an ontology for editing","text":"<p>Note: Creating a fork allows you to create your copy GitHub repository. This example provides instructions on forking the Mondo GitHub reposiitory. You can't break any of the Mondo files by editing your forked copy.</p> <ol> <li>On GitHub, navigate to https://github.com/monarch-initiative/mondo</li> <li>In the top-right corner of the page, click Fork.</li> </ol> <p></p> <ol> <li>When prompted 'Where should we fork mondo', choose your own repo (eg Nicole Vasilevsky).</li> <li>Be careful if you have multiple forks (i.e. the original and your own personal fork, as this can cause confusion).</li> <li> <p>Clone your forked repo:</p> </li> <li> <p>If you have GitHub Desktop installed - click Code -&gt; Open with GitHub Desktop</p> </li> <li> <p>How are you planning to use this fork? To contribute to parent project</p> </li> <li> <p>In GitHub Desktop, create a new branch:</p> </li> <li> <p>Click Current Branch - &gt; New Branch</p> </li> </ol> <p></p> <ol> <li> <p>Give your branch a name, like c-path-training-1</p> </li> <li> <p>You will make changes to the Mondo on the branch of your local copy.</p> </li> </ol> <p>Further instructions on forking a repo</p>"},{"location":"howto/github-create-pull-request/","title":"Create a Pull Request in GitHub","text":"<p>This how-to gives guidance on how to make a Pull Request (PR). For more on PRs, and guidelines on what makes a good PR, see the GitHub 10 tips on Pull Requests guide.</p>"},{"location":"howto/github-create-pull-request/#overview","title":"Overview","text":""},{"location":"howto/github-create-pull-request/#github-workflows","title":"GitHub workflows","text":"<p>A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub.</p> <p></p> <p>A Git repo typically has a master or main branch that is not directly edited. Changes are made by creating a branch from Master (complete copy of the Master + its history) (either a direct branch or via a fork).</p>"},{"location":"howto/github-create-pull-request/#branch-vs-fork","title":"Branch vs Fork","text":"<p>You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners.\u00a0 If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. See the Appendix for instructions on how to make a fork.</p> <p>If you have permission from the owners, you can instead make a new branch.</p>"},{"location":"howto/github-create-pull-request/#what-is-a-pull-request","title":"What is a Pull Request?","text":"<p>A Pull Request (PR) is an event in Git where a contributor (you!) asks a maintainer of a Git repository to review changes (e.g. edits to an ontology file) they want to merge into a project (e.g. the owl file) (see reference). Create a pull request to propose and collaborate on changes to a repository. These changes are proposed in a branch, which ensures that the default branch only contains finished and approved work. See more details here.</p>"},{"location":"howto/github-create-pull-request/#committing-pushing-and-making-pull-requests","title":"Committing, pushing and making pull requests","text":"<ol> <li> <p>See these instructions on cloning an ontology repo and creating a branch using GitHub Dekstop.</p> </li> <li> <p>Review: Once changes are made to the ontology file, they can be viewed in GitHub Desktop.</p> </li> <li> <p>Before committing, check the diff. An example diff from the Cell Ontology (CL) is pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and consider asking the ontology editor team for help instead.</p> </li> </ol> <p>Example 1 (Cell Ontology):</p> <p></p> <p>Example 2 (Mondo):</p> <p></p> <ol> <li> <p>Commit message: Before Committing, you must add a commit message. In GitHub Desktop in the Commit field in the lower left, there is a subject line and a description.</p> </li> <li> <p>Give a very descriptive title: Add a descriptive title in the subject line. For example: add new class ONTOLOGY:ID [term name] (e.g. add new class MONDO:0000006 heart disease)</p> </li> <li> <p>Write a great summary of what the change is in the Description box, referring to the issue. The sentence should clearly state how the issue is addressed.</p> </li> <li> <p>To link the issue, you can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages.</p> </li> <li> <p>Note: 'Fixes' and \"Closes' are case-insensitive.</p> </li> <li> <p>If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Addresses'. The commit will be associated with the correct ticket but the ticket will remain open. 7.NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed.</p> </li> </ol> <p></p> <ol> <li> <p>Click Commit to [branch]. This will save the changes to the ontology edit file.</p> </li> <li> <p>Push: To incorporate the changes into the remote repository, click Publish branch.</p> </li> </ol>"},{"location":"howto/github-create-pull-request/#make-a-pull-request","title":"Make a Pull Request","text":"<ol> <li> <p>Click: Create Pull Request in GitHub Desktop</p> </li> <li> <p>This will automatically open GitHub Desktop</p> </li> <li> <p>Click the green button 'Create pull request'</p> </li> <li> <p>You may now add comments to your pull request.</p> </li> <li> <p>The CL editors team will review your PR and either ask for changes or merge it.</p> </li> <li> <p>The changes will be available in the next release.</p> </li> </ol>"},{"location":"howto/idrange/","title":"Setting up your ID Ranges","text":""},{"location":"howto/idrange/#setting-id-ranges-in-your-ontology","title":"Setting ID ranges in your ontology","text":"<ol> <li> <p>Curators and projects are assigned specific ID ranges within the prefix for your ontology. See the README-editors.md for your ontology</p> </li> <li> <p>An example: uberon-idranges.owl</p> </li> <li> <p>NOTE: You should only use IDs within your range.</p> </li> <li> <p>If you have only just set up this repository, modify the idranges file and add yourself or other editors.</p> </li> </ol>"},{"location":"howto/idrange/#setting-id-ranges","title":"Setting ID ranges","text":""},{"location":"howto/idrange/#protege-56","title":"Protege 5.6","text":"<p>Prot\u00e9g\u00e9 5.6 can now automatically set up the ID range for a given user by exploiting the <code>ONT-idranges.owl</code> file, if it exists. ONT is the name of the ontology you are editing (for example, in UBERON, the file is named <code>uberon-idranges.owl</code>).  The file is automatically created by the ODK, so that users shouldn\u2019t need worry about it.</p> <p>This Protege version looks at the ID range file and matches your user name in Protege to the names in the file to automatically set up your ID range. Thus as long as this information matches you no longer need to manually set the ID range. You will get a message if your user name does not match one in the file asking you to pick an ID range.</p> <p>Note: If you are switching from an old Protege version to Protege 5.6, you may need to reset your range to the last used ID rather than just the full range or Protege would try to fill in gaps in the range.</p>"},{"location":"howto/idrange/#protege-550-or-below","title":"Protege 5.5.0 or below","text":"<p>ID ranges need to be manually set in Protege 5.6.0 or below, as described below.</p>"},{"location":"howto/idrange/#setting-id-ranges-in-protege-in-protege-550-or-below","title":"Setting ID ranges in Protege in Protege 5.5.0 or below","text":"<ol> <li> <p>Once you have your assigned ID range, you need to configure Protege so that your ID range is recorded in the Preferences menu. Protege does not read the idranges file.</p> </li> <li> <p>In the Protege menu, select Preferences.</p> </li> <li> <p>In the resulting pop-up window, click on the New Entities tab and set the values as follows.</p> </li> <li> <p>In the Entity IRI box:</p> </li> </ol> <p>Start with: Specified IRI: http://purl.obolibrary.org/obo</p> <p>Followed by: <code>/</code></p> <p>End with: <code>Auto-generated ID</code></p> <ol> <li>In the Entity Label section:</li> </ol> <p>Same as label renderer: IRI: http://www.w3.org/2000/01/rdf-schema#label</p> <ol> <li> <p>In the Auto-generated ID section:</p> </li> <li> <p>Numeric</p> </li> <li> <p>Prefix <code>GO_</code></p> </li> <li> <p>Suffix: leave this blank</p> </li> <li> <p>Digit Count <code>7</code></p> </li> <li> <p>Start: see go-idranges.owl. Only paste the number after the <code>GO:</code> prefix. Also, note that when you paste in your GO ID range, the number will automatically be converted to a standard number, e.g. pasting 0110001 will be converted to 110,001.)</p> </li> <li> <p>End: see go-idranges.owl</p> </li> <li> <p>Remember last ID between Protege sessions: ALWAYS CHECK THIS</p> </li> </ol> <p>(Note: You want the ID to be remembered to prevent clashes when working in parallel on branches.)</p>"},{"location":"howto/install-protege/","title":"5 step installation guide for Prot\u00e9g\u00e9","text":"<ol> <li>Click here to find latest release of Prot\u00e9g\u00e9. Choose and download the appropriate files for your operating system (Linux, macOS, Windows, platform independent).</li> <li>Use the PGP signature contained in the appropriate file with an <code>.asc</code> extension to verify the integrity of the downloaded Prot\u00e9g\u00e9 version.</li> <li>Decompress the downloaded <code>.zip</code> or <code>.tar.gz</code> file with tools appropriate for your operating system.</li> <li> <p>Follow the steps as needed by your operating system to install the Prot\u00e9g\u00e9 application. For example, on macOS: drag and drop <code>Prot\u00e9g\u00e9.app</code> to the <code>Applications</code> folder and replace any older versions of the software. You may need to right click <code>Prot\u00e9g\u00e9.app</code> and then choose <code>Open</code> from the menu to authorise the programme to run on your machine. Alternatively, go to <code>Preferences -&gt; Security -&gt; General</code>. You need to open the little lock, then click <code>Mac stopped an application from Running (Prot\u00e9g\u00e9)</code> -&gt; <code>Open anyways</code>.</p> </li> <li> <p>Adjust memory settings if necessary. Memory settings can now be adjusted in a jvm.conf configuration file that can be located either in the .protege/conf directory under your home directory, or in the conf directory within the application bundle itself. For example, to set the maximum amount of memory available for Prot\u00e9g\u00e9 to, say, 12GB, put the following in the jvm.conf file: <pre><code>max_heap_size=12G\n</code></pre></p> </li> <li>On macOS, you can find the file is here:</li> </ol> <pre><code>/Applications/Prote\u0301ge\u0301.app/Contents/conf/jvm.conf\n</code></pre> <p>Edit this part:</p> <pre><code># Uncomment the line below to set the maximal heap size to 8G\n#max_heap_size=8G\n</code></pre>"},{"location":"howto/installing-elk-in-protege/","title":"Install Elk 0.5 in Protege","text":"<p>Note: This is only needed for Protege 5.5.0 or below. Protege 5.6 has ELK already installed.</p> <ul> <li> <p>Click here to get the latest Protege Plugin latest build (this is available on the bottom of ELK pages. This will download a zipped file.)</p> </li> <li> <p>When downloaded, unzip and copy puli and elk jars (two .jar files) in the unpacked directory.</p> </li> <li>Paste these files in your Protege plugin directory.</li> <li> <p>Remove old org.semanticweb.elk.jar</p> </li> <li> <p>Install ELK plugin on Mac:</p> </li> </ul> <p>This can be done via one of two ways:</p> <p>Approach 1</p> <ol> <li>In Terminal:    <code>open ~/.Protege, then click on plugins</code></li> <li>Click on plugins</li> <li>Copy and paste the two files into the plugins directory</li> <li>Remove old elk.jar (Ex. org.semanticweb.elk.jar)</li> <li>Restart Protege. You should see ELK 0.5 installed in your Reasoner menu.</li> </ol> <p>Approach 2</p> <ol> <li>Paste these files in your Protege plugin directory. This is in one of two locations:</li> <li>~/.Protege/plugins (note this is usually hidden from finder, but you can see it in the terminal) or</li> <li>Go to Protege in Applications (Finder), right click, 'Show package contents' -&gt; Java -&gt; plugins</li> <li>If you go to <code>~/.Protege</code> and a directory called <code>plugins</code> does not exist in this folder, you can create it.</li> <li>Copy and paste the two files into the plugins directory</li> <li>Remove old elk.jar (Ex. org.semanticweb.elk.jar)</li> <li>Restart Protege. You should see ELK 0.5 installed in your Reasoner menu.</li> </ol> <p>Important: it seems Elk 0.5. Does not work with all versions of Protege, in particular, 5.2 and below. These instructions were only tested with Protege 5.5.</p>"},{"location":"howto/installing-elk-in-protege/#video-explanation","title":"Video Explanation","text":""},{"location":"howto/merge-terms/","title":"Merging Terms","text":"<p>NOTE This documentation is incomplete, for now you may be better consulting the GO Editor Docs</p> <p>For instructions on obsoleting terms (without merging/replacing with a new term, see obsoletion how to guide.)</p>"},{"location":"howto/merge-terms/#merging-ontology-terms","title":"Merging Ontology Terms","text":"<p>See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions.</p> <p>Note Before performing a merge, make sure that you know all of the consequences that the merge will cause. In particular, be sure to look at child terms and any other terms that refer to the \u2018obsoleted\u2019 term. In many cases a simple merge of two terms is not sufficient because it will result in equivalent classes for child terms. For example if obsoleted term X is going to be merged into target term Y and \u2018regulation of X\u2019 and \u2018regulation of Y\u2019 terms exist, then you will need to merge the regulation terms in addition to the primary terms. You will also need to edit any terms that refer to the obsoleted term to be sure that the names and definitions are consistent.</p>"},{"location":"howto/merge-terms/#manual-workflow","title":"Manual Workflow","text":"<ol> <li>Find the ID of the term in which the obsoleted term will be merged<ul> <li>Navigate to 'winning' term using the Search box. Copy the ID of the winning term somewhere.</li> </ul> </li> <li>Duplicate annotations from the obsoleted terms<ul> <li>Navigate to the term to be obsoleted.</li> <li>Right click on it and select <code>Duplicate class</code> then OK in the pop up window. This should create a class with the exact same name.</li> <li>On the duplicated class (you can see this by (CL:XXXX) within your range added), right click and select <code>Change IRI (Rename)</code></li> <li>Copy the ID of the winning term (obtained in Step 1).</li> <li>Be sure to use the underscore <code>_</code> in the identifier instead of the colon <code>:</code>, for example: <code>GO_1234567</code>. Make sure that the 'change all entities with this URI' box is checked.</li> <li>Navigate to the winning term IRI, all annotations should be merged.</li> </ul> </li> <li>Change obsoleted term label to a synonym<ul> <li>In the annotations box of the winning term there are now two terms with labels 'rdfs:label'. Click the <code>o</code> to change the label of the obsoleted term.</li> <li>In the resulting pop-up window, select the appropriate synonym label from the list on the left:<ol> <li><code>has_broad_synonym</code></li> <li><code>has_exact_synonym</code></li> <li><code>has_narrow_synonym</code></li> <li><code>has_related_synonym</code> (if unsure, this is the safest choice)</li> </ol> </li> </ul> </li> <li>Remove duplicated or inappropriate annotations<ul> <li>Check the definition, if there are multiple entries, remove the obsoleted one by clicking on the <code>x</code> on the right.</li> <li>Check the subclasses and remove inappropriate/duplciated ones by clicking on the <code>x</code> on the right.</li> <li>Check list of synonyms and remove inappropriate/duplciated ones by clicking on the <code>x</code> on the right.</li> <li>Note down the created_by and created_date (there can only be one value per term for each of these fields; this will be useful if you need to pick one after the merge is done).</li> </ul> </li> <li>Obsolete old term<ul> <li>Obsolete the old term by following instructions found in obsoletion how to guide..</li> <li>Ensure that you add a <code>rdfs:comment</code> that states that term was duplicated and to refer to the new new.</li> <li>Ensure that you add a <code>term replaced by</code> annotations as per the instructions and add the winning merged term.</li> </ul> </li> <li>Synchronize the reasoner and make sure there are no terms that have identical definitions as a result of the merge. These are displayed with an 'equivalent' sign <code>\u2261</code> in the class hierarchy view on the left hand panel.</li> <li>Save changes.</li> </ol> <p>See Daily Workflow section for commit, push and merge instructions.</p>"},{"location":"howto/merge-terms/#merge-using-owltools","title":"Merge using owltools","text":"<p>To use owltools will need to have Docker installed and running (see instructions here).</p> <p>This is the workflow that is used in Mondo.</p> <ol> <li>Create a branch and name it issue-### (for example issue-2864)</li> <li>Open Protege</li> <li>Prepre the owltools command:    <code>owltools --use-catalog mondo-edit.obo --obsolete-replace [CURIE 1] [CURIE 2] -o -f obo mondo-edit.obo</code></li> </ol> <p>CURIE 1 = term to be obsoleted CURIE 2 = replacement term (ie term to be merged with)</p> <p>For example: If to merge MONDO:0023052 ectrodactyly polydactyly with MONDO:0009156 ectrodactyly-polydactyly syndrome, the command is:</p> <p><code>owltools --use-catalog mondo-edit.obo --obsolete-replace MONDO:0023052 MONDO:0009156 -o -f obo mondo-edit.obo</code></p> <ol> <li>In Terminal, navigate to your ontology directory: src/ontology</li> <li>Run your owltools command</li> <li>Check the output in GitHub desktop</li> <li>Open a new version of your ontology edit file in Protege</li> <li>Search for the term that was obsoleted</li> <li>Add 'term tracker item' (type xsd:anyURI) with a link to the GitHub issue that requested the obsoletion.</li> <li>Add an obsoletion reason: use the annotation property 'has obsolescence reason' and write 'terms merged' in the literal field.</li> <li>Search for the 'term replaced by' term</li> <li>Delete the old ID</li> <li>Review the annotations to ensure there are no duplicate annotations. If there are, they should be merged.</li> <li>Review the subClassOf assertions, and make sure there are no duplicates. If there are, they should be merged.</li> <li>When reviewing the diff, make sure there is not an Alt ID. The diff should only show additions to the merged term and the obsoletion</li> </ol> <p>TROUBLESHOOTING: Travis/Jenkins errors</p> <ol> <li>Merging a term that is used as 'replaced by' for an obsolete term</li> </ol> <p><code>:: ERROR: ID-mentioned-twice:: GO:0030722      :: ERROR: ID-mentioned-twice:: GO:0048126        GO:0030722 :: ERROR: has-definition: missing definition for id</code></p> <p>The cause of this error is that Term A (GO:0048126) was obsoleted and had replace by Term B (GO:0030722). The GO editor tried to merge Term B into a third term term C (GO:0007312). The Jenkins checkk failed because 'Term A replaced by' was an alternative_id rather than by a main_id. Solution: In the ontology, go to the obsolete term A and replace the Term B by term C to have a primary ID as the replace_by.</p>"},{"location":"howto/obsolete-term/","title":"Obsoleting an Existing Ontology Term","text":"<p>See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions.</p> <p>Warning: Every ontology has their procedures on how they obsolete terms (eg notice periods, notification emails, to_be_obsolete tags, etc.), this how-to guide only serves as a guide on how obsolete a term directly on protege.</p> <p>For instructions on how to merge terms (i.e., replace a term with another term in the ontology), see instructions here.</p>"},{"location":"howto/obsolete-term/#pre-obsoletion-process-or-basic-obsoletion-etiquette","title":"PRE OBSOLETION PROCESS (or basic obsoletion etiquette)","text":"<ol> <li> <p>Check if the term (or any of its children) is being used for annotation:</p> </li> <li> <p>Go to your ontology browser of choice, search for the term, either by label or ID</p> </li> <li>See which other ontologies use the to be obsolete term</li> <li> <p>Notify affected groups (usually by adding an issue in their tracker)</p> </li> <li> <p>Check if the term is used elsewhere in the ontology</p> </li> <li>In Prot\u00e9g\u00e9, navigate to the term to be obsolete and go to the 'Usage' tab to see if that ID is used elsewhere.</li> <li>If the term is a parent to other terms or is used in logical definitions, make sure that another term replaces the obsolete term</li> </ol>"},{"location":"howto/obsolete-term/#obsoletion-process-manual","title":"OBSOLETION PROCESS (Manual)","text":"<p>Warning: some ontologies give advance notice on terms that will be obsoleted through the annotation 'scheduled for obsoletion on or after' instead of directly obsoleting the term. Please check with the conventions of your ontology before obsoleting a term.</p> <ol> <li>Navigate to the term to be obsoleted.</li> </ol> <p></p> <ol> <li>Select Edit &gt; Deprecate entity...</li> </ol> <p></p> <ol> <li>A deprecation wizard will pop up, in here, select GO style, and select continue (note this is specifc to GO style ontologies, if you are working with an OBI style ontology, there is an option for that too, if not use basic. For this how to, we will follow GO style)</li> </ol> <p></p> <ol> <li>Next, enter your reason for deprecation. For this, we advice for you to enter the github issue. (eg https://github.com/obophenotype/cell-ontology/issues/####) This will appear as a rdfs:comment</li> </ol> <p></p> <ol> <li>Next enter a replacement entity if there is one. This will automatically replace axioms in the ontology with the term, and add an 'item replaced by' axiom on the obsolete term.</li> </ol> <p></p> <ol> <li>Your obsolete term should now be stripped of its logical axioms and should look similar to the figure below.</li> </ol> <p></p> <ol> <li>Add any additional annotations needed - this is specific to ontologies and you should consult the conventions of the ontology you are working on.</li> </ol> <p>Examples of additional annotations to add:</p> <ul> <li> <p>IAO:0000233 \"term tracker item\" (type xsd:anyURI) - link to GitHub issue</p> </li> <li> <p>IAO:0000231 \"has_obsolence_reason\" (type: IAO:0000225 \"obsolescence reason specification\") - its value should be an IRI taken from the table below</p> </li> <li> <p>add \u2018OBSOLETE.\u2019 to the term definition: In the 'Description' window, click on the <code>o</code> on the right-hand side of the definition entry. In the resulting window, in the Literal tab, at the beginning of the definition, type: <code>OBSOLETE.</code></p> </li> <li> <p>if the obsoleted term was not replaced by another term in the ontology, but there are existing terms that might be appropriate for annotation, add those term IDs in the 'consider' tag: In the 'Annotations' window, select <code>+</code> to add an annotation. In the resulting menu, select <code>consider</code> and enter the ID of the replacement term.</p> <p>NOTE: Here you have to add the ID of the entity as an <code>xsd:string</code>, e.g. GO:0005819, not the term label.</p> </li> </ul> IRI CURIE Label Definition Editor note obo:IAO_0000103 IAO:0000103 failed exploratory term The term was used in an attempt to structure part of the ontology but in retrospect failed to do a good job Terms created as placeholders with the expectation of being permanent should instead be deprecated using the \"placeholder removed\" (obo:IAO_0000226) justification. obo:IAO_0000226 IAO:0000226 placeholder removed The term was created to temporarily stand in for a semantic purpose, but is no longer needed, typically due to another permanent term being defined. This reason is appropriate for a temporary term even if no permanent term is defined; for example if the need for the term no longer exists. If a permanent (replacement) term is defined, the deprecated term should reference the replacement with IAO:0100001. If no replacement is defined, it may be appropriate to reference alternative terms with oboInOwl:consider. obo:IAO_0000227 IAO:0000227 terms merged The term has been combined with one or more other terms to create a more encompassing (merged) term. The deprecated term should reference the merged term with obo:IAO_0100001. The replacement term should reference the original terms as sources using obo:RO_0001000 (derives_from). An editor note should explain what were the merged terms and the reason for the merge. obo:IAO_0000228 IAO:0000228 term imported The term has been replaced by a term imported from another ontology. This is to be used when the original term has been replaced by a term imported from an other ontology. The deprecated term should reference the imported term with obo:IAO_0100001. obo:IAO_0000229 IAO:0000229 term split The term has been split into two or more new terms. This is to be used when a term has been split in two or more new terms. An editor note should indicate the reason for the split. The deprecated term should reference the replacement terms with obo:IAO_0100001. obo:OMO_0001000 OMO:0001000 out of scope The term was added to the ontology on the assumption it was in scope, but it turned out later that it was not. This obsolesence reason should be used conservatively. Typical valid examples in disease ontologies are: unnecessary class grouping; mistaked creation of a disease associated to a phenotype."},{"location":"howto/obsolete-term/#obsolete-a-class-using-protege-make-entity-obsolete-function","title":"Obsolete a class (using Protege 'Make entity obsolete' function)","text":"<ol> <li>Navigate to the term to be obsoleted.</li> <li>In the Protege edit menu-&gt; Make entity obsolete</li> <li>Prepend the definition with OBSOLETE. For example, OBSOLETE. Chronic form of myeloproliferative neoplasm.</li> <li> <p>Add a statement about why the term was made obsolete: In the 'Annotations' window, select <code>+</code> to add an annotation. In the resulting menu, select <code>rdfs:comment</code> and select Type: <code>Xsd:string</code>.     Consult the wiki documentation for suggestions on standard comments:</p> <pre><code> - [http://wiki.geneontology.org/index.php/Curator_Guide:_Obsoletion](http://wiki.geneontology.org/index.php/Curator_Guide:_Obsoletion)\n\n - [http://wiki.geneontology.org/index.php/Obsoleting_GO_Terms](http://wiki.geneontology.org/index.php/Obsoleting_GO_Terms)\n\n - [http://wiki.geneontology.org/index.php/Editor_Guide](http://wiki.geneontology.org/index.php/Editor_Guide)\n</code></pre> </li> <li> <p>If the obsoleted term was replaced by another term in the ontology: In the 'Annotations' window, select <code>+</code> to add an annotation. In the resulting menu, select <code>term replaced by</code> and enter the ID of the replacement term.</p> </li> <li> <p>If the obsoleted term was not replaced by another term in the ontology, but there are existing terms that might be appropriate for annotation, add those term IDs in the 'consider' tag: In the 'Annotations' window, select <code>+</code> to add an annotation. In the resulting menu, select <code>consider</code> and enter the ID of the replacement term.</p> <p>NOTE: Here you have to add the ID of the entity as an <code>xsd:string</code>, e.g. GO:0005819, not the term label.</p> </li> <li> <p>Add any additional annotations needed - this is specific to ontologies and you should consult the conventions of the ontology you are working on.</p> </li> </ol> <p>Examples of additional annotations to add:</p> <ul> <li>IAO:0000233 term tracker item (type xsd:anyURI) - link to GitHub issue</li> <li>has_obsolence_reason</li> </ul> <p>See Daily Workflow section for commit, push and merge instructions.</p>"},{"location":"howto/odk-add-orcidio-module/","title":"Import ORCIDIO","text":""},{"location":"howto/odk-add-orcidio-module/#adding-an-orcidio-import-to-your-ontology-with-odk","title":"Adding an ORCIDIO import to your ontology with ODK","text":"<p>The Open Researcher and Contributor Identifier (ORCID) is a global, unambiguous way to identify a researcher. ORCID URIs (e.g., https://orcid.org/0000-0003-4423-4370) can therefore be used to unambigously and actionably attribute various aspects of ontology terms in combination with DC Terms or IAO predicates. However, URIs themselves are opaque and it is difficult to disambiguate to which person an ORCID corresponds when browsing an ontology (e.g., in Prot\u00e9g\u00e9).</p> <p>ORCIDIO is an ontology that declares ORCID URIs as named individuals and associates basic metadata (e.g., name, description) to each such that tools like Prot\u00e9g\u00e9 can display a human-readable label rather than the URI itself as in the following example.</p> <p></p> <p>In this guide, we discuss how to add ORCIDIO to your ODK setup.</p>"},{"location":"howto/odk-add-orcidio-module/#1-include-orcidio-as-an-import-into-the-odk-config-file","title":"1. Include ORCIDIO as an import into the ODK config file","text":"<p>In your ODK configuration (e.g. <code>src/ontology/myont-odk.yaml</code>), add the following to the <code>import_group</code>:</p> <pre><code>import_group:\n  annotation_properties:\n    - rdfs:label\n    - dc:description\n    - dc:source\n    - IAO:0000115\n  products:\n    - id: orcidio\n      mirror_from: https://w3id.org/orcidio/orcidio.owl\n      module_type: filter\n      base_iris:\n        - https://orcid.org/\n</code></pre> <p>The list of annotation properties, in particular <code>dc:source</code>, is important for the <code>filter</code> module to work (ORCIDIO relies heavily on axiom annotations for provenance).</p>"},{"location":"howto/odk-add-orcidio-module/#2-update-your-catalog","title":"2. Update your catalog","text":"<p>TODO: \"as usual\" should be re-written to cross-link to another guide about updating the catalog (or don't say as usual to keep this more self-contained) As usual, add a statement into your catalog (<code>src/ontology/catalog-v001.xml</code>):</p> <pre><code> &lt;uri name=\"http://purl.obolibrary.org/obo/ro/imports/orcidio_import.owl\" uri=\"imports/orcidio_import.owl\"/&gt;\n</code></pre>"},{"location":"howto/odk-add-orcidio-module/#3-update-the-edit-file","title":"3. Update the edit file","text":"<p>TODO: \"as usual\" should be re-written to cross-link to another guide about updating the edit file (or don't say as usual to keep this more self-contained) As usual, add an imports declaration to your edit file (<code>src/ontology/myont-edit.owl</code>):</p> <pre><code>Import(&lt;http://purl.obolibrary.org/obo/ro/imports/orcidio_import.owl&gt;)\n</code></pre> <p>TODO: link to explanation of base merging strategy Note: This is not necessary when using the <code>base merging</code> strategy (you will know what this means when you do use it).</p>"},{"location":"howto/odk-add-orcidio-module/#4-configure-your-seed","title":"4. Configure your seed:","text":"<p>Add a new SPARQL query: <code>src/sparql/orcids.sparql</code>. This is used to query for all ORCIDs used in your ontology.</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT DISTINCT ?orcid\nWHERE {\n  VALUES ?property {\n    &lt;http://purl.org/dc/elements/1.1/creator&gt;\n    &lt;http://purl.org/dc/elements/1.1/contributor&gt;\n    &lt;http://purl.org/dc/terms/creator&gt;\n    &lt;http://purl.org/dc/terms/contributor&gt; \n  }\n  ?term ?property ?orcid . \n  FILTER(isIRI(?term))\n}\n</code></pre> <p>Next, overwrite your ORCID seed generation to using this query by adding the following to your <code>src/ontology/myont.Makefile</code> (not <code>Makefile</code>!):</p> <pre><code>$(IMPORTDIR)/orcidio_terms_combined.txt: $(SRCMERGED)\n    $(ROBOT) query -f csv -i $&lt; --query ../sparql/orcids.sparql $@.tmp &amp;&amp;\\\n    cat $@.tmp | sort | uniq &gt;  $@\n</code></pre> <p>For your specific use-case, it may be necessary to tweak this SPARQL query, for example if your ORCIDs are used on axiom annotation level rather than entity annotation level.</p>"},{"location":"howto/odk-add-orcidio-module/#5-updating-config-and-orcidio","title":"5. Updating Config and ORCIDIO","text":"<p>Now run to apply your ODK changes:</p> <pre><code>sh run.sh make update_repo\n</code></pre> <p>This will update a number of files in your project, such as the autogenerated <code>Makefile</code>.</p> <p>Lastly, update your ORCIDIO import to apply the changes:</p> <pre><code>sh run.sh make refresh-orcidio\n</code></pre> <p>Commit all the changes to a branch, wait for continuous integration to finish, and enjoy your new ORCIDIO import module.</p>"},{"location":"howto/odk-create-repo/","title":"Creating a new Repository with the Ontology Development Kit","text":"<p>This is instructions on how to create an ontology repository in GitHub. This will only need to be done once per project. You may need assistance from someone with basic unix knowledge in following instructions here.</p> <p>We will walk you though the steps to make a new ontology project</p>"},{"location":"howto/odk-create-repo/#1-install-requirements","title":"1. Install requirements","text":"<ul> <li>docker: Install Docker and make sure its runnning properly, for example by typing <code>docker ps</code> in your terminal or command line (CMD). If all is ok, you should be seeing something like:</li> </ul> <pre><code>CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n</code></pre> <ul> <li>git, for example bundled with GitHub Desktop</li> </ul>"},{"location":"howto/odk-create-repo/#2-download-the-wrapper-script-and-pull-latest-odk-version","title":"2. Download the wrapper script and pull latest ODK version","text":"<ul> <li>Linux/Mac: seed-via-docker.sh</li> <li>PC: seed-via-docker.bat</li> <li>Make sure to save the wrapper script in your working directory and that the filetype is correct.</li> <li>You should have git installed - for the repo command to work perfectly, it requires a <code>.gitconfig</code> file in your user directory!</li> <li>First, make sure you have Docker running (you will see the Docker whale in your toolbar on a Mac)</li> <li>To make sure you have the latest version of the ODK installed, run in the command line</li> </ul> <p><code>docker pull obolibrary/odkfull</code></p> <p>NOTE The very first time you run this it may be slow, while docker downloads necessary images. Don't worry, subsequent runs should be much faster!</p> <p>NOTE Windows users, occasionally it has been reported that files downloaded on a Windows machine get a wrong file ending, for example <code>seed-via-docker.bat.txt</code> instead of <code>seed-via-docker.bat</code>, or, as we will see later, <code>project.yaml.txt</code> instead of <code>project.yaml</code>. If you have problems, double check your files are named correctly after the download!</p>"},{"location":"howto/odk-create-repo/#3-run-the-wrapper-script","title":"3. Run the wrapper script","text":"<p>You can either pass in a configuration file in YAML format that specifies your ontology project setup, or you can pass arguments on the command line. You can use <code>dir</code> in your command line on PC to ensure that your wrapper script, .gitconfig, and project.yaml (if you so choose) are all in the correct directory before running the wrapper script.</p>"},{"location":"howto/odk-create-repo/#unix-max-linux","title":"Unix (Max, Linux)","text":"<p>Passing arguments on the command line:</p> <pre><code>./seed-via-docker.sh -d po -d ro -d pato -u cmungall -t \"Triffid Behavior ontology\" triffo\n</code></pre> <p>Using a the predefined project.yaml file:</p> <pre><code>./seed-via-docker.sh -C examples/triffo/project.yaml\n</code></pre>"},{"location":"howto/odk-create-repo/#windows","title":"Windows","text":"<p>Passing arguments on the command line:</p> <pre><code>seed-via-docker.bat -d po -d ro -d pato -u cmungall -t \"Triffid Behavior ontology\" triffo\n</code></pre> <p>Using a the predefined project.yaml config file:</p> <pre><code>seed-via-docker.bat -C project.yaml\n</code></pre>"},{"location":"howto/odk-create-repo/#general-instructions-for-both-linux-and-windows","title":"General instructions for both Linux and Windows","text":"<ul> <li>Instead of <code>-u cmungall</code> you should be using your own username (i.e. <code>-u nico</code>), for example for your GitHub or GitLab hosting sites.</li> <li>You can add a -c (lowercase) just before the -C (capital c) in the command to first delete any previous attempt to generate your ontology with the ODK, and then replaces it with a completely new one. So, <code>-c</code> stands for <code>clean</code> or \"clean up previous attempts before running again\" and <code>-C</code> stands for \"the next parameter is the relative path to my config file\".</li> <li>In general, we now always recommend the use of config files. The ODK has a rich set of configuration options, most of which can only be set through the config file, but in general the config also serves as documentation and will help with updating your ontology at later stages.   To create a config file, you can download for example project.yaml by clicking on the link and then typing <code>command+s</code> on Mac or <code>ctrl+s</code> on Windows to save it in the same directory as your <code>seed-via-docker</code> script.   Then you can open the file with a text editor like Notepad++, Atom, Sublime or even nano, and adapt it to your project. Other more comprehensive examples can be found here.</li> </ul> <p>This will create your starter files in <code>target/triffid-behavior-ontology</code>. It will also prepare an initial release and initialize a local repository (not yet pushed to your Git host site such as GitHub or GitLab).</p>"},{"location":"howto/odk-create-repo/#problems","title":"Problems?","text":"<p>There are three frequently encountered problems at this stage:</p> <ol> <li>No local Git configuration</li> <li>Spaces is user path</li> <li>During download, your filenames got changed (Windows)</li> </ol>"},{"location":"howto/odk-create-repo/#no-local-git-configuration","title":"No local Git configuration","text":"<p>On GNU/Linux and macOS, the <code>seed-via-docker.sh</code> script uses <code>git config</code> to automatically obtain the username and email to use in the initial commit. That information is typically stored in a <code>~/.gitconfig</code> file in your user account\u2019s home directory.</p> <p>If you do not have such a file, you may create and initialise it with:</p> <pre><code>git config --global user.name Alice\ngit config --global user.email alice@example.org\n</code></pre> <p>If for some reason you do not wish to set a Git username and email globally, or you wish to use a different username and email than those already set in your <code>~/.gitconfig</code> file, you may explicitly pass a username and an email when you call the <code>seed-via-docker.sh</code> script as follows:</p> <pre><code>ODK_GITNAME=Alice ODK_GITEMAIL=alice@example.org ./seed-via-docker.sh ...\n</code></pre> <p>On Windows, the <code>seed-via-docker.bat</code> script expects to find the Git configuration file in <code>%userprofile%/.gitconfig. This is _not_ configurable, so if your Git configuration file is located elsewhere, you need to change the path in the</code>seed-via-docker.bat` script directly. The script looks like the following:</p> <pre><code>docker run -v %userprofile%/.gitconfig:/root/.gitconfig -v %cd%:/work -w /work --rm -ti obolibrary/odkfull /tools/odk.py seed %*\n</code></pre> <p><code>%userprofile%/.gitconfig</code> should be changed to the correct path of your local <code>.gitconfig</code> file.</p>"},{"location":"howto/odk-create-repo/#spaces-is-user-path","title":"Spaces is user path","text":"<p>We have had reports of users having trouble if there paths (say, <code>D:\\data</code>) contain a space symbol, like <code>D:/Dropbox (Personal)</code> or similar. In this case, we recommend to find a directory you can work in that does not contain a space symbol.</p> <p>You can customize at this stage, but we recommend to first push the changes to you Git hosting site (see next steps).</p>"},{"location":"howto/odk-create-repo/#during-download-your-filenames-got-changed-windows","title":"During download, your filenames got changed (Windows)","text":"<p>Windows users, occasionally it has been reported that files downloaded on a Windows machine get a wrong file ending, for example <code>seed-via-docker.bat.txt</code> instead of <code>seed-via-docker.bat</code>, or, as we will see later, <code>project.yaml.txt</code> instead of <code>project.yaml</code>. If you have problems, double check your files are named correctly after the download!</p>"},{"location":"howto/odk-create-repo/#4-push-to-git-hosting-website","title":"4. Push to Git hosting website","text":"<p>The development kit will automatically initialize a git project, add all files and commit.</p> <p>You will need to create a project on you Git hosting site.</p> <p>For GitHub:</p> <ol> <li>Go to: https://github.com/new</li> <li>The owner MUST be the org you selected with the <code>-u</code> option. The name MUST be the one you set with <code>-t</code>, just with lower case letters and dashes instead of spaces. In our example above, the name \"Triffid Behavior Ontology\" translates to <code>triffid-behavior-ontology</code>.</li> <li>Do not initialize with a README (you already have one)</li> <li>Click Create</li> <li>See the section under \"\u2026or push an existing repository from the command line\"</li> </ol> <p>For GitLab:</p> <ol> <li>Go to: https://gitlab.com/projects/new</li> <li>The owner MUST be the org you selected with the <code>-u</code> option. The name MUST be the one you set with <code>-t</code>.</li> <li>Do not initialize with a README (you already have one)</li> <li>Click 'Create project'</li> <li>See the section under \"Push an existing Git repository\"</li> </ol> <p>Follow the instructions there. E.g. (make sure the location of your remote is exactly correct!).</p> <pre><code>cd target/triffo\ngit remote add origin https://github.com/matentzn/triffid-behavior-ontology.git\ngit branch -M main\ngit push -u origin main\n</code></pre> <p>Note: you can now mv <code>target/triffid-behavior-ontology</code> to anywhere you like in your home directory. Or you can do a fresh checkout from github.</p>"},{"location":"howto/odk-create-repo/#alternative-recommendation-for-github-by-matentzn","title":"Alternative recommendation for GitHub by @matentzn","text":"<p>I generally feel its easier and less error prone to deviate from the standard instructions above. I keep having problems with git, passwords, typose etc, so I tend to do it, inofficially, as follows:</p> <ol> <li>When my repo is created I go to my GitHub Desktop</li> <li>I then do File &gt; Add local repository, and select the directory which contains my newly created repo (e.g. <code>target/triffo</code>).</li> <li>I then Click on \"Publish repository\".</li> <li>If I want the code to be public, I deselect \"Keep this code private\". By default, the repo will be uploaded to my own user profile on GitHub, but I can also select another Organization I have access to in the respective Dropdown menu.</li> <li>NOTE: there seem to be some issues with pushing a GitHub Workflow file recently - you may be asked by GitHub Desktop to provide an additional permission to push the Workflow file.</li> </ol>"},{"location":"howto/odk-create-repo/#next-steps-edit-and-release-cycle","title":"Next Steps: Edit and release cycle","text":"<p>In your repo you will see a README-editors.md file that has been customized for your project. Follow these instructions.</p>"},{"location":"howto/odk-create-repo/#obo-library-metadata","title":"OBO Library metadata","text":"<p>The assumption here is that you are adhering to OBO principles and want to eventually submit to OBO. Your repo will contain stub metadata files to help you do this.</p> <p>You can create pull requests for your ontology on the OBO Foundry. See the <code>src/metadata</code> file for more details.</p> <p>For more documentation, see http://obofoundry.org</p>"},{"location":"howto/odk-create-repo/#additional","title":"Additional","text":"<p>You will want to also:</p> <ul> <li>enable GitHub actions</li> </ul> <p>See the README-editors.md file that has been generated for your project.</p>"},{"location":"howto/odk-migrate-to-odk/","title":"How to Migrate an Existing Ontology Repository to the Ontology Development Kit","text":"<p>These are instructions on how to migrate an existing ontology repository hosted on GitHub or GitLab to an ODK based workflow. They are heavily based on (and are often identical to) the instructions provided in Creating a new Repository with the Ontology Development Kit. You may need assistance from someone with basic unix knowledge in following the instructions here.</p>"},{"location":"howto/odk-migrate-to-odk/#1-install-requirements","title":"1. Install requirements","text":"<ul> <li>Getting set up with Docker and the Ontology Development Kit.</li> <li>Install Prot\u00e9g\u00e9</li> <li>OPTIONAL (Beta Testing): Install the ODK runner for   better user experience. In the Linux shell you can do this with:<ul> <li><code>mkdir ~/tools</code> (to create a ~/tools directory)</li> <li><code>wget wget https://github.com/gouttegd/odkrunner/releases/latest/download/odkrun-linux -O ~/tools/odk -O ~/tools/odk</code>   (to download and save the ODK runner binary file into the ~/tools directory.)</li> <li><code>chmod +x ~/tools/odk</code> (to make the binary file executable)</li> <li><code>export PATH=$PATH:~/tools</code> (to add the ~/tools folder to your path for the current   session)<ul> <li>Assuming you use Bash as your shell (the Ubuntu default), you could add the ~/tools folder permanently to your path in your Bash config file with:   <pre><code>echo \"\n# Adding ODK runner script living in ~/tools to PATH\nexport PATH=\\\"\\$PATH:~/tools\\\"\" &gt;&gt; ~/.bashrc\n</code></pre>   If you\u2019re using another shell, check its documentation to know how to add a directory to its executable search path.</li> </ul> </li> </ul> </li> </ul>"},{"location":"howto/odk-migrate-to-odk/#2-get-the-ontology-you-want-to-migrate-into-the-right-format","title":"2. Get the ontology you want to migrate into the right format","text":"<p>CONTEXT: In the ODK, the ontology files being released to the public are not edited manually. Instead, there exists an \"editor file\" of your ontology, which, as the name suggests, is the one you edit in Prot\u00e9g\u00e9, your text editor or via ROBOT templates, the one where you import external ontologies or separate ontology modules via import statements. From this editor file, the public release files will be created automatically by ODK.</p> <p>Now, your current ontology file will most likely be serialized in RDF/XML (.owl) or Turtle (.ttl), but the editor file in ODK is serialized in functional syntax (.ofn). For migrating to ODK, it thus is best to convert your current ontology file to functional syntax as well. There are two benefits for doing this. For one, it will allow you to just copy and paste the content of your current ontology file to the editor file that will be created by ODK in one of the next steps. Second, the compact declaration section in the beginning of the OFN serialization allows you to quickly see external terms (those that have been reused from other ontologies), which will help you to build your import modules.</p> <p>The conversion to OFN can best be done with ROBOT, which is provided in ODK. If you installed the ODK runner previously, you can simply run the following command (replace [PATH TO YOUR \u201cOLD\u201d ONTOLOGY FILE] with the actual path to your \u201cold\u201d ontology file):</p> <pre><code>odk robot convert \\\n--input [PATH TO YOUR \u201cOLD\u201d ONTOLOGY FILE] \\\n--format ofn --output [PATH TO YOUR \u201cOLD\u201d ONTOLOGY FILE].ofn\n</code></pre> <ul> <li>If you didn't install the ODK runner, you need to download the ODK wrapper script into your   working directory with</li> <li><code>wget https://oboacademy.github.io/obook/resources/odk.sh</code>. Then check that it works by   running <code>sh odk.sh robot --version</code>, which should return the version of ROBOT (e.g.: <code>ROBOT  version 1.9.6</code>). Now, you can convert your current ontology file with:</li> </ul> <pre><code>sh odk.sh robot convert \\\n--input [PATH TO YOUR \u201cOLD\u201d ONTOLOGY FILE] \\\n--format ofn --output [PATH TO YOUR \u201cOLD\u201d ONTOLOGY FILE].ofn\n</code></pre> <p>In the below example of the Chemical Methods Ontology (CHMO), you can see that it declares terms from the following namespaces:</p> <ul> <li>classes from BFO,FIX, CHEBI, IAO, MS, OBSC, OBI and REX</li> <li>object properties from BFO, IAO, OBI, and RO</li> <li>and annotation properties from various other ontologies.</li> </ul> <pre><code>Declaration(Class(obo:BFO_0000140)\nDeclaration(Class(obo:CHEBI_50803))\nDeclaration(Class(obo:CHMO_0010020))\nDeclaration(Class(obo:FIX_0001068))\nDeclaration(Class(obo:IAO_0000140))\nDeclaration(Class(obo:MS_1000073))\nDeclaration(Class(obo:OBCS_0000058))\nDeclaration(Class(obo:OBI_0600014))\nDeclaration(Class(obo:REX_0000188))\nDeclaration(ObjectProperty(obo:BFO_0000052))\nDeclaration(ObjectProperty(obo:CHMO_0002922))\nDeclaration(ObjectProperty(obo:IAO_0000136))\nDeclaration(ObjectProperty(obo:OBI_0000293))\nDeclaration(ObjectProperty(obo:OBI_0000299))\nDeclaration(ObjectProperty(obo:OBI_0000312))\nDeclaration(ObjectProperty(obo:OBI_0000417))\nDeclaration(ObjectProperty(obo:RO_0002411))\nDeclaration(ObjectProperty(obo:RO_0002418))\nDeclaration(AnnotationProperty(obo:BFO_0000179))\nDeclaration(AnnotationProperty(obo:BFO_0000180))\nDeclaration(AnnotationProperty(obo:IAO_0000115))\nDeclaration(AnnotationProperty(chmo:isDefinedBy))\nDeclaration(AnnotationProperty(chmo:non-mining_synonym))\nDeclaration(AnnotationProperty(dc:title))\nDeclaration(AnnotationProperty(terms:license))\nDeclaration(AnnotationProperty(oboInOwl:SynonymTypeProperty))\nDeclaration(AnnotationProperty(rdfs:label))\nDeclaration(AnnotationProperty(owl:deprecated))\n</code></pre> <p>Based on this and assuming that all native terms within CHMO use the CHMO namespace (obo:CHMO_), we can derive that CHMO needs to have import modules for BFO, RO, CHEBI, IAO, OBI, MS, OBSC, REX and FIX to properly import the terms from these ontologies. To reuse the standard OBO annotation properties, like IAO:0000115 or oboInOwl:SynonymTypeProperty, you should import the complete OBO Metadata Ontology instead of re-declaring them in your editor file.</p>"},{"location":"howto/odk-migrate-to-odk/#3-create-your-odk-config-file","title":"3. Create your ODK config file","text":"<p>Create a config file (aka project.yaml) in your working directory, according to this schema, which is usually named yourOntologyID-odk.yaml.</p> <p>For example, if we want to migrate CHMO we would create a file called chmo-odk.yaml with this content to start out with:</p> <pre><code>## ontology metadata ##\nid: chmo\ntitle: Chemical Methods Ontology\ndescription: \"CHMO, the chemical methods ontology, describes methods used to collect data in chemical experiments, such as mass spectrometry and electron microscopy prepare and separate material for further analysis, such as sample ionisation, chromatography, and electrophoresis synthesise materials, such as epitaxy and continuous vapour deposition It also describes the instruments used in these experiments, such as mass spectrometers and chromatography columns. It is intended to be complementary to the Ontology for Biomedical Investigations (OBI).\"\ncontact:  batchelorc [at] rsc [.] org\ncreators:\n  - Royal Society of Chemistry\nlicense: https://creativecommons.org/licenses/by/4.0/\n\n## general ODK Settings ##\nrepo: rsc-cmo\ngit_main_branch: master\ngithub_org: rsc-ontologies\nci:\n  - github_actions\ndocumentation:\n  documentation_system: mkdocs\nexport_formats:\n  - owl\n  - obo\nreasoner: ELK\nrelease_artefacts:\n  - full\n  - base\nrobot_java_args: '-Xmx8G'  # max RAM to be used by Robot in ODK\n\n## CHMO specific Settings ##\nimport_group:\n  products:\n    - id: bfo\n      module_type: mirror\n    - id: ro\n      use_base: true\n      slme_individuals: exclude\n    - id: omo\n      module_type: mirror\n    - id: iao\n      make_base: true\n    - id: obi\n    - id: ms\n    - id: fix\n    - id: rex\n    - id: obsc\n    - id: chebi\n      is_large: true\n      use_gzipped: true\n      make_base: true\n</code></pre>"},{"location":"howto/odk-migrate-to-odk/#4-seeding-your-odk-repository-locally","title":"4. Seeding your ODK Repository Locally","text":"<p>To create a new ODK repository within your working directory based on the project YAML file you just created, you just have to call the following commands (replacing [PATH TO YOUR PROJECT YAML FILE] with the actual path to your project YAML file).</p> <ul> <li> <p>Using the seed-via-docker wrapper script (which is downloaded in this call, and thus   needs internet connection):</p> <pre><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/INCATools/ontology-development-kit/master/seed-via-docker.sh)\" --clean -C [PATH TO YOUR PROJECT YAML]\n</code></pre> </li> <li> <p>Using ODK runner:</p> <pre><code>odk seed --clean -C [PATH TO YOUR PROJECT YAML]\n</code></pre> </li> </ul> <p>This will create a folder called target in which you will find your new ODK repository structure with all the necessary files and folders as well as some first GIT commits already made. For more details, see also this OBOOK section.</p> <p>NOTE: This step may take a while and might seem to you that it stalled, because there is no output on the console when the script downloads the ontologies you specified in your import_group in the background. So, if you need to import many and/or large ontologies, as is the case in our example with ChEBI, be patient and wait until the script is done. Also, the target folder is not owned by the Linux user who called this command but by the root user. You thus might get a rights permission error, when you try to rerun the script in case something went wrong and would have to delete the target folder before a rerun with root permission.</p>"},{"location":"howto/odk-migrate-to-odk/#5-push-your-odk-repository-to-github-gitlab","title":"5. Push your ODK Repository to GitHub / GitLab","text":"<p>If you want to create a brand-new repository on GitHub / GitLab, you can just follow the steps explained here.</p>"},{"location":"howto/odk-migrate-to-odk/#6-make-a-new-branch-to-work-in","title":"6. Make a new Branch to Work in","text":"<p>If you want to keep your existing GitHub / GitLab repository, used your local clone as working directory for the above steps and have not done the following already, then you should now:</p> <ul> <li>Make a new branch and check it out.</li> <li>Think about how to integrate existing files and folders of your current repository into the   new ODK repo structure. (E.g. you might want to archive previous release files in a   dedicated folder or might want to integrate documentation related files into the <code>docs</code> folder   created by ODK to automatically build a GitHub pages documentation.)</li> <li>Copy all but the .git folder from the directory created by the seeding step (e.g.   <code>target/chmo/</code>) into the top level of your new branch.</li> </ul> <p>Even if you created a completely new repository, you should now do the next steps in a new branch.</p>"},{"location":"howto/odk-migrate-to-odk/#6-migrating-the-content-into-the-editor-file","title":"6. Migrating the Content into the Editor File","text":"<ul> <li>Open the editor file (.e.g. <code>target/chmo/src/ontology/chmo-edit.owl</code>) in your text editor and Prot\u00e9g\u00e9.</li> <li>From your previously converted \u201cold\u201d OFN ontology file, cut all declared terms (classes, object, data &amp;   annotation properties and individuals) that use the namespace of your ontology and paste them into the editor file.<ul> <li>Start by cutting the term declarations, then the classes, and so on.</li> <li>Check that you make no mistakes in these cutting &amp; paste steps. Whenever you save the editor file in your    text editor, Prot\u00e9g\u00e9 will ask you to reload the editor file. Click yes, and if this causes an error, you    did something wrong.</li> </ul> </li> </ul>"},{"location":"howto/odk-migrate-to-odk/#7-building-your-import-modules","title":"7. Building your Import Modules","text":"<p>In the term declaration section of your \u201cold\u201d OFN ontology file there should now only be those terms left, that are external and which need to be imported via import modules. To build the latter, you first need to provide these terms in the empty text files that were created by ODK in the seeding step in the <code>src/ontology/imports</code> folder (e.g. iao_terms.txt). Each term that should be imported must be listed in its respective text file as a new row via its term IRI. Optionally, for better human-readbility, you can provide a label after the term IRI via a comment. E.g. if you want to import the IAO object property \u201cis about\u201d you need to have this row in your iao_terms.txt:</p> <pre><code>http://purl.obolibrary.org/obo/IAO_0000136 # is about\n</code></pre> <p>Assuming you have specified all terms you want to import in their respective import text file, you can delete  your \"old\" OFN ontology file now, as it should be almost empty anyway and has outlived its purpose. </p> <p>Now, it is time to build your import modules from the text files you just populated. There are two ways of doing  this, as described  in this section of the OBOOK.</p> <ul> <li>You can either call the ODK command with which all import modules are build/updated at once using:</li> </ul> <pre><code>../src/ontology/$ sh run.sh make refresh-imports\n</code></pre> <ul> <li>or you can only update a specific import module by calling:</li> </ul> <pre><code>../src/ontology/$ sh run.sh make refresh-%\n</code></pre> <p>where the \u201c%\u201d is the placeholder for the id of the ontology from which you import. E.g. to build/update your   IAO module, you\u2019d have to call:</p> <pre><code>../src/ontology/$ sh run.sh make refresh-iao\n</code></pre> <ul> <li>With the above commands ODK will first download the whole ontology from which ODK will build your   import module. If you ran this command previously just recently, and thus already have the most current mirror   of that ontology within your mirror folder (<code>scr/ontology/mirror</code>), you can skip this download/mirror step by   instead calling:</li> </ul> <pre><code>../src/ontology/$ sh run.sh make no-mirror-refresh-imports\n</code></pre> <p>for building all import modules at once, or:</p> <pre><code>../src/ontology/$ sh run.sh make no-mirror-refresh-%\n</code></pre> <p>for building only a specific one.</p> <ul> <li>If you use ODK runner, just replace <code>sh run.sh</code> with <code>odk</code> in any of the above commands.</li> </ul> <p>In most cases the default way of building import modules in ODK via the extraction method called SLME-BOT will be the best option. In some cases this default can be too \u201cnoisy\u201d by also importing terms you don\u2019t need/want. With the <code>module_type</code> &amp; <code>module_type_slme</code> parameters in your project.yaml you can tweak the default import module build behavior of ODK, which relies on knowing what ROBOT can do, see also:</p> <ul> <li>http://robot.obolibrary.org/extract</li> <li>http://robot.obolibrary.org/filter</li> <li>http://robot.obolibrary.org/remove</li> </ul> <p>The easiest ways would be to try <code>TOP</code> or <code>STAR</code> instead of the default <code>BOT</code> as value for the <code>module_type_slme</code> parameter and then refresh your imports to see if you still got all the terms you wanted, including their axioms and not any terms that are unrelated.</p> <p>You also have the option to use <code>module_type: custom</code> and define your own ROBOT code for building an import module within your custom makefile (e.g. like this).</p> <ul> <li>However, this can be quite challenging to get right, as you\u2019d probably not want to accidentally miss   any asserted axioms.</li> </ul> <p>Check if everything worked as expected by reloading the editor file in Prot\u00e9g\u00e9 when you refreshed an import module.</p> <p>NOTE: If you need to add a new import dependency to your project.yaml (e.g. you added another ontology dependency to the <code>import_group</code> section), you need to run:</p> <pre><code>../src/ontology/$ sh run.sh make update_repo\n</code></pre> <p>AND you need to add this newly added ontology dependency also to the import declaration sections of your <code>catalog-v001.xml</code> and your editor file.</p>"},{"location":"howto/odk-migrate-to-odk/#8-merge-your-branch","title":"8. Merge your Branch","text":"<p>Once you got your import modules (dependencies) right, you can merge the branch.</p>"},{"location":"howto/odk-migrate-to-odk/#9-make-a-release","title":"9. Make a Release","text":"<p>Make a release and update the metadata needed for your purl system to resolve (e.g. OBO PURL system can also resolve to your import modules) see also:  http://pato-ontology.github.io/pato/odk-workflows/ReleaseWorkflow</p>"},{"location":"howto/odk-migrate-to-odk/#10-join-the-odk-slack-channel","title":"10. Join the ODK Slack Channel","text":"<p>Come to the #ontology-development-kit Slack channel to get help (it is best to have an open repo so others have something to look at for helping to fix  problems).</p>"},{"location":"howto/odk-migrate-to-odk/#11-updating-your-odk-environment","title":"11. Updating your ODK Environment","text":"<p>To update your ODK environment follow this HowTo.</p>"},{"location":"howto/odk-migrate-to-odk/#good-to-know","title":"Good to Know","text":"<p>Your cheat sheet for the Frequently Used ODK Commands.</p>"},{"location":"howto/odk-migrate-to-odk/#contributors","title":"Contributors","text":"<ul> <li>Philip Str\u00f6mert (GitHub: @StroemPhi)</li> <li>Nicolas Matentzoglu (GitHub: @matentzn)</li> <li>Damien Goutte-Gattat (GitHub: @gouttegd)</li> </ul>"},{"location":"howto/odk-setup/","title":"Getting set up with Docker and the Ontology Development Kit","text":""},{"location":"howto/odk-setup/#installation","title":"Installation","text":""},{"location":"howto/odk-setup/#for-windows","title":"For Windows","text":"<ul> <li>Follow the instructions here. Note that you should have Windows 10 Professional installed for this to work. We are not sure Docker Desktop works at all with Windows 10 Home, but we have not tried in a while. If you know what you are doing, you could try to configure Docker toolbox, but we have had many issues with it, and do not recommend it unless absolutely necessary.<ul> <li>If you are unable to install Docker Desktop on your Windows PC (e.g. no admin rights or prohibited by the IT department of your institution) but you have the ability to use the Windows Hyper-V-Manager (possible w/o admin rights) or another virtualization tool, such as VirtualBox, you could set up a Linux virtual machine (VM) to use ODK. We recommend using Lubuntu, as it won't need much computing resources. Although you cannot install Docker Desktop in such a VM, you can install the Docker Engine, which suffices to proceed with the next step. </li> <li>A much more convenient way to use a virtual Linux environment (with admin rights) is via the Windows Linux Subsystem (WSL) in Windows 10 and above. Installing it and the Docker Engine will allow you to use ODK in a Linux shell environment, while working with Prot\u00e9g\u00e9, GitHub Desktop and other possibly helpful apps like PyCharm, in your regular Windows GUI environment.</li> </ul> </li> <li>Once installed, you should be able to open your command line and download the ODK.</li> <li>Click on your Windows symbol (usually in bottom left corner of screen), type \"cmd\" and you should be able to see and open the Command Line tool.</li> <li>in the command line type, type <code>docker pull obolibrary/odkfull</code>. This will download the ODK (will take a few minutes, depending on you internet connection).</li> <li>Executing something in a Docker container can be \"wordy\", because the docker container requires quite a few parameters to be run. To make this easier, we prepared a wrapper script here. You can download this file by clicking on <code>Raw</code>, and then, when the file is open in your browser, CTRL+S to save it. Ideally, you save this file in your project directory, the directory you will be using for your exercises, as it will only allow you to edit files in that very same directory (or one of its sub-directories).</li> <li>Setting the memory:   Typical issues (WSL 1 vs 2)</li> </ul>"},{"location":"howto/odk-setup/#for-maclinux","title":"For Mac/Linux","text":"<ul> <li>Install docker: Install Docker following the official instructions.</li> <li>Make sure its running properly, for example by typing <code>docker ps</code> in your terminal or command line (CMD). If all is ok, you should be seeing something like:</li> </ul> <pre><code>CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n</code></pre> <ul> <li>Run <code>docker pull obolibrary/odkfull</code> on your command line to install the ODK. This will take while.</li> <li>Download an ODK wrapper script. The odk.sh has further instruction on how to best use it.</li> <li>Now you are ready to go to a directory containing the odk.sh wrapper script and running <code>sh odk.sh robot --version</code> to see whether it works.</li> <li>The ODK wrapper script is generally useful to have: you can for example enter a ODK container, similar to a virtual machine,   by simply running <code>sh odk.sh bash</code> (to leave the ODK container again, simply run <code>exit</code> from within the container). On Windows, use <code>run.bat bash</code> instead.   However, for many of the ontologies we develop, we already ship an ODK wrapper script in the ontology repo, so we dont need the odk.sh or odk.bat file.   That file is usually called <code>run.sh</code> or <code>run.bat</code> and can be found in your ontology repo in the <code>src/ontology</code> directory   and can be used in the exact same way.</li> </ul>"},{"location":"howto/odk-setup/#problems-with-memory-important","title":"Problems with memory (important)","text":"<p>One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. There are two potential causes for out-of-memory errors:</p> <ol> <li>The application (for example, the ODK release run) needs more memory than assigned to <code>JAVA</code> inside the ODK docker container. This memory is set as part of the ODK wrapper files, i.e. <code>src/ontology/run.bat</code> or <code>src/ontology/run.sh</code>, usually with <code>ODK_JAVA_OPTS</code>.</li> <li>The application needs more memory than is assigned to your docker installation. On most systems (apart from a handful fo Windows ones based on WSL), you have to set docker memory in the docker preferences. That happens here is that the Java memory above may be set to something like 10GB, while the maximum docker memory is set to 8GB. If the application needs, say, 9GB to run, you have assigned enough Java memory, but docker does not permit more than 8 to be used.</li> </ol> <p>Out-of-memory errors can take many forms, like a Java OutOfMemory exception, but more often than not it will appear as something like an <code>Error 137</code>.</p>"},{"location":"howto/odk-setup/#solving-memory-issues","title":"Solving memory issues","text":""},{"location":"howto/odk-setup/#setting-memory-limits","title":"Setting memory limits:","text":"<p>There are two places you need to consider to set your memory:</p> <ol> <li>Your ODK wrapper script (see above), i.e. odk.bat, odk.sh or src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding    <code>robot_java_args: '-Xmx8G'</code> to your src/ontology/cl-odk.yaml file, see for example here.</li> <li>Set your docker memory. By default, it should be about 10-20% more than your <code>robot_java_args</code> variable. You can manage your memory settings    by right-clicking on the docker whale in your system bar--&gt;Preferences--&gt;Resources--&gt;Advanced, see picture below.</li> </ol> <p></p>"},{"location":"howto/odk-setup/#more-intelligent-pipeline-design","title":"More intelligent pipeline design","text":"<p>If your problem is that you do not have enough memory on your machine, the only solution is to try to engineer the pipelines a bit more intelligently, but even that has limits: large ontologies require a lot of memory to process when using ROBOT. For example, handling ncbitaxon as an import in any meaningful way easily consumes up to 12GB alone. Here are some tricks you may want to contemplate to reduce memory:</p> <ul> <li><code>robot query</code> uses an entirely different framework for representing the ontology, which means that whenever you use ROBOT query, for at least a short moment, you will have the entire ontology in memory twice. Sometimes you can optimse memory by seperating <code>query</code> and other <code>robot</code> commands into seperate commands (i.e. not chained in the same <code>robot</code> command).</li> <li> <p>The <code>robot reason</code> command consumes a lot of memory. <code>reduce</code> and <code>materialise</code> potentially even more. Use these only ever in the last possible moment in a pipeline.</p> </li> <li> <p>`</p> </li> </ul>"},{"location":"howto/odk-update/","title":"Updating ODK","text":"<p>A new version of the Ontology Development Kit (ODK) is out? This is what you should be doing:</p> <ol> <li>Install the latest version of ODK by pulling the ODK docker images. In your terminal, run:</li> </ol> <pre><code>docker pull obolibrary/odkfull\n</code></pre> <ol> <li>To update your repository, go to your <code>src/ontology</code> directory.</li> </ol> <pre><code>cd myrepo/src/ontology\n</code></pre> <ol> <li>Create a new git branch in your usual way (optional)</li> </ol> <p>Now run the update command TWICE (the first time it may fail, as the update command needs to update itself).</p> <pre><code>sh run.sh make update_repo\nsh run.sh make update_repo\n</code></pre> <ol> <li>Edit the following file: <code>.github/workflows/qc.yml</code> (from the top level of your repository) and make sure that it is using the latest version of the ODK.</li> </ol> <p>For example, <code>container: obolibrary/odkfull:v1.3.0</code>, if <code>v1.3.0</code>. Is the latest version. If you are unsure what the latest version is, you can find that information here: https://hub.docker.com/r/obolibrary/odkfull/tags</p> <ol> <li> <p>OPTIONAL: if you have any other GitHub actions you would like to update to the latest ODK, now is the time! All of your GitHub actions can be found in the <code>.github/workflows/</code> directory from the top level of your repo.</p> </li> <li> <p>Review all the changes and commit them, and make a PR the usual way. 100% wait for the PR to pass QC - ODK updates can be significant!</p> </li> <li> <p>Send a reminder to all other ontology developers of your repo and tell them to install the latest version of ODK (step 1 only).</p> </li> </ol>"},{"location":"howto/ontology-overview/","title":"How to prepare OBO Academy ontology overview","text":""},{"location":"howto/ontology-overview/#overview","title":"Overview","text":"<p>This 'how to' guide provides a template for an Ontology Overview for your ontology. Please create a markdown file using this template and share it in your ontology repository, either as part of your ReadMe file or as a separate document in your documentation. The Ontology Overview should include the following three sections:</p> <ol> <li>Scope</li> <li>Curation and governance workflows</li> <li>How the ontology is used in practice</li> </ol>"},{"location":"howto/ontology-overview/#scope","title":"Scope","text":"<ul> <li>Describe the domain and scope of ontology.</li> <li>For example, the Mondo ontology covers concepts in the area of diseases across species and integrates disease terminologies from several underlying sources.</li> <li>Include a figure of upper level terms (critical: give a list of all the high level terms that the ontology covers (1-2 levels). Eg Mondo: disease or disorder, disease susceptibility, disease characteristic).</li> </ul> <ul> <li>Include a figure with exemplary term (using OBO graph)</li> </ul>"},{"location":"howto/ontology-overview/#curation-and-governance-workflows","title":"Curation and governance workflows","text":""},{"location":"howto/ontology-overview/#ontology-curation","title":"Ontology Curation","text":"<p>Describe the ontology level curation, ie how to add terms. For example, terms are added to the ontology via:</p> <ul> <li>Manual additions via Protege</li> <li>ROBOT templates</li> <li>DOSDP templates</li> </ul>"},{"location":"howto/ontology-overview/#governance","title":"Governance","text":"<ul> <li>How do people request new terms or changes</li> <li>How do people contribute terms directly (ie ROBOT templates, etc) (if applicable)</li> </ul> <p>Note: There is no need for details about QC, ODK unless it is related to curation (ie pipeline that automatically generates mappings, include that)</p>"},{"location":"howto/ontology-overview/#how-the-ontology-used-in-practice","title":"How the ontology used in practice","text":"<p>Include 1-3 actual use cases. Please provide concrete examples.</p> <p>For example:</p> <ol> <li>this group uses the ontology to annotate this data for this purpose</li> <li>this group uses the ontology to compute phenotypic similarity for prediction of related diseases</li> <li>The ontology is used for named entity recognition (NER) as a dictionary as a synonym source</li> </ol>"},{"location":"howto/open-science-engineer/","title":"How to be an Open Science Engineer - maximising impact for a better world","text":"<p>Contributors:</p> <ul> <li>Nicole Vasilevsky</li> <li>Nicolas Matentzoglu</li> <li>Bradley Varner</li> </ul> <p>Status: This is a working document! Feel free to add more content!</p> <p>The Open Science Engineer contributes to the collection and standardisation of publicly available scientific knowledge through curation, community-building and data, ontology and software engineering.</p> <p>Open Science and all its sub-divisions, including Open Data and Open Ontologies, are central to tackling global challenges from rare disease to climate change. Open licenses are only part of the answer - the really tough part is the standardisation of data (including the unification of ontologies, the FAIRification of data and adoption of common semantic data models) and the organisation of a global, fully decentralised community of Open Science engineers. Here, we will discuss some basic principles on how we can maximise our impact as members of a global community combating the issues of our time:</p> <ol> <li>Principle of Collaboration: How do we create a welcoming and inclusive environment for implementing social workflows and deepen our ties across project boundaries?</li> <li>Principle of Upstream Fixing: How can we maximise benefits to the global community by pushing fixes as far upstream as possible?</li> <li>Principle of No-ownership: How do we develop a sense of co-, or -no, ownership for community driven ontologies and ontology tools?</li> </ol> <p>We discuss how to best utilise social workflows to achieve positive impact. We will try to convince you that building a close collaborative international community by spending time on submitting and answering issues on GitHub, helping on Stack Overflow and other online platforms, or just reaching out and donating small amounts of time to other open science efforts can make a huge difference.</p>"},{"location":"howto/open-science-engineer/#table-of-contents","title":"Table of contents","text":"<ul> <li>Principle of Collaboration</li> <li>Principle of Upstream fixing</li> <li>Principle of No-ownership</li> <li>TLDR - Summary</li> </ul>"},{"location":"howto/open-science-engineer/#tutorials","title":"Tutorials","text":""},{"location":"howto/open-science-engineer/#icbo-2022-presentation","title":"ICBO 2022 presentation","text":"<p>For a quick 10 minute overview:</p>"},{"location":"howto/open-science-engineer/#monarch-obo-training-tutorial","title":"Monarch OBO training Tutorial","text":"<p>How to be an open science ontologist</p> <p></p>"},{"location":"howto/open-science-engineer/#principle-of-collaboration","title":"Principle of Collaboration","text":"<p>The heart and soul of a successful Open Science culture is collaboration. The relative isolation into which many projects are forced due to limitations imposed by certain kinds of funding makes it even more important to develop effective social, collaborative workflows. This involve effective online communication, vocal appreciation (likes, upvotes, comments), documentation and open-ness.</p>"},{"location":"howto/open-science-engineer/#question-answering-and-documentation","title":"Question answering and documentation","text":"<ol> <li>When you find an answer on Stack Overflow, GitHub issues (or even Hacker News) that helps you, upvote it. It cannot be stressed enough how important this is to let good answers float up to the top (become more visible), and recognise the time and energy people spend formulating answers. If you do not have a Stack Overflow account, make one now.</li> <li>If you cannot find an answer to a question on GitHub or Stack Overflow, and you figure out a solution consider asking the question and answering it yourself! Consider this: if you provide an answer that will result in only 6 open science people spending one hour less solving a problem (a conservative estimate), you saved the taxpayer 6 hours of salary (not to speak of improving the quality of the solutions)!</li> <li>Answer questions on GitHub issues even when on trackers not in your purview. People often falsely assume that they should not try to answer queries in an open source project that they are not directly involved with. This is wrong! Open source developers highly (!) appreciate it when you chip in on answering queries, from dealing with errors and exceptions to \"How do I?\" kind of queries. Get involved on other peoples issue trackers!</li> <li>Before asking questions on slack or issue trackers, always do a basic search first. Consider (1) the project's documentation, (2) open and closed issues on the issue tracker of the project your query is related to. If you do not find the information you need try and craft questions that are concise yet give sufficient context. To enable future users to find these answers, consider using the issue tracker instead of slack! Also it is important to remember that the people that answer your questions have to spend time they could have spend on other Open Science work!</li> <li>Make yourself responsible for continuously improving open science documentation. If a question was answered on slack or in an issue, consider a 10 minute detour to update the documentation of the project to reflect the solution if you think it could be helpful. This is the only way to scale Open Science projects: as the user base increases, providing support on a 1:1 basis will soon be infeasible.</li> <li>Good documentation is as important as good quality code, and everyone can help out, even if it is \"only\" about formatting, typos and adding additional links!</li> </ol>"},{"location":"howto/open-science-engineer/#social-conduct-and-review","title":"Social Conduct and review","text":"<ol> <li>Be overly generous with likes. Being a Open Science Engineer can be quite a lonely affair: hundreds of unanswered issues and questions, pull requests that remain un-reviewed for months, projects for which you never really get credit (want to be a QC engineer, anyone?). Even if you do not have time to respond to all issues you scroll through in a day, add a like if you find an answer useful. This goes for slack comments, Tweets and random Pull Requests you have nothing to do with as well. And, perhaps most importantly star all GitHub repos that are useful to you (you could, for example, scroll through https://github.com/topics/obofoundry and star all ontologies and ontology projects useful in your work)! This is a huge deal as it significantly motivates other OSEs and therefore provides fuel to the Open Science movement.</li> <li>Reduce work for others as much as possible by communicating clearly.<ul> <li>Take the time to write clear responses with just the right amount of detail. The goal of your communication is to get the point across as swiftly as possible, and misunderstandings are a huge time killer.</li> <li>Again: Sloppy, quick responses can cause more work than not responding at all!</li> <li>Use bullet lists to structure your response, and checklists for action items.</li> <li>Hide unnecessary details with <code>&lt;details&gt;</code> tag: <code>&lt;details&gt;&lt;summary&gt;[click arrow to expand]&lt;/summary&gt;</code>. See example here</li> <li>Be generous linking external issues to provide context. Example: </li> </ul> </li> <li>Be positive and generous with gratitude and attribution<ul> <li>If a member of the community opens an issue, always thank them for the issue right away. We need to encourage people to open issues.</li> <li>Don't let issues linger without any response. It is better to respond with a thank you and some instructions for the issuer on what to do to fix the issue themselves than to let it linger. We need our stakeholders to be more involved - first time issuers are especially vulnerable and may not bother to come back if they are ignored.</li> <li>When giving feedback, be positive, friendly and constructive.</li> <li>Show appreciation: thank users for issues and thank a PR reviewer for their review. Always.</li> <li>Attribute, attribute, attribute. If someone helped you sort something out emphasise this publicly!</li> </ul> </li> </ol>"},{"location":"howto/open-science-engineer/#openness","title":"Openness","text":"<ol> <li>Promote truly open communication:<ul> <li>Contribute your thoughts openly so other people can benefit from it.</li> <li>Don't put issues on Slack where the public can't see it. Consider moving interesting discussions on Slack into a more public space, like GitHub discussions.</li> <li>Create public tickets which can be searched and referred to later.</li> </ul> </li> </ol>"},{"location":"howto/open-science-engineer/#principle-of-upstream-fixing","title":"Principle of Upstream Fixing","text":"<p>Maximising impact of your changes is by far the best way you can benefit society as an Open Science Engineer. Open Science projects are a web of mutually dependent efforts, for example:</p> <ul> <li>Ontologies re-use terms and axioms from other ontologies  </li> <li>Software packages provide functionality that help building Knowledge Graphs, ensuring the quality of ontology releases and extract new insights from existing knowledge and scientific facts.  </li> <li>Projects use ontologies for indexing data and making them discoverable.  </li> </ul> <p>The key to maximising your impact is to push any fixes as far upstream as possible. Consider the following projects and the way they depend on each other (note that this is a gross simplification for illustration; in reality the number of dependencies is much higher):</p> <p></p> <p>Let's think of the following (entirely fabricated) scenario based on the image above.</p> <ol> <li>Open Targets provides evidence for a gene association with some disease.</li> <li>This association is only supported by the IMPC data source which associates mouse and human phenotypes using the phenodigm algorithm which is based on semantic similarity.</li> <li>The semantic similarity scores are computed using cross-species axioms provided by the Monarch Initiative, especially uPheno.</li> <li>The cross-species logical axioms provided by Monarch depend on a range of lexical and logical approaches. This means the structure of uPheno is directly influenced by the naming and synonyms provided by species specific phenotype ontologies (SSPOs), such as the Human Phenotype Ontology (HPO).</li> </ol> <p>It is, therefore, possible that:</p> <ol> <li>A faulty synonym is accidentally added to the HPO...</li> <li>...which causes a faulty logical axiom candidate in uPheno (again missed by QC measures)...</li> <li>...which causes a faulty similarity value in Monarch...</li> <li>...which causes a wrong disease-gene association in IMPC...</li> <li>...which ultimately leads to a faulty piece of evidence in Open Targets.</li> </ol> <p>Imagine a user of Open Targets that sees this evidence, and reports it to Open Targets as a bug. Open Targets could take the easy way out: remove the erroneous record from the database permanently. This means that the IMPC (itself with hundreds of dependent users and tools), Monarch (again with many dependents), uPheno and HPO (with probably thousands of dependents) would still carry forward that (tiny) mistake.  This is the basic idea of maximising impact through Upstream Fixing: The higher up-stream (up the dependency graph) an error is fixed, the more cumulative benefit there is to a huge ecosystem of tools and services.</p> <p>An even better fix would be to have each fix to the ontology result in a new, shared quality control test. For example, some errors (duplicate labels, missing definition, etc) can be caught by automated testing. Here is a cool story.</p>"},{"location":"howto/open-science-engineer/#case-study-external-contribution-and-upstream-fixing","title":"Case Study: External contribution and upstream fixing","text":"<ul> <li>Over time, we have developed QC checks that ensure that the same exact synonym cannot be shared between two classes. However the checks where not perfect..</li> <li><code>@vasvir</code> (GitHub name), a member of the global community reached out to us on Uberon: https://github.com/obophenotype/uberon/issues/2424.      https://github.com/obophenotype/uberon/pull/2640</li> <li>Instead of fixing the discovered issue by ourselves, we invited @vasvir to fix the issues himself. We gave him some instructions on how to proceed, leading not only to eight new pull requests, but also an entirely new Quality Control check that augments the existing checks with case-insensitivity (<code>Gasserian ganglion</code> and <code>gasserian ganglion</code> where previously considered distinct). Note: before the PRs, @vasvir did not speak any SPARQL.     </li> <li>Members of our team helped @vasvir to see his first pull requests through by instructing them how to use the technology (robot, ODK, docker, SPARQL), and follow our pull request conventions.</li> </ul> <p>Instead of simply deleting the synonyms for his NLP projects, <code>@vasvir</code> instead decided to report the issues straight to the source. This way, hundreds, if not thousands of projects will directly or indirectly benefit from him!</p>"},{"location":"howto/open-science-engineer/#other-examples-of-upstream-fixing","title":"Other examples of upstream fixing","text":"<p>Example 1: While curating Mondo, Nicole identified issues relevant to Orphanet and created this issue.</p> <p></p> <p>Example 2: There is overlap between Mondo and Human Phenotype Ontology and the Mondo and HPO curators tag each other on relevant tickets.</p> <p></p> <p>Example 3: In Mondo, if new classifications are made, Mondo curators report this back to the source ontology to see if they would like to follow our classification.</p> <p></p>"},{"location":"howto/open-science-engineer/#conclusions-upstream-fixing","title":"Conclusions: Upstream Fixing","text":"<p>Have you ever wondered how much impact changing a synonym from <code>exact</code> to <code>related</code> could have? Or the addition of a precise mapping? The fixing of a typo in a label? It can be huge. And this does not only relate to ontologies, this goes for tool development as well. We tend to work around bugs when we are building software. Instead, or at least in addition to, we should always report the bug at the source to make sure it gets fixed eventually.</p> <p></p>"},{"location":"howto/open-science-engineer/#principle-of-no-ownership","title":"Principle of No-ownership","text":"<p>Many of the resources we develop are financed by grants. Grants are financed in the end by the taxpayer. While it is occasionally appropriate to protect open work with creative licenses, it rarely makes sense to restrict access to Open Ontologies work - neither to commercial nor research exploitation (we may want to insist on appropriate attribution to satisfy our grant developers).</p> <p>On the other side there is always the risk of well-funded commercial endeavours simply \"absorbing\" our work - and then tying stakeholders into their closed, commercial ecosystem. However, this is not our concern. We cannot really call it stealing if it is not really ours to begin with! Instead of trying to prevent unwanted commercialisation and closing, it is better to work with corporations in pre-competitive schemes such as Pistoia Alliance or Allotrope Foundation and lobby for more openness. (Also, grant authorities should probably not allow linking scientific data to less than totally open controlled vocabularies.)</p> <p>Here, we invite you to embrace the idea that ontologies and many of the tools we develop are actually community-driven, with no particular \"owners\" and \"decision makers\". While we are not yet there (we don't have sufficiently mature governance workflows for full fledged onto-communism), and most ontologies are still \"owned\" by an organisation that provides a major source of funding, we invite you to think of this as a preliminary state. It is better to embrace the idea of \"No-ownership\" and figure out social workflows and governance processes that can handle the problems of decision making.</p>"},{"location":"howto/open-science-engineer/#take-responsibility-for-your-community-ontologies","title":"Take responsibility for your community (ontologies)","text":"<ol> <li>Ensure that you see your issues and pull requests through to the end. No one will do this for you. Remember - contributors to open source projects, especially ontologies, have their own agendas, and do not automatically care about other peoples work.</li> <li> <p>Feel empowered to nudge reviewers or experts to help. Get that issue answered and PR merged whatever it takes!</p> <p>Example: After waiting for the PR to be reviewed, Meghan kindly asked Nicole if she should find a different reviewer.  1. Find review buddies. For every ontology you seek to contribute to pair up with someone who will review your pull requests and you will review their pull requests. Sometimes, it is very difficult to get anyone to review your pull request. Reach out to people directly, and form an alliance for review. It is fun, and you learn new things (and get to know new people!). 1. Be proactive </p> <ul> <li>Problem of decentralization and lack of hierarchial organisation needs proactive and brave decision makers.  </li> <li>No one will do your work for you. See your pull requests and issues through all the way to the release!  </li> <li>Learn the tools necessary to make basic fixes - just try it/do it.  </li> <li>Always have your index finger on the Edit button when reading documentation. There is always something to fix, including typos and content. </li> </ul> </li> </ol>"},{"location":"howto/open-science-engineer/#reduce-your-fear-of-breaking-the-ontology","title":"Reduce your fear of \"breaking the ontology\".","text":"<ol> <li>Most of our ontologies have many checks in place, and GitHub has version control. Nothing has ever been broken to the point where it can't be fixed. (Remember to work on a branch!)     Example: The QC checks on this PR failed 6 times before it passed.     </li> <li>Perfect is the enemy of good enough.  </li> <li>It's okay if your PR is rejected. Mentally, prepare yourself for having the PR rejected. This is fine - the community always looks to the best possible way to change the ontology. The next PR will be accepted!  </li> <li>If you make a mistake, and it is pointed out during pull request review, consider adding an appropriate QC check to prevent the issue from happening again. And who knows - maybe you have an opportunity to fix past mistakes!  </li> </ol>"},{"location":"howto/open-science-engineer/#tldr-summary","title":"TLDR - Summary","text":"<ul> <li>Principle of Collaboration<ul> <li>Upvote answers, on Stack Overflow, GitHub and any other open communication platform.</li> <li>Get involved on other peoples issue trackers.</li> <li>Always do a basic search before asking.</li> <li>Continuously improve Open Science documentation.</li> <li>Be overly generous with likes.</li> <li>Always strive to reduce work for other members of the community.</li> <li>Be positive and generous with gratitude and attribution.</li> <li>Promote open communication (less slack, more GitHub).</li> </ul> </li> <li>Principle of Upstream fixing<ul> <li>The key to maximising your impact is to push any fixes as far upstream as possible.</li> <li>When you experience a problem, always report it to the immediate source. If you can report it as high upstream as possible.</li> <li>In a perfect world, provide a fix in the form of a pull request.</li> </ul> </li> <li>Principle of No-ownership<ul> <li>See your issues and pull requests through to the end (dont drop the ball, no one will do your work for you)</li> <li>Feel empowered to nudge reviewers until they tell you not to.</li> <li>Find review buddies (this is really helpful to organise community work).</li> <li>Be proactive... and brave.</li> <li>Reduce your fear of breaking the ontology.</li> <li>Reduce your fear of getting a pull request rejected.</li> <li>Reduce other peoples fear of breaking the ontology by adding additional QC checks.</li> </ul> </li> </ul>"},{"location":"howto/prettify/","title":"Prettify markdown files","text":""},{"location":"howto/prettify/#description","title":"Description","text":"<p>Prettier standardizes the representation and formatting of Markdown. More information is available at https://prettier.io/. Note, these instructions are for a Mac.</p>"},{"location":"howto/prettify/#install-npm","title":"Install npm","text":"<p>If you do not have npm installed, this can be installed using homebrew (if you have homebrew installed).</p> <p><code>brew install node</code></p>"},{"location":"howto/prettify/#install-prettier-locally","title":"Install Prettier locally","text":"<ol> <li>Run <code>npm install --save-dev --save-exact prettier</code></li> </ol>"},{"location":"howto/prettify/#prettify-your-files","title":"Prettify your files","text":"<ol> <li>Create a new branch</li> <li>Navigate to your root directory containing obook</li> <li>Run <code>npx prettier --write .</code></li> <li>Commit to your branch and create a pull request</li> </ol>"},{"location":"howto/protege-browse-search/","title":"Browsing and Searching","text":""},{"location":"howto/protege-browse-search/#open-the-ontology-in-protege","title":"Open the ontology in Prot\u00e9g\u00e9","text":"<p>Note: Windows users should open Protege using run.bat Note: For the purpose of this how-to, we will be using MONDO as the ontology</p> <ol> <li>Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo)</li> <li>When you open Protege, you will be on the Active Ontology tab</li> <li>Note the Ontology IRI field. The IRI is used to identify the ontology on the Web.</li> </ol> <p></p>"},{"location":"howto/protege-browse-search/#the-protege-ui","title":"The Prot\u00e9g\u00e9 UI","text":"<p>The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations.</p> <p>To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab.</p> <p></p> <p>Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. \u00a0For most normal usage you should answer no. This will open in a new window.</p> <p>The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information.</p> <p></p>"},{"location":"howto/protege-browse-search/#running-the-reasoner","title":"Running the reasoner","text":"<p>Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner:</p> <p></p>"},{"location":"howto/protege-browse-search/#entities-tab","title":"Entities tab","text":"<p>You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below).</p> <p></p> <p>The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query.</p>"},{"location":"howto/protege-browse-search/#searching-in-protege","title":"Searching in Protege","text":"<p>You can search for any entity using the search bar on the right:</p> <p></p> <p>The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together.</p> <p></p> <p>Here's an example search for 'COVID-19': </p> <p>It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g.</p> <p></p> <p>In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom.</p> <p>Try to find these specific classes:</p> <ul> <li>'congenital heart disease'</li> <li>'Kindler syndrome'</li> <li>'kidney failure'</li> </ul> <p>Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019.</p> <ul> <li>Try searching for \u2018br car and see what kind of results are returned.</li> <li>Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term?</li> </ul> <p>Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.</p>"},{"location":"howto/respectful-collaboration/","title":"Respectful Collaboration","text":""},{"location":"howto/respectful-collaboration/#how-to-build-respectful-collaborative-environments-on-github","title":"How to build respectful collaborative environments on Github","text":"<p>WARNING: This is an incomplete page which will be gradually populated with content.</p> <ol> <li>Consider a Code of Conduct for your project. Having an explicit Code of Conduct in an ontology repository supports maintaining respectful and inclusive interactions, particularly in collaborative workflows that often span diverse international communities and cultures. Ontology development typically involves extensive discussions, consensus-building, and iterative refinement, processes that thrive on clear, respectful, and open communication. By adopting a CoC, you explicitly affirm the repository's commitment to respectful conduct, inclusivity, and professional collaboration, helping reduce misunderstandings, conflicts, and barriers to contribution. Ultimately, this strengthens community cohesion, fosters trust among contributors, and contributes significantly to the sustainability and effectiveness of collaborative ontology engineering efforts. Learn how to add a CoC to your project here.</li> </ol>"},{"location":"howto/revert-commit/","title":"How to revert a commit using GitHub Desktop","text":""},{"location":"howto/revert-commit/#prerequisites","title":"Prerequisites","text":"<p>You need to have a GitHub account GitHub and download GitHub Desktop</p>"},{"location":"howto/revert-commit/#background","title":"Background","text":""},{"location":"howto/revert-commit/#reversing-a-commit","title":"Reversing a commit","text":"<ol> <li>Create a new branch in GitHub Desktop (Important step! Never make changes to the master branch!)</li> <li>Click on History tab.</li> <li>Find commit you want to revert.</li> <li>Right click. Click on 'Reverse Changes in Commit' from the drop-down list.</li> <li>Fix any conflicts.</li> <li>Commit branch, create pull request and assign a reviewer.</li> </ol>"},{"location":"howto/revert-commit/#contributors","title":"Contributors","text":"<ul> <li>Arwa Ibrahim</li> </ul>"},{"location":"howto/review-disease-mappings/","title":"Reviewing disease mappings for Biocurators and Clinicians","text":""},{"location":"howto/review-disease-mappings/#description","title":"Description","text":"<p>This guide provides guidelines on how to rapidly review large scale efforts to create mappings between ontology classes. </p> <p>Mapping can be created using tools like OAK to generate a bunch of mapping candidates. A reviewer then needs to determines whether the mapping is correct or not. </p> <p>Refer to Are these two entities the same? A guide. for a comprehensive guide on determining a specific mapping.</p>"},{"location":"howto/review-disease-mappings/#guidelines","title":"Guidelines","text":"<ol> <li>Depending on the format of the table you are reviewing, it is recommended that you open the table in a google spreadsheet, to allow for collaborative work and to allow yourself to add notes in a new column, or add formatting or color coding.</li> <li>Hide any columns that are not relevant, to make it easier to quickly compare the mappings. For example, in the table below, you may want to hide the mapping_justification column, mapping_tool. You can also move the columns around so the labels are next to each other. </li> <li>Look at the confidence score for the mapping and look at the lower confidence scores first, as these are more likely to have issues.</li> <li>Review the label of the mappings. Note, matching on the label is not always sufficient to conclude two terms are equivalent. Ideally, reviewers should compare the definitions of each term to ensure they have the same meaning.</li> <li>Review how the match was made. If it is a lexical match, matching on labels, it is likely the mapping is correct but this should ideally be confirmed by reviewing the definitions as mentioned above.</li> <li>If matches are made based on shared xrefs, these should be carefully reviewed because not all mapping between terminologies are intended to be equivalent xrefs. For example: MONDO:0000509 non-syndromic intellectual disability was mapped to DOID:0081098 autosomal recessive intellectual developmental disorder 13 based on the shared xref OMIM:613192. The Mondo class is broader than this DO class and this is not an exact mapping.</li> <li>Watch out for matching on acronyms. Acronyms can mean a lot of different things and sometimes the mapping tools will incorrectly match on acronyms.</li> </ol>"},{"location":"howto/review-disease-mappings/#example-mappings","title":"Example mappings","text":"subject_id subject_label predicate_id object_id object_label mapping_justification mapping_tool confidence subject_match_field object_match_field match_string comment ID A oboInOwl:hasDbXref &gt;A oboInOwl:source &gt;A sssom:object_label MONDO:0000159 bone marrow failure syndrome MONDO:equivalentTo NCIT:C165614 Bone Marrow Failure Syndrome semapv:LexicalMatching oaklib 0.849778895 rdfs:label rdfs:label bone marrow failure syndrome LEXMATCH MONDO:0000376 respiratory system cancer MONDO:equivalentTo NCIT:C4571 Malignant Respiratory System Neoplasm semapv:LexicalMatching oaklib 0.8 oio:hasExactSynonym rdfs:label malignant respiratory system neoplasm LEXMATCH MONDO:0000437 cerebellar ataxia MONDO:equivalentTo NCIT:C26702 Ataxia semapv:LexicalMatching oaklib 0.8 oio:hasExactSynonym rdfs:label ataxia LEXMATCH MONDO:0000541 jejunal adenocarcinoma MONDO:equivalentTo NCIT:C181158 Jejunal Adenocarcinoma semapv:LexicalMatching oaklib 0.849778895 rdfs:label rdfs:label jejunal adenocarcinoma LEXMATCH MONDO:0000543 ovarian melanoma MONDO:equivalentTo NCIT:C178441 Ovarian Melanoma semapv:LexicalMatching oaklib 0.849778895 rdfs:label rdfs:label ovarian melanoma LEXMATCH MONDO:0000665 apraxia MONDO:equivalentTo NCIT:C180557 Apraxia semapv:LexicalMatching oaklib 0.849778895 rdfs:label rdfs:label apraxia LEXMATCH MONDO:0000705 Clostridium difficile colitis MONDO:equivalentTo NCIT:C180523 Clostridium difficile Infection semapv:LexicalMatching oaklib 0.8 oio:hasExactSynonym rdfs:label clostridium difficile infection LEXMATCH MONDO:0000736 dyschromatosis universalis hereditaria MONDO:equivalentTo NCIT:C173131 Dyschromatosis Universalis Hereditaria semapv:LexicalMatching oaklib 0.849778895 rdfs:label rdfs:label dyschromatosis universalis hereditaria LEXMATCH"},{"location":"howto/review-pull-request/","title":"How to review a pull request","text":"<p>Pull Requests are GitHub's mechanism for allowing one person to propose changes to a file (which could be a chunk of code, documentation, or an ontology) and enabling others to comment on (review) the proposed changes. You can learn more about creating Pull Requests (PRs) here; this document is about reviewing other people's PRs.</p> <p>One key aspect of reviewing pull requests (aka code review or ontology change review) is that the purpose is not just to improve the quality of the proposed change. It is also about building shared coding habits and practices and improving those practices for all engineers (ontology and software) across a whole organisation (effectively building the breadth of project knowledge of the developers and reducing the amount of hard-to-understand code). </p> <p>Reviewing is an important aspect of open science and engineering culture that needs to be learned and developed. In the long term, this habit will have an effect on the growth and impact of our tools and ontologies comparable to the engineering itself.</p> <p>It is central to open science work that we review other people's work outside our immediate team. We recommend choosing a few people with whom to mutually review your work, whether you develpo ontologies, code or both. It is of great importance that pull requests are addressed in a timely manner, ideally within 24 hours of the request. The requestor is likely in the headspace of being receptive to changes and working hard to get the code fixed when they ask for a code review.</p>"},{"location":"howto/review-pull-request/#overarching-workflow","title":"Overarching workflow","text":"<ol> <li> <p>Understand the Context: First, read the description of the pull request (PR). It should explain what changes have been made and why. Understand the linked issue or task related to this PR. This will help you understand the context of the changes.</p> </li> <li> <p>Check the Size: A good PR should not be too large, as this makes it difficult to understand the full impact of the changes. If the PR is very large, it may be a good idea to ask the author to split it into smaller, more manageable PRs.</p> </li> <li> <p>Review the Code: Go through the code changes line by line. Check the code for clarity, performance, and maintainability. Make sure the code follows the style guide and best practices of your project. Look out for any potential issues such as bugs, security vulnerabilities, or performance bottlenecks.</p> </li> <li> <p>Check the Tests: The PR should include tests that cover the new functionality or changes. Make sure the tests are meaningful, and they pass. If the project has a continuous integration (CI) system, all tests should pass in the CI environment. In some cases, manual testing may be helpful (see below).</p> </li> <li> <p>Check the Documentation: If the PR introduces new functionality, it should also update the documentation accordingly. Even for smaller changes, make sure that comments in the code are updated.</p> </li> <li> <p>Give Feedback: Provide constructive feedback on the changes. If you suggest changes, explain why you think they are necessary. Be clear, respectful, and concise. Remember, your goal is to help improve the quality of the code.</p> </li> <li> <p>Follow Up: After you have provided feedback, check back to see if the author of the PR has made the suggested changes. You might need to have a discussion or explain your points further.</p> </li> <li> <p>Approve/Request Changes: If you are satisfied with the changes and all your comments have been addressed, approve the PR. If not, request changes and explain what should be done before the PR can be approved.</p> </li> <li> <p>Merge the PR: Once the PR is approved and all CI checks pass, it can be merged into the main branch. If your project uses a specific merge strategy (like squash and merge or rebase and merge), make sure it's followed.</p> </li> </ol>"},{"location":"howto/review-pull-request/#how-to-review-the-code-ontology-changes-thoroughly","title":"How to review the code / ontology changes thoroughly","text":"<ol> <li>Ensure that the PR links to a related issue that explains the context of the PR. If there is no issue, request that an issue be created to motivate the change in the PR (gently - accept if the answer is negative).</li> <li>Understand the Context: Begin by familiarizing yourself with the purpose of the changes. Read the description of the pull request, any linked issues or tasks, and understand the feature or bug that the pull request is addressing.</li> <li>Ensure that all changes in the PR are intentional. Changes should be small. If there are a lot of unrelated changes, in particular line ending changes, serialisation changes in ontology (e.g. a lot of added <code>xsd:string</code> declarations), request before doing a review to reduce the changes to only the changes pertaining to the specific issue at hand.</li> <li>Review the Code Structure: Look at the overall structure of the code. Check if the code is organized logically and consistently, follows the project's coding conventions, and the changes are made in the right place.<ul> <li>For software, consider factors like whether the code is in an approporiate location (files, modules). If there are no specific coding standards for the project, suggest that such standards be created.</li> <li>For ontologies, check that the change is to the right file (edit file, DOSDP patterns, ROBOT templates etc.).  </li> </ul> </li> <li>Check the Code Quality: Review the code in detail. Look for any programming errors, potential performance issues, or security vulnerabilities. The code should be clean, efficient, and easy to understand. Pay attention to the naming conventions, error handling, edge cases, and potential bugs.<ul> <li>The two items below are where you should spend most of your time:</li> </ul> </li> <li>Review the Tests: Ensure that the PR includes tests and that they cover all important aspects of the new code. Check if all tests pass, and if the project has a continuous integration (CI) system, ensure all CI checks pass as well.<ul> <li>For ontologies, it is essential that a bug fix is augmented by a test that can recognise the same bug happening again in the future!</li> <li>For code, it is a bit of a judgment call whether a test is needed, but in general, we have to have at least thorough integration tests that touch the feature affected by the pull request.</li> </ul> </li> <li>Test the Changes Manually (use with care): Depending on the change, it may be a good idea to check out the PR branch and test the changes manually. This can help catch issues that are not covered by automated tests, or may be necessary if the diff is too large for a normal review. Note that this is an instance of Guru Testing and should only be used when there is a sense that automated testing is incomplete. If at all possible, an automated test should be run immediately after the manual review is done. Examples of where this kind of manual review may be appropriate are cases where, for example, a lot of the class hierarchy of an ontology is affected by a change. Manual review may catch issues such as missing superclasses which are hard to capture through automated testing.</li> <li>Provide Constructive Feedback: Provide clear, respectful, and constructive feedback. Highlight the parts of the code that you think are good and the parts that need improvement. Request changes if necessary, and approve the PR once you're satisfied with the changes.</li> </ol>"},{"location":"howto/review-pull-request/#how-to-review-a-pull-request-in-5-min","title":"How to review a pull request in 5 min","text":"<p>In many cases, we may not have the time to perform a proper code review. In that case, try at least to achieve this:</p> <ol> <li>Ensure that the PR has a related issue that explains the context of the PR (see above).</li> <li>Ensure that all changes in the PR are intentional (see above).</li> <li>File and structure overview: Scan through the files and directories that have been modified. Note the overall structure of the changes, and look for any unusual modifications (e.g., changes in areas not related to the stated purpose of the PR, or large diffs that are not explained by the PR description).</li> <li>Random code sampling: Instead of trying to read every line, pick a few sections of code at random to review. Pay attention to the cleanliness of the code, and see if there are any glaring issues or departures from the project's coding conventions.</li> <li>Check automated tests and results: Review the tests that have been added or modified. Check the results of the tests and automated build processes, if available. Even in a quick review, the status of the tests can provide useful information about the quality of the changes.</li> <li>Provide high-level feedback: Based on your quick review, provide high-level feedback. This could include praising good practices you've noticed, pointing out major concerns or areas that seem off, or simply acknowledging the work with a comment.</li> </ol>"},{"location":"howto/review-pull-request/#video-explanations","title":"Video explanations","text":""},{"location":"howto/review-pull-request/#how-to-review-an-obsoletion-pr-in-mondo","title":"How to review an Obsoletion PR in Mondo","text":""},{"location":"howto/review-pull-request/#how-to-review-a-merge-pr-in-mondo","title":"How to review a merge PR in Mondo","text":""},{"location":"howto/run-odk-with-podman/","title":"Running ODK with Podman","text":"<p>Some employers, in particular outside academia, may discourage the use of Docker via Docker Desktop for licensing reasons. Podman is an open source container, pod, and container image management engine. While there are some fundamental differences between Podman and Docker, a basic user can treat them as largely equivalent, for example by creating an alias from Podman's native <code>podman</code> command to Docker's <code>docker</code>.</p> <p>Aliases work well for running <code>docker</code> commands directly but it create some issues around automatic processes in ODK since a locally set alias is not visible from ODK's <code>run.sh</code> script. As a result, a command like</p> <pre><code>sh run.sh make prepare_release -B\n</code></pre> <p>may give you the error message</p> <pre><code>./run.sh: line 68: docker: command not found\n\nPlease remember to update your ODK image from time to time: https://oboacademy.github.io/obook/howto/odk-update/.\n</code></pre> <p>To fix this, instead create a symlink between <code>docker</code> and the Podman executable. To do this, run</p> <pre><code>which podman\n</code></pre> <p>to find where podman is installed, then navigate to the Podman executable</p> <pre><code>cd /path/to/podman/bin\n</code></pre> <p>Now create the symlink using (requires admin privileges)</p> <pre><code>sudo ln -s podman docker\n</code></pre> <p>After this, running <code>ls -la</code> in the same directory should include a line like</p> <pre><code>docker -&gt; podman\n</code></pre> <p>and <code>which docker</code> should return <pre><code>/path/to/podman/bin/docker\n</code></pre></p> <p>Finally, ensure that the symlink is somewhere on your path (if it isn't already!) by running</p> <pre><code>export PATH=$PATH:/path/to/podman/bin\n</code></pre> <p>Restart your shell and you should find that your ODK <code>run.sh</code> script works again as expected.</p>"},{"location":"howto/run-odk-with-podman/#warning","title":"Warning!","text":"<p>The commands above were tested in <code>zsh</code> shell on a Mac. They should also work in an <code>bash</code> shell, on Mac or Linux. They may not work on Windows.</p> <p>You could also edit the ODK <code>run.sh</code> script to use <code>podman</code> instead of <code>docker</code>, which would work with the previously mentioned <code>alias</code> solution but this is NOT ADVISABLE as the change would be overwritten every time the repo is updated. Plus, if your <code>run.sh</code> is committed into your Git repo, the change would affect all your fellow ontology editors.</p>"},{"location":"howto/screenshot/","title":"Create Screenshot and paste into an issue","text":""},{"location":"howto/screenshot/#description","title":"Description","text":"<p>The instructions below describe how to capture a screenshot of your screen, either your entire screen or a partial screenshot. These can be pasted into GitHub issues, pull requests or any markdown file.</p>"},{"location":"howto/screenshot/#screenshot-instructions-mac","title":"Screenshot Instructions (Mac)","text":"<ol> <li>Full screen: Hit the Command, Shift and 3 keys together to take a screenshot of the entire screen</li> </ol> <ol> <li>Partial screen: Hit the Shift, Control, Command and 4 keys together to take a screenshot of a portion of the screen. Crosshairs will appear and select the portion you would like to capture.</li> </ol> <ol> <li>Paste the partial screenshot into comment box in GitHub.</li> </ol>"},{"location":"howto/screenshot/#video-explanation-partial-screenshot-on-a-mac","title":"Video explanation: Partial screenshot on a Mac","text":""},{"location":"howto/screenshot/#screenshot-instructions-pc","title":"Screenshot Instructions (PC)","text":""},{"location":"howto/screenshot/#use-the-snipping-tool-available-with-windows-10","title":"Use the Snipping Tool (available with Windows 10)","text":"<ol> <li>The easiest way to elicit the tool is to press Windows logo key + Shift + S to open a dialog window to select the type of screen capture you would like to do. 1. Options exist for free form, rectangular, window or full screen snips.</li> <li>After snipping, a notification dialog pops up so the user can select the location to save or annotate the screenshot. Otherwise, the default setting is to save to the clipboard.</li> </ol>"},{"location":"howto/screenshot/#use-your-keyboard","title":"Use your keyboard","text":"<p>Different keyboards have different keys. One of the following options should work:</p> <ol> <li>Fn + Print Screen is an option on some keyboards</li> <li>PrtScn or PrtSc</li> <li>Hit Shift, Window, S keys together to take a screenshot. You will be able to resize the screenshot as needed.</li> </ol>"},{"location":"howto/screenshot/#reference","title":"Reference","text":"<ul> <li>Wikihow Take a Screenshot on a Mac</li> <li>Using snipping tools to capture screenshots is described here.</li> </ul>"},{"location":"howto/set-up-protege/","title":"Setup Protege 5.6.2","text":"<p>(This was adopted from the Gene Ontology editors guide and Mondo documentation). Updated 2023-08-16 by Nicole Vasilevsky</p>"},{"location":"howto/set-up-protege/#mac-instructions","title":"Mac Instructions","text":"<p>These instructions are for Mac OS</p>"},{"location":"howto/set-up-protege/#protege-version","title":"Protege version","text":"<p>As of February 2023, OBO ontology editors are using Protege version 5.6.2.</p>"},{"location":"howto/set-up-protege/#download-and-install-protege","title":"Download and install Protege","text":"<ul> <li>Get Protege from protege.stanford.edu</li> <li>Unzip and move the Protege app to your Applications folder.</li> <li>See Install_Protege5_Mac for more instructions and troubleshooting common problems.</li> </ul>"},{"location":"howto/set-up-protege/#increase-memory-in-protege-562","title":"Increase memory in Protege 5.6.2","text":"<p>Protege needs at least 4G of RAM to cope with large ontologie like Mondo, ideally use 12G or 16G if your machine can handle it. Edits to the Protege configuration files will not take effect until Protege is restarted.</p> <ol> <li>If running from Protege.app on a Mac, open the /Applications/Protege-5.6.2/Prot\u00e9g\u00e9.app/Contents/conf/jvm.conf file</li> <li>set the heap size to 12G: max_heap_size=12G</li> </ol>"},{"location":"howto/set-up-protege/#increase-memory-in-protege-550","title":"Increase memory in Protege 5.5.0","text":"<ol> <li>If running from Protege.app on a Mac, open the /Applications/Protege-5.5.0/Prot\u00e9g\u00e9.app/Contents/info.plist file</li> <li>Below the line: <code>&lt;string&gt;-Xss16M&lt;/string&gt;</code></li> <li>Insert another line: <code>&lt;string&gt;-Xmx12G&lt;/string&gt;</code></li> </ol> <p>Some Mac users might find that the edits need to be applied to <code>/Applications/Prot\u00e9g\u00e9.app/Contents/Info.plist</code>.</p>"},{"location":"howto/set-up-protege/#pc-instructions","title":"PC Instructions","text":"<p>Taken in part from Memory Management with Prot\u00e9g\u00e9 by Michael DeBellis. Updated by Nicole Vasilevsky.</p> <p>The following instructions will probably not work if Prot\u00e9g\u00e9 was installed from the platform independent version, which does not include the Java Runtime Environment or a Windows .exe launcher.</p> <ul> <li>Visit https://protege.stanford.edu/</li> <li>Click orange DOWNLOAD NOW button</li> <li>Click gray Download for Windows button on subsequent page</li> <li>Register if desired, or skip registration </li> <li>Find <code>Protege-&lt;version&gt;-win.zip</code></li> <li>most likely in your Downloads directory (or in This PC directory)</li> <li>current version is Protege-5.5.0-win.zip</li> <li>Unzip the downloaded file with your favorite file compression utility</li> <li>You should see the Protege application and a file called 'run'</li> <li>To open Protege, double-click the 'run' file. This will open cmd.exe and it will take a bit to open the file.</li> <li>The fonts used by Prot\u00e9g\u00e9 may be very small, especially on some high resolution monitors. To increase the font size, go to  File -&gt; Preferences -&gt; Renderer -&gt; Font size.</li> <li>Exit Prot\u00e9g\u00e9 after confirming that it can be launched.</li> <li>There should be a <code>Protege.l4j.ini</code> in the same directory as <code>Protege.exe</code>. Opening large ontologies like MONDO will require an increase to Protege's default maximum Java heap size, which is symbolized as <code>-Xmx&lt;size&gt;</code>. 4GB is usually adequate for opening MONDO, as long as 4GB of free memory is really available on your system before you launch Prot\u00e9g\u00e9! Allocating even more memory will improve some tasks, like reasoning. You can check your available memory by launching the Windows Task Manager, clicking on the More details button on the bottom of the window and then checking the Performance tab at the top of the window.</li> <li>It's recommended to make a backup of <code>Protege.l4j.ini</code> before editing</li> </ul> <p>Open <code>Protege.l4j.ini</code> with a lightweight text editor like Atom or Sublime. Using notepad.exe instead might work, but may change character encodings or the character(s) used to represent End of Line.  </p> <p>After increasing the memory available to Prot\u00e9g\u00e9, <code>Protege.l4j.ini</code> might look like this. </p> <pre><code>-Xms200M\n-Xmx4G\n-Xss16M\n</code></pre> <p>Note that there is no whitespace between <code>-Xmx</code>, the numerical amount of memory, and the Megabytes/Gigabytes suffix. Don't forget to save. </p> <p>Taking advantage of the memory increase requires that Prot\u00e9g\u00e9 is shut down and relaunched, if applicable. The methods discussed here may not apply if Prot\u00e9g\u00e9  is launched through any method other than double clicking <code>Protege.exe</code> from the folder where the edited <code>Protege.l4j.ini</code> resides.</p>"},{"location":"howto/set-up-protege/#note-on-increasing-memory","title":"Note on increasing memory","text":"<p>If you have issues opening Protege, then reduce the memory, try 10G (or lower) instead.</p>"},{"location":"howto/set-up-protege/#add-elk-reasoner","title":"Add ELK reasoner","text":"<p>See instructions here. Note: Protege 5.6.1 has the ELK reasoner installed.</p>"},{"location":"howto/set-up-protege/#instructions-for-new-protege-users","title":"Instructions for new Protege users","text":""},{"location":"howto/set-up-protege/#setting-your-id-range","title":"Setting your ID range","text":"<p>See instructions here.</p>"},{"location":"howto/set-up-protege/#user-details","title":"User details","text":"<ol> <li><code>User name</code> Click <code>Use supplied user name:</code> add your name (ie nicolevasilevsky)</li> <li>Check <code>Use Git user name when available</code></li> <li>Add <code>ORCID</code>. Add the ID number only, do not include https://, ie 0000-0001-5208-3432</li> </ol>"},{"location":"howto/set-up-protege/#setting-username-and-auto-adding-creation-date","title":"Setting username and auto-adding creation date","text":"<ol> <li>In the Protege menu, go to <code>Preferences</code> &gt; <code>New Entities Metadata</code> tab</li> <li>Check <code>Annotate new entities with creator (user)</code> box</li> <li><code>Creator property</code> Add http://purl.org/dc/terms/contributor</li> <li><code>Creator value</code> Select Use ORCID</li> <li><code>Date property</code> Add http://purl.org/dc/terms/date</li> <li><code>Date value format</code> Select ISO-8601</li> </ol>"},{"location":"howto/set-up-protege/#install-protege-obo-plugin","title":"Install Protege OBO plugin","text":"<p>This plugin enables some extra functionality, such as the option to obsolete entities from the menu. To install it:</p> <ol> <li>Go to <code>File &gt; Check for plugins...</code>.</li> <li>Click on <code>OBO Annotations Editor</code> and click on <code>Install</code>.</li> <li>Restart Protege for the plugin to be active.</li> <li>You should now have the option to obsolete entities in <code>Edit &gt; Make entity obsolete</code>.</li> <li>You can see a list of all installed plugins in <code>Preferences &gt; Plugins</code>.</li> </ol>"},{"location":"howto/setup-docker/","title":"Set up docker","text":"<ul> <li>Follow the instructions here (for a Mac) or here (for Windows).</li> <li>Once installed, you should be able to open your command line and download the ODK.<ul> <li>Open Terminal</li> <li>in the command line type, type <code>docker pull obolibrary/odkfull</code>. This will download the ODK (will take a few minutes, depending on you internet connection).</li> </ul> </li> <li>Setting the memory:   Set memory ~60% of your system memory, for example, if you have 16GB of RAM, then you should assign 10-11.</li> </ul>"},{"location":"howto/setup-ontology-development-odk/","title":"Getting set up to manage ontology pipelines with the ODK","text":"<ol> <li>Set up docker and install the ODK (howto)</li> <li>To warm up with ODK development, follow the ODK Tutorial here</li> </ol>"},{"location":"howto/switching-ontologies/","title":"Switching Ontologies in Protege","text":"<p>By: Nicole Vasilevsky</p> <p>Note: This applies to Protege 5.5.0 and below. Protege 5.6 manages ID ranges for you automatically and these instructions are not needed.</p>"},{"location":"howto/switching-ontologies/#description","title":"Description","text":"<p>When you edit an ontology, you need to make sure you are using the correct prefix and your assigned ID range for that on ontology. Protege (unfortunately) does not remember the last prefix or ID range that you used when you switch between ontologies. Therefore we need to manually update this each time we switch ontologies.</p>"},{"location":"howto/switching-ontologies/#instructions","title":"Instructions","text":"<ol> <li>When you switch to a new ontology file, open your preferences in Protege (File -&gt; Preferences).</li> <li>Be sure you are on the New entities tab.</li> <li>Add the Prefix for the ontology you are working on.</li> <li>If you don't know your ID range, go to the ID ranges file for that ontology (it should be in <code>src/ontology/[ontology-name]-idranges.owl</code>. (For example, src/ontology/mondo-idranges.owl.)</li> <li>Copy and paste in the start and end values for your ID range.</li> </ol>"},{"location":"howto/switching-ontologies/#tips","title":"Tips","text":"<ol> <li>I work on many ontologies, so I keep a note in OneNote (or Evernote) that keeps track of all my ID ranges for quick reference.</li> <li>You don't need to track the last ID that was used, Protege will know to pick the next ID in your range. For example, if your ID range is 8000000 to 8999999, you can enter that as your range, even if you have already added 10 terms within your range. Protege will know to assign the next ID as 8000011.</li> </ol>"},{"location":"howto/switching-ontologies/#video-explanation","title":"Video Explanation","text":""},{"location":"howto/term-request/","title":"Make term requests to existing ontologies","text":""},{"location":"howto/term-request/#prerequisites","title":"Prerequisites","text":"<p>You need to have a GitHub account to make term requests. Sign up for a free GitHub account.</p>"},{"location":"howto/term-request/#background","title":"Background","text":""},{"location":"howto/term-request/#recommended-reading","title":"Recommended reading","text":"<p>This guide on How to select and request terms from ontologies by Chris Mungall provides some helpful background and tips for making term requests.</p>"},{"location":"howto/term-request/#why-make-a-new-term-request","title":"Why make a new term request?","text":"<p>Ontologies are under constant development and are continuously expanded and iterated upon. You may discover that a term you need is not available in your preferred ontology. In this case, please make a new term request to the ontology.</p>"},{"location":"howto/term-request/#making-term-requests-to-existing-ontologies","title":"Making term requests to existing ontologies","text":"<p>In the following text below, we describe best practices for making a term request to an ontology. In general, requests for new terms are make on the ontology GitHub issue tracker. For example, this is the GitHub issue tracker for the Uberon Anatomy onology.</p> <p>Note: These are suggestions and not strict rules. We appreciate your contributions to extending and improving ontologies. Following best guidelines is appreciated by the curators and developers, and assists them in addressing your issue more quickly. However, we understand if you are not always able to follow these best practices. Please add as much information as possible, and if there are any questions, the ontology developer may follow up with you for further clarification.</p>"},{"location":"howto/term-request/#making-a-new-term-request","title":"Making a new term request","text":"<ol> <li>Go to the ontology issue tracker in GitHub</li> <li>Select New issue</li> <li>Pick appropriate template (if applicable)</li> <li>If there is a template, fill in the information that is requested on the template below each header.</li> <li>General information that should be included in a new term request:</li> <li>Preferred label: Your preferred name or label for the new term. Note- new term request should not match existing terms or synonyms.</li> <li>Parent: The parent or superclass for that term. Remember that ontologies use subsumption reasoning, meaning that a subclass/child will inherit all the properties of the parent. In most ontologies, terms can have multiple classification, means terms can be classified under more than one parent.       Note: You can use a ontology search enginge like OLS to double check your class does not already exist and to search for parent terms in your respective ontology.</li> <li>Definition: Please write a concise definition of your term (see this guide on writing good ontology definitions).</li> <li>Definition database cross-reference(s): Indicate the source or database cross-reference(s) or source for the definition, such as a PubMed ID (PMID) or reference to a website.</li> <li>Synonym(s): an alternative term that has the same or closely related meaning for your new term. Please indicate the synonym scope (see more details below).</li> <li>Synonym database cross-reference(s): Provide a database cross-reference or source for the synonym, if applicable.</li> <li>Your ORCID or the URL for your working group, if applicable. If you do not have an ORCID, you can sign up for one for free here.       Note: You can link your ORCID in your GitHub profile.</li> <li>Comments: You can add any additional comments at the end. Please indicate if the comment should be included as a 'comment' annotation on the ontology term.</li> <li>Click Submit New Issue</li> <li>An ontology curator will review your issue and follow up with you if more information is needed.</li> </ol>"},{"location":"howto/term-request/#synonym-scopes","title":"Synonym scopes:","text":"<ul> <li>Exact - an exact match</li> <li>Narrow - more specific term</li> <li>Broad - more general term</li> <li>Related - a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct</li> </ul>"},{"location":"howto/term-request/#formatting","title":"Formatting:","text":"<ol> <li>For most ontologies, the preferred term labels should be lowercase (unless it is a proper name or abbreviation)</li> <li>Write the request below the prompts on the template so the Markdown formatting displays properly</li> <li>Synonyms should be lowercase (with exceptions above)</li> <li>Definition source - if from PubMed, please use the format PMID:XXXXXX (no space)</li> <li>Include the ID and label for the parent term</li> </ol>"},{"location":"howto/term-request/#submitting-other-issues","title":"Submitting other issues","text":"<ul> <li>Users may want to request other types of changes to an ontology such as Mondo beyond just adding a new term.</li> <li>Other types of requests may include changes to the classification, typos, bugs, etc.</li> <li>Some ontologies have other templates available on their issue tracker. Select the appropriate template. If there is not an appropriate template available, scroll to the bottom and select 'open a blank issue'.</li> </ul>"},{"location":"howto/term-request/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> </ul>"},{"location":"howto/update-import/","title":"Update Imports Workflow","text":"<p>This page discusses how to update the contents of your imports using the ODK, like adding or removing terms.</p> <p>Note: This is a specialised how-to for ODK managed ontologies and is replicated from ODK docs to consolidate workflows in the obook. Not all ontologies use ODKs and many ontologies have their own workflows for imports, please also check with your local ontology documents and/or developers.</p> <p>Note: The extract function in ROBOT can also be used to extract subsets from onotlogies for modular imports without the use of the ODK. For details on that, please refer to the ROBOT documentation</p>"},{"location":"howto/update-import/#importing-a-new-term","title":"Importing a new term","text":"<p>Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\".</p> <p>Importing a new term is split into two sub-phases:</p> <ol> <li>Declaring the terms to be imported</li> <li>Refreshing imports dynamically</li> </ol>"},{"location":"howto/update-import/#declaring-terms-to-be-imported","title":"Declaring terms to be imported","text":"<p>There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be):</p> <ol> <li>Protege-based declaration</li> <li>Using term files</li> <li>Using the custom import template</li> </ol>"},{"location":"howto/update-import/#protege-based-declaration","title":"Protege-based declaration","text":"<p>This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container. This approach also applies to ontologies that use base module import approach.</p> <ol> <li>Open your ontology (edit file) in Protege (5.5+).</li> <li>Select 'owl:Thing'</li> <li>Add a new class as usual.</li> <li>Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906.</li> <li>Click 'OK'</li> </ol> <p></p> <p>Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here), the metadata (labels, definitions, etc) for this term are imported from the respective external source ontology and becomes visible in your ontology.</p>"},{"location":"howto/update-import/#using-term-files","title":"Using term files","text":"<p>Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in <code>src/ontology/go_import.owl</code>, you will also have an associated term file <code>src/ontology/go_terms.txt</code>. You can add terms in there simply as a list:</p> <pre><code>GO:0008150\nGO:0008151\n</code></pre> <p>Now you can run the refresh imports workflow) and the two terms will be imported.</p>"},{"location":"howto/update-import/#using-the-custom-import-template","title":"Using the custom import template","text":"<p>This workflow is appropriate if:</p> <ol> <li>You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above).</li> <li>You wish to augment your imported ontologies with additional information. This requires a cautionary discussion.</li> </ol> <p>To enable this workflow, you add the following to your ODK config file (<code>src/ontology/cl-odk.yaml</code>), and update the repository (using <code>sh run.sh make update_repo</code>):</p> <pre><code>use_custom_import_module: TRUE\n</code></pre> <p>Now you can manage your imported terms directly in the custom external terms template, which is located at <code>src/templates/external_import.owl</code>. Note that this file is a ROBOT template, and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully.</p> <p>The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say current import <code>APOLLO_SV:00000480</code>, and you wish to import <code>APOLLO_SV:00000532</code>, you simply add a row like this:</p> <pre><code>ID  Entity Type\nID  TYPE\nAPOLLO_SV:00000480  owl:Class\nAPOLLO_SV:00000532  owl:Class\n</code></pre> <p>When the imports are refreshed see imports refresh workflow, the term(s) will simply be imported from the configured ontologies.</p> <p>Now, if you wish to extent the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the <code>ID</code> and <code>ENTITY</code> columns and (b) ensure that the ROBOT template is valid otherwise, see here.</p> <p>WARNING. Note that doing this is a widespread antipattern (see related issue). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file (see format variant documentation for explanation on what base file is)</p>"},{"location":"howto/update-import/#refresh-imports","title":"Refresh imports","text":"<p>If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example):</p> <p>First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory).</p> <pre><code>cd src/ontology\n</code></pre> <p>Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed.</p> <pre><code>sh run.sh make PAT=false imports/go_import.owl -B\n</code></pre> <p>Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above:</p> <pre><code>sh run.sh make refresh-go\n</code></pre> <p>Note that in case you changed the defaults, you need to add <code>IMP=true</code> and/or <code>MIR=true</code> to the command below:</p> <pre><code>sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B\n</code></pre> <p>If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. <code>go.owl</code> for your go import) you can set <code>MIR=false</code> instead, which will do the exact same thing as the above, but is easier to remember:</p> <pre><code>sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B\n</code></pre>"},{"location":"howto/update-import/#using-the-base-module-approach","title":"Using the Base Module approach","text":"<p>Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it. A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this:</p> <p>Imagine this being the full Uberon ontology:</p> <pre><code>Axiom 1: BFO:123 SubClassOf BFO:124\nAxiom 1: UBERON:123 SubClassOf BFO:123\nAxiom 1: UBERON:124 SubClassOf UBERON 123\n</code></pre> <p>The base file is the set of all axioms that are about UBERON terms:</p> <pre><code>Axiom 1: UBERON:123 SubClassOf BFO:123\nAxiom 1: UBERON:124 SubClassOf UBERON 123\n</code></pre> <p>I.e.</p> <pre><code>Axiom 1: BFO:123 SubClassOf BFO:124\n</code></pre> <p>Gets removed.</p> <p>The base file pipeline is a bit more complex then the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it.</p> <p>Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we</p> <ol> <li>First obtain the base (ususally by simply downloading it, but there is also an option now to create it with ROBOT)</li> <li>We merge all base files into one big pile</li> <li>Then we extract a single module <code>imports/merged_import.owl</code></li> </ol> <p>The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml.</p> <p>To check if your ontology uses this method, check src/ontology/cl-odk.yaml to see if <code>use_base_merging: TRUE</code> is declared under <code>import_group</code></p> <p>If your ontology uses Base Module approach, please use the following steps:</p> <p>First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you)</p> <p>Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory).</p> <pre><code>cd src/ontology\n</code></pre> <p>Then refresh imports by running</p> <pre><code>sh run.sh make imports/merged_import.owl\n</code></pre> <p>Note: if your mirrors are updated, you can run <code>sh run.sh make no-mirror-refresh-merged</code></p> <p>This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and your one of the local devs should pick this up and run the import for you.</p> <p>Lastly, restart Protege, and the term should be imported in ready to be used.</p>"},{"location":"images/","title":"About using images in Git/GitHub","text":"<p>There are two places you'll probaby want to use images in GitHub, in issue tracker and in markdown files, html etc. The way you handle images in these contexts is quite different, but easy once you get the hang of it.</p>"},{"location":"images/#in-markdown-files-and-html-etc","title":"In markdown files (and html etc)","text":"<p>All images referenced in static files such as html and markdown need to be referenced using a URL; dragging and dropping is not supported and could actually cause problems. Keeping images in a single directory enables them to be referenced more readily. Sensible file names are highly recommended, preferably without spaces as these are hard to read when encoded.</p> <p>An identical file, named in two different ways is shown as an example below. They render in the same way, but the source \"code\" looks ugly when spaces are used in file names.</p> <p>Eg.</p> encoding needed no encoding needed <code>![](github%20organizations%20teams%20repos.png</code> <code>![](github-organizations-teams-repos.png)</code> <p>In this example, the filename is enough of a 'url' because this file (https://ohsu-library.github.io/github-tutorial/howto/images/index.md) and the images are in the same directory https://ohsu-library.github.io/github-tutorial/howto/images/.</p> <p>To reference/embed an image that is not in the same directory, a more careful approach is needed.</p>"},{"location":"images/#referencing-images-in-your-repository-and-elsewhere","title":"Referencing images in your repository and elsewhere","text":"Absolute path referencing Relative path referencing <code>![](https://github.com/OHSU-Library/github-tutorial/raw/master/docs/other-images/owl.jpg)</code> <code>![](other-images/owl.jpg)</code> <p>Each instance of <code>../</code> means 'go up one level' in the file tree.</p> <p>It is also possible to reference an image using an external URL outside your control, in another github organization, or anywhere on the web, however this method can be fragile if the URL changes or could lead to unintended changes. Therefore make your own copies and reference those unless:</p> <ul> <li>You're sure that referencing the originals will not end in broken links or surprising content.</li> <li>Copying the image is prohibited</li> <li>The images are too large to make copying worth the hassle/expense.</li> </ul> <p>For example, it is not clear for how long the image below will manage to persist at this EPA link, or sadly, for how long the image will even be an accurate reflection of the current situation in the arctic. https://www.epa.gov/sites/production/files/styles/microsite_banner/public/2016-12/epa-banner-images/science_banner_arctic.png</p> <p></p>"},{"location":"images/#in-github-issue-tracker","title":"In GitHub issue tracker","text":"<p>Images that are embedded into issues can be dragged and dropped in the GitHub issues interface. Once you've done so, it will look something like this with GitHub assigning an arbitrary URL (githubuserassets) for the image.</p> <p><code>![](screenshot-of-images-in-issues.png)</code></p> <p></p>"},{"location":"images/#sizing-images","title":"Sizing images","text":"<p>Ideally, a Markdown document is renderable in a variety of output formats and devices. In some cases, it may be desirable to create non-portable Markdown that uses HTML syntax to position images. This limits the longevity of the artifact, but may be necessary sometimes. We describe how to manage this below.</p> <p>In order to size images, use the native html syntax: <code>width =</code> with the &lt;img src=, as per below.</p> <p><code>&lt;img src=\"https://github.com/monarch-initiative/monarch-app/raw/master/image/Phenogrid3Compare.png\" width=\"53\"&gt;</code></p> <p></p>"},{"location":"images/#back-to-home","title":"Back to Home","text":""},{"location":"lesson/analysing-linked-data/","title":"Analysing Linked Data (Fundamentals)","text":""},{"location":"lesson/analysing-linked-data/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/analysing-linked-data/#prerequisites","title":"Prerequisites","text":"<ul> <li>Review tutorial on Ontology Theory</li> </ul>"},{"location":"lesson/analysing-linked-data/#preparation","title":"Preparation","text":"<ul> <li>Essential</li> <li>Linked Data Engineering: Week 1</li> <li>Support</li> <li>Programming Historian Linked Data tutorial</li> <li>Original Whitepaper (Tim Berners Lee et al)</li> <li>Educational curriculum for Linked Data</li> <li>Tools: Browse through the tools and standards listed in the Semantic Engineer Toolbox.</li> </ul>"},{"location":"lesson/analysing-linked-data/#learning-objectives","title":"Learning objectives","text":"<ul> <li>Advanced SPARQL</li> <li>Term enrichment</li> <li>Semantic similarity</li> <li>Named Entity Recognition</li> </ul>"},{"location":"lesson/analysing-linked-data/#tutorials","title":"Tutorials","text":""},{"location":"lesson/analysing-linked-data/#the-linked-data-landscape-from-an-obo-perspective-standards-services-and-tools","title":"The Linked Data landscape from an OBO perspective: Standards, Services and Tools","text":"<p>In the following we will look a bit at the general Linked Data landscape, and name some of its flagship projects and standards. It is important to be clear that the Semantic Web field is a very heterogenous one:</p>"},{"location":"lesson/analysing-linked-data/#flagship-projects-of-the-wider-semantic-web-community","title":"Flagship projects of the wider Semantic Web community","text":"<ul> <li>Linked Open Data (LOD) cloud: The flagship project of the Semantic Web. An attempt to make all, or anyways a lot, of Linked Data accessible in one giant knowledge graph. A good overview can be found in this medium article. Note that some people seem to think that the Semantic Web is (or should be) the Linked Open Data cloud. I would question this view, but I am not yet decided what my position is.</li> <li>Schema.org: General purpose vocabulary for entities on the web, founded by Google, Microsoft, Yahoo and Yandex. To get a better sense of the types of entities and relationships covered see here.</li> <li>DBpedia: Project that extracts structured data from Wikipedia and makes it available as a giant knowledge graph. The associated ontology, similar to schema.org, covers entities encountered in common sense knowledge.</li> <li>Wikidata: Free and open knowledge base that can be edited in much the same way as Wikipedia is edited.</li> </ul> <p>While these Semantic Web flagship projects are doubtlessly useful, it is sometimes hard to see how they can help for your biomedical research. We rarely make use of them in our day to day work as ontologists, but there are some notable exceptions:</p> <ul> <li>Where our work involves modelling environmental factors, we sometimes use wikidata as a standard way to refer for example to countries.</li> <li>For some more common sense knowledge use cases, such as nutrition, consider augmenting your knowledge graph with data from wikidata or dbpedia. While they may be a bit more messy and not directly useful for exploration by humans, it is quite possible that Machine Learning approaches can use the additional context provided by these knowledge graphs to improve embeddings and deliver more meaningful link predictions.</li> <li>Some OBO ontologies are already on Wikidata - perhaps you can find additional synonyms and labels which help with your data mapping problems!</li> </ul>"},{"location":"lesson/analysing-linked-data/#where-the-obo-and-semantic-web-communities-are-slightly-at-odds","title":"Where the OBO and Semantic Web communities are slightly at odds","text":"<p>The OBO format is a very popular syntax for representing biomedical ontologies. A lot of tools have been built over the years to hack OBO ontologies on the basis of that format - I still work with it on a daily basis. Although it has semantically been proven to be a subset of OWL (i.e. there is a lossless mapping of OBO into OWL) and can be viewed as just another syntax, it is in many ways idiosyncratic. For starters, you wont find many, if any, IRIs in OBO ontologies. The format itself uses CURIEs which are mapped to the general OBO PURL namespace during transformation to OWL. For example, if you see MONDO:0003847 in an OBO file, and were to translate it to OWL, you will see this term being translated to http://purl.obolibrary.org/obo/MONDO_0003847. Secondly, you have a bunch of built-in properties like BROAD or ABBREVIATION that mapped to a vocabulary called oboInOwl (oio). These are pretty non-standard on the general Semantic Web, and often have to be manually mapped to the more popular counterparts in the Dublin Core or SKOS namespaces.</p> <p>Having URIs as identifiers is not generally popular in the life sciences. As discussed elsewhere, it is much more likely to encounter CURIEs such as MONDO:0003847 than URIs such as http://purl.obolibrary.org/obo/MONDO_0003847 in biomedical databases.</p> <p></p>"},{"location":"lesson/analysing-linked-data/#useful-tools-for-biomedical-research","title":"Useful tools for biomedical research","text":"<p>Why does the biomedical research, and clinical, community care about the Semantic Web and Linked Data? There are endless lists of applications that try to apply semantic technologies to biomedical problems, but for this week, we only want to look at the broader picture. In our experience, the use cases where Semantic Web standards are applied successfully are:</p> <ul> <li>Where to find ontologies: Ontology repositories</li> <li>OBO Foundry Ontology Library</li> <li>BioPortal<ul> <li>CPT Story. The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes.</li> </ul> </li> <li>AgroPortal: Like BioPortal, but focussed on the Agronomy domain.</li> <li>Linked Open Data Vocabularies (LOV): Lists the most important vocabularies in the Linked Data space, such as Dublin Core, SKOS and Friend-of-a-Friend (FOAF).</li> <li>Where to find terms: Term browsers</li> <li>OLS: The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work.</li> <li>Ontobee: The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. A key difference between Ontobee and OLS/Bioportal is that Ontobee limits hierarchical relationships to is_a. This means if you are browsing ontologies such as GO, Uberon, CL, ENVO, you will not see part-of links in the hierarchy, and these links are crucial for understanding these ontologies.</li> <li>AberOWL: Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS.</li> <li>identifiers.org: A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here, and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with.</li> <li>Curate biomedical data. There are a lot of different tools in this space - which we will discuss in a bespoke unit later in the course. Examples:</li> <li>isatools: The open source ISA framework and tools help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies.</li> <li>RightField: System for curating ontology terms in Excel spreadsheets.</li> <li>CEDAR Templates: Basically a templating system that allows to create templates to record metadata, for example in a lab setting, of course with ontology integration.</li> <li>Other examples of tabular data to RDF converters, but new ones coming up every year.</li> <li>Building ontologies</li> <li>Populous/Webulous: A system to maintain/generate ontologies from spreadsheets. The idea was to basically to define patterns in a (now mostly dead) language called OPPL, and then apply them to spreadsheets to generate OWL axioms. EBI recently discontinued the service, as there is a general exodus to Google Sheets + ROBOT templates instead.</li> <li>ROBOT templates + Google Sheets and Cogs: A lightweight approach based on a set of tools that allows curating ontologies in spreadsheets (e.g. Google Sheets) which are converted into OWL using ROBOT.</li> <li>DOSDP tools + Dead Simple Design Patterns (DOSDP): Similar to ROBOT templates, DOSDPs (which really should be called DOSDTs, because they are not really design patterns; they are ontology templates), another system that allows the generation of OWL axioms based on spreadsheet data.</li> <li>Cleaning messy data</li> <li>OpenRefine: I have not myself used this ever, but some of my colleagues have. OpenRefine allows you to upload (spreadsheet) data, explore it and clean it (going as far as reconciling terms using Wikidata concepts).</li> </ul>"},{"location":"lesson/analysing-linked-data/#which-biomedical-ontologies-should-we-use","title":"Which biomedical ontologies should we use?","text":"<p>As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \"10 simple rules for selecting a Bio-ontology\" by Malone et al, but I would further recommend to ask yourself the following:</p> <ul> <li>Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation).</li> <li>Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently ploy-hierarchical (such as Mondo), while others strive to be single inheritance (DO, ICD).</li> <li>Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data.</li> </ul> <p>Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being payed every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS!) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them.</p> <p>We will discuss ontologies in the medical, phenomics and genomics space in more detail in a later session of the course.</p>"},{"location":"lesson/analysing-linked-data/#other-interesting-links","title":"Other interesting links","text":"<ul> <li>Linked Data in e-Government</li> <li>Industrial Ontologies Foundry: Something like the OBO Foundry for Industrial Ontologies</li> <li>OntoCommons: An H2020 CSA project dedicated to the standardisation of data documentation across all domains related to materials and manufacturing.</li> </ul>"},{"location":"lesson/analysing-linked-data/#basic-linked-data-and-semantic-web-concepts-for-the-semantic-data-engineer-in-the-biomedical-domain","title":"Basic Linked data and Semantic Web Concepts for the Semantic Data Engineer in the Biomedical Domain","text":"<p>In this section we will discuss the following:</p> <ul> <li>Introductory remarks</li> <li>The advantages of globally unique identifiers</li> <li>Some success stories of the Semantic Web in the biomedical domain</li> <li>Some basic concepts you should probably have heard about</li> <li>The ecosystem of the Semantic Web: Standards, Technologies and Research Areas</li> <li>Typical tasks of Semantic Data Engineers in the biomedical domain</li> </ul>"},{"location":"lesson/analysing-linked-data/#introduction","title":"Introduction","text":"<p>Note of caution: No two Semantic Web overviews will be equivalent to each other. Some people claim the Semantic Web as an idea is an utter failure, while others praise it as a great success (in the making) - in the end you will have to make up your own mind. In this section I focus on parts of the Semantic Web step particularly valuable to the biomedical domain, and I will omit many relevant topics in the wider Semantic Web area, such as Enterprise Knowledge Graphs, decentralisation and personalisation, and many more. Also, the reader is expected to be familiar with the basic notions of the Semantic Web, and should use this overview mainly to tie some of the ideas together.</p> <p>The goal of this section is to give the aspiring Semantic Data Engineer in the biomedical domain a rough idea of key concepts around Linked Data and the Semantic Web insofar as they relate to their data science and and data engineering problems. Even after 20 years of Semantic Web research (the seminal paper, conveniently and somewhat ironically behind a paywall, was published in May 2001), the area is still dominated by \"academic types\", although the advent of the Knowledge Graph is already changing that. As I already mentioned above, no two stories of what the Semantic Web is will sound the same. However, there are a few stories that are often told to illustrate why we need semantics. The OpenHPI course names a few:</p> <ul> <li>\"From the web of documents to the web of data\" tells the story of how the original web is essentially a huge heap of (interlinked) natural language text documents which are very hard to understand for search engines: Does the word \"Jaguar\" on this site refer to the car or the cat? Clarifying in your web page that the word Jaguar refers to the concept of \"Jaguar the cat\", for example like this: <code>&lt;span about=\"dbpedia:Jaguar\"&gt;Jaguar&lt;/span&gt;</code>, will make it easier for the search engine to understand what your site is about and link it to other relevant content. From this kind of mark-up, structured data can be extracted and integrate into a giant, worldwide database, and exposed through SPARQL endpoints, that can then be queried using a suitable query language.</li> <li>\"From human to machine understandable\": as a Human, we know that a Jaguar is a kind of cat, and all cats have four legs. If you ask a normal search engine: \"Does a Jaguar have four legs?\" it will have a tough time to answer this question correctly (if it cannot find that exact statement anywhere). That is why we need proper semantics, some kind of formalism such that a \"machine\" can deduce from the statements \"Jaguar is a cat; Cat has four legs\" that \"Jaguar has four legs\".</li> <li>The \"Semantic Layer Cake\": a box/brick diagram showing how Semantic Web Technologies are stacked on top of each other. An engineering centric view that has been used countless times to introduce the Semantic Web, but rarely helped anyone to understand what it is about.</li> </ul> <p>I am not entirely sure anymore that any of these ways (web of data, machine understanding, layered stack of matching standards) to motivate the Semantic Web are particularly effective for the average data scientists or engineer. If I had to explain the Semantic Web stack to my junior self, just having finished my undergraduate, I would explain it as follows (no guarantee though it will help you).</p> <p>The Semantic Web / Linked Data stack comprises roughly four components that are useful for the aspiring Semantic (Biomedical) Data Engineer/Scientist to distinguish:</p>"},{"location":"lesson/analysing-linked-data/#a-way-to-refer-to-things-including-entities-and-relations-in-a-global-namespace","title":"A way to refer to things (including entities and relations) in a global namespace.","text":"<p>You, as a scientist, might be using the term \"gene\" to refer to basic physical and functional unit of heredity, but me, as a German, prefer the term \"Gen\". In the Semantic Web, instead of natural language words, we prefer to use URIs to refer to things such as https://www.wikidata.org/wiki/Q7187: if you say something using the name https://www.wikidata.org/wiki/Q7187, both your German and Japanese colleagues will \"understand\" what you are referring to. More about that in the next chapter.</p>"},{"location":"lesson/analysing-linked-data/#lots-loaaaads-of-ways-to-make-statements-about-things","title":"Lots (loaaaads!) of ways to make statements about things.","text":"<p>For example, to express \"a mutation of SHH in humans causes isolated microphthalmia with coloboma-5\" you could say something like (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://purl.obolibrary.org/obo/RO_0004020 | \"has basis in dysfunction of\"]--&gt;(https://identifiers.org/HGNC:10848 | \"SSH (gene)\"). Or you could say: (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://www.w3.org/2000/01/rdf-schema#subClassOf | \"is a\"]--&gt;(http://purl.obolibrary.org/obo/MONDO_0003847 | \"Mendelian Disease\"). If we use the analogy of \"language\", then the URIs (above) are the words, and the statements are sentences in a language. Unfortunately, there are many languages in the Semantic Web, such as OWL, RDFS, SKOS, SWRL, SHACL, SHEX, and dialects (OWL 2 EL, OWL 2 RL) and a plethora of formats, or serialisations (you can store the exact same sentence in the same language such as RDF, or OWL, in many different ways)- more about that later. In here lies also one of the largest problems of the Semantic Web - lots of overlapping standards means, lots of incompatible data - which raises the bar for actually being able to seamlessly integrate \"statements about things\" across resources.</p>"},{"location":"lesson/analysing-linked-data/#collections-of-statements-about-things-that-somehow-belong-together-and-provide-some-meaning-or-context-for-those-things","title":"Collections of statements about things that somehow belong together and provide some meaning, or context, for those things.","text":"<p>Examples include:</p> <ul> <li>controlled vocabularies, that define, for example, how to refer to a disease (e.g., we use http://purl.obolibrary.org/obo/MONDO_0012709 to refer to \"isolated microphthalmia with coloboma 5\"),</li> <li>terminologies which describe how we humans refer to a disease (How is it called in German? Which other synonyms are used in the literature? How is the term defined in the medical literature?),</li> <li>taxonomies which define how diseases are related hierarchically (\"microphthalmia, isolated, with coloboma 5 is a kind of Mendelian disease\"),</li> <li>ontologies which further define how diseases are defined in terms of other entities, for example \"microphthalmia, isolated, with coloboma 5 is a Mendelian disease that has its basis in the dysfunction of SSH\".</li> <li>Note: In practice, when we say \"ontology\", we mean all of the above together - it is, however, good to know that they are somewhat distinct, and that there are different \"languages\" that can be used for each of these distinctions.</li> </ul>"},{"location":"lesson/analysing-linked-data/#tools-that-do-something-useful-with-these-collections-of-statements","title":"Tools that do something useful with these collections of statements.","text":"<p>For example (as always, non exhaustive):</p> <ul> <li>Efficient storage (triple stores, in-memory ontology APIs, other databases). Similar to traditional SQL databases, the Semantic Web comes with a number of database solutions that are optimised to deliver \"semantic content\".</li> <li>Semantically aware querying: Very similar to traditional SQL (which, incidentally is often great to query semantic data), there are various ways to \"interrogate\", or query, your Linked Data, such as SPARQL, DL Querying, Ontology-based data access (OBDA).</li> <li>Subsetting (module/subset extraction): Often, ontologies (or other collections of Linked Data statements) are very large and cover a lot of entities and knowledge that is not important to your work. There are a number of techniques that allow you to extract meaningful subsets for your use case; for example, you may be interested to get all the information you can about Mendelian diseases, but you don't care about common diseases (e.g. see ROBOT extract).</li> <li>Visualisation: As a Data Scientist, you are used to looking at your data in tabular form - while a lot of information stored in ontologies can still be inspected this way, in general semantic data is best perceived as a graph - which are notoriously hard to visualise. Fortunately, a lot of Linked Data, in particular ontologies in the biomedical domain are predominantly tree-shaped (you have a disease, and underneath sub-diseases). Term browsers like OLS typically render ontologies as trees.</li> <li>Data linking/matching: This is key in particular in the biomedical sciences, as there is almost never just one way to refer to the same thing. In my experience, a good rule of thumb is that there are 3-6, e.g. 3-6 URIs that refer to \"Mendelian Disease\", all of which need to be matched together to integrate data across resources. There are many approaches to ontology matching - none of which are anywhere near perfect.</li> <li>Automated error checking and validation (Syntax, Structure (SHEX, SHACL), logical Consistency (DL Reasoner)): Naturally, writing sentences in any language is hard in the beginning, but this is even more true for highly complex languages such as OWL. In my experience, no-one can write flawless OWL without the help of automatic syntax and semantics checking, at least not consistently. Validation tools are a crucial part of your Semantic Engineering toolbox.</li> <li>Translate between languages: Often we need to translate from one language, for example OWL, to another, for example SKOS to integrate divergent resources. Translations in the Semantic Web context are nearly always lossy (there are always things you can say in one language, but not in another), but they may be necessary nevertheless.</li> <li>Discovery of terms (OLS, BioPortal): If you are curating terms, you need to know what ID (URI) to use for \"isolated microphthalmia with coloboma 5\". For that, term browsers such as OLS are perfect. Just type in your natural language search term, and you will find a series of suggestions for URIs you can use for your curation.</li> <li>Discovery of vocabularies OBO Foundry ontology library, BioPortal): we will have a section later on on how to select appropriate ontologies for your use case, but the general problem of finding vocabularies, or ontologies, is answered by ontology repositories or libraries. Naturally, our favourite ontology library is the OBO Foundry ontology library which contain a lot of high quality ontologies for the biomedical domain.</li> <li>Make implicit knowledge explicit, aka reasoning:</li> <li>Deductive (DL Reasoning, Rule-based reasoning such as Datalog, SWRL). One of the major selling points for OWL, for example, in the biomedical domain is the ability to use logical reasoning in a way that is sound (only gives you correct inferences, at all times) and complete (all hidden implications are found, at all times, by the reasoner) - this is particularly great for medical knowledge where mistakes in computer algorithms can have devastating effects. However, I am slowly coming to the conviction that sound and complete reasoning is not the only form of deductive reasoning that is useful - many rule languages can offer value to your work by unveiling hidden relationships in the data without giving such strong \"logical guarantees\".</li> <li>Inductive (Machine Learning approaches, Knowledge Graph Representation Learning). The new frontier - we will discuss later in the course how our ontology-powered Knowledge Graphs can be leveraged to identify drug targets, novel gene to phenotype associations and more, using a diverse set of Machine Learning-based approaches.</li> </ul> <p>This week will focus on 1 (identifiers) and 4 (applications) - 2 (languages and standards) and 3 (controlled vocabularies and ontologies) will be covered in depth in the following weeks.</p> <p>Note on the side: Its not always 100% clear what is meant by Linked Data in regular discourse. There are some supposedly \"clear\" definitions (\"method for publishing structured data\", \"collection of interrelated datasets on the Web\"), but when it comes down to the details, there is plenty of confusion (does an OWL ontology constitute Linked Data when it is published on the Web? Is it Linked Data if it does not use RDF? Is it Linked Data if it is less than 5-star - see below). In practice all these debates are academic and won't mean much to you and your daily work. There are entities, statements (context) being said about these entities using some standard (associated with the Semantic Web, such as OWL or RDFS) and tools that do something useful with the stuff being said.</p>"},{"location":"lesson/analysing-linked-data/#when-i-say-mendelian-disease-i-mean-httppurlobolibraryorgobomondo_0003847","title":"When I say \"Mendelian Disease\" I mean http://purl.obolibrary.org/obo/MONDO_0003847","text":"<p>One of the top 5 features of the Semantic Web (at least in the context of biomedical sciences) is the fact that we can use URIs as a global identifier scheme that is unambiguous, independent of database implementations, independent of language concerns to refer to the entities in our domain.</p> <p>For example, if I want to refer to the concept of \"Mendelian Disease\", I simply refer to http://purl.obolibrary.org/obo/MONDO_0003847 - and everyone, in Japan, Germany, China or South Africa, will be able to \"understand\" or look up what I mean. I don't quite like the word \"understanding\" in this context as it is not actually trivial to explain to a human how a particular ID relates to a thing in the real world (semiotics). In my experience, this process is a bit rough in practice - it requires that there is a concept like \"Mendelian Disease\" in the mental model of the person, and it requires some way to link the ID http://purl.obolibrary.org/obo/MONDO_0003847 to that \"mental\" concept - not always as trivial as in this case (where there are standard textbook definitions). The latter is usually achieved (philosophers and linguists please stop reading) by using an annotation that somehow explains the term - either a label or some kind of formal definition - that a person can understand. In any case, not trivial, but thankfully not the worst problem in the biomedical domain where we do have quite a wide range of shared \"mental models\" (more so in Biology than Medical Science..). Using URIs allows us to facilitate this \"understanding\" process by leaving behind some kind of information at the location that is dereferenced by the URI (basically you click on the URI and see what comes up). Note that there is a huge deal of compromise already happening across communities. In the original Semantic Web community, the hope was somehow that dereferencing the URI (clicking on it, navigating to it) would reveal structured information about the entity in question that could used by machines to understand what the entity is all about. In my experience, this was rarely ever realised in the biomedical domain. Some services like Ontobee expose such machine readable data on request (using a technique called content negotiation), but most URIs simply refer to some website that allow humans to understand what it means - which is already a huge deal. For more on names and identifiers I refer the interested reader to James Overton's OBO tutorial here.</p> <p>Personal note: Some of my experienced friends in the bioinformatics world say that \"IRI have been more pain than benefit\". It is clear that there is no single thing in the Semantic Web is entirely uncontested - everything has its critics and proponents.</p>"},{"location":"lesson/analysing-linked-data/#the-advent-of-the-curie-and-the-bane-of-the-curie-map","title":"The advent of the CURIE and the bane of the CURIE map","text":"<p>In reality, few biological resources will contain a reference to http://purl.obolibrary.org/obo/MONDO_0003847. More often, you will find something like <code>MONDO:0003847</code>, which is called a CURIE. You will find CURIEs in many contexts, to make Semantic Web languages easier to read and manage. The premise is basically that your document contains a prefix declaration that says something like this:</p> <pre><code>PREFIX MONDO: &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\n</code></pre> <p>which allows allows the interpreter to unfold the CURIE into the IRI:</p> <pre><code>MONDO:0003847 -&gt; http://purl.obolibrary.org/obo/MONDO_0003847\n</code></pre> <p>In reality, the proliferation of CURIEs has become a big problem for data engineers and data scientists when analysing data. Databases rarely, if ever, ship the CURIE maps with their data required to understand what a prefix effectively stands for, leading to a lot of guess-work in the daily practice of the Semantic Data Engineer (if you ever had to distinguish ICD: ICD10: ICD9: UMLS:, UMLSCUI: without a prefix map, etc you will know what I am talking about). Efforts to bring order to this chaos, essentially globally agreed CURIE maps (e.g. prefixcommons), or ID management services such as identifiers.org exist, but right now there is no one solution - prepare yourself to having to deal with this issue when dealing with data integration efforts in the biomedical sciences. More likely than not, your organisation will build its own curie map and maintain it for the duration of your project.</p>"},{"location":"lesson/analysing-linked-data/#semantic-web-in-the-biomedical-domain-success-stories","title":"Semantic Web in the biomedical domain: Success stories","text":"<p>There are probably quite a few divergent opinions on this, but I would like to humbly list the following four use cases as among the most impactful applications of Semantic Web Technology in the biomedical domain.</p>"},{"location":"lesson/analysing-linked-data/#light-semantics-for-data-aggregation","title":"Light Semantics for data aggregation.","text":"<p>We can use hierarchical relations in ontology to group data. For example, if I know that http://purl.obolibrary.org/obo/MONDO_0012709 (\"microphthalmia, isolated, with coloboma 5\") http://www.w3.org/2000/01/rdf-schema#subClassOf (\"is a\") http://purl.obolibrary.org/obo/MONDO_0003847 (\"Mendelian Disease\"), then a specialised Semantic Web tool called a reasoner will know that, if I ask for all genes associated with Mendelian diseases, you also want to get those associated with \"microphthalmia, isolated, with coloboma 5\" specifically (note that many query engines such as SPARQL with RDFS entailment regime have simple reasoners embedded in them, but we would not call them \"reasoner\" - just query engine).</p>"},{"location":"lesson/analysing-linked-data/#heavy-semantics-for-ontology-management","title":"Heavy Semantics for ontology management.","text":"<p>Ontologies are extremely hard to manage and profit from the sound logical foundation provided by the Web Ontology Language (OWL). We can logically define our classes in terms of other ontologies, and then use a reasoner to classify our ontology automatically. For example, we can define abnormal biological process phenotypes in terms of biological processes (Gene Ontology) and classify our phenotypes entirely using the classification of biological processes in the Gene Ontology (don't worry if you don't understand a thing - we will get to that in a later week).</p>"},{"location":"lesson/analysing-linked-data/#globally-unique-identifiers-for-data-integration","title":"Globally unique identifiers for data integration.","text":"<p>Refer to the same thing the same way. While this goal was never reached in total perfection, we have gotten quite close. In my experience, there are roughly 3-6 ways to refer to entities in the biomedical domain (like say, ENSEMBL, HGNC, Entrez for genes; or SNOMED, NCIT, DO, MONDO, UMLS for diseases). So while the \"refer to the same thing the same way\" did not truly happen, a combination of standard identifiers with terminological mappings, i.e. links between terms, can be used to integrate data across resources (more about Ontology Matching later). Again, many of my colleagues disagree - they don't like IRIs, and unfortunately, you will have to build your own position on that.</p> <p>Personal note: From an evolutionary perspective, I sometimes think that having 2 or 3 competing terminological systems is better than 1, as the competition also drives the improvements in quality, but there is a lot of disagreement on this.</p>"},{"location":"lesson/analysing-linked-data/#coordinated-development-of-mutually-compatible-ontologies-across-the-biomedical-domain-the-open-biological-and-biomedical-ontologies-obo-foundry","title":"Coordinated development of mutually compatible ontologies across the biomedical domain: The Open Biological and Biomedical Ontologies (OBO) Foundry.","text":"<p>The OBO Foundry is a community-driven effort to coordinate the development of vocabularies and ontologies across the biomedical domain. It develops standards for the representation of terminological content (like standard properties), and ontological knowledge (shared design patterns) as well as shared systems for quality control. Flagship projects include:</p> <ul> <li>The Relation Ontology (RO) for the standardisation of relationships that connect entities in biomedical ontologies.</li> <li>The Core Ontology for Biology and Biomedicine (COB): upper ontology to align key entities used throughout biomedical ontologies.</li> <li>The OBO Metadata Ontology for aligning ontology metadata properties across OBO ontologies.</li> <li>The OBO Persistent Identifier System: an Identifier scheme for persistent URIs used by many ontologies on the web. The system is used to refer to terms as well as ontologies and their versions.</li> <li>OBO Dashboard: a system for the monitoring and continued improvement of OBO ontologies with automated Quality Control checks.</li> </ul>"},{"location":"lesson/analysing-linked-data/#semantic-web-and-linked-data-things-you-should-have-heard-about","title":"Semantic Web and Linked Data: Things you should have heard about","text":"<ul> <li>The Semantic Web Layer Cake: A iconic, colourful graphic that describes the layered design of the semantic web, from URIs to Logic. Its not particularly useful, but as a Semantic Web Explorer, you should have seen it.</li> <li>Linked Data is mostly referred to as a \"method for publishing data\", a key concept in the Semantic Web domain, coined by Tim Berners Lee in 2006. Related concepts:</li> <li>Linked Data Principles:<ul> <li>Use URIs as names for things</li> <li>Use HTTP URIs so that people can look up those names.</li> <li>When someone looks up a URI, provide useful information.</li> <li>Include links to other URIs. so that they can discover more things.</li> </ul> </li> <li>5-Star system<ol> <li>make your stuff available on the Web (whatever format) under an open license</li> <li>make it available as structured data (e.g., Excel instead of image scan of a table)</li> <li>use non-proprietary formats (e.g., CSV instead of Excel)</li> <li>use URIs to denote things, so that people can point at your stuff</li> <li>link your data to other data to provide context</li> </ol> </li> <li>FAIR data: Principles defined in 2016, somewhat orthogonal to Linked Data Principles. A nice tutorial, also going a bit more in depth into identifiers than what we did in this section, can be found here. The idea of FAIR data is probably more impactful in the biomedical and pharmaceutical world then the idea of Linked Data. While there are some (slighltly irritating) voices on the sidelines that say that \"It can't be FAIR if its not RDF\", it is probably true that a nicely formatted CSV file on the Web is at least as useful as a (hard to understand) RDF dump containing the same data. Worldwide collaborations between major pharmaceutical corporations promoting FAIR data, such as the Pistoia Alliance do mention Semantic Web Technologies in their White papers, but keep the jargon a bit more hidden from the general public. Data, according to the FAIR principles, should be:</li> <li>Findable (machine readable metadata, etc)</li> <li>Accessible (open authentication, authorisation)</li> <li>Interoperable (integrated with other data, closely related to controlled vocabularies and linked data)</li> <li>Reusable (metadata, license, provenance)</li> <li>World Wide Web Consortium (W3C): The World Wide Web Consortium (W3C) is an international community that develops open standards, in particular many of those (but not all!) pertaining to the Semantic Web.</li> </ul>"},{"location":"lesson/analysing-linked-data/#the-ecosystem-of-linked-data-and-semantic-web-standards-technologies-and-research-areas","title":"The Ecosystem of Linked Data and Semantic Web: Standards, Technologies and Research Areas","text":"<p>In the following, we will list some of the technologies you may find useful, or will be forced to use, as a Semantic Data Engineer. Most of these standards will be covered in the subsequent weeks of this course.</p> Standard Purpose Use case Web Ontology Language (OWL) Representing Knowledge in Biomedical Ontologies All OBO ontologies must be provided in OWL as well. Resource Description Framework (RDF) Model for data interchange. Triples, the fundamental unit of RDF, are ubiquitous on the Semantic Web SPARQL Query Language for RDF A standard query language for RDF and RDFS. Primary query language to interrogate RDF/RDFS/Linked Data on the Web. Simple Knowledge Organization System (SKOS) Another, more lightweight, knowledge organisation system in many ways competing with OWL. Not as widely used in the biomedical domain as OWL, but increasing uptake of \"matching\" vocabulary (skos:exactMatch, etc). RDF-star A key shortcoming of RDF is that, while I can in principle say everything about everything, I cannot directly talk about edges, for example to attribute provenance: \"microphthalmia, isolated, with coloboma 5 is kind of Mendelian disease\"--source: Wikipedia Use cases here. JSON-LD A method to encoding linked data in JSON format. (Very useful to at least know about). RDFa W3C Recommendation to embed rich semantic metadata in HTML (and XML). I have to admit - in 11 years Semantic Web Work I have not come across much use of RDFa in the biomedical domain. But @jamesaoverton is using it in his tools! <p>A thorough overview of all the key standards and tools can be found on the Awesome Semantic Web repo.</p> <p>For a rough sense of current research trends it is always good to look at the accepted papers at one of the major conferences in the area. I like ISWC (2020 papers), but for the aspiring Semantic Data Engineering in the biomedical sphere, it is probably a bit broad and theoretical. Other interesting specialised venues are the Journal of Biomedical Semantics and the International Conference on Biomedical Ontologies, but with the shift of the focus in the whole community towards Knowledge Graphs, other journals and conferences are becoming relevant.</p> <p>Here are a few key research areas, which are, by no means (!), exhaustive.</p> <ul> <li>How can we combine data and knowledge from different ontologies/knowledge graphs?</li> <li>Ontology/Knowledge graph alignment: How can we effectively link to ontologies, or knowledge graphs, together?</li> <li>Ontology merging: combine two ontologies by corresponding concepts and relations.</li> <li>Ontology matching: A sub-problem of ontology alignment, namely the problem of determining whether two terms (for example two diseases) from different ontologies should be linked together or not.</li> <li>How can we integrate data from unstructured and semistructured sources such as documents or spreadsheets?</li> <li>Named Entity Recognition (NER): the process of identifying a named \"thing\" in a text.</li> <li>Entity linking: The task of associating a named entity, for example the result of a Named Entity Recognition algorithm, or the column of a spreadsheet, to a concept in an ontology. For example, the value \"Mendelian Disease\" is linked to the concept http://purl.obolibrary.org/obo/MONDO_0003847.</li> <li>Relationship extraction: Once you have identified the genes and diseases in your Pubmed abstracts, you will want to understand how they related to each other. Is the gene the \"basis in dysfunction of\" the disease? Or just randomly co-occurs in the sentence?</li> <li>Note: Many of the problems in this category are typically associated with the domain of Natural Language Processing rather than Semantic Web.</li> <li>How can we generate insight from semantically integrated data?</li> <li>Knowledge Graphs and Machine Learning<ul> <li>Knowledge Graph Embeddings. The number one hype topic in recent years: How do you get from a graph of interrelated entities to a faithful representation in a vector space (basically numbers), so that Machine Learning algorithms can do their magic?</li> <li>Link predication: Based on what we know, which are the best drug targets for my rare disease of interest?</li> </ul> </li> <li>Logical reasoning: While the research on deductive reasoning, at least the more \"hard-core\" Description Logic kind, seems to be a bit more quiet in recent years (maybe I am wrong here, I just see much less papers coming through my Google Scholar alerts now then I used to), there is still a lot going on in this domain: more efficient SPARQL engines, rule-based reasoning such as the recently commercialised RDFox reasoner and many more.</li> <li>Other research areas (not in any way exhaustive):</li> <li>Web decentralisation and privacy:<ul> <li>Solid: Solid (Social Linked Data) is a web decentralization project led by Tim Berners-Lee, with the aim of true ownership of personal data and improved privacy. \"Pods\" are like secure personal web servers for data from which application can request data.</li> <li>Shape validation: It is very difficult to validate huge Knowledge Graphs of interrelated data efficiently (by validate we can mean a lot of things, such as making sure that your cat does not accidentally end up as someone's \"Mendelian Disease\"). Shape languages such as Shex and SHACL are poised to solve this problem, but the research is ongoing.</li> </ul> </li> <li>New standards and tools: There is always someone proposing a new semantic standard for something or building a new kind of triple store, SPARQL extension or similar.</li> </ul>"},{"location":"lesson/analysing-linked-data/#typical-jobs-of-semantic-data-engineers-in-the-biomedical-domain","title":"Typical Jobs of Semantic Data Engineers in the biomedical domain","text":"<p>It is useful to get a picture of the typical tasks a Semantic Data Engineer faces when building ontologies are Knowledge Graphs. In my experience, it is unlikely that any particular set of tools will work in all cases - most likely you will have to try and assemble the right toolchain for your use case and refine it over the lifetime of your project. The following are just a few points for consideration of tasks I regularly encountered - which may or may not overlap with the specific problems you will face.</p>"},{"location":"lesson/analysing-linked-data/#finding-the-right-ontologies","title":"Finding the right ontologies","text":"<p>There are no simple answers here and it very heavily depends on your use cases. We are discussing some places to look for ontologies here, but it may also be useful to simply upload the terms you are interested in to a service like Zooma and see what the terms map to at a major database provider like EBI.</p>"},{"location":"lesson/analysing-linked-data/#finding-the-right-data-sources","title":"Finding the right data sources","text":"<p>This is much harder still than it should have to be. Scientific databases are scattered across institutions that often do not talk to each other. Prepare for some significant work in researching the appropriate databases that could benefit your work, using Google and the scientific literature.</p>"},{"location":"lesson/analysing-linked-data/#extending-existing-ontologies","title":"Extending existing ontologies","text":"<p>It is rare nowadays that you will have to develop an ontology entirely from scratch - most biomedical sub-domains will have some kind of reasonable ontology to build upon. However, there is often a great need to extend existing ontologies - usually because you have the need of representing certain concepts in much more detail, or your specific problem has not been modelled yet - think for example when how disease ontologies needed to be extended during the Coronavirus Crisis. Extending ontologies usually have two major facets:</p> <ol> <li>If at all possible you should seek to contribute new terms, synonyms and relationships to the ontologies you seek to extend directly. Here, you can use GitHub to make issues requesting new terms, but more boldly, you can also add new terms yourself. We will teach you how to do that in one of the next weeks.</li> <li>If the knowledge is considered out of scope for the ontology to be extended (for example because the terms are considered too detailed), then you will maintain your own \"branch\" of the ontology. Many tools such as the Ontology Development Kit and ROBOT can help you maintain such a branch but the general instinct should be:</li> <li>Make a public GitHub repo.</li> <li>Reach out to the developers of the main ontology</li> <li>Stay in touch and coordinate releases</li> </ol>"},{"location":"lesson/analysing-linked-data/#mapping-data-into-ontologiesknowledge-graphs","title":"Mapping data into ontologies/knowledge graphs","text":"<p>Also sometimes more broadly referred to as \"data integration\", this problem involves a variety of tasks, such as:</p> <ol> <li>Named Entity Recognition. If you have a set of documents, such as PubMed abstracts or clinical notes, you may have to first identify the parts of speech that refer to clinical entities.</li> <li>Entity Linking: Once you have identified the biomedical entities of interest, you may want to link them to your existing knowledge graph. This process is sometimes called entity mapping or data mapping as well. Very often, this task is not fully automated. We have worked on projects where we used approaches to Entity Linking to suggest good mappings to users, which then had to confirm or reject them. It is also good to understand that not all entity linking must be vertical (i.e. between \"equivalent\" entities). Very often, there is no equivalent entity in your knowledge graph to map to, and here you need to decide whether to (a) create a new entity in the knowledge graph to map to or (b) map to a broader entity (for example \"microphthalmia, isolated, with coloboma 5\" to \"Mendelian Disease\"). What is more efficient / useful solely depends on your use case!</li> </ol>"},{"location":"lesson/analysing-linked-data/#build-application-ontologies","title":"Build application ontologies","text":"<p>To make your data discoverable, it is often useful to extract a view from the ontologies you are using (for example, Gene Ontology, Disease Ontology) that only contains the terms and relationships of relevance to your data. We usually refer to this kind of ontology as an application ontology, or an ontology specific to your application, which will integrate subsets of other ontologies. This process will typically involve the following:</p> <ul> <li>Define a seed, or a set of terms you want to import from your external ontologies of interest.</li> <li>Extract relevant subsets from ontologies using this seed (for example using ROBOT extract).</li> <li>Combine and potentially link these subsets together.</li> <li>Frameworks such as the Ontology Development Kit can help with this task, see for example the Coronavirus Vocabulary maintained by EBI.</li> </ul>"},{"location":"lesson/analysing-linked-data/#leverage-ontologies-and-knowledge-graphs-for-you-data-analysis-problems","title":"Leverage ontologies and knowledge graphs for you data analysis problems","text":"<p>There are many ways your semantic data can be leveraged for data analysis, but in my experience, two are particularly central:</p> <ol> <li>Data grouping and search: make data about \"microphthalmia, isolated, with coloboma 5\" available when searching for data about \"Mendelian Disease\".</li> <li>Link prediction: Figure out what additional knowledge is hidden in your data that can drive your research (e.g. possible new therapies or drug targets).</li> </ol>"},{"location":"lesson/analysing-linked-data/#additional-materials-and-resources","title":"Additional materials and resources","text":"<p>The open courses of the Hasso Plattner Institute (HPI) offer introductions into the concepts around Linked Data, Semantic Web and Knowledge Engineering. There are three courses of relevance to this weeks topics, all of which overlap significantly.</p> <ul> <li>Knowledge Engineering and the Web of Data: The oldest (2015), of the courses, but the most thorough when it comes to logical foundations, semantics and OWL. We will come back to this course in Weeks 4 and 5.</li> <li>Linked Data Engineering: Overlaps a lot with the Knowledge Engineering and the Web of Data course, with a bit more RDF/Linked Data focus.</li> <li>Knowledge Graphs: The most up-to-date of the three courses (2020), and will be referred to again in Week 12 of our course here.</li> </ul>"},{"location":"lesson/analysing-linked-data/#contributors","title":"Contributors","text":"<ul> <li>add name/ORCID here</li> </ul>"},{"location":"lesson/automating-ontology-workflows/","title":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking","text":""},{"location":"lesson/automating-ontology-workflows/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/automating-ontology-workflows/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ontology Pipelines</li> </ul>"},{"location":"lesson/automating-ontology-workflows/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>In this course, you will learn the basics of automation in and around the OBO ontology world - and beyond. The primary goal is to enable ontology pipeline developers to plan the automation of their ontology workflows and data pipelines, but some of the materials are very general and apply to scientific computing more widely. The course serves also as a prerequisite for advanced application ontology development.</p>"},{"location":"lesson/automating-ontology-workflows/#learning-objectives","title":"Learning objectives","text":"<ul> <li>Unix shell</li> <li><code>make</code></li> <li>Advanced Git, GitHub</li> </ul>"},{"location":"lesson/automating-ontology-workflows/#preparation","title":"Preparation","text":"<p>Please complete the following tutorials.</p> <ul> <li>The Unix Shell (~4.5 hours)</li> <li>Version Control with Git (~3 hours)</li> <li>Introduction to GitHub</li> </ul>"},{"location":"lesson/automating-ontology-workflows/#tutorials","title":"Tutorials","text":""},{"location":"lesson/automating-ontology-workflows/#thinking-automation","title":"Thinking \"Automation\"","text":"<p>By: James Overton</p> <p>Automation is part of the foundation of the modern world. The key to using and building automation is a certain way of thinking about processes, how they can be divided into simple steps, and how they operate on inputs and outputs that must be exactly the same in some respects but different in others.</p> <p>In this article I want to make some basic points about automation and how to think about it. The focus is on automation with software and data, but not on any particular software or data. Some of these points may seem too basic, especially for experienced programmers, but in 20+ years of programming I've never seen anybody lay out these basic points in quite this way. I hope it's useful.</p>"},{"location":"lesson/automating-ontology-workflows/#the-basics","title":"The Basics","text":"<p>\"automatos\" from the Greek: \"acting of itself\"</p> <p>Automation has two key aspects:</p> <ol> <li>make the input the same</li> <li>process the inputs in the same way</li> </ol> <p>The second part is more visible, and tends to get more attention, but the first part is at least as important. While automation makes much of the modern world possible, it is not new, and there are serious pitfalls to avoid. No system is completely automatic, so it's best to think of automation on a spectrum, and starting thinking about automation at the beginning of a new project.</p>"},{"location":"lesson/automating-ontology-workflows/#examples-of-automation","title":"Examples of Automation","text":"<p>To my mind, the word \"automation\" brings images of car factories, with conveyor belts and robotic arms moving parts and welding them together. Soon they might be assembling self-driving (\"autonomous\") cars. Henry Ford is famous for making cars affordable by building the first assembly lines, long before there were any robots. The essential steps for Ford were standardizing the inputs and the processes to get from raw materials to a completed car. The history of the 20th century is full of examples of automation in factories of all sorts.</p> <p>Automation was essential to the Industrial Revolution, but it didn't start then. We can look to the printing press. We can look to clocks, which regimented lives in monasteries and villages. We can think of recipes, textiles, the logistics of armies, advances in agriculture, banking, the administration of empires, and so on. The scientific revolution was built on repeatable experiments published in letters and journal articles. I think that the humble checklist is also an important relative of automation.</p> <p>Automation is not new, but it's an increasingly important part of our work and our lives.</p>"},{"location":"lesson/automating-ontology-workflows/#software-automation-is-special","title":"Software Automation is Special","text":"<p>Software is almost always written as source code in text files that are compiled and/or interpreted as machine code for a specific set of hardware. Software can drive machines of all sorts, but a lot of software automation stays inside the computer, working on data in files and databases, and across networks. We'll be focused on this kind of software automation, transforming data into data.</p> <p>The interesting thing about this is that source code is a kind of data, so there are software automation workflows that operate on data that defines software. The upshot is that you can have automation that modifies itself. Doing this on a large scale introduces a lot of complexity, but doing it on a small scale can be a clean solution to certain problems.</p> <p>Another interesting thing about software is that once we solve an automation problem once we can copy that solution and apply it again and again for almost zero cost. We don't need to build a new factory or a new threshing machine. We can just download a program and run it. Henry Ford could make an accurate estimate of how long it would take to build a car on his assembly line, but software development is not like working on the assembly line, and estimating time and budget for software development is notoriously hard. I think this is because software developers aren't just executing automation, they're building new automation for each new project.</p> <p>Although we talk about \"bit rot\", and software does require maintenance of a sort, software doesn't break down or wear out in the same ways that physical machines do. So while the Industrial Revolution eliminated many jobs, it also created different jobs, building and maintaining the machines. It's not clear that software automation will work the same way.</p> <p>Software automation is special because it can operate on itself, and once complete can be cheaply copied. Software development is largely about building automated systems of various sorts, usually out of many existing pieces. We spend most of our time building new systems, or modifying an existing system to handle new inputs, or adapting existing software to a new use case.</p>"},{"location":"lesson/automating-ontology-workflows/#the-dangers-of-automation","title":"The Dangers of Automation","text":"<p>To err is human; to really foul things up requires a computer.</p> <p>An obvious danger of automation is that machines are faster than humans, so broken automation can often do more damage more quickly than a human can. A related problem is that humans usually have much more context and depth of experience, which we might call \"common sense\", and a wider range of sensory inputs than most automated systems. This makes humans much better at recognizing that something has gone wrong with a process and that it's time to stop.</p> <p>New programmers soon learn that a simple program that performs perfectly when the input is in exactly the right format, becomes a complex program once it's updated to handle a wide range of error conditions. In other words, it's almost always much harder to build automation that can gracefully handler errors and problems than it is to automate just the \"happy path\". Old programmers have learned through bitter experience that it's often practically impossible to predict all the things that can go wrong with an automated system in practise.</p> <p>I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. -- Abraham Maslow</p> <p>A less obvious danger of automation comes from the sameness requirement. When you've built a great piece of automation, perfectly suited to inputs of a certain type, it's very tempting to apply that automation more generally. You start paying too much attention to how things are the same, and not enough attention to their differences. You may begin to ignore important differences. You may surrender your common sense and good judgment, to save yourself the work of changing the automated system or making an exception.</p> <p>Bureaucracies are a form of automation. Everyone has had a bad experience filling out some form that ignores critical information, and with some bureaucrat who would not apply common sense and make an exception.</p> <p>Keep all this in mind as you build automated systems: a broken machine can do a lot of damage very quickly, and a system built around bad assumptions can do a lot of hidden damage.</p>"},{"location":"lesson/automating-ontology-workflows/#a-spectrum-of-automation","title":"A Spectrum of Automation","text":"<p>Let's consider a simple case of automation with software, and build from the most basic sort of automation to a full-fledged system.</p> <p>Say you have a bunch of text files in a directory, each containing minutes from meetings that we had together over the years. You can remember that I talked about a particular software package that might solve a problem that you just discovered, but you can't remember the name.</p>"},{"location":"lesson/automating-ontology-workflows/#1-ad-hoc","title":"1. Ad Hoc","text":"<p>The first thing you try is to just search the directory. On a Mac you would open the Finder, navigate to the directory, and type \"James\" into the search bar. Unfortunately that gives too many results: all the files with the minutes for a meeting where I said something.</p> <p>The next thing to do is double click some text files, which would open them in Text Edit program, and skim them. You might get lucky!</p> <p>You know that I the meeting was in 2019, so you can try and filter for files modified in that year. Unfortunately the files have been updated at different times, so the file dates aren't useful.</p> <p>Now if each file was named with a consistent pattern, including the meeting date, then it would be simple to filter for files with \"2019\" in the name. This isn't automation, but it's the first step in the right direction. Consistent file names are one way to make inputs the same so that you can process them in the same way.</p> <p>Let's say it works: you filter for files from 2019 with \"James\" in them, skim a few, and find a note where I recommended using Pandoc to convert between document formats. Mission accomplished!</p>"},{"location":"lesson/automating-ontology-workflows/#2-notes","title":"2. Notes","text":"<p>Next week you need to do something very similar: Becky mentioned a website where you can find an important dataset. It's basically the same problem with different inputs. If you remember exactly what you did last time, then you can get the job done quickly. As the job gets more complicated and more distant in time, and as you find yourself doing similar tasks more often, it's nice to have notes about what you did and how you did it.</p> <p>If I'm using a graphical user interface (GUI) then for each step I'll note the program I used, and the menu item or button I clicked, e.g. \"Preferences &gt; General &gt; Font Size\", or \"Search\" or \"Run\". If I'm using a command-line interface (CLI) then I'll copy-paste the commands into my notes.</p> <p>I often keep informal notes like this in a text file in the relevant directory. I name the file \"notes.txt\". A \"README\" file is similar. It's used to describe the contents of a directory, often saying which files are which, or what the column headers for a given table mean.</p> <p>Often the task is more complicated and requires one or more pieces of software that I don't use every day. If there's relevant documentation, I'll put a link to it in my notes, and then a short summmary of exactly what I did.</p> <p>In this example I look in the directory of minutes and see my \"notes.txt\" file. I read that and remember how I filtered on \"2019\" and searched for \"James\". This time I filter on \"2020\" and search for \"Becky\", and I find the website for the dataset quickly enough.</p> <p>As a rule of thumb, it might take you three times longer to find your notes file, write down the steps you took, and provide a short description, than it would to just do the job without taking notes. When you're just taking notes for yourself, this often feels like a waste of time (you'll remember, right?!), and sometimes it is a bit of a waste. If you end up using your notes to help with similar tasks in the future, then this will likely be time well spent.</p> <p>As a rule of thumb, it might take three times longer to write notes for a broader audience than notes for just yourself. This is because you need to take into account the background knowledge of your reader, including her skills and assumptions and context, and especially the possible misunderstandings that you can try to avoid with careful writing. I often start with notes for just myself and then expand them for a wider audience only when needed.</p>"},{"location":"lesson/automating-ontology-workflows/#3-checklist","title":"3. Checklist","text":"<p>When tasks get more complicated or more important then informal notes are not enough. The next step on the spectrum of automation is the humble checklist.</p> <p>The most basic checklists are for making sure that each item has been handled. Often the order isn't important, but lists are naturally ordered from top to bottom, and in many cases that order is useful. For example, my mother lays out her shopping lists in the order of the aisles in her local grocery store, making it easier to get each item and check it off without skipping around and perhaps having to backtrack.</p> <p>I think of a checklist as a basic form of automation. It's like a recipe. It should lay out the things you need to start, then proceed through the required steps in enough detail that you can reproduce them. In some sense, by using the checklist you are becoming the \"machine\". You are executing an algorithm that should take you from the expected inputs to the expected output.</p> <p>Humble as the checklist is, there's a reason that astronauts, pilots, and surgical teams live by their checklists. Even when the stakes are not so high, it's often nice to \"put your brain on autopilot\" and just work the checklist without having to remember and reconsider the details of each step.</p> <p>A good checklist is more focused than a file full of notes. A checklist has a goal at the end. It has specific starting conditions. The steps have been carefully considered, so that they have the proper sequence, and none are missing. Perhaps most importantly, a checklist helps you break a complex task down into simple parts. If one of the parts is still too complex, then break it down again into a nested checklist (really a sort of tree structure).</p> <p>Checklists sometimes include another key element of automation: conditionals. A shopping list might say \"if there's a sale on crackers, then buy three boxes\". If-then conditions let our automated systems adapt to circumstances. The \"then\" part is just another step, but the \"if\" part is a little different. It's a test to determine whether a condition holds. We almost always want the result of the test to be a simple True or False. Given a bunch of inputs, some of which pass the test and some of which fail it, we can think of the test as determining some way in which all the things that pass are the same and all the things that fail are the same. Programmers will also be familiar with more complex conditionals such as if-then-else, if-elseif-else, and \"case\", which divide process execution across multiple \"branches\".</p> <p>As a rule of thumb, turning notes into a checklist will likely take at least three times as long as simply writing the notes. If the checklist is for a wider audience, expect it to take three times as long to write, for the same reasons mentioned above for notes.</p> <p>If a task is simple and I can hold all the steps in my head, and I can finish it in one sitting without distractions, then I won't bother with a checklist. But more and more I find myself writing myself a checklist before I begin any non-trivial tasks. I use bullet points in my favourite text editor, or sometimes the Notes app on my iPhone. I lay out the steps in the expected order, and I check them off as I go. Sometimes I start making the checklist days before I need it, so I have lots of time to think about it and improve it. If there's a job that I'm worried about, breaking it down into smaller pieces usually helps to make the job feel more manageable. Actually, I try to start every workday by skimming my (long) To Do list, picking the most important tasks, and making a checklist for what I want to get done by quitting time.</p>"},{"location":"lesson/automating-ontology-workflows/#3-checkscript","title":"3. Checkscript","text":"<p>\"Checkscript\" is a word that I think I made up, based on insights from a couple of sources, primarily this blog post on \"Do-nothing scripting: the key to gradual automation\" This is where \"real\" automation kicks in, writing \"real\" code and stuff, but hopefully you'll see that it's just one more step on the spectrum of automation that I'm describing.</p> <p>The notes and checklists we've been discussing are just text in your favourite text editor. A checkscript is a program. It can be written in whatever programming language you prefer. I'll give examples in Posix Shell, but that blog post uses Python, and it really doesn't matter. You start with a checklist (in your mind at least). The first version of your program should just walk you through your checklist. The program should walk you through each step of your checklist, one by one. That's it.</p> <p>Here's a checkscript based on the example above. It just prints the first step (<code>echo</code>), waits for you to press any key (<code>read</code>), then prints the next step, and so on.</p> <pre><code>###!/bin/sh\n\necho \"1. Use Finder to filter for files with '2019' in the name\"\nread -p \"Press enter to continue\"\n\necho \"2. Use finder to search file content for 'James'\"\nread -p \"Press enter to continue\"\n\necho \"3. Open files in Text Edit and search for 'James'\"\nread -p \"Press enter to continue\"\n\necho \"Done!\"\n</code></pre> <p>So far this is just a more annoying way to use a checklist. The magic happens once you break the steps down into small enough pieces and realize that you know how to tell the computer to do some of the steps instead of doing them all yourself.</p> <p>For example, you know that the command-line tool <code>grep</code> is used for searching the contents of files, and that you can use \"fileglob\"s to select just the files that you want to search, and that you can send the output of <code>grep</code> to another file to read in your favourite text editor. Now you know how to automate the first two steps. The computer can just do that work without waiting for you:</p> <pre><code>###!/bin/sh\n\ngrep \"James\" *2019* &gt; search_results.txt\n\necho \"1. Open 'search_results.txt' in Text Edit and search for 'James'\"\nread -p \"Press enter to continue\"\n\necho \"Done!\"\n</code></pre> <p>Before we were using the Finder, and it is possible to write code to tell the Finder to filter and seach for files. The key advantage of <code>grep</code> here is that we send the search results to another file that we can read now or save for later.</p> <p>This is also a good time to mention the advantage of text files over word processor files. If the minutes were stored in Word files, for example, then Finder could probably search them and you could use Word to read them, but you wouldn't be able to use <code>grep</code> or easily output the results to another file. Unix tools such as <code>grep</code> treat all text files the same, whether they're source code or meeting minutes, which means that these tools work pretty much the same on any text file. By keeping your data in Word you restrict yourself to a much smaller set of tools and make it harder to automate you work with simple scripts like this one.</p> <p>Even if you can't get the computer to run any of the steps for you automatically, a checkscript can still be useful by using variables instead of repeating yourself:</p> <pre><code>###!/bin/sh\n\nFILE_PATTERN=\"*2019*\"\nFILE_CONTENTS=\"James\"\n\necho \"1. Use Finder to filter for files with '${FILE_PATTERN}' in the name\"\nread -p \"Press enter to continue\"\n\necho \"2. Use finder to search file content for '${FILE_CONTENTS}'\"\nread -p \"Press enter to continue\"\n\necho \"3. Open files in Text Edit and search for '${FILE_CONTENTS}'\"\nread -p \"Press enter to continue\"\n\necho \"Done!\"\n</code></pre> <p>Now if I want to search for \"Becky\" I can just change the FILE_CONTENTS variable in one place. I find this especially useful for dates and version numbers.</p> <p>This is pretty simple for a checkscript, with very few steps. A more realistic example would be if there were many directories containing the minutes of many meetings, maybe in different file formats and with different naming conventions. In order to be sure that we're searching all of them we might need a longer checkscript.</p> <p>Writing and using a checkscript instead of a checklist will likely take (you guessed it) about three times as long. But the magic of the checkscript is in the title of the blog post I mentioned: \"gradual automation\". Once you have a checkscript, you can run through it all manually, but you can also automate bits a pieces of the task, saving yourself time and effort next time.</p>"},{"location":"lesson/automating-ontology-workflows/#5-script","title":"5. Script","text":"<p>A \"script\" is a kind of program that's easy to edit and run. There are technical distinctions to be made between \"compiled\" programs and \"interpreted\" programs, but they turn out to be more complicated and less helpful than they seem at first. Technically, a checkscript is just a script that waits for you to do the hard parts. In this section I want to talk about \"fully automated\" or \"standalone\" scripts that you just provide some input and execute.</p> <p>Most useful programs are useful because they call other programs (in the right ways). I like shell scripts because they're basically just commands that are copied and pasted from work I was doing on the command-line. It's really easy to call other programs.</p> <p>To continue our example, say that our minutes were stored in Word files. There are Python libraries for this, such as python-docx. You can write a little script using this library that works like <code>grep</code> to search for specified text in selected files, and output the results to a search results file.</p> <p>As you add more and more functionality to a script it can become unwieldy. Scripts work best when they have a simple \"flow\" from beginning to end. They may have some conditionals and some loops, but once you start seeing nested conditionals and loops, then your script is doing too much. There are two main options to consider:</p> <ol> <li>break your script into smaller, simpler scripts</li> <li>build a specialized tool: the next step on the spectrum of automation</li> </ol> <p>The key difference between a checkscript and a \"standalone\" script is handling problems. A checkscript relies on you to supervise it. A standalone script is expected to work properly without supervision. So the script has to be designed to handle a wider range of inputs and fail gracefully when it gets into trouble. This is a typical case of the \"80% rule\": the last 20% takes 80% of the time. As a rule of thumb, expect it to take three times as long to write a script that can run unsupervised than it takes you to write a checkscript that does \"almost\" the same thing.</p>"},{"location":"lesson/automating-ontology-workflows/#6-specialized-tool","title":"6. Specialized Tool","text":"<p>When your script needs nested conditionals and loops, then it's probably time to reach for a programming language that's designed to write code \"in the large\". Some languages such as Python can make a pretty smooth transition from a script in a single file to a set of files in a module, working together nicely. You might also choose another language that can provide better performance or efficiency.</p> <p>It's not just the size and the logical complexity of your script, consider its purpose. The specialized tools that I have in mind have a clear purpose that helps guide their design. This also makes them easier to reuse across multiple projects.</p> <p>I often divide my specialized tools into two parts: a library and a command-line interface. The library can be used in other programs, and contains the most distinctive and important functionality. But the command-line interface is essential, because it lets me use my specialized tool in the shell and in scripts, so I can build more automation on top of it.</p> <p>Writing a tool in Java or C++ or Rust usually takes longer than a script in shell or Python because there are more details to worry about such as types and efficient memory management. In return you usually get more reliability and efficiency. But as a rule of thumb, expect it to take three times as long to write a specialized tool than it would to \"just\" write the script. On the other hand, if you already have a script that does most of what you want, and you're already familiar with the target you are moving to, then it can be fairly straightforward to translate from the script to the specialized tool. That's why it's often most efficient to write a prototype script first, do lots of quick experiments to explore the design space, and when you're happy with the design then start on the \"production\" version.</p>"},{"location":"lesson/automating-ontology-workflows/#7-workflow","title":"7. Workflow","text":"<p>The last step in the spectrum of automation is to bring together all your scripts into a single \"workflow\". My favourite tool for this is the venerable Make. A <code>Makefile</code> is essentially a bunch of small scripts with their input and output files carefully specified. When you ask Make to build a given output file, it will look at the whole tree of scripts, figure out which input files are required to build your requested output file, then which files are required to build those files, and so on until it has determined a sequence of steps. Make is also smart enough to check whether some of the dependencies are already up-to-date, and can skip those steps. Looking at a <code>Makefile</code> you can see everything broken down into simple steps and organized into a tree, through which you can trace various paths. You can make changes at any point, and run Make again to update your project.</p> <p>I've done this all so many times that now I often start with a <code>Makefile</code> in an empty directory and build from there. I try experiments on the command line. I make notes. I break the larger task into parts with a checklist. I automate the easy parts first, and leave some parts as manual steps with instructions. I write little scripts in the <code>Makefile</code>. I write larger scripts in the <code>src/</code> directory. If these get too big or complex, I start thinking about building a specialized tool. (And of course, I store everything in version control.) It takes more time at the beginning, but I think that I usually save time later, because I have a nice place to put everything from the start.</p> <p>In other words, I start thinking about automation at the very beginning of the project, assuming from the start that it will grow, and that I'll need to go back and change things. With a mindset for automation, from the start I'm thinking about how the inputs I care about are the same and different, which similarities I can use for my tests and code, and which differences are important or unimportant.</p>"},{"location":"lesson/automating-ontology-workflows/#conclusion","title":"Conclusion","text":"<p>In the end, my project isn't ever completely automated. It doesn't \"act of itself\". But by making everything clear and explicit I'm telling the computer how to do a lot of the work and other humans (or just my future self) how to do the rest of it. The final secret of automation, especially when it comes to software and data, is communication: expressing things clearly for humans and machines so they can see and do exactly what you did.</p>"},{"location":"lesson/automating-ontology-workflows/#scientific-computing-an-overview","title":"Scientific Computing: An Overview","text":"<p>By: James Overton</p> <p>By \"scientific computing\" we mean using computers to help with key aspect of science such as data collection, cleaning, interpretation, analysis, and visualization. Some people use \"scientific computing\" to mean something more specific, focusing on computational modelling or computationally intensive analysis. We'll be focusing on more general and day-to-day topics: how can a scientist make best use of a computer to do their work well?</p> <p>These three things apply to lots of fields, but are particularly important to scientists:</p> <ul> <li>reliability</li> <li>reproducibility</li> <li>communication</li> </ul> <p>It should be no surprise that automation can help with all of these. When working properly, computers make fewer mistakes than people, and the mistakes they do make are more predictable. If we're careful, our software systems can be easily reproduced, which means that an entire data analysis pipeline can be copied and run by another lab to confirm the results. And scientific publications are increasingly including data and code as part of the review and final publication process. Clear code is one of the best ways to communicate detailed steps.</p> <p>Automation is critical to scientific instruments and experiments, but we'll focus on the data processing and analysis side: after the data has been generated, how should you deal with it.</p> <p>Basic information management is always important:</p> <ul> <li>community standard file formats</li> <li>consistent file naming</li> <li>documentation, READMEs</li> <li>backups</li> <li>version control</li> </ul> <p>More advanced data management is part of this course:</p> <ul> <li>consistent use of versioned</li> <li>software</li> <li>reference data</li> <li>terminology</li> <li>controlled vocabularies</li> <li>data dictionaries</li> <li>ontologies</li> </ul> <p>Some simple rules of thumb can help reduce complexity and confusion:</p> <ul> <li>make space</li> <li>firm foundations</li> <li>one-way data flow</li> <li>plan for change</li> <li>test from the start</li> <li>documentation is also for you</li> </ul>"},{"location":"lesson/automating-ontology-workflows/#make-space","title":"Make Space","text":"<p>When starting a new project, make a nice clean new space for it. Try for that \"new project smell\".</p> <ul> <li>I always create a new directory on my computer.</li> <li>I almost always create a new GitHub repository.</li> <li>I usually create a README and a Makefile, right away.</li> </ul> <p>It's not always clear when a project is really \"new\" or just a new phase of an old project. But try to clear some space to make a fresh start.</p>"},{"location":"lesson/automating-ontology-workflows/#firm-foundations","title":"Firm Foundations","text":"<p>A lot of data analysis starts with a reference data set. It might be a genome or a proteome. It might be a corpus. It might be a set of papers or data from those papers.</p> <p>Start by finding that data and selecting a particular version of it. Write that down clearly in your notes. If possible, include a unique identifier such as a (persistent) URL or DOI. If that's not possible, write down the steps you took. If the data isn't too big, keep a copy of it in your fresh new project directory. If the data is a bit too big, keep a compressed copy in a <code>zip</code> or <code>gz</code> file. A lot of software is perfectly happy to read directly from compressed files, and you can compress or uncompress data using piped commands in your shell or script. If the data is really too big, then be extra careful to keep notes on exactly where you can find it again. Consider storing just the hashes of the big files, so you can confirm that they have exactly the same contents.</p> <p>If you know from the start that you will need to compare your results with someone else's, make sure that you're using the same reference data that they are. This may require a conversation, but trust me that it's better to have this conversation now than later.</p>"},{"location":"lesson/automating-ontology-workflows/#one-way-data-flow","title":"One-Way Data Flow","text":"<p>It's much easier to think about processes that flow in one direction. Branches are a little trickier, but usually fine. The real trouble comes with loops. Once a process loops back on itself it's much more difficult to reason about what's happening. Loops are powerful, but with great power comes great responsibility. Keep the systems you design as simple as possible (but no simpler).</p> <p>In practical terms:</p> <ul> <li>Try not to read then write to the same file.   If you have to, try to append rather than overwrite.   This is one reason why I prefer tables on disk to databases.</li> <li>Don't hesitate to write intermediate files.   These are very useful for testing and debugging.   When you're \"finished\" you can comment out these steps.</li> </ul>"},{"location":"lesson/automating-ontology-workflows/#plan-for-change","title":"Plan for Change","text":"<p>It's very tempting: you could automate this step, or you could just do it manually. It might take three times as long to automate it, right? So you can save yourself some precious time by just opening Excel and \"fixing\" things by hand.</p> <p>Sometimes that bet will pay off, but I lose that bet most of the time. I tend to realize my mistake only at the last minute. The submission deadline is tomorrow but the core lab \"fixed\" something and they have a new version of the dataset that we need to use for the figures. Now I really don't have time to automate, so I'm up late clicking through Excel again and hoping that I remembered to redo all the changes that I made last time.</p> <p>Automating the process would have actually saved me time, but more importantly it would have avoided a lot of stress. By now I should know that the dataset will almost certainly be revised at the last minute. If I have the automation set up, then I just update the data, run the automation again, and quickly check the results.</p>"},{"location":"lesson/automating-ontology-workflows/#test-from-the-start","title":"Test from the Start","text":"<p>Tests are another thing that take time to implement.</p> <p>One of the key benefits to tests is (again) communication. When assessing or trying out some new piece of software I often look to the test files to see examples of how the code is really used, and the shape of the inputs and outputs.</p> <p>There's a spectrum of tests that apply to different parts of your system:</p> <ul> <li>unit tests: individual functions and methods</li> <li>regression tests: ensure that fixed bugs do not reappear</li> <li>integration tests: end-to-end functionality</li> <li>performance tests: system speed and resource usage</li> <li>acceptance tests: whether the overall system meets its design goals</li> </ul> <p>Tests should be automated. The test suite should either pass or fail, and if it fails something needs to be fixed before any more development is done. The automated test suite should run before each new version is committed to version control, and ideally more often during development.</p> <p>Tests come with costs:</p> <ul> <li>development cost of writing the tests</li> <li>time and resources spent running the tests</li> <li>maintenance costs of updating the tests</li> </ul> <p>The first is obvious but the other two often more important. A slow test suite is annoying to run, and so it won't get run. A test suite that's hard to update won't get updated, and then failures will be ignored, which defeats the entire purpose.</p>"},{"location":"lesson/automating-ontology-workflows/#documentation-is-also-for-you","title":"Documentation is also for You","text":"<p>I tend to forget how bad a memory I have. In the moment, when I'm writing brilliant code nothing could be more obvious than the perfect solution that is pouring forth from my mind all over my keyboard. But when I come back to that code weeks, months, or years later, I often wonder what the heck I was thinking.</p> <p>We think about the documentation we write as being for other people, but for a lot of small projects it's really for your future self. Be kind to your future self. They may be even more tired, even more stressed than you are today.</p> <p>There's a range of different forms of documentation, worth a whole discussion of its own. I like this four-way distinction:</p> <ul> <li>tutorials: getting started, basic concepts, an overview</li> <li>how-to guides: how to do common tasks</li> <li>explanation: why does it work this way?</li> <li>reference: looking up the details</li> </ul> <p>You don't need all of these for your small project, but consider a brief explanation of why it works the way it does (aimed at a colleague who knows your field well), and some brief notes on how-to do the stuff this project is for. These could both go in the README of a small project.</p>"},{"location":"lesson/automating-ontology-workflows/#additional-materials-and-resources","title":"Additional materials and resources","text":"<ul> <li>A whirlwind introduction to the command line</li> <li>Programming with Python</li> <li>Oh My Git!</li> </ul>"},{"location":"lesson/automating-ontology-workflows/#contributors","title":"Contributors","text":"<ul> <li>Nico Matentzoglu</li> <li>James Overton</li> </ul>"},{"location":"lesson/chatgpt-ontology-curation/","title":"Leveraging ChatGPT for ontology curation","text":""},{"location":"lesson/chatgpt-ontology-curation/#lesson-leveraging-chatgpt-for-ontology-curation","title":"Lesson: Leveraging ChatGPT for ontology curation","text":"<p>In this lesson, we will take a look at the generative capabilities of LLM's in general and ChatGPT in particular, to try and get a beginning sense on how to leverage it to enhance ontology curation workflows.</p> <p>The goal of the lesson is to give a mental model of what ChatGPT and LLMs are used for (ignoring details on how they work), contextualise the public discourse a bit, and then move on to looking at some concrete examples at its potential for improving curation activties.</p> <p>To achieve this we engaged in a dialog with ChatGPT to generate almost the entire content of the lesson. The lesson authors provide the general \"structure\" of the lesson, provided to ChatGPT as a series of prompts, and get ChatGPT to provide the content. This content is obviously not as good as it could have been if it was created by a human with infinite resources, but we hope it does get the following points across:</p> <ol> <li>You can get 80% of the way in 20% (or less, in my case) of the time (it took me 90 minutes to write the lesson, while the same scope of lesson would have taken me 8-10 hours without ChatGPT</li> <li>Beware of hallucinations in particular</li> <li>ChatGPT is not only good for generating natural language text, but also structured content</li> <li>Using ChatGPT is fun, which drastically reduces the mental barrier of using it in a professional context</li> </ol> <p>We believe that from a user perspective, prompt engineering will be the most important skill that need to be learned when dealing with generative AI. Not just ChatGPT (which generates text), but also tools that generate images from text such as DALL-E or Midjourney, so this is what we will focus on. In the long term, applications like Monarchs OntoGPT will do some of the heavy lifting around writing perfect prompts, but it seems pretty clear that some basic knowledge of prompt engineering will be useful, or even necessary, for a long time to come.</p> <p>For a reference of effective ChatGPT prompts for ontology development see here.</p> <p>Note: - ChatGPT is rapidly evolving. The moment we add an answer, it will probably be outdated. For example, I created the first version of this tutorial on April 17th 2023. On May 27th, almost all answers ChatGPT is giving are completely different from the ones given in the first round. This is also important to remember when building applications around ChatGPT. - Note: https://open-assistant.io/chat is free and can be used to follow this tutorial instead of ChatGPT.</p>"},{"location":"lesson/chatgpt-ontology-curation/#contributors","title":"Contributors","text":"<ul> <li>Sierra Moxon</li> <li>Chris Mungall</li> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/chatgpt-ontology-curation/#skills-you-will-learn-in-this-lesson","title":"Skills you will learn in this lesson","text":"<ul> <li>Write basic prompts for ChatGPT and similar LLM-based chatbots</li> <li>Understand the basic principles of text in -&gt; text out of LLM-based programs</li> <li>Understand practically the risks of hallucination</li> <li>Motivate yourself to explore the question: \"How, if at all, can I use this technology to become more efficient/effective in my work? (there is no general answer to that yet!)\"</li> </ul>"},{"location":"lesson/chatgpt-ontology-curation/#conventions","title":"Conventions","text":"<p>Prompts</p> <p></p> <p>We use quote syntax with the prompt icon to indicate a concrete prompt for ChatGPT</p> <p>Comments</p> <p></p> <p>We use quote syntax with the comment icon to indicate a comment by the author</p> <p>Replies by ChatGPT</p> <p>Replies are given in normal text form. All text after the table of contents, apart from comments, prompts and the section on executable workflows are generated by ChatGPT.</p>"},{"location":"lesson/chatgpt-ontology-curation/#prequisites","title":"Prequisites","text":"<ul> <li>Have a very basic understanding of how ChatGPT works.</li> </ul>"},{"location":"lesson/chatgpt-ontology-curation/#preparation","title":"Preparation","text":"<ul> <li>If you have a tiny bit of a technical inclination, we recommend the ca. 60 minute OpenAI/DeepLearning.ai course. It has zero fluff, and is very focussed on some of the core use cases of using ChatGPT as an API.</li> <li>We recommend to spend 30-60 minutes studying the excellent Learn Prompting, a Free, Open Source Course on Communicating with Artificial Intelligence.</li> <li>Take a look at this nice introduction to prompt engineering by anthropic</li> <li>If you want to replicate the contents of this lession, you have to make an account at https://chat.openai.com/</li> </ul>"},{"location":"lesson/chatgpt-ontology-curation/#lesson","title":"Lesson","text":"<ul> <li>Basic concepts and why should you care?</li> <li>What is Prompt Engineering?</li> <li>Applications of LLMs and ChatGPT around ontology development</li> <li>How can we, concretely, enhance the ontology curation process?</li> <li>Using ChatGPT for ontology mappings</li> <li>Generating pre-coordinated expressions and schema instances</li> </ul> <p>None of the text in this section is generated with ChatGPT.</p> <p>In essence, an LLM takes as an input a piece of text, and returns text as an output. A \"prompt\" is a  piece of text that is written by an agent. This can be a human, or a software tool, or a combination of the two. In most cases, a human agent will pass the prompt to a specialised tool that pre-processes the prompt in certain ways (like translating it, adding examples, structuring it and more) before passing it to the large language model (LLM). For example, a when a chatbot tool like ChatGPT receives a prompt, it processes the prompt in certain ways, than leveraging the trained LLM to generate the text (which is probably postprocessed) and passed back to the human agent.</p> <p></p> <p>There are an infinite number of possible tools you can imagine following this rough paradigm. Monarch's own ontogpt, for example, receives the prompt from the human agent, then augments the prompt in a certain way (by adding additional instructions to it) before passing the augmentd prompt to an LLM like gpt3.5 (or lately even gpt4), which generates an instance of a curation schema. This is a great example for an LLM generating not only human readable text, but structured text. Another example for this is to ask an LLM to generate, for example, a SPARQL query to obtain publications from Wikidata.</p> <p>Given the wide range of applications LLMs can serve, it is important to get a mental model of how these can be leveraged to improve our ontology and data curation workflows. It makes sense for our domain (semantic engineering and curation) to distinguish four basic models of interacting with LLMs (which are technically not much different):</p> <ol> <li>Using LLM-based tools as advisors (endpoint humans)</li> <li>Using LLM-based tools as assistants (endpoint humans)</li> <li>Using LLM-based tools to extract information for automated processing (endpoint application)</li> </ol> <p>Using LLMs as advisors has a huge number of creative applications. An advisor in this sense is a machine that \"knows a lot\" and helps you with your own understanding of the world.  Large language models trained on a wide range of inputs are particularly interesting in this regard because of the immense breadth of their knowledge (rather than depth), which is something that can be difficult to get from human advisors. For example, the authors of this article have used ChatGPT and other LLM-based chatbots to help with understanding different domains, and how they might relate to knowledge management and ontologies in order to give specific career advice or to prepare for scientific panel discussions. For ontology curators, LLMs can be used to generate arguments for certain classification decisions (like a disease classification) or even suggest a mapping.</p> <p>Using LLMs as assistants is probably the most important use of LLM-based tools at the moment, which includes aspects like summarising texts, generating sometimes boring, yet important, creative work (documentation pages, tutorials, blog-posts etc). It is probably not a good idea, at least as of May 2023, to defer to LLM-based tools to classify a term in an ontology, for example because of its tendency to hallucinate. Despite many arguments to the contrary LLMs are not databases. They are programs to generate text.</p> <p>Using LLMs to extract information, similar to \"LLMs as assistants\", is, similar to the above, also about automating certain tasks, but the endpoint is not a software program rather than a human. It is the most important basic model of LLMs for us curators and software engineers to understand, because it is, in essence, the one that threatens our current work-life the most: What happens if LLMs become better at extracting structured knowledge from papers (or similarly generate software codes for user stories) than us? It is important that this thought is not ignored out of fear, but approached with a realistic and positive mindset.</p> <p>Training. Apart from the fact that LLMs take text as an input and return text as an output, it is important to be aware how they are trained.</p> <p></p> <p>The basic technique for trainig is called \"next token prediction\". In essence, tokens in the text such as words or phrases are masked out by the training function and the LLM is trained to correctly predict these masked out phrases given the previous words in the sentence (or paragraph). The corpus used for this training ranges from specialised databases all the way to a large chunk of the publicly accessible textual information on the internet. Again, it is important to realise that text does not have to be composed of natural language sentences - learning how to generate programming code and structured data works much the same way.</p> <p>In the following, we will ask ChatGPT or Open Assistant a few questions that are relevant to this lesson, which allows us to make certain points. The main point is that a lot of the work for preparing a useful tutorial can be deferred to ChatGPT is an assistant!  Many of the questions explore how these tools could relate to us biocurators and semantic engineers in particular, but non of this should be viewed as written in stone. The most important take away from this lesson is that you should play with these tools yourself!</p> <p></p> <p>Can you explain in 3 simple sentences what an LLM is and how it works? <code>[Note this question was asked to [Open Assistant](https://open-assistant.io/chat/) instead of ChatGPT.]</code></p> <p>An LLM (Langauge Modell) is a AI model that can be trained to perform specific tasks such as speech recognition, text generation or machine translation. It's called a \"model\" because it is being trained on large sets of data to predict the next step based on previously learned relationships between inputs and outputs. LLMs are designed to recognize patterns in human language by processing input data through multiple layers of artificial neural networks that simulate the way neurons communicate in the brain. The training process involves feeding vast amounts of text into the model so that it can learn to identify underlying structures within the language itself. This allows the model to make predictions about likely next words or phrases given previous context. By continually adjusting its internal weights and biases using gradient descent techniques, the LLM becomes increasingly proficient at generating coherent text based on statistical probabilities derived from its knowledge of the language.</p> <p></p> <p>What is interesting here: the LLM actually made a typo, or rather, three: <code>Langauge Modell</code> - missing the first L (large), Langauge instead of Language and Modell with double L (which is German). Note that both words are spellied correctly later in the text.</p> <p></p> <p>What are the 5 core concepts relevant for end-users to understand LLMs?</p> <ol> <li>Language Understanding and Generation: LLMs can understand human language inputs and generate text-based responses, allowing for interactive conversations.</li> <li>Contextual Understanding: LLMs consider the context of the conversation to provide relevant and coherent responses, taking into account previous messages or prompts.</li> <li>Pretraining: LLMs learn from vast amounts of text data during pretraining, which helps them acquire a general understanding of language, grammar, and context.</li> <li>Limitations and Potential Bias: LLMs may provide incorrect or biased information as they generate responses based on patterns learned from training data. Users should be mindful of these limitations and critically evaluate the information provided.</li> <li>Responsible Use and Privacy: Users should use LLMs responsibly, following platform guidelines and being cautious when sharing personal or sensitive information during interactions. Data privacy should be considered while engaging with LLMs.</li> </ol> <p></p>"},{"location":"lesson/chatgpt-ontology-curation/#what-is-prompt-engineering","title":"What is prompt engineering?","text":"<p>Describe the idea of prompt engineering in simple terms for an audience of domain experts</p> <p>Prompt engineering is the process of carefully crafting input queries, or \"prompts,\" to guide the behavior of a large language model (LLM) like GPT-4, ensuring that it provides useful, relevant, and accurate responses. As domain experts, you understand the importance of asking the right questions to extract valuable information. Similarly, with prompt engineering, the goal is to design questions or statements in a way that maximizes the LLM's ability to generate meaningful, high-quality output.</p> <p>Essentially, prompt engineering serves as a \"communication bridge\" between humans and the LLM, allowing the model to better understand the intent behind the query and generate the desired response. This can involve refining the wording, providing context, or specifying the format of the desired answer, among other techniques.</p> <p>By focusing on the art and science of prompt engineering, researchers and practitioners can better harness the potential of LLMs and overcome some of the challenges related to biases, incomplete knowledge, and misleading responses.</p> <p>Providing an effective prompt to ChatGPT is an art form, and as resources like learnprompting.org testify, one that requires cultivation and care. The better you become at writing prompts, the more value you will draw from ChatGPT for your daily ontology curation tasks. How effective our prompts are depends on the exact language models used in ChatGPT (at the moment 3.5 or 4), which means that this lesson will keep evolving.</p>"},{"location":"lesson/chatgpt-ontology-curation/#what-are-some-basic-tips-for-writing-good-prompts-for-chatgpt","title":"What are some basic tips for writing good prompts for ChatGPT?","text":"<p>In the excellent 60 minute OpenAI, Deeplearning.ai course on Prompt Engineering, we learn that we should be (1) writing clear an specific instructions and (2) give ChatGPT \"time to think\".</p> <ul> <li>Talking to a LLM is like talking to a teenager</li> <li>Give clear and specific instructions.</li> <li>Use different words to get better results the second time.</li> <li>Give the LLM time to think.</li> </ul> <p></p>"},{"location":"lesson/chatgpt-ontology-curation/#applications-of-chatgpt-for-ontology-development","title":"Applications of ChatGPT for ontology development","text":"<p>What are some potential applications of ChatGPT for ontology development?</p> <p>ChatGPT can be a valuable tool for ontology development in various ways. Some potential applications include:</p> <ol> <li>Concept identification: ChatGPT can help identify and extract relevant concepts from large volumes of unstructured text, providing a list of entities, relationships, and attributes that can be included in an ontology.</li> <li>Semantic relation extraction: ChatGPT can be employed to detect semantic relationships between concepts, such as \"is-a\" or \"part-of\" relations, facilitating the construction of the ontology's hierarchical structure.</li> <li>Ontology validation: ChatGPT can assist in validating and refining the structure of an ontology by providing human-like feedback on its consistency and coherence, suggesting improvements, or identifying potential gaps or redundancies.</li> <li>Automated ontology population: ChatGPT can be used to generate instances of entities and relationships based on examples or descriptions, helping to populate the ontology with relevant data.</li> <li>Ontology alignment and merging: ChatGPT can help identify similarities and differences between different ontologies, supporting the process of ontology alignment and merging, which is essential when integrating information from diverse sources.</li> <li>Natural language interfaces: ChatGPT can be utilized to develop natural language interfaces for interacting with ontologies, allowing users to query and explore the ontology using conversational language rather than complex formal queries.</li> <li>Ontology documentation: ChatGPT can assist in generating human-readable documentation for ontologies, including explanations of concepts, relationships, and properties, making it easier for users to understand and work with the ontology.</li> <li>Ontology learning: ChatGPT can be trained to learn from examples or other sources of knowledge, supporting the semi-automatic or automatic creation of ontologies based on the input data.</li> </ol> <p>By leveraging the capabilities of ChatGPT, ontology developers can streamline the development process, enhance the quality of their ontologies, and facilitate their adoption and use across various domains.</p> <p></p>"},{"location":"lesson/chatgpt-ontology-curation/#how-can-we-concretely-enhance-the-ontology-curation-process","title":"How can we, concretely, enhance the ontology curation process?","text":"<p>A normal workflow of ontology curation is this:</p> <ol> <li>A concept is required by a biocurator to describe a piece of data, for example, a disease concept, or an anatomical entity.</li> <li>The biocurator requests the new concept (or term) from the ontology curator, providing a label and perhaps a scientific reference</li> <li>The ontology curator then adds that concept to the ontology using a tool like Protege, adding a human-readable definition, synonyms and provenance metadata. Most importantly, they define the concept logically, often using a formalism such as OWL specifying super-classes, and other logical relationships</li> <li>Lastly, the ontology curator runs a number of quality checks to ensure that the added concepts is logically coherent (e.g. the reasoner places it in the intended part of of the class hierarchy).</li> </ol> <p>How can ChatGPT be used concretely to help with that process?</p> <p>ChatGPT can be incorporated into various stages of the ontology curation workflow to assist biocurators and ontology curators in streamlining and improving the process. Here's how ChatGPT can be used concretely at each step:</p> <ol> <li>Concept identification: ChatGPT can help the biocurator identify relevant concepts based on the data they are working with. By providing information or descriptions related to the data, the biocurator can receive a list of potential concepts or terms, including disease concepts or anatomical entities, that might be suitable for the ontology.</li> <li>Term request and communication: ChatGPT can facilitate communication between the biocurator and the ontology curator. It can be used to draft the term request, including the label, scientific reference, and any other relevant information, ensuring that the request is clear and complete.</li> <li>Concept addition and metadata generation: ChatGPT can assist ontology curators in generating human-readable definitions, synonyms, and provenance metadata for the new concept. By providing the initial label and scientific reference, ChatGPT can produce candidate definitions, suggest synonyms, and even help draft metadata descriptions. Additionally, it can provide suggestions for logical relationships, such as super-classes and other connections, using its understanding of the formalism, like OWL.</li> <li>Quality checks and coherence: While ChatGPT may not directly run quality checks like a reasoner, it can help ontology curators verify the coherence of the added concepts by providing human-like feedback on the logical relationships and overall structure. It can suggest potential issues, such as redundancies or inconsistencies, that the curator may want to address to ensure that the concept is logically coherent and correctly placed in the class hierarchy.</li> </ol> <p>By integrating ChatGPT into the ontology curation workflow, biocurators and ontology curators can benefit from its natural language understanding and generation capabilities, making the process more efficient and accurate. However, it is essential to keep in mind that ChatGPT should be used as a complementary tool and not as a replacement for human expertise or specialized software designed for ontology development and curation.</p> <p></p>"},{"location":"lesson/chatgpt-ontology-curation/#using-chatgpt-for-ontology-mappings","title":"Using ChatGPT for ontology mappings","text":"<p>What about ontology mappings?</p> <p>LLMs like ChatGPT can be used to identify equivalent or similar concepts across different ontologies by analyzing concept labels, definitions, and other metadata. They can understand and process natural language descriptions and compare them to find potential matches or alignments between concepts. Specifically, LLMs can:</p> <ul> <li>Perform text similarity analysis: By comparing the textual descriptions of concepts, LLMs can calculate similarity scores, identifying closely related concepts across different ontologies.</li> <li>Understand contextual relationships: LLMs can identify and consider the context in which concepts are used, making it easier to match concepts that have similar meanings or roles within their respective ontologies.</li> <li>Leverage synonyms and alternative terms: LLMs can use synonyms or alternative terms provided in the ontology metadata to improve concept matching, ensuring that equivalent concepts with different labels can still be identified.</li> <li>Exploit structured knowledge: LLMs can be trained to understand and work with formalisms like OWL and RDF, which can help them identify matching concepts based on the structured information within ontologies.</li> </ul> <p>By leveraging these capabilities, LLMs can assist ontology curators in the challenging task of concept matching, ultimately enhancing the ontology mapping and alignment process.</p> <p></p> <p>Can you provide an example of mapping a concept from the Human Phenotype Ontology to the Mammalian Phenotype Ontology?</p> <p>Let's consider the concept \"Abnormality of the ear\" in HPO.</p> <ol> <li>HPO Concept:</li> <li>ID: HP:0000598</li> <li>Label: Abnormality of the ear</li> <li>Definition: A structural or functional anomaly of the ear.</li> </ol> <p>Our goal is to find a matching or similar concept in the Mammalian Phenotype Ontology (MPO).</p> <ol> <li>After analyzing the MPO, we find the following concept:</li> <li>ID: MP:0002161</li> <li>Label: abnormal ear morphology</li> <li>Definition: any structural anomaly of the external or middle ear</li> </ol> <p>In this case, both concepts describe abnormalities of the ear in their respective domains (human and mammalian). The labels and definitions are similar, indicating that these concepts might be considered equivalent or closely related.</p> <p>Thus, we can propose the following mapping:</p> <ul> <li>HPO: HP:0000598 (Abnormality of the ear) &lt;=&gt; MPO: MP:0002161 (abnormal ear morphology)</li> </ul> <p>COMMENT. The 27th May version of ChatGPT is giving a different answer:</p> <p>Sure, let's consider an example for this mapping:</p> <p>Human Phenotype Ontology (HPO) is designed to classify and annotate the phenotypic abnormalities encountered in human disease. Each term in HPO describes a specific phenotypic abnormality, such as 'HP:0004322 - Short stature' which describes an abnormality of body height that is less than -2 standard deviations compared to the appropriate reference population.</p> <p>Mammalian Phenotype Ontology (MPO), on the other hand, is used in the annotation of data from mammalian phenotyping experiments, typically in model organisms like mice. Its equivalent term for short stature might be 'MP:0001265 - decreased body size' which describes a mammal with an overall smaller body size.</p> <p>So, in this case, 'HP:0004322 - Short stature' in HPO could be mapped to 'MP:0001265 - decreased body size' in MPO.</p> <p>Please note that this is a simplified example and the actual mapping process could be more complex, due to the nuanced differences between human and mammalian phenotypes and the structure of these ontologies. For actual mappings, it is essential to ensure the descriptions and definitions align appropriately, and the context of use is correctly considered. </p> <p>Also, there may not always be a one-to-one correspondence between the terms in different ontologies. Sometimes, one term in an ontology may correspond to multiple terms in another, or there may not be an equivalent term at all. This is why ontological mapping is a complex task requiring domain knowledge and careful consideration.</p> <p></p> <p>Can you express the above mapping in SSSOM?</p> <p>The Simple Standard for Sharing Ontology Mappings (SSSOM) is a standard that enables sharing and comparing ontology mappings using a simple tabular format (TSV). Here's the example mapping from Human Phenotype Ontology (HPO) to Mammalian Phenotype Ontology (MPO) expressed in SSSOM format:</p> <pre><code>subject_id  subject_label                 predicate_id  object_id  object_label            match_type  confidence\nHP:0000598  Abnormality of the ear        skos:closeMatch  MP:0002161  abnormal ear morphology  Lexical     0.90\n</code></pre> <p>In this case, I have used skos:closeMatch as the predicate to represent the close relationship between the two concepts. The match_type is \"Lexical\" since the mapping is based on similar labels and definitions. The confidence score is set to 0.90 to reflect a high degree of similarity between the concepts, but this value may vary depending on the actual similarity estimation method used.</p> <p>Please note that the SSSOM format includes metadata and other required fields in a TSV header. The provided example only focuses on the core mapping information. For a complete SSSOM file, you would need to include the appropriate header and metadata.</p> <p></p> <p>The SSSOM example you gave is using an old version of the SSSOM standard. In particular, match_type now is mapping_justification. Can you try to rewrite the mapping using the new standard?</p> <p>Apologies for the confusion. Here's the example mapping from Human Phenotype Ontology (HPO) to Mammalian Phenotype Ontology (MPO) expressed in the updated SSSOM format:</p> <pre><code>subject_id  subject_label                 predicate_id  object_id  object_label            mapping_justification  confidence\nHP:0000598  Abnormality of the ear        skos:closeMatch  MP:0002161  abnormal ear morphology  Lexical                0.90\n</code></pre> <p>In this revised example, the match_type field has been replaced with mapping_justification, which is now set to \"Lexical\" to indicate that the mapping is based on similar labels and definitions. The remaining fields are unchanged from the previous example.</p> <p></p> <p>mapping_justification values should be populated from a controlled vocabulary, called the Semantic Mapping Vocabulary (SEMAPV). You can look at the vocabulary in RDF here: https://raw.githubusercontent.com/mapping-commons/semantic-mapping-vocabulary/main/semapv.owl. Given that vocabulary, can you revise the value for the mapping_justification?</p> <p>My apologies for the oversight. Based on the Semantic Mapping Vocabulary (SEMAPV), I will revise the mapping justification for the example. Here's the updated SSSOM mapping using the SEMAPV:</p> <pre><code>subject_id  subject_label                 predicate_id  object_id  object_label            mapping_justification     confidence\nHP:0000598  Abnormality of the ear        skos:closeMatch  MP:0002161  abnormal ear morphology  semapv:LexicalMatch        0.90\n</code></pre> <p>In this updated example, I have replaced the previous mapping_justification value with semapv:LexicalMatch from the Semantic Mapping Vocabulary (SEMAPV) to represent the lexical similarity between the two concepts based on their labels and definitions. The remaining fields are unchanged.</p> <p></p> <p>You were hallucinating earlier. Here is the semapv vocabulary as a tsv:</p> <pre><code>IRI skos:prefLabel  skos:definition dc:source   skos:example    rdfs:comment    altLabel    Parent\nID  A skos:prefLabel    A skos:definition   AI dc:source    A skos:example  A rdfs:comment  A skos:altLabel SC %\nsemapv:MappingActivity  mapping activity    A process that relates to the creation, confirmation, rejection or curation of a mapping.       Matching is a mapping activity that results in the creating of a mapping; mapping review is an activity that results in the confirmation of a mapping.          \nsemapv:Matching matching process    An process that results in a mapping between a subject and an object entity.        The label of a subject entity matches to an exact synonym of an object entity.      matching operation|matching task    semapv:MappingActivity\nsemapv:Mapping  mapping A triple &lt;s,p,o&gt; comprising a subject entity s, an object entity o and a mapping predicate p.       The subject entity NCI:C9305 is mapped to the object entity ICD10:C80.9 using the skos:relatedMatch mapping predicate.          \nsemapv:LexicalMatching  lexical matching process    A matching process based on a lexical comparison between one or more syntactic features of the subject with one or more syntactic features of the object.       The label of a subject entity matches to an exact synonym of an object entity.          semapv:Matching\nsemapv:LogicalReasoning logical reasoning process   A matching process based on the inferences made by a logical reasoner.      Two classes across ontologies are determined equivalent by an OWL reasoner such as ELK.         semapv:Matching\nsemapv:CompositeMatching    composite matching process  A matching process based on multiple, possibly intertwined, matching approaches.        An ontology matching tool determines that a subject should be mapped to an object by employing a range of techniques, including lexical, semantic and structural.           semapv:Matching\nsemapv:UnspecifiedMatching  unspecified matching process    A matching process based on an unspecified comparison.      A mapping between a subject and an object was established, but it is no longer clear how or why.            semapv:Matching\nsemapv:SemanticSimilarityThresholdMatching  semantic similarity threshold-based matching process    A matching process based on a minimum threshold of a score from a comparison based on a semantic similarity algorithm.      A match between a subject and an object entity was established because they had a Jaccard score higher than 0.8 based on the set of (common) superclasses.          semapv:Matching\nsemapv:LexicalSimilarityThresholdMatching   lexical similarity threshold-based matching process A lexical matching process based on a minimum threshold of a score from a comparison based on a lexical similarity algorithm.       A match between a subject and an object was established because they had a Levenshtein score higher than 0.8.           semapv:Matching\nsemapv:StructuralMatching   structural matching process     https://doi.org/10.1016/j.websem.2009.11.002    A match between a subject and an object was established because of the similarity of their structural features, e.g., the number of direct property of a class. Structural matching does not involve looking at \"values\" of properties.     semapv:Matching\nsemapv:InstanceBasedMatching    instance-based matching process A matching process based on individual representations (or instances).  https://doi.org/10.1007/978-3-642-38721-0   A match between a subject A and an object B was established because they share the same instances.          semapv:Matching\nsemapv:BackgroundKnowledgeBasedMatching background knowledge-based matching process A matching process that exploits background knowledge from external resources, commonly referred to as background knowledge resources. This approach is also known as indirect matching, BK-based matching or context-based matching.   https://doi.org/10.1016/j.websem.2018.04.001    A match between a subject A and an object B was established because they appear equivalent under consideration of externally provided background knowledge.         semapv:Matching\nsemapv:MappingChaining  mapping chaining-based matching process A matching process based on the traversing of multiple mappings.        A match between a subject A and an object B was established because A was mapped to C, C was mapped to D and D was mapped to B.         semapv:Matching\nsemapv:MappingReview    mapping review  A process that is concerned with determining if a mapping \u201ccandidate\u201d (otherwise determined) is reasonable/correct.     A match between a subject A and an object B was established elsewhere, and a human reviewer determined that the mapping is true (or false) based on an independent evaluation.          semapv:MappingActivity\nsemapv:ManualMappingCuration    manual mapping curation An matching process that is performed by a human agent and is based on human judgement and domain knowledge.        A human curator determines that a subject should be mapped to an object by virtue of their domain expertise.            semapv:Matching\nsemapv:MatchingPreprocessing    matching process pre-processing A preliminary processing of inputs prior to performing matching.                    semapv:MappingActivity\nsemapv:Stemming stemming    The process of reducing a word to its word stem.                    semapv:Normalization\nsemapv:Lemmatization    lemmatization   The process of reducing the different forms of a word to one single form.   https://doi.org/10.1007/978-3-642-38721-0               semapv:Normalization\nsemapv:StopWordRemoval  stop-word removal   A preprocessing method transforming a string by removing a list of stop words.  https://doi.org/10.1007/978-3-642-38721-0   The stop-words \u201cof, and, is, a\u201d are removed from the label field of the subject entity in the mapping.          semapv:Normalization\nsemapv:RegexRemoval regex removal   A preprocessing method transforming a string by matching a regular expression and then removing that match.     The regex match \u201c[ -]phenotype\u201d is removed from the label field of the subject entity in the mapping.           semapv:Normalization\nsemapv:RegexReplacement regex replacement   A preprocessing method transforming a string by matching a regular expression and then replacing that match with a specified substitution string.       The regex match \u201c[ -]phenotype\u201d is replaced by \u201c-disease\u201d for the label field of the subject entity in the mapping.         semapv:Normalization\nsemapv:MatchingPostProcessing   matching process post-processing    A process applied to a set of mappings, usually with the intention of changing it.                  semapv:MappingActivity\nsemapv:ThresholdFiltering   threshold filtering                     semapv:MatchingPostProcessing\nsemapv:StableMarriageFiltering  stable marriage filtering                       semapv:MatchingPostProcessing\nsemapv:CardinalityFiltering cardinality filtering                       semapv:MatchingPostProcessing\nsemapv:LogicalConsistencyFiltering  logical consistency filtering                       semapv:MatchingPostProcessing\nsemapv:Normalization    normalization   A family of preprocessing techniques for reducing strings to be compared to a common format.    https://doi.org/10.1007/978-3-642-38721-0               semapv:MatchingPreprocessing\nsemapv:CaseNormalization    case normalization  A normalization technique converting each alphabetic character in a string to their lower case counterpart. https://doi.org/10.1007/978-3-642-38721-0   The string 'Ontology' is normalised to 'ontology' (lower case).         semapv:Normalization\nsemapv:DiacriticsSuppression    diacritics suppression  A normalization technique replacing diactritic signs (e.g. accents, hats) with their most frequent replacements.    https://doi.org/10.1007/978-3-642-38721-0   The string 'Prot\u00e9g\u00e9' is normalised to 'Protege'.            semapv:Normalization\nsemapv:BlankNormalisation   blank normalisation A normalization technique replacing all blank characters, such as space, tabulation, carriage return (or sequences of these) into a single blank character. https://doi.org/10.1007/978-3-642-38721-0   The string 'ontology  matching' (two spaces between 'ontology' and 'matching) becomes 'ontology matching' (one space).          semapv:Normalization\nsemapv:LinkStripping    link stripping  A normalization technique replacing specific links between words, such as apostrophes, dashes, underscore, etc into dashes or blanks.   https://doi.org/10.1007/978-3-642-38721-0   The string 'alzheimer's' becomes 'alzheimers'.          semapv:Normalization\nsemapv:DigitSuppression digit suppression   A normalization technique removing all numbers in a string.     https://doi.org/10.1007/978-3-642-38721-0   The string 'ontology1' becomes 'ontology'.          semapv:Normalization\n</code></pre> <p></p> <p>Can you use this to pick the right mapping justification?</p> <p>Apologies for the confusion earlier. Based on the provided SEMAPV vocabulary as a TSV, I will revise the mapping justification for the example. Here's the updated SSSOM mapping using the correct SEMAPV:</p> <pre><code>subject_id  subject_label                 predicate_id  object_id  object_label            mapping_justification                    confidence\nHP:0000598  Abnormality of the ear        skos:closeMatch  MP:0002161  abnormal ear morphology  semapv:LexicalMatching                    0.90\nIn this updated example, I have replaced the previous mapping_justification value with semapv:LexicalMatching from the provided Semantic Mapping Vocabulary (SEMAPV) to represent the lexical similarity between the two concepts based on their labels and definitions. The remaining fields are unchanged.\n</code></pre> <p>Please remember that a complete SSSOM file would require the appropriate header and metadata in addition to the core mapping information provided in the example.</p> <p></p> <p>Can you add 4 more examples to that SSSOM table</p> <p>Sure, here are 4 more examples of mappings between the Human Phenotype Ontology (HPO) and the Mammalian Phenotype Ontology (MPO) using SSSOM format:</p> <pre><code>subject_id  subject_label                         predicate_id  object_id  object_label                      mapping_justification                    confidence\nHP:0000598  Abnormality of the ear                skos:closeMatch  MP:0002161  abnormal ear morphology            semapv:LexicalMatching                    0.90\nHP:0000365  Hearing impairment                    skos:closeMatch  MP:0006325  hearing loss                      semapv:LexicalMatching                    0.92\nHP:0001382  Joint hypermobility                   skos:closeMatch  MP:0004927  increased joint flexibility       semapv:LexicalMatching                    0.85\nHP:0001392  Hepatomegaly                          skos:closeMatch  MP:0002764  enlarged liver                    semapv:LexicalMatching                    0.88\nHP:0001250  Seizures                              skos:closeMatch  MP:0002064  seizures                          semapv:LexicalMatching                    0.95\n</code></pre> <p>These examples illustrate mappings between HPO and MPO concepts based on lexical matching, where the similarity between the labels and definitions is considered. Note that the confidence values are provided as an example and may differ in real-world scenarios.</p> <p></p> <p>Can you print only the table using markdown table syntax in a way that I can copy paste it</p> subject_id subject_label predicate_id object_id object_label mapping_justification confidence HP:0000598 Abnormality of the ear skos:closeMatch MP:0002161 abnormal ear morphology semapv:LexicalMatching 0.90 HP:0000365 Hearing impairment skos:closeMatch MP:0006325 hearing loss semapv:LexicalMatching 0.92 HP:0001382 Joint hypermobility skos:closeMatch MP:0004927 increased joint flexibility semapv:LexicalMatching 0.85 HP:0001392 Hepatomegaly skos:closeMatch MP:0002764 enlarged liver semapv:LexicalMatching 0.88 HP:0001250 Seizures skos:closeMatch MP:0002064 seizures semapv:LexicalMatching 0.95 <p></p> <p>Can you add 100 more examples, this time only providing a markdown table that I can copy paste, with no further comments around it?</p> <p>Creating 100 mappings with accuracy requires domain expertise and a comprehensive understanding of the ontology mappings. However, I can provide you with 20 examples to help you understand the format. Please note that these examples may not be completely accurate and should not be used for any critical applications.</p> subject_id subject_label predicate_id object_id object_label mapping_justification confidence HP:0000010 Megabladder skos:closeMatch MP:0000547 enlarged bladder semapv:LexicalMatching 0.85 HP:0000014 Renal agenesis skos:closeMatch MP:0000548 absent kidney semapv:LexicalMatching 0.82 HP:0000015 Bladder diverticulum skos:closeMatch MP:0000556 bladder diverticula semapv:LexicalMatching 0.88 HP:0000016 Urethral atresia skos:closeMatch MP:0000565 closed urethra semapv:LexicalMatching 0.90 HP:0000020 Renal duplication skos:closeMatch MP:0000571 duplex kidney semapv:LexicalMatching 0.84 HP:0000021 Hypospadias skos:closeMatch MP:0000573 abnormal penis morphology semapv:LexicalMatching 0.76 HP:0000022 Urethral stricture skos:closeMatch MP:0000574 urethral obstruction semapv:LexicalMatching 0.86 HP:0000025 Polycystic kidney dysplasia skos:closeMatch MP:0000576 polycystic kidney semapv:LexicalMatching 0.92 HP:0000026 Hyperechogenic kidneys skos:closeMatch MP:0000581 abnormal kidney echogenicity semapv:LexicalMatching 0.81 HP:0000027 Renal cortical microcysts skos:closeMatch MP:0000584 renal cortical cysts semapv:LexicalMatching 0.89 HP:0000028 Renal tubular atrophy skos:closeMatch MP:0000585 atrophic renal tubules semapv:LexicalMatching 0.87 HP:0000031 Renal hypoplasia skos:closeMatch MP:0000591 small kidney semapv:LexicalMatching 0.79 HP:0000032 Renal hypoplasia/aplasia skos:closeMatch MP:0000592 renal hypoplasia semapv:LexicalMatching 0.91 HP:0000033 Proximal tubulopathy skos:closeMatch MP:0000593 abnormal proximal tubule semapv:LexicalMatching 0.75 AT THIS POINT CHATGPT JUST STOPPED GENERATING CONTENT. <p></p>"},{"location":"lesson/chatgpt-ontology-curation/#generating-pre-coordinated-expressions-and-schema-instances","title":"Generating pre-coordinated expressions and schema instances","text":"<p>Here are a few slots in a schema: quality, anatomical_entity and chemical_entity</p> <pre><code>quality:\ndescription: The property being measured, or changes in this property, for example, amount, level, increased amount, decreased concentration\nannotations:\n    prompt.example: amount, level, increased amount, decreased concentration\nrange: Quality\n\nanatomical_entity:\ndescription: The anatomical location that the chemical entity is measured in\nrange: AnatomicalEntity\nannotations:\n    prompt.example: liver, heart, brain, finger\n\nchemical_entity:\ndescription: The chemical entity that is being measured\nrange: ChemicalEntity\nannotations:\n    prompt.example: lysine, metabolite\n</code></pre> <p></p> <p>Can you create a YAML file with those three elements as keys, and extract the contents of the string \"increased blood glucose levels\" into as values to these keys? Output should be just a simple yaml file, like:</p> <pre><code>quality: concentration\nanatomical_entity: liver\nchemical_entity: lysine\n</code></pre>"},{"location":"lesson/chatgpt-ontology-curation/#todo-finish-this-section","title":"TODO FINISH THIS SECTION","text":""},{"location":"lesson/chatgpt-ontology-curation/#from-chat-to-exectutable-workflows-what-we-need-to-do-to-leverage-llms","title":"From Chat to exectutable workflows: what we need to do to leverage LLMs","text":"<p>The above tutorial was a fun case study using ChatGPT with GPT-4. 95% of the content provided was generated by ChatGPT with GPT-4. While certainly not as great as possible, it took a solid ontology engineer (<code>@matentzn</code>) about 90 minutes to write this lesson, which would have usually cost him more than 8 hours.</p> <p>It is clear that learning how to talk to AI, the process we refer to as \"prompt engineering\" is going to be absolutely essential for ontology curators moving forward - as LLMs improve and understand even complex languages like OWL better, perhaps as important as ontology modelling itself. I dont think there is any doubt that enganging is a good amount of play and study on this subject is both fun and hugely beneficial.</p> <p>All that said, perceiving LLMs through the lens of a chat bot leaves a lot of potential unexplored. For example, if ChatGPT (or LLMs in general) can generate structured data, why not implement this directly into our curation tools (like Protege)? Tools like GitHub co-pilot are already used to making good programmers a lot more effective, but so far, these tools focus on development environments where the majority of the generated content is text (e.g. software code), and not so much heavily UI driven ones like Protege.</p> <p>A lot of blog posts have circulated recently on Twitter and LinkedIn explored the potential of LLMs to generate RDF and OWL directly. It is already clear that LLMs can and will do this moving forward. For ontology curation specifically, we will need to develop executable workflows that fit into our general ontology curation process. As a first pass, some members of our community have developed OntoGPT. We will explore how to use OntoGPT in a future lesson.</p>"},{"location":"lesson/chatgpt-ontology-curation/#some-more-thoughts-on-hallucinations","title":"Some more thoughts on hallucinations","text":"<p>Update 27 May 2023: It seems that complaints wrt to hallucinations, the chat part of ChatGPT is a bit more sensitive to database like queries:</p> <p></p>"},{"location":"lesson/chatgpt-ontology-curation/#cool-applications-the-authors-of-this-tutorial-used","title":"Cool applications the authors of this tutorial used","text":"<ul> <li>The key is that it is generative: the key thing is that we are generating text that is human like</li> <li>As databases, LLMs are very inefficient (see excellent talk by Denny Vrandecic) and Hallucinate too often (Chat)</li> <li>Writing emails: shortening</li> <li>Writing documentations:</li> <li>Writing docs for a softare tool. Adding background info and context.</li> <li>Find a good human-readable name for a cluster in ML clustering</li> <li>Named entity recognition and knowledge extraction</li> <li>Career planning and discovering stuff you didnt know about at all</li> <li>ChatGPT for contextualising work and writing use cases</li> <li>Using it for curation research - but doing it right</li> <li>Don't ask it to do the curation for you, get it to help you find arguments for and against a modelling decision (\"I will always have to recurate anyways, so what is the point?\")</li> <li>Always remember that ChatGPT is not a database. It hallucinates and changes its mind all the time. Get it to generate text for you that could be useful in your work.</li> </ul>"},{"location":"lesson/chatgpt-ontology-curation/#additional-materials","title":"Additional materials","text":"<ul> <li>Tutorial on using OntoGPT</li> <li>Reference of effective prompts for ChatGPT</li> </ul> <p>(Current) Limitations:</p> <ul> <li>8 Things to know about large language models</li> <li>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/","title":"Contributing to OBO ontologies","text":""},{"location":"lesson/contributing-to-obo-ontologies/#prerequisites","title":"Prerequisites","text":"<p>Participants will need to have access to the following resources and tools prior to the training:</p> <ul> <li>GitHub account - Register for a free GitHub account here</li> <li>Protege - Install the latest version of Protege, download it from the GitHub repository here</li> <li>Install GitHub Desktop - Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#preparation","title":"Preparation","text":"<ul> <li>Review tutorial on Ontology Term Use</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: How to contribute terms to existing ontologies.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#learning-objectives","title":"Learning objectives","text":"<ul> <li>How to use GitHub</li> <li>GitHub workflows</li> <li>Branch vs Fork</li> <li>How to create GitHub Issues</li> <li>Understand basic Open Source etiquette</li> <li>Reading READMEs</li> <li>Understand basics of ontology development workflows</li> <li>Browsing and Searching in Protege</li> <li>Add new terms to an ontology<ul> <li>Initial Protege setup</li> <li>Protege editing</li> <li>The Class description view</li> </ul> </li> <li>Use GitHub: make pull requests</li> <li>Understand ontology design patterns</li> <li>Use templates: ROBOT, DOSDP (under development)</li> <li>Basics of OWL</li> <li>Logic and debugging</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#tutorials","title":"Tutorials","text":""},{"location":"lesson/contributing-to-obo-ontologies/#monarch-obo-training-tutorials","title":"Monarch OBO training Tutorials","text":""},{"location":"lesson/contributing-to-obo-ontologies/#contributing-to-ontologies-part-1","title":"Contributing to Ontologies Part 1","text":""},{"location":"lesson/contributing-to-obo-ontologies/#contributing-to-ontologies-part-2","title":"Contributing to Ontologies Part 2","text":""},{"location":"lesson/contributing-to-obo-ontologies/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/contributing-to-obo-ontologies/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> <li>Rebecca Jackson</li> <li>Melissa Haendel</li> <li>Chris Mungall</li> <li>David Osumi-Sutherland</li> <li>Matt Yoder</li> <li>Carlo Torniai</li> <li>Simon Jupp</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#use-github","title":"Use GitHub","text":""},{"location":"lesson/contributing-to-obo-ontologies/#github-workflows","title":"GitHub workflows","text":"<p>GitHub - distributed version control (Git) + social media for geeks who like to build code/documented collaboratively.</p> <p>A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub.</p> <p></p> <p>A Git repo typically has a master or main branch that is not directly editing. Changes are made by creating a branch from Master (complete copy of the Master + its history).</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#branch-vs-fork","title":"Branch vs Fork","text":"<p>You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners.\u00a0 If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo.</p> <p>If you have permission from the owners, you can instead make a new branch. For this training, we gave you access to the repository. See the Appendix for instructions on how to make a fork.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#create-github-issues","title":"Create GitHub Issues","text":"<ol> <li>Go to GitHub tracker for the ontology where you'd like to create an issue</li> <li>Select New issue</li> <li>Pick appropriate template (if applicable)</li> <li>Fill in the information that is requested on the template below each header</li> <li>For a new term request, please include:</li> <li>The parent ID and label</li> <li>A definition in the proper format</li> <li>Sources/cross references for synonyms</li> <li>Your ORCID</li> <li>Add any additional comments at the end</li> <li>If you are requesting changes to an existing term, include as much information as possible, including the term ID and label.</li> <li>If you use a template, an ontology curator may automatically be assigned.</li> </ol> <p>Tip: you can easily obtain term metadata like OBO ID, IRI, or the term label by clicking the three lines above the Annotations box (next to the term name) in Protege, see screenshot below. You can also copy the IRI in markdown, which is really convenient for pasting into GitHub.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#video-explanation","title":"Video Explanation","text":"<p>See this example video on creating a new term request to the Mondo Disease Ontology:</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#basic-open-source-etiquette","title":"Basic Open Source etiquette","text":"<ul> <li>Keep in mind that open source ontology repositories on GitHub are public and open to all.</li> <li>Be respectful in your requests and comments.</li> <li>Do not include any private information.</li> <li>GitHub sends notifications to your email, and you can respond via your email client. Keep in mind, the responses are posted publicly. Be sure to delete your email signature that includes any personal information, like your email address or phone number.</li> <li>Many ontologies have limited resources and personnel for development and maintenance. Please be patient with your requests.</li> <li>If your ticket/request has been unanswered for a long period of time, feel free to kindly check in by commenting on the ticket.</li> <li>Including a deadline or priority on the ticket can help the ontology curators with triaging tickets.</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#reading-readmes","title":"Reading READMEs","text":"<p>A README is a text file that introduces and explains a project. It is intended for everyone, not just the software or ontology developers. Ideally, the README file will include detailed information about the ontology, how to get started with using any of the files, license information and other details. The README is usually on the front page of the GitHub repository.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#basics-of-ontology-development-workflows","title":"Basics of ontology development workflows","text":""},{"location":"lesson/contributing-to-obo-ontologies/#ontology-development-workflows","title":"Ontology development workflows","text":"<p>The steps below describe how to make changes to an ontology.</p> <ol> <li>Go to the GitHub repository for your ontology, and clone the repository. The example below describes how to clone the Mondo Disease Ontology repo, but this can be applied to any ontology that is stored in GitHub.</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#clone-the-mondo-repo","title":"Clone the Mondo repo","text":"<ol> <li>Open the Mondo GitHub repository</li> <li>Click Code</li> </ol> <ol> <li>Click 'Open with GitHub Desktop'</li> </ol> <ol> <li>You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos.</li> <li>This will open GitHub Desktop and the repo should start downloading. This could take some time depending on how big the file is and how much memory your computer has.</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#create-a-branch-using-github-desktop","title":"Create a branch using GitHub Desktop","text":"<ol> <li>Click the little arrow in Current Branch</li> <li>Click New Branch</li> <li>Give your branch a name: training-initials (ie <code>training-NV</code>)</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#open-the-ontology-edit-file-in-protege","title":"Open the Ontology edit file in Protege","text":"<ol> <li>Open Protege</li> <li>Go to: File -&gt; Open</li> <li>Navigate to [ontology-name]/src/ontology/[ontology-name]-edit.obo and open this file in Protege. For example: mondo/src/ontology/mondo-edit.obo</li> <li>Note: all ontologies that use the Ontology Development Kit (ODK) will have the 'edit' files stored in the same folder path: src/ontology/[ontology-name]-edit.owl (or [ontology-name]-edit.obo)</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#browsing-and-searching-in-protege","title":"Browsing and Searching in Protege","text":"<p>The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#open-the-mondo-in-protege","title":"Open the Mondo in Prot\u00e9g\u00e9","text":"<p>Note: Windows users should open Protege using run.bat</p> <ol> <li>Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo)</li> <li>When you open Protege, you will be on the Active Ontology tab</li> <li>Note the Ontology IRI field. The IRI is used to identify the ontology on the Web.</li> </ol> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#the-protege-ui","title":"The Prot\u00e9g\u00e9 UI","text":"<p>The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations.</p> <p>To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab.</p> <p></p> <p>Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. \u00a0For most normal usage you should answer no. This will open in a new window.</p> <p>The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#running-the-reasoner","title":"Running the reasoner","text":"<p>Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner:</p> <p></p> <p>For more details on why it is important to have the reasoner on when using the editors version of an ontology, see the Reasoning reference guide. But for now, you don't need a deeper understanding, just be sure that you always have the reasoner on.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#entities-tab","title":"Entities tab","text":"<p>You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below).</p> <p></p> <p>The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#searching-in-protege","title":"Searching in Protege","text":"<p>You can search for any entity using the search bar on the right:</p> <p></p> <p>The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together.</p> <p></p> <p>Here's an example search for 'COVID-19': </p> <p>It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g.</p> <p></p> <p>In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom.</p> <p>Try to find these specific classes:</p> <ul> <li>'congenital heart disease'</li> <li>'Kindler syndrome'</li> <li>'kidney failure'</li> </ul> <p>Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019.</p> <ul> <li>Try searching for \u2018br car and see what kind of results are returned.</li> <li>Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term?</li> </ul> <p>Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#use-github-make-pull-requests","title":"Use GitHub: make pull requests","text":""},{"location":"lesson/contributing-to-obo-ontologies/#committing-pushing-and-making-pull-requests","title":"Committing, pushing and making pull requests","text":"<ol> <li> <p>Changes made to the ontology can be viewed in GitHub Desktop.</p> </li> <li> <p>Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the ontology editors for help instead.</p> </li> </ol> <p>Example 1:</p> <p></p> <ol> <li>Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:0001012 episodic angioedema with eosinophilia</li> </ol> <p>NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages.</p> <ol> <li> <p>Note: 'Fixes' and \"Closes' are case-insensitive.</p> </li> <li> <p>If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed.</p> </li> <li> <p>Click Commit to [branch]. This will save the changes to the cl-edit.owl file.</p> </li> <li> <p>Push: To incorporate the changes into the remote repository, click Publish branch.</p> </li> </ol> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#add-new-terms-to-an-ontology","title":"Add New Terms to an Ontology:","text":"<p>The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#setup","title":"Setup","text":""},{"location":"lesson/contributing-to-obo-ontologies/#setting-preferences-for-new-entities","title":"Setting Preferences for New entities","text":"<p>Ontology terms have separate names and IDs. The names are annotation values (labels) and the IDs are represented using IRIs. The OBO foundry has a policy on IRI (or ID) generation (http://www.obofoundry.org/principles/fp-003-uris.html). You can set an ID strategy using the \"New Entities\" tab under the Prot\u00e9g\u00e9 Preferences -- on the top toolbar, click the \"Prot\u00e9g\u00e9 dropdown, then click Preferences.</p> <p></p> <p>Set your new entity preferences precisely as in the following screenshot of the New Entities tab.</p> <p>Note - you have been assigned an ID range in the Mondo idranges file\u00a0 - you should be able to find your own range assigned there.</p> <p>DIY (only if you know what you are doing!)</p> <p>To add your own ID ranges:</p> <p>Go into src/ontology</p> <p>create a branch</p> <p>Find and edit mondo-idranges.owl by adding the following:</p> <pre><code>Datatype: idrange:10 #update this to next following integer from previous\n\n    Annotations:\n        allocatedto: \"Your Name\" #change to your name\n\n    EquivalentTo:\n    xsd:integer[&gt;= 0806000 , &lt;= 0806999]. #add a range of 999 above the previous integer\n</code></pre> <p>Be sure to change \"Your Name\" to your actual name! And note that this value should almost always be an individual, and not an organization or group.</p> <p>create a pull request and add matentzn or nicolevasilevsky as a reviewer</p> <p>proceed to setting up as below:</p> <p></p> <p>Specified IRI: http://purl.obolibrary.org/obo/</p> <p>Note - if you edit more than one ontology in Protege, you will need to update your Preferences for each ontology before you edit.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#setting-preferences-for-user-details","title":"Setting Preferences for User details","text":"<p>User name: click Use supplied user name and enter your username in the field below</p> <p>Click Use Git user name when available</p> <p>In the ORCID field, add your ORCID ID (in the format 0000-0000-0000-0000)</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#setting-preferences-for-new-entities-metadata","title":"Setting Preferences for New entities metadata","text":"<p>The current recommendation of the OBO Foundry Technical Working Group is that an editor who creates a new term SHOULD add a <code>http://purl.org/dc/terms/contributor</code> annotation, set to the ORCID or GitHub username of the editor, and a <code>http://purl.org/dc/terms/date</code> annotation, set to the current date.</p> <p>You can have Prot\u00e9g\u00e9 automatically add those annotations by setting your preferences to match the screenshot below, in the New entities metadata tab (under preferences).</p> <p>If you do not have an ORCID, register for for free here: https://orcid.org/</p> <p></p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#protege-editing","title":"Protege editing","text":""},{"location":"lesson/contributing-to-obo-ontologies/#creating-a-new-class","title":"Creating a new class","text":"<p>Before you start:</p> <ul> <li> <p>make sure you are working on a branch - see quick guide here.</p> </li> <li> <p>make sure you have the editor's file open in Protege as detailed here.</p> </li> </ul> <p>New classes are created in the Class hierarchy panel on the left.</p> <p>There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle).</p> <p>Practice adding a new term:</p> <p>We will work on these two tickets:</p> <ol> <li>https://github.com/monarch-initiative/mondo/issues/616</li> <li>https://github.com/monarch-initiative/mondo/issues/2541</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#httpsgithubcommonarch-initiativemondoissues616","title":"https://github.com/monarch-initiative/mondo/issues/616","text":"<ul> <li> <p>Search for the parent term 'hypereosinophilic syndrome' (see search guide if you are unsure how to do this).</p> </li> <li> <p>When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'hypereosinophilic syndrome'</p> </li> </ul> <p></p> <p>A dialog will popup. Name this new subclass: migratory muscle precursor. Click \"OK\" to add the class.</p> <p></p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#adding-annotations","title":"Adding annotations","text":"<p>Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. CL includes a pre-declared set of annotation properties. The most commonly used annotations are below.</p> <ul> <li>rdfs:label</li> <li>definition</li> <li>has_exact_synonym</li> <li>has_broad_synonym</li> <li>has_narrow_synonym</li> <li>has_related synonym</li> <li>database_cross_reference</li> <li>rdfs:comment</li> </ul> <p>Note, most of these are bold in the annotation property list:</p> <p></p> <p>Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK.</p> <p>Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns.</p> <p></p> <p></p> <p>Definitions in Mondo should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space)</p> <p>To add a dbxref to the definition:</p> <ul> <li>Click the @ symbol next to the definition</li> <li>Click the + button next in the pop-up window</li> <li>Scroll up on the left hand side until you find 'database_cross_reference', and click it</li> <li>Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces)</li> <li>Click OK</li> <li>Add the additional dbxref: GARD:0013029</li> <li>The dbxrefs should appear as below.</li> </ul> <p></p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#add-synonyms-and-database-cross-reference","title":"Add Synonyms and Database cross reference","text":"<ol> <li>Add synonyms</li> <li>Click the add annotations button</li> <li>Add the following synonyms as 'has_exact_synonym':<ul> <li>EAE</li> <li>Gleich's syndrome</li> <li>Gleich syndrome</li> </ul> </li> <li>All synonyms in Mondo should have a dbxref on the synonym</li> <li>Click the @ symbol next to the synonym</li> <li>Click the + button</li> <li>Add the dbxref to each synonym: GARD:0013029</li> <li>Add database cross reference</li> <li>Click the add annotations button</li> <li>Add the following database_cross_reference':<ul> <li>GARD:0013029</li> </ul> </li> <li>Click the @ symbol next to the synonym</li> <li>Click the + button<ul> <li>Add source: MONDO:equivalentTo</li> </ul> </li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#the-class-description-view","title":"The Class description view","text":"<p>We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class.</p> <p>Note the Anonymous Ancestors. This is a difficult concept we will return to later, and the contents of this portion may seem confusing at first (some of these may be clearer after you complete the \"Basics of OWL\" section below). These are OWL expressions that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the expression from. For many ontologies, you will see some quite abstract expressions in here inherited from upper ontologies, but these can generally be ignored for most purposes.</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#revising-a-superclass","title":"Revising a superclass:","text":"<ol> <li> <p>If you want to revise the superclass, click the 'o' symbol next to the superclass and replace the text. Try to revise 'musculoskeletal system disease' to\u00a0 'disease by anatomical system'.</p> </li> <li> <p>If you want to delete a superclass, click the 'x' button next to the superclass. Delete the 'disease by anatomical system' superclass.</p> </li> </ol> <p>Close this window without saving.</p> <p>Save your work.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#make-a-pull-request","title":"Make a Pull Request","text":"<ol> <li> <p>Click: Create Pull Request in GitHub Desktop</p> </li> <li> <p>This will automatically open GitHub Desktop</p> </li> <li> <p>Click the green button 'Create pull request'</p> </li> <li> <p>You may now add comments to your pull request.</p> </li> <li> <p>The CL editors team will review your PR and either ask for changes or merge it.</p> </li> <li> <p>The changes will be available in the next release.</p> </li> </ol> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#ontology-design-patterns","title":"Ontology design patterns","text":"<p>Dead Simple Ontology Design Patterns (DOSDPs) are specifications, written in yaml format, that specify how ontology terms should be created (see article here). They can be used to:</p> <ul> <li>generate documentation</li> <li>generate new terms</li> <li>retrofit existing ontology terms</li> </ul> <p>DOSDPs have some key features:</p> <ul> <li>Description: that describes the purpose of the patterns</li> <li>Examples: Provides examples of terms that use the Patterns</li> <li>Declared classes: these are the classes that are used in the pattern. Any subclass of the declared class can be used in this pattern.</li> <li>Declared relationships: the relationships used in the logical axioms</li> <li>vars: the variable classes that are used in the pattern. Any subclass of the 'var' can be used in this pattern.</li> <li>Pattern for class name, annotations, text definition and equivalentTo (logical definition): Ontology classes are used as 'fillers' to create new classes that conform to the specific patterns.</li> </ul> <p>Examples of design patterns are available here:</p> <ul> <li>uPheno pattern library</li> <li>Mondo Disease Ontology pattern library</li> </ul> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#use-templates-robot-dosdp","title":"Use templates: ROBOT, DOSDP","text":"<p>under development</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#basics-of-owl","title":"Basics of OWL","text":"<ul> <li>Clone the Ontologies 101 repository, then open the folder <code>BDK14_exercises</code> from your file system</li> <li>Open <code>basic-subclass/chromosome-parts.owl</code> in Prot\u00e9g\u00e9, then do the following exercises:<ul> <li>Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions</li> <li>Disjointness</li> <li>Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology</li> <li>OWL Class Restrictions</li> </ul> </li> <li>Open <code>basic-restriction/er-sec-complex.owl</code> in Prot\u00e9g\u00e9, then do the following exercise:<ul> <li>Basic Restrictions</li> </ul> </li> <li>Open <code>basic-dl-query/cc.owl</code> in Prot\u00e9g\u00e9, then do the following exercises:<ul> <li>DL Query Tab - note that <code>owl:Nothing</code> is defined as the very bottom node of an ontology, therefore the DL query results will show <code>owl:Nothing</code> as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of <code>owl:Nothing</code> and therefore unsatisfiable (more on that below).</li> <li>Basic DL Queries</li> </ul> </li> <li>Open <code>basic-classification/ubiq-ligase-complex.owl</code> in Prot\u00e9g\u00e9, then do the following exercises:<ul> <li>Basic Classification</li> </ul> </li> <li>Read (I can't get no) satisfiability (~10 minutes)</li> <li>Optional: Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner?</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#logic-and-debugging","title":"Logic and debugging","text":"<p>Below are exercises to demonstrate how to:</p> <ul> <li>Add equivalent axioms (logical definitions) to ontology terms</li> <li>Run the reasoner and view the inferred hierarchy</li> <li>Debugging and viewing explanations</li> </ul>"},{"location":"lesson/contributing-to-obo-ontologies/#practice-adding-logic-and-debugging","title":"Practice adding logic and debugging","text":"<p>These instructions will use the Mondo disease ontology as an example.</p>"},{"location":"lesson/contributing-to-obo-ontologies/#practice-1","title":"Practice 1","text":""},{"location":"lesson/contributing-to-obo-ontologies/#add-new-terms-with-an-equivalance-axiom-to-mondo","title":"Add New Terms with an Equivalance Axiom to Mondo:","text":""},{"location":"lesson/contributing-to-obo-ontologies/#creating-a-new-class_1","title":"Creating a new class","text":"<p>New classes are created in the Class hierarchy panel on the left.</p> <p>There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle).</p> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#practice-adding-a-new-term","title":"Practice adding a new term:","text":""},{"location":"lesson/contributing-to-obo-ontologies/#add-the-new-term-mycotoxin-allergy","title":"Add the new term 'mycotoxin allergy'","text":"<ol> <li>Navigate to the Mondo repo in GitHub Desktop, create a branch, and open mondo-edit.obo in Protege.</li> <li>Search for the parent term 'allergic disease' (see search guide if you are unsure how to do this).</li> <li>When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'allergic disease'</li> <li>A dialog will popup. Name this new subclass: mycotoxin allergy. Click \"OK\" to add the class.</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#add-annotations-and-a-logical-axiom","title":"Add annotations and a logical axiom","text":"<p>Equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/</p> <p>For this class, we want to follow the design pattern for allergy.</p> <ol> <li>Review this pattern before proceeding.</li> <li>Based on the pattern specifications, add a text definition to this term.</li> <li>Add the database cross reference to this term: MONDO:patterns/allergy</li> <li>Add a synonym that is consistent with this pattern.</li> </ol> <p></p> <ol> <li>Add the equivalence axiom according to the pattern specifications.</li> </ol> <p></p> <p></p> <ol> <li>Run the reasoner</li> <li>View the inferred hierarchy. You should see a child of owl:Nothing (and you should see this in the Description pane as well.)</li> </ol> <p></p> <p></p> <ol> <li>Click on the ? button next to the owl:Nothing in the Description view</li> <li>The explanation tells you the reason why this is unsatisfiable</li> </ol> <p></p> <ol> <li>Mycotoxin is a child of a 'specifically dependent continuant' and and 'independent continuant' is disjoint with 'specifically dependent continuant'.</li> <li>This logical axiom uses the relation 'realized in response to stimulus'</li> <li>Click on this relation in Protege (you can click on the relation name in the equivalence axiom, or you can click on the Object properties tab and search for this relation.</li> <li>There are domain and range restrictions on this property, where the range has to be a material enity. Chebi mycotoxin is actually a role, so it is not a material entity.</li> </ol> <p></p> <ol> <li>Go back to the Classes or Entities pane and remove the equivalence axiom and run the reasoner again. You should now see no unsatisfiable classes.</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#practice-2","title":"Practice 2","text":""},{"location":"lesson/contributing-to-obo-ontologies/#practice-adding-a-new-term_1","title":"Practice adding a new term:","text":""},{"location":"lesson/contributing-to-obo-ontologies/#add-the-new-term-acquired-alacrima","title":"Add the new term 'acquired alacrima'","text":"<ol> <li>Add 'acquired candidiasis as a subclass of MONDO_0002026 candidiasis.</li> </ol>"},{"location":"lesson/contributing-to-obo-ontologies/#add-annotations-and-a-logical-axiom_1","title":"Add annotations and a logical axiom","text":"<p>As noted above, equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/</p> <p>For this class, we want to follow the design pattern for acquired.</p> <ol> <li>Review this pattern before proceeding.</li> <li>Based on the pattern specifications, add a text definition to this term.</li> <li>Add the database cross reference to the definition: MONDO:patterns/allergy</li> <li>Add the equivalence axiom according to the pattern specifications.</li> <li>Run the reasoner</li> <li>View the inferred hierarchy.</li> </ol> <p></p>"},{"location":"lesson/contributing-to-obo-ontologies/#further-reading","title":"Further reading","text":"<ul> <li>Debugging ontologies using OWL reasoning</li> <li>Chris Mungall on how to write great textual definitions</li> </ul>"},{"location":"lesson/developing-an-obo-ontology/","title":"Developing an OBO Reference Ontology","text":""},{"location":"lesson/developing-an-obo-ontology/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course?","text":"<p>Develop skills to lead a new or existing OBO project, or reference ontology develoment.</p>"},{"location":"lesson/developing-an-obo-ontology/#learning-objectives","title":"Learning objectives","text":"<ul> <li>detailed knowledge of OBO principles and best practises</li> <li>use OBO Dashboard</li> <li>use OBO Registry</li> <li>use PURL system</li> </ul>"},{"location":"lesson/developing-an-obo-ontology/#prerequisites","title":"Prerequisites","text":"<ul> <li>Review tutorial on Ontology Development Automation</li> </ul>"},{"location":"lesson/developing-an-obo-ontology/#preparation","title":"Preparation","text":"<p>Please complete the following and then continue with this tutorial below:</p> <ul> <li>ROBOT Mini-Tutorial, part 2</li> <li>Software Carpentry: Automation and Make</li> </ul>"},{"location":"lesson/developing-an-obo-ontology/#ontology-development","title":"Ontology Development","text":""},{"location":"lesson/developing-an-obo-ontology/#description","title":"Description","text":"<p>By the end of this session, you should be able to:</p> <ul> <li>Merge ontology modules &amp; imports with <code>robot merge</code></li> <li>Create a classified version of an ontology with <code>robot reason</code></li> <li>Add metadata to an ontology with <code>robot annotate</code></li> <li>Create a simple release workflow using ROBOT commands in a Makefile</li> <li>Create a new ontology with ODK</li> </ul>"},{"location":"lesson/developing-an-obo-ontology/#what-is-an-ontology-release","title":"What is an ontology release?","text":"<p>Like software, official OBO Foundry ontologies have versioned releases. This is important because OBO Foundry ontologies are expected to be shared and reused. Since ontologies are bound to change over time as more terms are added and refined, other developers need stable versions to point to so that there are no surprises. OBO Foundry ontologies use GitHub releases to maintain these stable copies of older versions.</p> <p>Generally, OBO Foundry ontologies maintain an \"edit\" version of their file that changes without notice and should not be used by external ontology developers because of this. The edit file is used to create releases on a (hopefully) regular basis. The released version of an OBO Foundry ontology is generally a merged and reasoned version of the edit file. This means that all modules and imports are combined into one file, and that file has the inferred class hierarchy actually asserted. It also often has some extra metadata, including a version IRI. OBO Foundry defines the requirements for version IRIs here.</p>"},{"location":"lesson/developing-an-obo-ontology/#the-release-workflow-process-should-be-stable-and-can-be-written-as-a-series-of-steps-for-example","title":"The release workflow process should be stable and can be written as a series of steps. For example:","text":"<ol> <li>Update modules from templates</li> <li>Merge ontology modules &amp; the main edit file into one</li> <li>Assert the inferred class hierarchy</li> <li>Add a version IRI &amp; other important metadata</li> </ol>"},{"location":"lesson/developing-an-obo-ontology/#this-series-of-steps-can-be-turned-into-robot-commands","title":"This series of steps can be turned into ROBOT commands:","text":"<ol> <li><code>robot template</code></li> <li><code>robot merge</code></li> <li><code>robot reason</code></li> <li><code>robot annotate</code></li> </ol> <p>Since we can turn these steps into a series of commands, we can create a <code>Makefile</code> that stores these as \"recipes\" for our ontology release!</p> <ul> <li>Review the ROBOT commands:</li> <li><code>report</code> and <code>query</code></li> <li><code>convert</code>, <code>extract</code>, and <code>template</code></li> <li><code>merge</code>, <code>reason</code>, <code>annotate</code>, and <code>diff</code></li> <li>Chaining ROBOT commands</li> <li>Specifying custom prefixes</li> <li>Review Software Carpentry course content</li> <li>Use the ODK to bootstrap a new ontology</li> <li>Introduction to the OBO Foundry Registry</li> </ul>"},{"location":"lesson/developing-an-obo-ontology/#contributors","title":"Contributors","text":"<ul> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/developing-application-ontologies/","title":"Developing an Application Ontology","text":""},{"location":"lesson/developing-application-ontologies/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/developing-application-ontologies/#prerequisites","title":"Prerequisites","text":"<ul> <li>Review tutorial on Ontology Contribution</li> </ul>"},{"location":"lesson/developing-application-ontologies/#preparation","title":"Preparation","text":"<ul> <li>TBD</li> </ul>"},{"location":"lesson/developing-application-ontologies/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: Combining ontology subsets for use in a project.</p>"},{"location":"lesson/developing-application-ontologies/#learning-objectives","title":"Learning objectives","text":"<ul> <li>manage GitHub</li> <li>manage ontology imports</li> <li>use ROBOT extract: MIREOT, SLME</li> <li>use ROBOT report</li> <li>pruning trees</li> </ul>"},{"location":"lesson/developing-application-ontologies/#tutorials","title":"Tutorials","text":"<ul> <li>in person or video (link videos here as they become available)</li> </ul>"},{"location":"lesson/developing-application-ontologies/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/developing-application-ontologies/#contributors","title":"Contributors","text":"<ul> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/entity-matching/","title":"Linking across vocabularies: Semantic Entity Matching","text":""},{"location":"lesson/entity-matching/#summary","title":"Summary","text":"<p>All across the biomedical domain, we refer to domain entities (such as chemicals or anatomical parts) using identifiers, often from controlled vocabularies.</p> <p>The decentralised evolution of scientific domains has led to to the emergence of disparate \"semantic spaces\" with different annotation practices and reference vocabularies and formalisms. </p> <p></p> <p>To bridge between these spaces, entity mappings have emerged, which link, for example, genes from HGNC to ENSEMBL, diseases between OMIM and Mondo and anatomical entities between FMA and Uberon.</p> <p>Entity matching is the process of establishing a link between an identifier in one semantic space to an identifier in another. There are many cultures of thought around entity matching, including Ontology Matching, Entity Resolution and Entity Linking.</p>"},{"location":"lesson/entity-matching/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Fundamentals</li> <li>Basic tutorials</li> <li>Introduction to entity mapping</li> <li>How are mappings collected in practice?</li> <li>A typical workflow for matching</li> <li>Examples</li> </ul>"},{"location":"lesson/entity-matching/#overview-of-the-terminology","title":"Overview of the terminology","text":"Concept Definition Semantic space A not widely used concept to denote a cluster of related data that can be interpreted using the same ontology. Ontology matching The task of determining corresponding entities across ontologies. Entity mapping Determining and documenting the correspondence of an entity in one semantic space to another. Schema mapping Determining and documenting the translation rules for converting an entity from one semantic space to another. Ontology alignment An ontology alignment is a set of term mappings that links all concepts in a source ontology to their appropriate correspondence in a target ontology, if any. Knowledge graph matching More or less the same as ontology matching - for knowledge graphs Thesaurus building Involves assigning natural language strings (synonym) to a code in a knowledge organisation system (like a taxonomy, terminology, or ontology) Named Entity Recognition and Entity Linking Involve recognising entities (such as diseases) in text and linking them to some identifier. Entity resolution/record linkage Involves determining if records from different data sources represent, in fact, the same entity Schema matching Determines if two objects from different data models (schema elements, schema instances) are semantically related. Value Set Mapping Determines and documents the correspondence of two Value Sets and their respective values (i.e. a 2-level mapping!)."},{"location":"lesson/entity-matching/#fundamentals","title":"Fundamentals","text":"<p>The excellent OpenHPI course on Knowledge Engineering with Semantic Web Technologies gives a good overview:</p> <p>Another gentle overview on Ontology Matching was taught as part of the Knowledge &amp; Data course at Vrije Universiteit Amsterdam. </p> <p></p>"},{"location":"lesson/entity-matching/#basic-tutorials","title":"Basic tutorials","text":"<ol> <li>Mapping curation with SSSOM</li> <li>Introduction to processing mappings with SSSOM and sssom-py CLI</li> <li>Introduction to matching with OAK lexmatch</li> </ol>"},{"location":"lesson/entity-matching/#introduction-to-entity-mapping","title":"Introduction to Entity Mapping","text":"<p>In the following, we consider a entity a symbol that is intended to refer to a real world entity, for example:</p> <p></p> <ul> <li>an ontology id like OMOP:441554 corresponds to the concept of \"Vascular calcification\" in the real world. Note that OMOP:441554 may be annotated with the <code>rdfs:label</code> \"Friedreichs Ataxia\".  The label itself is not necessarily a term - it could change, for example to \"Friedreichs Ataxia (disease)\", and still retain the same meaning.</li> <li>\"<code>Friedreich's Ataxia</code>\" (example on the left) may be a term in my controlled vocabulary which I understand to correspond to that respective disease (not all controlled vocabularies have IDs for their terms).  This happens for example in clinical data models that do not use formal identifiers to refer to the values of slots in their data model, like \"MARRIED\" in /datamodel/marital_status.</li> <li>Examples of entities: <ul> <li>IDs of classes in an ontology</li> <li>elements of a clinical value set</li> <li>elements of clinical terminologies such as Z63.1</li> <li>Elements of a schema</li> </ul> </li> <li>TLDR: entities are symbols that correspond to things in the world, and that correspondence is not subject to change.  Labels of such entities can change without changing the meaning of the entity.</li> </ul> <p>In our experience, there are roughly four kinds of mappings:</p> <ul> <li>string-string: Relating one string, or label, to another string, or label. Understanding such mappings is fundamental to understanding all the other kinds of mappings.</li> <li>_string-entity: Relating a specific string or \"label\" to their corresponding entity in a terminology or ontology. We usually refer to these as synonyms, but there may be other words used in this case.</li> <li>entity-entity_: Relating an entity, for example a class in an ontology, to another entity. This is what most people in the ontology domain would understand when thy hear \"ontology mappings\".</li> <li>complex mappings: Relating two sets of entities. For example <code>cheese sandwich (wikidata:Q2734068)</code> to <code>sandwich (wikidata:Q111836983)</code> and <code>cheese wikidata:Q10943</code>.  These are the rarest and most complicated kinds of mappings and are out of scope for this lesson.</li> </ul> <p>In some ways, these four kinds of mappings can be very different. We do believe, however, that there are enough important commonalities such as common features, widely overlapping use cases and overlapping toolkits to consider them together. In the following, we will discuss these in more detail, including important features of mappings and useful tools.</p>"},{"location":"lesson/entity-matching/#important-features-of-mappings","title":"Important features of mappings","text":"<p>Mappings have historically been neglected as second-class citizens in the medical terminology and ontology worlds - the metadata is insufficient to allow for precise analyses and clinical decision support, they are frequently stale and out of date, etc. The question \"Where can I find the canonical mappings between X and Y\"? is often shrugged off and developers are pointed to aggregators such as OxO or UMLS which combine manually curated mappings with automated ones causing \"mapping hairballs\".</p> <p>There are many important metadata elements to consider, but the ones that are by far the most important to consider one way or another are:</p> <ul> <li>Precision: Is the mapping exact, broad or merely closely related?</li> <li>Confidence: Do I trust the mapping? Was is done manually by an expert in my domain, or by an algorithm?</li> <li>Source version: Which version of the entity (or its corresponding ontology) was mapped? Is there a newer mapping which has a more suitable match for my entity?</li> </ul> <p>Whenever you handle mappings (either create, or re-use), make sure you are keenly aware of at least these three metrics, and capture them. You may even want to consider using a proper mapping model like the Simple Shared Standard for Ontology Mappings (SSSOM) which will make your mappings FAIR and reusable.</p>"},{"location":"lesson/entity-matching/#string-string-mappings","title":"String-string mappings","text":"<p>String-string mappings are mappings that relate two strings. The task of matching two strings is ubiquitous for example in database search fields (where a user search string needs to be mapped to some strings in a database). Most, if not all effective ontology matching techniques will employ some form of string-string matching. For example, to match simple variations of labels such as \"abnormal heart\" and \"heart abnormality\", various techniques such as Stemming and bag of words can be employed effectively. Other techniques such as edit-distance or Levenshtein can be used to quantify the similarity of two strings, which can provide useful insights into mapping candidates.</p>"},{"location":"lesson/entity-matching/#string-entity-mappings-synonyms","title":"String-entity mappings / synonyms","text":"<p>String-entity mappings relate a specific string or \"label\" to their corresponding term in a terminology or ontology. Here, we refer to these as \"synonyms\", but there may be other cases for string-entity mappings beyond synonymy.</p> <p>There are a lot of use cases for synonyms so we will name just a few here that are relevant to typical workflows of Semantic Engineers in the life sciences. </p> <p>Thesauri are reference tools for finding synonyms of terms. Modern ontologies often include very rich thesauri, with some ontologies like Mondo capturing more than 70,000 exact and 35,000 related synonyms. They can provide a huge boost to traditional NLP pipelines by providing synonyms that can be used for both Named Entity Recognition and Entity Resolution. Some insight on how, for example, Uberon was used to boost text mining can be found here.</p>"},{"location":"lesson/entity-matching/#entity-entity-mappings-ontology-mappings","title":"Entity-entity mappings / ontology mappings","text":"<p>Entity-entity mappings relate a entity (or identifier), for example a class in an ontology, to another entity, usually from another ontology or database. The entity-entity case of mappings is what most people in the ontology domain would understand when they hear \"ontology mappings\". This is also what most people understand when they here \"Entity Resolution\" in the database world - the task of determining whether, in essence, two rows in a database correspond to the same thing (as an example of a tool doing ER see deepmatcher, or py-entitymatcher). For a list standard entity matching toolkit outside the ontology sphere see here. </p>"},{"location":"lesson/entity-matching/#monarch-obo-training-tutorials","title":"Monarch OBO Training Tutorials","text":""},{"location":"lesson/entity-matching/#introduction-to-semantic-entity-matching","title":"Introduction to Semantic Entity Matching","text":""},{"location":"lesson/entity-matching/#how-are-mappings-collected-in-practice","title":"How are mappings collected in practice?","text":"<p>Mappings between terms/identifiers are typically collected in four ways:</p> <ol> <li>Automatically, using terminological matchers (of which ontology matchers one the most important category in our domain).</li> <li>Manually, by dedicated curators.</li> <li>By re-using existing mappings. Some semantic spaces have been mapped by organisations such as the Monarch Initiative or UMLS, and it is often worth re-viewing existing mappings before building ones own.</li> <li>Using a mix of automated and manual approaches, where automated approaches typically generate \"mapping candidates\" which are \"double checked\" by humans before used in production systems.</li> </ol> <p>The main trade-off for mappings is very simple: 1. Automated mappings are very error prone (not only are they hugely incomplete, they are also often faulty). 1. Human curated mappings are very costly.</p> <p>--&gt; The key for any given mapping project is to determine the highest acceptable error rate, and then distribute the workload between human and automated matching approaches. We will discuss all three ways of collecting mappings in the following.</p> <p>Aside from the main tradeoff above, there are other issues to keep in mind: - Manually curated mappings are far from perfect. Most of the cost of mapping review lies in the decision how thorough a mapping should be reviewed. For example, a human reviewer may be tasked with reviewing 1000 mappings. If the acceptable error rate is quite high, the review may simply involve the comparison of labels (see here), which may take around 20 seconds. A tireless reviewer could possibly accept or dismiss 1000 mappings just based on the label in around 6 hours. Note that this is hardly better than what most automated approaches could do nowadays. - Some use cases involve so much data that manual curation is nearly out of the question.</p>"},{"location":"lesson/entity-matching/#manual-curation-of-mappings","title":"Manual curation of mappings","text":"<p>It is important to remember that matching in its raw form should not be understood to result in semantic mappings. The process of matching, in particular lexical or fuzzy semantic matching is error prone and usually better treated as resulting in candidates for mappings. This means that when we calculate the effort of a mapping project, we should always factor in the often considerable effort required by a human to verify the correctness of a candidate mapping. There are many tools that can help with this process, for example by filtering out conflicting lower-confidence mappings, but in the end the reality is this: due to the fact that source and target do not share the same semantics, mappings will always be a bit wobbly. There are two important kinds of review which are very different:</p> <ol> <li>False positive review (wrong mappings). The goal here is to identify mappings that do not work for our use case, or do not use the correct semantic mapping relation. For example <code>orange juice [wikidata:Q219059]</code> and <code>orange juice (unpasteurized) [FOODON:00001277]</code> may not be considered as the same thing in the sense of <code>skos:exactMatch</code>. </li> <li>False negative review (missing mappings). The goal here is to understand if we can do anything to improve the matching process by tuning it. For very simple matchers like <code>oak lexmatch</code> this usually involves hacking labels and synonyms by removing or replacing words. More sophisticated matchers like Agreement Maker Light (AML) have many more tuning options, and it requires patience and expertise to find the right ones. One good approach here is to include semantically or lexically similar matches in the results, and review if generally consistent patterns of lexical variation can be spotted.  For example: <code>orange juice (liquid) [FOODON:00001001]</code> seems to be exactly what <code>orange juice [wikidata:Q219059]</code> is supposed to mean. The labels are not the same, but lexically similar: a simple lexical distance metric like Levenshtein could have been used to identify these.</li> </ol> <p>Tip: always keep a clear visible list of unmapped classes around to sanity check how good your mapping has been so far.</p>"},{"location":"lesson/entity-matching/#automated-matching","title":"Automated matching","text":"<p>There are many (many) tools out there that have been developed for entity matching. A great overview can be found in Euzenats Ontology Matching. Most of the matchers apply a mix of lexical and semantic approaches.</p> <p>As a first pass, we usually rely on a heuristic that an exact match on the label is strong evidence that the two entities correspond to the same thing. Obviously, this cannot always be the case <code>Apple</code> (the fruit) and <code>Apple</code> (the company) are two entirely different things, yet a simple matching tool (like OAK <code>lexmatch</code>) would return these as matching. The reason why this heuristic works in practice is because we usually match between already strongly related semantic spaces, such as two gene databases, two fruit ontologies or two disease terminologies. When the context is narrow, lexical heuristics have a much lower chance to generate excessively noisy mappings.</p> <p>After lexical matchings are created, other techniques can be employed, including syntactic similarity (match all entities which have labels that are more than 80% similar and end with <code>disease</code>) and semantic similarity (match all entities whose node(+graph)-embedding have a cosine similarity of more than 80%). Automated matching typically results in a large number of false positives that need to be filtered out using more sophisiticated approaches for mapping reconciliation.</p> <p></p>"},{"location":"lesson/entity-matching/#a-typical-workflow-for-matching","title":"A typical workflow for matching","text":"<ol> <li>Collecting existing mappings from external sources, if they exist.</li> <li>Generating mapping candidates using automated matchers (seed mappings).</li> <li>Iteratively refining mappings while keeping track of:<ul> <li>Anchor mappings (known correct mappings)</li> <li>Overestimation mappings (mapping candidates)</li> <li>Negative mappings (known false mappings, as deterimed by tools or human review)</li> </ul> </li> </ol> <p>The refinement step may involve automated approaches that are sensitive to the logical content of the sources involved (for example by ensuring that the result does not result in equivalence cliques, or unsatisfiable classes), but more often than not, human curators are employed to curate the mapping candidates generated by the various automated approaches.</p> <p></p>"},{"location":"lesson/entity-matching/#some-examples-of-domain-specific-mapping-of-importance-to-the-biomedical-domain","title":"Some examples of domain-specific mapping of importance to the biomedical domain","text":""},{"location":"lesson/entity-matching/#phenotype-ontology-mappings","title":"Phenotype ontology mappings","text":"<p>Mapping phenotypes across species holds great promise for leveraging the knowledge generated by Model Organism Database communities (MODs) for understanding human disease. There is a lot of work happening at the moment (2021) to provide standard mappings between species specific phenotype ontologies to drive translational research (example). Tools such as Exomiser leverage such mappings to perform clinical diagnostic tasks such as variant prioritisation. Another app you can try out that leverages cross-species mappings is the Monarch Initiatives Phenotype Profile Search.</p>"},{"location":"lesson/entity-matching/#disease-ontology-mappings","title":"Disease ontology mappings","text":"<p>Medical terminology and ontology mapping is a huge deal in medical informatics (example). Mondo is a particularly rich source of well provenanced disease ontology mappings.</p>"},{"location":"lesson/entity-matching/#further-reading","title":"Further reading","text":"<ul> <li>A great overview can be found in \"Tackling the challenges of matching biomedical ontologies\" (Faria et al 2018)</li> <li>A yearly competition of ontology matching systems is held by the Ontology Alignment Evaluation Initiative (OAEI). The challenge results are a useful guide to identifying systems for matching you may want to try.</li> <li>The most comprehensive work for anyone serious about entity matching is Euzenats Ontology Matching.</li> </ul>"},{"location":"lesson/getting-hands-on/","title":"Getting Hands on with Ontologies","text":""},{"location":"lesson/getting-hands-on/#prerequisites","title":"Prerequisites","text":"<p>Sign up for a free GitHub account</p>"},{"location":"lesson/getting-hands-on/#preparation","title":"Preparation","text":"<p>No advance preparation is necessary.</p> <p>Optional: If you are unfamiliar with ontologies, this introduction to ontologies explanation may be helpful.</p>"},{"location":"lesson/getting-hands-on/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: The purpose of this lesson is to train biomedical researchers on how to find a term, what to do if they find too many terms, how to decide on which term to use, and what to do if no term is found.</p>"},{"location":"lesson/getting-hands-on/#learning-objectives","title":"Learning objectives","text":"<ul> <li>Understand open source ontology community development</li> <li>Identify the best ontology or ontologies for annotation of biomedical data</li> <li>Find ontology terms and chose the best ontology term</li> <li>Make new term requests</li> </ul>"},{"location":"lesson/getting-hands-on/#lesson","title":"Lesson","text":""},{"location":"lesson/getting-hands-on/#introduction-to-community-based-ontology-development","title":"Introduction to Community Based Ontology Development","text":"<p>This how to guide on How to be an Open Science Engineer - maximizing impact for a better world has a lot of details about the philosophy behind open science ontology engineering. Some key points are summarized below.</p> <ul> <li>The ontologies that I will promote in this lesson are part of the OBO Foundry, a community of ontology developers that are committed to developing open, freely available, community ontologies under a share set of guiding principles, that ensure the ontologies are interoperable, scientifically accurate, amongst other qualities.</li> <li>A key feature of the success of our ontologies is the community who contributes to them. (I claim they are successful because they are widely used in biomedical curation, databases, bioinformatics and computational analyses.)</li> <li>Principle of Collaboration:<ul> <li>These ontologies rely on community contributions and many are not funded. We value your expertise in making new term requests, requesting changes or commenting on existing tickets.</li> <li>The OBO Academy has a lot of self-paced learning material, if you are inclined to learn how to contribute directly to ontologies.</li> <li>Join Slack channels and email lists.</li> <li>Reduce work for others as much as possible by communicating clearly.</li> <li>Be positive and generous with gratitude and attribution.</li> <li>Promote truly open communication: Create public tickets which can be searched and referred to later.</li> </ul> </li> <li>Principle of Upstream Fixing<ul> <li>Report an bugs or errors to the source ontology.</li> <li>Make term requests for any missing terms.</li> </ul> </li> <li>Principle of No Ownership:<ul> <li>Ensure that you see your issues through to the end.</li> <li>Feel free to nudge ontology curators.</li> </ul> </li> </ul>"},{"location":"lesson/getting-hands-on/#where-to-find-ontology-terms-and-how-to-chose-the-right-terms","title":"Where to find ontology terms and how to chose the right terms","text":"<p>See lesson on Using Ontologies and Ontology Terms</p>"},{"location":"lesson/getting-hands-on/#how-to-make-new-term-requests","title":"How to make new term requests","text":"<p>See How to guide on Make term requests to existing ontologies</p>"},{"location":"lesson/getting-hands-on/#exercise","title":"Exercise","text":"<ol> <li>Sign up for a GitHub account if you do not already have one. (Some of the exercises can be done without being signed into GitHub).</li> <li>Search for an ontology term in OLS: <ul> <li>Search for 'exhaust exposure'. Note how many results you get. Do you have a sense of how to select the appropriate term for your annotations?</li> </ul> </li> <li>Make a new term request for a missing term:<ul> <li>Suppose you want a more general term than already exists: exhuast exposure. Using the instructions in How to guide on Make term requests to existing ontologies, make a new term request to an appropriate GitHub repository for an ontology, such as ECTO. (Note, since this is just for practice, please do not submit the ticket.)</li> </ul> </li> </ol>"},{"location":"lesson/getting-hands-on/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> </ul>"},{"location":"lesson/hackathon/","title":"OBO Academy Hackathon","text":""},{"location":"lesson/hackathon/#goalslearning-outcomes","title":"Goals/Learning outcomes","text":"<ul> <li>Strengthen social workflows in creating Pull Requests (PRs), including:<ul> <li>creating a PR</li> <li>writing a description of the PR</li> <li>Reviewing a PR</li> <li>Adding suggestions using suggestion mode</li> </ul> </li> <li>Foster a sense of shared efforts in open science </li> </ul>"},{"location":"lesson/hackathon/#prerequisites","title":"Prerequisites","text":"<p>Pull Requests lesson</p>"},{"location":"lesson/hackathon/#outline","title":"Outline","text":"<ol> <li>(Nico) Show structure of OBOOK</li> <li>Naming conventions of How To guides</li> <li>How to create a new how to guide</li> <li>(Nicole) Present the issues: OBOOK need volunteer</li> <li>(Students) Use Slack to find a buddy to review your PRs during this Hackathon</li> <li>Work through issues together</li> </ol>"},{"location":"lesson/hackathon/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/licensing-data-and-software/","title":"Licensing Data and Software","text":""},{"location":"lesson/licensing-data-and-software/#learning-objectives","title":"Learning objectives","text":"<p>Develop context in order to make informed licensing decisions for data and software.</p>"},{"location":"lesson/licensing-data-and-software/#prerequisites","title":"Prerequisites","text":"<p>None.</p>"},{"location":"lesson/licensing-data-and-software/#preparation","title":"Preparation","text":"<p>None.</p>"},{"location":"lesson/licensing-data-and-software/#description","title":"Description","text":"<p>The slide deck for this presentation is currently here.</p>"},{"location":"lesson/licensing-data-and-software/#further-reading-and-resources","title":"Further reading and resources","text":"<p>The Debian Free Software Guidelines https://wiki.debian.org/DebianFreeSoftwareGuidelines  https://wiki.debian.org/DFSGLicenses </p> <p>The Free Software Foundation (v/different from Debian) https://www.fsf.org/licensing/  https://www.gnu.org/licenses/license-list.html </p> <p>Creative Commons https://creativecommons.org/share-your-work/  https://creativecommons.org/share-your-work/cclicenses/ </p> <p>(Re)usable Data Project (shameless plug) https://reusabledata.org https://doi.org/10.1371/journal.pone.0213090</p> <p>Fair use cases https://fairuse.stanford.edu/overview/fair-use/cases/ </p>"},{"location":"lesson/licensing-data-and-software/#contributors","title":"Contributors","text":"<ul> <li>Seth Carbon</li> <li>(Re)usable Data Project</li> </ul>"},{"location":"lesson/linkml-helps-with-ontology-use/","title":"Using Ontologies in Practice - LinkML can help!","text":""},{"location":"lesson/linkml-helps-with-ontology-use/#what-are-ontologies-anyway","title":"What are ontologies, anyway?","text":"<p>Most researchers have heard of ontologies, but that may be where their familiarity ends. What exactly are ontologies, and how are they used in practice? What do you gain by using ontologies? Do you have to be an ontologist to use ontologies?? Read on to find out!</p> <p>Researchers often ask these sorts of questions:</p> <p></p> <p>An ontology is a sophisticated, organized hierarchical controlled vocabulary that categorizes and defines relationships between concepts. Ontologies are used to organize and share data in many fields of science and beyond; they are a valuable tool for making datasets generated in the field or at the bench usable and reproducible by others.. Ontologies provide the following key features:</p> <ul> <li>establishes a shared vocabulary with precisely defined terms and concepts. </li> <li>categorizes concepts in a hierarchical manner, making it easier to navigate and find relevant information.</li> <li>promotes data from a collection of isolated values to a component of a broader network of related concepts,</li> <li>bringing together results from different sources and different fields, enabling more sophisticated queries and analyses.</li> </ul>"},{"location":"lesson/linkml-helps-with-ontology-use/#how-are-ontologies-used-to-model-data","title":"How are ontologies used to model data?","text":"<p>Scientific data usually has a data model (schema) -- a blueprint that describes how the data is organized and the relationships between the different elements within a database. Data models can also describe how and when an ontology should be used in defining a row or data element. </p> <p>Let\u2019s look at an example of three different datasets from three different researchers all concerned with the microbial makeup of water samples in Oregon to illustrate how to use ontologies in data models (schemas) and make it possible to meaningfully combine data from different sources. </p> <p>For our example, let\u2019s say that Researcher A is taking samples from Lake Albert, Researcher B is taking samples from the Pacific Ocean, and Researcher C is taking samples from Crater Lake. Each researcher wants to record the depth where their sample was collected, and the types of bacteria they find in each sample. Researcher D is a data scientist who wants to use each of the three resulting datasets to answer some specific questions about bacterial compositions of water samples in Oregon.</p> <p></p> <p>Researchers A, B, and C collect their data using spreadsheets. They make up labels for each column in their spreadsheets, record their data, and summarize their findings in a lab notebook. </p> <p>To combine these three datasets and make use of their content together, Researcher D needs to identify and compare corresponding rows (this is sometimes called normalizing the data). The first thing Researcher D has to do is to convert information from the titles of the spreadsheets into data about each sample (in this case, whether the source of the sample is a lake or an ocean, because we know that lakes and oceans have very different bacterial compositions). </p> <p></p>"},{"location":"lesson/linkml-helps-with-ontology-use/#ontologies-provide-standardized-ways-to-describe-data","title":"Ontologies provide standardized ways to describe data","text":"<p>As she looks at the titles of the spreadsheets, Researcher D notes that while Lake Albert and Crater Lake are both lakes, they have very different salinity levels. Lake Albert is a saltwater lake, while Crater Lake is a freshwater lake. Knowing that she wants to group the data resulting from these three studies first by salinity and then by depth, Researcher D realizes that she might need even more attributes, beyond salinity, to describe the site these samples were taken from. Luckily, her colleague introduces her to the Environment Ontology (ENVO). ENVO offers a range of terms that scientists can use to describe environmental elements, such as ecosystems, habitats, and geographical features. By selecting the most specific ENVO term that describes the \u201ctype\u201d column in each row of her three datasets, Researcher D gives herself the ability to filter her data for samples from similar environments. Importantly, by using the relationships in the ontology, Researcher D can change the filters for samples to group similar samples together in a variety of ways (e.g. all the water samples, only the freshwater samples, only the lake samples, etc.). This standardization makes it possible to combine the different datasets generated by Researchers A, B and C, which opens the door to using more powerful search and analysis approaches.</p> <p>The figure below shows a portion of the ENVO ontology that describes types of bodies of water. The top-level term, \u201cwater body\u201d, has child classes \u201cmarine water body\u201d (the Pacific Ocean, for example, would fall into that class) and \u201clake\u201d. One type of lake is \u201csaline lake\u201d (there are also other types of lakes, including \u201cfreshwater lake\u201d, that are not shown in this small subset of ENVO).</p> <p></p>"},{"location":"lesson/linkml-helps-with-ontology-use/#ontologies-harmonize-datasets","title":"Ontologies harmonize datasets","text":"<p>Combining datasets from different sources is a lot more complicated if they don\u2019t represent information in the same way. After annotating all the data she has with ENVO terms, Researcher D notices that in the different spreadsheets, \u2018depth\u2019 is described three different ways. Each dataset (spreadsheet) represents an implicit model of the attributes of the environment where the sample was collected. By implicit model, we mean that by manually examining the spreadsheet, looking at its name, its column values and understanding where this data came from, we can make some educated assumptions about how to map the data to another, unrelated dataset. However, such assumptions can be flawed and are usually made by a person using the data outside the group that generated it. This process is also time consuming and often error prone. The larger the unconstrained dataset, the more likely it will be that a particular row or value of a cell will not match the assumptions the downstream data consumer has made about the implicit model. Moreover, when the dataset is used as input to an analysis tool, there\u2019s no human to guide the data interpretation. </p> <p>In order to make their data interpretable and usable by other researchers, software tools, and even their future selves, data creators (researchers) should be explicit about the data they generate. They should write definitions of column headers. They should include relevant information about the data that applies to every row (or object) in the dataset (in our example, they should include metadata about the sample locations in the dataset itself so Researcher D doesn\u2019t have to make assumptions about the lakes). They should specify when they expect cells in their dataset to be empty or possibly empty. They should specify when a cell can have more than one value, and if the multiple-valued cells are delimited by a specific character. They should define data types for each column (should the values be strings, integers, should they all have units, what are those units, etc.). </p> <p>In our example, because there is no structured way to enter or interpret the data in each column (and no documentation to help Researcher D understand each column and what data to expect in that column), there is no easy way for Researcher D to combine the data without going through each row manually to standardize it. This manual standardization step isn\u2019t a big deal when you only have three tiny spreadsheets as in our example, but imagine having to do that with dozens of spreadsheets, each with hundreds or even thousands of lines--that would be no fun at all!</p> <p></p> <p>Furthermore, because there isn\u2019t an explicit definition of each column in a spreadsheet (e.g. we expect integer values in centimeters in the depth column, we expect ENVO terms in the type column), different datasets could become uncombinable even within the same group or lab. For example, in the figure above, the \u201cdepth\u201d box shows different ways that sample depth might be recorded (cm, inches, feet), without the use of a schema to standardize and validate the entries.</p> <p>While ontologies provide the knowledge of the domain of interest, data models or schemas provide the blueprint for a domain or experiment or collection of individuals that share common characteristics or attributes that are programmatically validatable. We use ontologies in a data model to categorize different dimensions of our individual in a consistent way. We use data models to communicate our programmatic understanding of our data attributes and relationships  between different types of data. </p> <p>In a single data model representing a complex dataset, more than one ontology can be used to limit the values of a column in the dataset. For instance, the biological concept of \"Expression\" can be divided into various ontological domains and controlled vocabularies within a data model (or dataset).:</p> <ul> <li>An ontology of stages (when expressed) </li> <li>An ontology of anatomical entities (where expressed): e.g. Uber Anatomy Ontology (Uberon)</li> <li>An ontology of methods or assays (how we assessed expression), e.g. Materials and Methods Ontology (MMO)</li> <li>A controlled vocabulary of transcripts or genes (which gene or transcript was expressing), e.g. NCBIGene/Ensembl Gene Identifiers. </li> </ul>"},{"location":"lesson/linkml-helps-with-ontology-use/#linkml-is-a-shared-framework-for-data-modeling","title":"LinkML is a shared framework for data modeling","text":"<p>LinkML (which stands for \u201cLinked data Modeling Language\u201d) is an open, extensible modeling framework that allows computers and people to fully describe their data and work cooperatively to model, validate, and distribute data that is reusable and interoperable. As a flexible yet expressive standard for describing data models ranging from value sets and flat, checklist-style standards to complex normalized data structures that use polymorphism and inheritance, LinkML enables even non-developers to create data models, helping to bridge the gap between people of diverse expertise and enabling a shared language with which to express the critically important blueprint of an organization\u2019s data collection. </p> <p>LinkML uses ontologies in defining a schema as well as in constraining and validating data. LinkML is best illustrated through examples. Our data model for the Pacific Ocean Sample Dataset has implicit assumptions: for example, what values are allowed in the depth column (should they all be centimeter measurements? Should each cell include the units? Can the bacteria column include more than one thing? Is the comma a delimiter between different bacteria identifiers, or do some bacteria have commas in their name?, etc.) Below, we show how this spreadsheet data model would look if we used LinkML, with its explicit syntax, to describe it:</p> <p></p>"},{"location":"lesson/linkml-helps-with-ontology-use/#linkml-lets-you-describe-classes-instances-and-attributes","title":"LinkML lets you describe classes, instances and attributes","text":"<p>Each row of our spreadsheets has elements (columns) that describe the defining characteristics of each sample (in our case, its depth, salinity, bacterial composition, and sample type). In LinkML schemas, sets of elements are grouped together to make \u201cclasses.\u201d In the figure above, for example, \u201cBiosample\u201d is a LinkML class that we\u2019ve created to hold all the shared elements that make up each sample row in our spreadsheet. Conversely, each row of our spreadsheet is an instance of a Biosample class in our schema. </p> <p>In LinkML we can add a description to every class and to every element in the class with the \u201cdescription\u201d keyword. We can specify if an element can hold a range of values (string, integer, etc.) and whether we can expect one or many values in each \u201crow\u201d (or instance) of this class. </p> <p>If we want to constrain the acceptable values of a column--for example, restrict the \u201csample_type\u201d column to terms from ENVO--we can do that using a LinkML \u201cdynamic enumeration\u201d (specified above with the \u201cenums\u201d keyword). A dynamic enumeration is a set of values that specifies the acceptable inputs for this column. The \u201creachable from\u201d keyword in our enumeration definition tells software applications to expand the content of this enumeration to all the possible terms in ENVO. This means that we can write software that returns a validation error if a value in this column is not from ENVO. </p> <p>Additionally, LinkML can describe attributes using ontologies directly. For example, if we wanted to reuse the term \u201cdepth\u201d from ENVO in our model directly, we could specify that in our LinkML model using the <code>slot_uri</code> component (\u201cENVO:3100031\u201d in our example above represents a sample depth). Using this component, we are telling a computer or another researcher exactly what the definition of depth is in our dataset, and we are reusing that concept from an external source (in this case ENVO). This means less work for us as we are collecting the data, and less work for our collaborators to understand our data later. </p>"},{"location":"lesson/linkml-helps-with-ontology-use/#linkml-helps-you-make-implicit-assumptions-explicit","title":"LinkML helps you make implicit assumptions explicit","text":"<p>In our data model or schema, we should take care to make implicit assumptions that we\u2019ve made in organizing our dataset explicit using a language that is computable. We should reuse the work of ontologies to define classes if we want to use a specific concept in our schema but also add attributes/metadata to this concept. We should map our definition of a concept to the same concept in an external model (in this case it could be an ontology, in other cases it could be another LinkML Class). If we need to constrain a particular column/slot/value to use a specific ontology (e.g. the ENVO use case above), then we can use dynamic enumerations. </p>"},{"location":"lesson/linkml-helps-with-ontology-use/#schemas-and-ontologies-better-together","title":"Schemas and ontologies: better together","text":"<p>Utilizing ontologies involves structuring knowledge into defined terms and concepts, enabling efficient communication. Ontologies offer key functionalities: establishing shared vocabulary, hierarchical categorization, and enabling advanced data connections. Data models, or schemas, further complement ontologies by providing a standardized framework for labeling individual data points. This collaborative relationship between ontologies and data models transforms complex information into a well-organized and accessible resource across scientific domains.</p>"},{"location":"lesson/linkml-helps-with-ontology-use/#further-learning-materials","title":"Further learning materials","text":"<p>To understand the LinkML syntax and semantics in more depth, please check out the LinkML tutorial, and in particular, the guide for using ontologies in your LinkML schema, which includes many more examples:</p> <ul> <li>The LinkML tutorial: https://linkml.io/linkml/intro/tutorial.html</li> <li>Ontologies and LinkML in depth: https://linkml.io/linkml/faq/general.html#ontologies-as-values</li> </ul> <p>For more information on how other researchers have used ontologies, please see the main ontology-user pathway in the OBOOK (Open Biological and Biomedical Ontologies Organized Knowledge):</p> <ul> <li>https://oboacademy.github.io/obook/pathways/ontology-user/</li> </ul> <p>Tutorial (ICBO 2025): </p> <ul> <li>Annotating Data with Ontologies: LinkML Can Help (slides; video, 2hr 25min)</li> </ul>"},{"location":"lesson/linkml-helps-with-ontology-use/#contributors","title":"Contributors","text":"<ul> <li>Lead author: Sierra Moxon</li> <li>Editor: Nomi Harris</li> <li>PI: Chris Mungall</li> <li>Images: adapted from figures by Julie McMurray</li> </ul>"},{"location":"lesson/modelling-with-object-properties/","title":"Modeling with Object Properties","text":"<p>In this lesson, we will give an intuition of how to work with <code>object properties</code> in OBO ontologies, also referred to as \"relations\".</p> <p>We will cover, in particular, the following subjects:</p> <ol> <li>What is the role of object properties in OBO ontologies, and how should we model them?</li> <li>What is the relation ontology (RO), and how do we add object properties to it?</li> </ol> <p></p>"},{"location":"lesson/modelling-with-object-properties/#preparation","title":"Preparation","text":"<p>We have worked with the University of Manchester to incorporate the Family History Knowledge Base Tutorial fully into OBO Academy.</p> <p>This is it: OBOAcademy: Family History - Modelling with Object Properties.</p> <p>In contrast to the Pizza tutorial, the Family history tutorial focuses on modelling with individuals. Chapters 4, 5, 8 and 9 are full of object property modelling, and are not only great to get a basic understanding of using them in your ontology, but also give good hints at where OWL and object properties fall short. We refer to the FHKB in the following and expect you to have completed at least chapter 5 before reading on.</p>"},{"location":"lesson/modelling-with-object-properties/#the-role-of-object-properties-in-the-obo-sphere","title":"The Role of Object Properties in the OBO-sphere","text":"<p>To remind ourselves, there are three different types of relations in OWL:</p> <ol> <li>Data properties (DatatypeProperty) connect your classes and individuals to data values, such as strings or numbers. In OBO, these are the least frequently used kinds of properties, used for example by CIDO and ONS.</li> </ol> <p>For some example usage, run the following query in the ontobee OLS endpoint:</p> <p>http://www.ontobee.org/sparql</p> <pre><code>prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT distinct *\nWHERE {\nGRAPH ?graph_uri\n{ ?dp rdf:type owl:DatatypeProperty .\n  ?sub ?dp ?obj }\n}\n</code></pre> <p>Note that many uses of data properties across OBO are a bit questionable, for example, you do never want to attach a modification dates or similar to your classes using data properties, as these fall under OWL semantics. This means that logically, if a superclass has a relation using a DatatypeProperty, then this relation _holds for all subclasses of that class as well.</p> <ol> <li> <p>Annotation properties are similar to data properties, but they are outside of OWL semantics, i.e. OWL reasoners and reasoning do not care, in fact ignore, anything related to annotation properties. This makes them suitable for attaching metadata like labels etc to our classes and properties. We sometimes use annotation properties even to describe relationships between classes if we want reasoners to ignore them. The most typical example is IAO:replaced_by, which connects an obsolete term with its replacement. Widely used annotation properties in the OBO-sphere are standardised in the OBO Metadata Ontology (OMO).</p> </li> <li> <p>The main type of relation we use in OBO Foundry are object properties. Object properties relate two individuals or classes with each other, for example:</p> </li> </ol> <pre><code>OWLObjectPropertyAssertion(:part_of, :heart, :cardiovascular_system)\n</code></pre> <p>In the same way as annotation properties are maintained in OMO (see above), object properties are maintained in the Relation Ontology (RO).</p> <p>Object properties are of central importance to all ontological modelling in the OBO sphere, and understanding their semantics is critical for any put the most trivial ontologies. We assume the reader to have completed the Family History Tutorial mentioned above.</p> <p></p>"},{"location":"lesson/modelling-with-object-properties/#object-property-semantics-in-obo","title":"Object property semantics in OBO","text":"<p>In our experience, these are the most widely used characteristics we specify about object properties (OP):</p> <ol> <li>Sub-property: if an OP is a sub-property of another parent OP, it inherits all its semantic characteristics. Most importantly: if OP1 is a sub-property of OP2, then, if (a)--[OP1]--&gt;(b), we infer that (a)--[OP2]--&gt;(b).</li> <li>Domain: if OP has a domain C, it means that every time (a)--[OP]--&gt;(b), (a) must be a C. For example, <code>ecologically co-occurs with</code> in RO has the domain <code>'organism or virus or viroid'</code>, which means that whenever anything <code>ecologically co-occurs with</code> something else, it will be inferred to be a <code>'organism or virus or viroid'</code>.</li> <li>Range: if OP has a range C, it means that every time (a)--[OP]--&gt;(b), (b) must be a C. For example <code>produced by</code> has the domain <code>material entity</code>. Note that in ontologies, ranges are slightly less powerful then domains: If we have a class <code>Moderna Vaccine</code> which is SubClass of <code>'produced by' some 'Moderna'</code> we get that <code>Moderna Vaccine</code> is a <code>material entity</code> due to the domain constraint, but NOT that <code>Moderna</code> is a <code>material entity</code> due to the range constraint (explanation to this is a bit complicated, sorry).</li> <li>Transitivity: if an OP is transitive, it means that if (a)--[OP]--&gt;(b)--[OP]--&gt;(c), (a)--[OP]--&gt;(c). For example, if the eye is part of the head, which is part of the body, we can infer that the eye must be part of the body.</li> <li>Property chains: Similar to transitive properties, property chains allow us to bridge across multiple properties. The FHKB tutorial above is all about amazing property chains so you should have a deep understanding of these if you followed the tutorial.</li> </ol> <p>Other characteristics like functionality and symmetry are used across OBO ontologies, but not nearly to the same extend as the 5 described above.</p>"},{"location":"lesson/modelling-with-object-properties/#the-relation-ontology-ro","title":"The Relation Ontology (RO)","text":"<p>The Relation Ontology serves two main purposes in the OBO world:</p> <ol> <li>As a place to standardise object properties. The idea is this: many ontologies are modelling mereological relations, such as partonomies, which requires relationships such as \"part of\" and \"has part\". To ensure that ontologies are interoperable, we need to make sure that all ontologies use the same \"part of\" relationship. Historically this is not always been true, and still is not. At the time of this writing, running:</li> </ol> <pre><code>prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT distinct ?graph_uri ?s\nWHERE {\nGRAPH ?graph_uri\n{ ?s rdf:type owl:ObjectProperty ;\n   rdfs:label \"part of\" . }\n}\n</code></pre> <p>On the OntoBee SPARQL endpoint still reveals a number of ontologies using non-standard part-of relations. In our experience, most of these are accidental due to past format conversions, but not all. This problem was much worse before RO came along, and our goal is to unify the representation of key properties like \"part of\" across all OBO ontologies. The OBO Dashboard checks for object properties that are not aligned with RO.</p> <ol> <li>As a place to encode and negotiate object property semantics. Object properties (OP) can have domains and ranges, can have characteristics such as functionality and transitivity, see above. Arguing the exact semantics of an OP can be a difficult and lengthy collaborative process, esp. since OP semantics can have a huge impact on ontology reasoning. Detailed RO documentation (modelling patterns and practices) can be found in here. The process of how relationships are added to RO is discussed in the next section.</li> </ol>"},{"location":"lesson/modelling-with-object-properties/#adding-relationships-to-ro","title":"Adding relationships to RO","text":"<p>To add a relationship we usually follow the following process. For details, please refer to the RO documentation.</p> <ol> <li>Check whether the OP is already in RO. Search for synonyms - often the relationship you are looking exist, but under a different name. If you cant find the exact OP, see whether you can find similar OPs - this may help you also identify suitable parent OPs.</li> <li>Make an RO issue. Take care to not only describe the name of your relationship, but also intended application areas with examples, a good definition, potential parent relationships, domains and ranges. The more detail you provide, the easier it will be for the community to review your request.</li> <li>Make a pull request. This involves the same steps as usual. If you are unsure what annotations need to be added and how to reflect the intended semantics, it may be useful to look at past pull requests.</li> <li>Join our quarterly RO calls and check out the RO documentation.</li> </ol>"},{"location":"lesson/ontology-design/","title":"Ontology Design","text":""},{"location":"lesson/ontology-design/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/ontology-design/#prerequisites","title":"Prerequisites","text":"<p>Participants will need to have access to the following resources and tools prior to the training:</p> <ul> <li>GitHub account - register for a free GitHub account here</li> <li>Protege - Install Protege 5.5, download it here</li> <li>Install ELK 0.5</li> <li>Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop</li> </ul>"},{"location":"lesson/ontology-design/#preparation","title":"Preparation","text":"<ul> <li>Review tutorial on Ontology Term Use</li> <li>Review tutorial on Contributing to OBO Ontologies</li> <li>Clone Mondo repo: Follow these instructions to clone the Mondo repo</li> </ul>"},{"location":"lesson/ontology-design/#what-is-delivered-as-part-of-this-course","title":"What is delivered as part of this course","text":"<p>Description: This course will cover reasoning with OWL.</p>"},{"location":"lesson/ontology-design/#learning-objectives","title":"Learning objectives","text":"<p>At the end of this lesson, you should know how to do:</p> <ol> <li>Add existential restrictions</li> <li>Add defined classes</li> <li>Add disjoint axioms</li> <li>Debug unsatisfiable classes</li> </ol>"},{"location":"lesson/ontology-design/#tutorials","title":"Tutorials","text":"<p>OpenHPI Course Content</p> <ol> <li>Ontologies and Logic Videos 3.0-3.10 | Duration: ~3.5 hrs</li> <li>OWL, Rules, and Reasoning Videos 4.0-4.8 | Duration: ~2.7 hrs</li> </ol>"},{"location":"lesson/ontology-design/#additional-materials-and-resources","title":"Additional Materials and Resources","text":"<ul> <li>Monkeying around with OWL: Musings on building and using ontologies, posts by Chris Mungall</li> <li>Documentation on Cell Ontology relations</li> <li>Guidelines for writing definitions in Ontologies (paper)</li> <li>How to deal with unintentional equivalent classes: Cool post by Chris Mungall on how to deal with an important reasoning issue.</li> </ul>"},{"location":"lesson/ontology-design/#semantic-engineer-toolbox","title":"Semantic Engineer Toolbox","text":"<ul> <li>Protege</li> <li>ELK Protege Plugin</li> <li>GitHub Desktop</li> </ul>"},{"location":"lesson/ontology-design/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/ontology-design/#acknowledgement","title":"Acknowledgement","text":"<ul> <li>Content was adapted from Ontology 101 Tutorial</li> </ul>"},{"location":"lesson/ontology-design/#owl-class-restrictions","title":"OWL class restrictions","text":"<p>In OWL, we use object properties to describe binary relationships between two individuals (or instances). We can also use the properties to describe new classes (or sets of individuals) using restrictions. A restriction describes a class of individuals based on the relationships that members of the class participate in. In other words, a restriction is a kind of class, in the same way that a named class is a kind of class.</p> <p>For example, we can use a named class to capture all the individuals that are idiopathic diseases. But we could also describe the class of idiopathic disease as all the instances that are 'has modifier' idiopathic disease.</p> <p>In OWL, there are three main types of restrictions that can be placed on classes. These are quantifier restriction, cardinality restrictions, and hasValue restriction. In this tutorial, we will initially focus on quantifier restrictions.</p> <p>Quantifier restrictions are further categorized into two types, the existential and the universal restriction.</p> <ul> <li>Existential restrictions describe classes of individuals that participate in at least one relationship along a specified property to individuals that are members of a specified class. For example, the class of individuals that have at least one ( some ) 'has modifier' relationship to members of the <code>idiopathic disease</code> class. In Protege, the keyword 'some' is used to denote existential restrictions.</li> <li>Universal restrictions describe classes of individuals that for a given property only have relationships along this property to individuals that are members of a specified class. For example, we can say a cellular component is capable of many functions using the existential quantifier, however, OWL semantics assume that there could be more. We can use the universal quantifier to add closure to the existential. That is, we can assert that a cellular component is capable of these functions, and is only capable of those functions and no other. Another example is that the process of hair growth is found only in instances of the class Mammalia. In Protege the keyword 'only' is used.</li> </ul> <p>In this tutorial, we will deal exclusively with the existential (some) quantifier.</p>"},{"location":"lesson/ontology-design/#superclass-restrictions","title":"Superclass restrictions","text":"<p>Strictly speaking in OWL, you don't make relationships between classes, however, using OWL restrictions we essentially achieve the same thing.</p> <p>We wanted to capture the knowledge that the named class 'idiopathic achalasia' is an idiopathic disease. In OWL speak, we want to say that every instance of an ' idiopathic achalasia' is also an instance of the class of things that have at least one 'has modifier' relationship to an idiopathic disease. In OWL, we do this by creating an existential restriction on the idiopathic achalasia class.</p> <ol> <li>In the Entities tab, select 'idiopathic achalasia' in the class hierarchy and look at its current class description in the bottom right box.</li> <li>Note that there are two superclasses (as denoted by the SubClass Of list). ''gastroesophageal disease'' and 'has modifier' some idiopathic.</li> <li>Run the reasoner.</li> <li>You should see that this class is now inferred to be an idiopathic disease because of this SubClassOf (superclass) restriction.</li> </ol>"},{"location":"lesson/ontology-design/#equivalence-axioms-and-automatic-classification","title":"Equivalence Axioms and Automatic classification","text":"<p>This example introduces equivalence axioms or defined classes (also called logical definitions) and automatic classification.</p> <p>The example involves classification of Mendelian diseases that have a monogenic (single gene) varation. These equivalence axioms are based off the Mondo Design Pattern disease_series_by_gene.</p> <p>Constructs:</p> <ul> <li>and (intersection)</li> <li>equivalence (logical definitions)</li> <li>existential restrictions (e.g. 'disease has basis in dysfunction of')</li> </ul>"},{"location":"lesson/ontology-design/#add-an-equivalence-axiom-to-an-existing-mondo-term","title":"Add an equivalence axiom to an existing Mondo term","text":"<ol> <li>Create a new branch and open (or re-open) mondo-edit.obo</li> <li>Navigate to the class <code>'cardioacrofacial dysplasia 1'</code></li> <li>According to OMIM, this disease is caused by a variation in the gene PRKACA.</li> <li>We want to add an equivalence axiom that says every instance of this class is a type of <code>'cardioacrofacial dysplasia'</code> that has dysfunction in the PRKACA gene.</li> <li>To do this, click the + next to Equivalent To in the lower right Description box.</li> <li>Add the following equivalence axiom:    <code>'cardioacrofacial dysplasia' and ('disease has basis in dysfunction of' some PRKACA)</code></li> <li>Run the reasoner.</li> <li>You shouldn't see any change, but try deleting the superclass assertion to 'cardioacrofacial dysplasia' and re-running the reasoner.</li> <li>You should see that 'cardioacrofacial dysplasia' is an inferred superclass.</li> <li>Undo the last change, save your work, commmit, and create a pull request.</li> </ol>"},{"location":"lesson/ontology-design/#adding-classes-and-automatically-classifying-them","title":"Adding classes and automatically classifying them","text":"<p>For teaching purposes, let's say we need a new class that is 'fungal allergy'.</p> <ol> <li>Create a new branch and re-open mondo-edit.obo</li> <li>Add a new term under owl:Thing named 'fungal allergy'.</li> <li>Following the design pattern allergy.yaml, add the text definition, synonym and equivalentTo axiom, using the substance ECTO_0000524 'exposure to mycotoxin'.</li> <li>Run the reasoner and note where the class is automatically classified.</li> <li>Create a pull request and note in the PR what the parent class is.</li> </ol>"},{"location":"lesson/ontology-design/#debugging-automatic-classifications","title":"Debugging automatic classifications","text":"<ol> <li>On the same branch, add a new term under owl:Thing named 'oral cavity neoplasm'.</li> <li>Following the design pattern neoplasm_by_origin, add the term label and the equivalence axiom.</li> <li>Run the reasoner and note where the term in automatically classified. You should see it is under owl:Nothing.</li> <li>Click the ? next to owl:Nothing in the Description box to see the explanation.</li> <li>Can you determine why this is an unsatisfiable class?</li> <li>Create a pull request and add a comment explaining why this is unsatisfiable.</li> </ol>"},{"location":"lesson/ontology-design/#disjointness","title":"Disjointness","text":"<p>By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom.</p> <ol> <li>Create a branch in the Mondo repo (name it: disjoint-[your initials]. For example: disjoint-nv)</li> <li>Open the mondo-edit.obo file</li> <li>Per this ticket, we want to assert that infectious disease and syndromic disease are disjoint.</li> <li>To do this first search for and select the infectious disease class.</li> <li>In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the Expression editor, add 'syndromic disease' as disjoint with 'infectious disease'.</li> <li>Run the ELK reasoner.</li> <li>Scroll to the top of your hierarchy and note that owl:Nothing has turned red. This is because there are unsatisfiable classes.</li> </ol>"},{"location":"lesson/ontology-design/#review-and-fix-one-unsatisfiable-class","title":"Review and fix one unsatisfiable class","text":"<p>Below we'll review an example of one class and how to fix it. Next you should review and fix another one on your own and create a pull request for Nicole or Nico to review. Note, fixing these may require a bit of review and subjective decision making and the fix described below may not necessarily apply to each case.</p> <ol> <li>Review <code>Bickerstaff brainstem encephalitis</code>: To understand why this class appeared under owl:Nothing, first click the ? next to owl:Nothing in the Description box. (Note, this can take a few minutes).</li> </ol> <p></p> <ol> <li>The explanation is displayed above - it is because this class is a descedent of <code>Guillain-Barre syndrome</code>, which is a child of <code>syndromic disease</code>.</li> <li>Next, we have to ask if <code>Bickerstaff brainstem encephalitis</code> is an appropriate child of <code>regional variant of Guillain-Barre syndrome</code>. Note, Mondo integrates several disease terminologies and ontologies, and brought in all the subclass hierarchies from these source ontologies. To see the source of this superclass assertion, click the @ next to the assertion.</li> <li>This source came from Orphanet, see below.</li> </ol> <p></p> <ol> <li>Based on the text definition, there does not seem to be any suggestion that this disease is a type of Guillain-Barre syndrome.</li> <li>Assuming that this disease is not a type of Guillain-Barre syndrome, we should exclude the superclass <code>regional variant of Guillain-Barre syndrome</code> (see this paper and this paper. It seems a bit unclear what the relationship of BBE is to Guillain-Barre syndrome. This also brings into the question if a disease can be syndromic and an infectious disease - maybe this disjoint axiom is wrong, but let's not worry about this for the teaching purposes.)</li> <li>To exclude a superclass, follow the instructions here.</li> </ol>"},{"location":"lesson/ontology-development/","title":"Ontology Development: Release Management, Quality Control and Collaborative Methods","text":""},{"location":"lesson/ontology-development/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/ontology-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a GitHub account</li> <li>You have set up docker and installed the ODK (how to)</li> </ul>"},{"location":"lesson/ontology-development/#preparation","title":"Preparation","text":"<ul> <li>You have prepared your ODK set-up</li> <li>Read ODK reference overview</li> </ul>"},{"location":"lesson/ontology-development/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":""},{"location":"lesson/ontology-development/#learning-objectives","title":"Learning objectives","text":""},{"location":"lesson/ontology-development/#tutorials","title":"Tutorials","text":""},{"location":"lesson/ontology-development/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/ontology-development/#contributors","title":"Contributors","text":""},{"location":"lesson/ontology-fundamentals/","title":"Ontologies: Fundamentals","text":""},{"location":"lesson/ontology-fundamentals/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/ontology-fundamentals/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Prot\u00e9g\u00e9</li> </ul>"},{"location":"lesson/ontology-fundamentals/#preparation","title":"Preparation","text":"<ul> <li>Complete OpenHPI Week 5: Ontology Engineering videos 5.1, 5.2, and 5.4 - 5.6 (~2.5 hours)</li> <li>We are skipping 5.3: Ontology Learning and both sections on MORE Ontology Evaluation (5.7 and 5.8)</li> <li>Complete part of the Ontologies 101 Tutorial (~2 hours)</li> <li>Clone the Ontologies 101 repository, then open the folder <code>BDK14_exercises</code> from your file system</li> <li>Open <code>basic-subclass/chromosome-parts.owl</code> in Prot\u00e9g\u00e9, then do the following exercises:<ul> <li>Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions</li> <li>Disjointness</li> <li>Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology</li> <li>OWL Class Restrictions</li> </ul> </li> <li>Open <code>basic-restriction/er-sec-complex.owl</code> in Prot\u00e9g\u00e9, then do the following exercise:<ul> <li>Basic Restrictions</li> </ul> </li> <li>Open <code>basic-dl-query/cc.owl</code> in Prot\u00e9g\u00e9, then do the following exercises:<ul> <li>DL Query Tab - note that <code>owl:Nothing</code> is defined as the very bottom node of an ontology, therefore the DL query results will show <code>owl:Nothing</code> as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of <code>owl:Nothing</code> and therefore unsatisfiable (more on that below).</li> <li>Basic DL Queries</li> </ul> </li> <li>Open <code>basic-classification/ubiq-ligase-complex.owl</code> in Prot\u00e9g\u00e9, then do the following exercises:<ul> <li>Basic Classification</li> </ul> </li> <li>Read (I can't get no) satisfiability (~10 minutes)</li> <li>Optional: Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Don't spend too much time on this if you get stuck, we'll look at an example of an inconsistent ontology in our session.</li> </ul>"},{"location":"lesson/ontology-fundamentals/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: Learn the fundamentals of ontologies.</p>"},{"location":"lesson/ontology-fundamentals/#learning-objectives","title":"Learning objectives","text":"<ul> <li>OpenHPI course review: questions? (~15 minutes)</li> <li>OWL ontology serializations (\"formats\") (~15 minutes)</li> <li>Converting between serializations with <code>robot convert</code> (Review; ~15 minutes)</li> <li>Creating modules from existing ontologies (~30 minutes)</li> <li>What is a module?</li> <li>How do we use the modules in our ontologies?</li> <li>Extraction methods: MIREOT vs. SLME</li> <li>Creating a module to import with <code>robot extract</code> (Review; ~15 minutes)</li> <li>Ontology design patterns (~15 minutes)</li> <li>Real world example: Ontology for Biomedical Investigations (OBI)</li> <li>Using design patterns in <code>robot template</code> (Review; ~15 minutes)</li> <li>Including your modules in your ontology as imports</li> </ul>"},{"location":"lesson/ontology-fundamentals/#tutorials","title":"Tutorials","text":""},{"location":"lesson/ontology-fundamentals/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/ontology-fundamentals/#contributors","title":"Contributors","text":"<ul> <li>add name/ORCID here</li> </ul>"},{"location":"lesson/ontology-pipelines/","title":"Ontology Pipelines with ROBOT and SPARQL","text":""},{"location":"lesson/ontology-pipelines/#warning","title":"Warning","text":"<p>These materials are under construction and may be incomplete.</p>"},{"location":"lesson/ontology-pipelines/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install ROBOT so you can use it outside of Docker (scroll down to the end of the ROBOT page to find the Windows instructions)</li> <li>Optional Install ODK. The ODK includes ROBOT. In the more advanced parts of the course, you will need the ODK installed for some of the other dependencies it includes, and for Windows users it is often easier to follow the tutorials from inside the docker container rather than the Windows CMD.</li> <li>Familiarise yourself with the ROBOT documentation, to the point that you are aware of the various commands that exist.</li> </ul>"},{"location":"lesson/ontology-pipelines/#tutorials","title":"Tutorials","text":"<ul> <li>Complete the ROBOT Mini-Tutorial 1 to learn your first ROBOT commands: <code>convert</code>, <code>extract</code> and <code>template</code></li> <li>Complete the ROBOT Mini-Tutorial 2 to learn about <code>annotate</code>, <code>merge</code>, <code>reason</code> and <code>diff</code></li> <li>Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour)</li> </ul>"},{"location":"lesson/ontology-pipelines/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>There are two basic ways to edit an ontology:  1. Manually, using tools such as Protege, or  2. Using computational tools such as ROBOT. </p> <p>Both have their advantages and disadvantages: manual curation is often more practical when the required ontology change follows a non-standard pattern, such as adding a textual definition or a synonym, while automated approaches are usually much more scalable (ensure that all axioms in the ontology are consistent, or that imported terms from external ontologies are up-to-date or that all labels start with a lower-case letter). </p> <p>Here, we will do a first dive into the \"computational tools\" side of the edit process. We strongly believe that the modern ontology curator should have a basic set of computational tools in their Semantic Engineering toolbox, and many of the lessons in this course should apply to this role of the modern ontology curator. </p> <p>ROBOT is one of the most important tools in the Semantic Engineering Toolbox. For a bit more background on the tool, please refer to the paper ROBOT: A Tool for Automating Ontology Workflows.</p> <p>We also recommend to get a basic familiarity with SPARQL, the query language of the semantic web, that can be a powerful combination with ROBOT to perform changes and quality control checks on your ontology.</p>"},{"location":"lesson/ontology-pipelines/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/ontology-pipelines/#contributors","title":"Contributors","text":"<ul> <li>Becky Jackson</li> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/ontology-term-use/","title":"Using Ontologies and Ontology Terms","text":""},{"location":"lesson/ontology-term-use/#warning","title":"Warning","text":"<p>These materials are under construction and may be incomplete.</p>"},{"location":"lesson/ontology-term-use/#prerequisites","title":"Prerequisites","text":"<ul> <li>Sign up for a free GitHub account</li> </ul>"},{"location":"lesson/ontology-term-use/#preparation","title":"Preparation","text":"<ul> <li>None</li> </ul>"},{"location":"lesson/ontology-term-use/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: Using ontology terms for annotations and structuring data.</p>"},{"location":"lesson/ontology-term-use/#learning-objectives","title":"Learning objectives","text":"<ol> <li>Explain why ontologies are useful</li> <li>Find good ontologies: ontology repositories, OBO</li> <li>Find terms using ontology browsers</li> <li>Assess ontologies for use: license, quality</li> <li>Map local terminology to ontology terms</li> <li>Identify missing terms</li> <li>Make term requests to existing ontologies</li> <li>Understand the differences between IRIs, CURIEs, and labels</li> </ol>"},{"location":"lesson/ontology-term-use/#tutorials","title":"Tutorials","text":"<ul> <li>None</li> </ul>"},{"location":"lesson/ontology-term-use/#additional-materials-and-resources","title":"Additional materials and resources","text":"<ul> <li>How select and request terms from ontologies - Blog post by Chris Mungall</li> <li>Guidelines for writing definitions in Ontologies (paper)</li> <li>OntoTips - A guide by Chris Mungall covering various aspects of ontology engineering.</li> </ul>"},{"location":"lesson/ontology-term-use/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> </ul>"},{"location":"lesson/ontology-term-use/#1-why-ontologies-are-useful","title":"1. Why ontologies are useful","text":"<p>Ontologies provide a logical classification of information in a particular domain or subject area. Ontologies can be used for data annotations, structuring disparate data types, classifying information, inferencing and reasoning across data, and computational analyses.</p>"},{"location":"lesson/ontology-term-use/#difference-between-a-terminology-and-an-ontology","title":"Difference between a terminology and an ontology","text":""},{"location":"lesson/ontology-term-use/#terminology","title":"Terminology","text":"<p>A terminology is a collection of terms; a term can have a definition and synonyms.</p>"},{"location":"lesson/ontology-term-use/#ontology","title":"Ontology","text":"<p>An ontology contains a formal classification of terminology in a domain that provides textual and machine readable definitions, and defines the relationships between terms. An ontology is a terminology, but a terminology is not (necessarily) an ontology.</p> <p></p>"},{"location":"lesson/ontology-term-use/#2-finding-good-ontologies","title":"2. Finding good ontologies","text":"<p>Numerous ontologies exist. Some recommended sources to find community developed, high quality, and frequently used ontologies are listed below.</p> <ul> <li>OBO Foundry. Read more below</li> <li>The Ontology Lookup Service (OLS). The OLS contains over 200 ontologies.</li> <li>BioPortal. BioPortal aggregates almost 900 biomedical ontologies, and provides a search interface to look up terms. It is a popular repository for ontologies, but as only a fraction of the ontologies are reviewed by the OBO Foundry, you should carefully review any ontologies found on BioPortal before committing to use them.</li> <li>Ontobee. Ontobee indexes all 200+ OBO Foundry ontologies and is the default browser for OBO: For example, when you click http://purl.obolibrary.org/obo/IAO_0000112, you will be redirected to the a page in the Ontobee browser that describes the annotation property <code>example of usage</code>.</li> </ul> <p></p>"},{"location":"lesson/ontology-term-use/#3-ontology-repositories","title":"3. Ontology repositories","text":""},{"location":"lesson/ontology-term-use/#obo-foundry","title":"OBO Foundry","text":"<p>The OBO Foundry is a community of ontology developers that are committed to developing a library of ontologies that are open, interoperable ontologies, logically well-formed and scientifically accurate. OBO Foundry participants follow and contribute to the development of an evolving set of principles including open use, collaborative development, non-overlapping and strictly-scoped content, and common syntax and relations, based on ontology models that work well, such as the Gene Ontology (GO).</p> <p>The OBO Foundry is overseen by an Operations Committee with Editorial, Technical and Outreach working groups.</p>"},{"location":"lesson/ontology-term-use/#find-terms-using-ontology-browsers","title":"Find terms using ontology browsers","text":"<p>Various ontology browsers are available, we recommend using one of the ontology browsers listed below.</p> <ul> <li>Find terms:</li> <li>Ontology Lookup Service</li> <li>BioPortal</li> <li>Ontobee</li> </ul> <p></p>"},{"location":"lesson/ontology-term-use/#4-assessing-ontologies-for-use","title":"4. Assessing ontologies for use","text":"<p>Some considerations for determining which ontologies to use include the license and quality of the ontology.</p>"},{"location":"lesson/ontology-term-use/#license","title":"License","text":"<p>Licenses define how an ontology can legally be used or reused. One requirement for OBO Foundry Ontologies is that they are open, meaning that the ontologies are openly and freely available for use with acknowledgement and without alteration. OBO ontologies are required to be released under a Creative Commons CC-BY license version 3.0 or later, OR released into the public domain under CC0. The license should be clearly stated in the ontology file.</p>"},{"location":"lesson/ontology-term-use/#quality","title":"Quality","text":"<p>Some criteria that can be applied to determine the quality of an ontology include:</p> <ul> <li>Is there an ontology tracker to report issues? All open ontologies should have some form of an issue tracker to report bugs, make new term requests or request other changes to the ontology. Many ontologies use GitHub to track their issues.</li> <li>Is it currently active? Are there a large number of open tickets on the ontology tracker that have not been commented on or otherwise addressed? Are the tickets very old, have been sitting for years?</li> <li>Commmunity involvement On the issue tracker, is there evidence of community involvement, such as issues and comments from outside community members?</li> <li>Scientifically sound Does the ontology accurately represent the domain in a scientifically sound way?</li> </ul>"},{"location":"lesson/ontology-term-use/#how-to-determine-which-is-the-right-ontology-to-use","title":"How to determine which is the right ontology to use?","text":"<ul> <li>There are multiple ontologies that exist, start by selecting the appropriate ontology, then search and restrict your search to that ontology.</li> <li>Recommend using ontologies that are open and interoperable. Focusing on OBO foundry ontologies are a good place to start</li> <li>Make informed decision about which ontology to use</li> <li>Maybe the ontology you want to use does not have the term you want, so make a term request to that ontology</li> </ul>"},{"location":"lesson/ontology-term-use/#5-mapping-local-terminology-to-ontology-terms","title":"5. Mapping local terminology to ontology terms","text":"<p>Data can be mapped to ontology terms manually, using spreadsheets, or via curation tools such as:</p> <ul> <li>Zooma</li> <li>BioPortal Annotator</li> <li>Canto - a web-based literature curation tools</li> <li>Textpresso - designed for C. elegans curation</li> <li>OntoBrowser - an online collaborative curation tool</li> </ul> <p></p>"},{"location":"lesson/ontology-term-use/#6-identifying-missing-terms","title":"6. Identifying missing terms","text":"<p>The figure below by Chris Mungall on his blog post on How to select and request terms from ontologies  describes a workflow on searching for identifying missing terms from an ontology.</p> <p></p> <p></p>"},{"location":"lesson/ontology-term-use/#7-make-term-requests-to-existing-ontologies","title":"7. Make term requests to existing ontologies","text":"<p>See separate lesson on Making term requests to existing ontologies.</p> <p></p>"},{"location":"lesson/ontology-term-use/#8-differences-between-iris-curies-and-labels","title":"8. Differences between IRIs, CURIEs, and labels","text":""},{"location":"lesson/ontology-term-use/#uri","title":"URI","text":"<p>A uniform resource identifier (URI) is a string of characters used to identify a name or a resource.</p>"},{"location":"lesson/ontology-term-use/#url","title":"URL","text":"<p>A URL is a URI that, in addition to identifying a network-homed resource, specifies the means of acting upon or obtaining the representation.</p> <p>A URL such as this one:</p> <p>https://github.com/obophenotype/uberon/blob/master/uberon_edit.obo</p> <p>has three main parts:</p> <ol> <li>Protocol, e.g. https</li> <li>Host, e.g. github.com</li> <li>Path, e.g. /obophenotype/uberon/blob/master/uberon_edit.obo</li> </ol> <p>The protocol tells you how to get the resource. Common protocols for web pages are http (HyperText Transfer Protocol) and https (HTTP Secure). The host is the name of the server to contact (the where), which can be a numeric IP address, but is more often a domain name. The path is the name of the resource on that server (the what), here the Uberon anatomy ontology file.</p>"},{"location":"lesson/ontology-term-use/#iri","title":"IRI","text":"<p>A Internationalized Resource Identifiers (IRI) is an internet protocol standard that allows permitted characters from a wide range of scripts. While URIs are limited to a subset of the ASCII character set, IRIs may contain characters from the Universal Character Set (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. It is defined by RFC 3987.</p> <p>More information is available here.</p>"},{"location":"lesson/ontology-term-use/#curies","title":"CURIEs","text":"<p>A Compact URI (CURIE) consists of a prefix and a suffix, where the prefix stands in place of a longer base IRI.</p> <p>By converting the prefix and appending the suffix we get back to full IRI. For example, if we define the obo prefix to stand in place of the IRI as: http://purl.obolibrary.org/obo/, then the CURIE obo:UBERON_0002280 can be expanded to http://purl.obolibrary.org/obo/UBERON_0002280, which is the UBERON Anatomy term for \u2018otolith\u2019. Any file that contains CURIEs need to define the prefixes in the file header.</p>"},{"location":"lesson/ontology-term-use/#label","title":"Label","text":"<p>A label is the textual, human readable name that is given to a term, class property or instance in an ontology.</p>"},{"location":"lesson/phenotype-data/","title":"Basic concepts of phenotype data and ontologies","text":""},{"location":"lesson/phenotype-data/#basic-concepts-of-phenotype-data-integration","title":"Basic concepts of phenotype data integration","text":"<p>Part of the Phenotype data and the role of ontologies\" (OBO Academy Phenomics Series).</p> <p>Authors:</p> <ul> <li>James McLaughlin</li> <li>Nicolas Matentzoglu</li> </ul> <p>Last update: 30.03.2024.</p> <p>In this lesson, we will discuss:</p> <ul> <li>Core concepts, from characteristics and biological attributes to phenotypic abnormalities</li> <li>Different types of phenotype data capture in practice</li> <li>An overview on how the uPheno framework supports integrating all kinds of phenotype data into a computational framework</li> <li>A brief outlook and how computational phenotypes can be exploited for analysis</li> </ul> <p></p>"},{"location":"lesson/phenotype-data/#overview","title":"Overview","text":"<p>Phenotyping is, in essence, the process of recording the observable characteristics, or phenotypic profile, of a subject. There are many use cases for doing this task, for example: </p> <ul> <li>Clinicians have to record a patient's phenotypic profile to facilitate more accurate diagnosis</li> <li>Researchers have to record phenotypic profiles of model organisms to characterise them and assess interventions (genetic or drug or otherwise).</li> <li>Curators extract knowledge from scientific publications to build knowledge bases which contain associations between phenotypes and other data types.</li> </ul> <p>There are different schools and use cases that are concerned with how to record phenotypes in a structured manner:</p> <ul> <li>Quantified phenotypes can be recorded as a trait in combination with a measurement datum (\u201chead circumference\u201d, \u201c35 cm\u201d) or a qualified term expressing \u201cphenotypic change\u201d (\u201cincreased head circumference\u201d).</li> <li>We can express phenotype terms as \u201cpre-coordinated\u201d terms, like \u201cincreased head circumference\u201d or a \u201cpost-coordinated expression\u201d, like \u201chead\u201d, \u201ccircumference\u201d, \u201cincreased\u201d).</li> <li>Phenotype data can be standardised to varying degrees, from free text to ontology terms.</li> </ul> <p>In this lesson, we will introduce different concepts and categories around phenotype data, and provide an introduction on how to integrate them using the uPheno framework.</p>"},{"location":"lesson/phenotype-data/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of ontologies.</li> <li>Basic understanding of structured data.</li> </ul>"},{"location":"lesson/phenotype-data/#preparation","title":"Preparation","text":"<p>None.</p>"},{"location":"lesson/phenotype-data/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<ul> <li>A basic understanding of the core concepts around phenotype data, from characteristics, measurements and traits to phenotypic abnormalities and diseases.</li> <li>A basic understanding of how the uPheno framework integrates such data.</li> </ul>"},{"location":"lesson/phenotype-data/#table-of-contents","title":"Table of contents","text":"<ul> <li>Core concepts</li> <li>Phenotype Data in practice</li> <li>Integrating phenotype data</li> <li>The promise of integrated, computational phenotype data</li> </ul>"},{"location":"lesson/phenotype-data/#recording","title":"Recording","text":"<p>This is a recording from April 2024 covering the entire course:</p> <p></p>"},{"location":"lesson/phenotype-data/#core-concepts","title":"Core concepts","text":"<p>Duration: 12 min</p> <p>Goals of this section:</p> <ul> <li>Give a sense of the core features of phenotype data, including characteristics, traits and abnormalities.</li> <li>Materials.</li> </ul> <p></p>"},{"location":"lesson/phenotype-data/#phenotype-data-in-practice","title":"Phenotype Data in practice","text":"<p>Duration: 12 min</p> <p>Goals of this section:</p> <ul> <li>Give a sense of the contexts in which phenotype data is produced (research and clinical)</li> <li>Give a sense of the shape of different styles of phenotype data</li> <li>Materials.</li> </ul> <p></p>"},{"location":"lesson/phenotype-data/#integrating-phenotype-data","title":"Integrating phenotype data","text":"<p>Duration: 12 min</p> <p>Goals of this section:</p> <ul> <li>Show how the uPheno framework can be used to translate many forms of phenotype data into a coherent computational framework.</li> <li>Show how the uPheno framework can be enriched with knowledge about phenotypes.</li> <li>Materials.</li> </ul> <p></p>"},{"location":"lesson/phenotype-data/#the-promise-of-integrated-computational-phenotype-data-4-min","title":"The promise of integrated, computational phenotype data (4 min)","text":"<p>Duration: 4 min</p> <p>Goals of this section:</p> <ul> <li>Give a sense how the uPheno framework can be used computationally.</li> <li>Materials.</li> </ul>"},{"location":"lesson/rdf/","title":"Introduction to RDF","text":"<p>First Instructor: James Overton Second Instructor: Becky Jackson</p>"},{"location":"lesson/rdf/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/rdf/#description","title":"Description","text":"<p>Modelling and querying data with RDF triples, and working with RDF using tables</p>"},{"location":"lesson/rdf/#topics","title":"Topics","text":"<ul> <li>RDF modelling</li> <li>RDFS</li> <li>SPARQL</li> </ul>"},{"location":"lesson/rdf/#openhpi-course-content","title":"OpenHPI Course Content","text":"<p>OpenHPI Linked Data Engineering (2016)</p> <ul> <li>Lesson 2 RDF</li> <li>Lesson 4 SPARQL</li> </ul>"},{"location":"lesson/rdf/#software-carpentry-lessons","title":"Software Carpentry Lessons","text":"<p>Using Databases and SQL</p>"},{"location":"lesson/rdf/#new-material","title":"New Material","text":"<ul> <li>Tables and Triples</li> </ul>"},{"location":"lesson/rdf/#optional-material","title":"Optional material","text":"<ul> <li>Linking data</li> </ul>"},{"location":"lesson/rdf/#semantic-engineer-toolbox","title":"Semantic Engineer Toolbox","text":""},{"location":"lesson/semantic-database-fundamentals/","title":"Semantic Databases: Fundamentals","text":""},{"location":"lesson/semantic-database-fundamentals/#warning","title":"Warning","text":"<p>These materials are under construction and incomplete.</p>"},{"location":"lesson/semantic-database-fundamentals/#prerequisites","title":"Prerequisites","text":"<ul> <li>Review tutorial on Project Ontology Development</li> </ul>"},{"location":"lesson/semantic-database-fundamentals/#preparation","title":"Preparation","text":"<ul> <li>TBD</li> </ul>"},{"location":"lesson/semantic-database-fundamentals/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: Using ontology terms in a database.</p>"},{"location":"lesson/semantic-database-fundamentals/#learning-objectives","title":"Learning objectives","text":"<ul> <li>advanced term mapping</li> <li>ontology terms in SQL</li> <li>terminology table JOINs, constraints</li> <li>convert tables to triples</li> <li>triplestores</li> <li>knowledge graphs</li> </ul>"},{"location":"lesson/semantic-database-fundamentals/#tutorials","title":"Tutorials","text":"<ul> <li>in person or video (link videos here as they become available)</li> </ul>"},{"location":"lesson/semantic-database-fundamentals/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/semantic-database-fundamentals/#contributors","title":"Contributors","text":"<ul> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/synonyms/","title":"An Introduction to Synonyms in OBO Ontologies","text":""},{"location":"lesson/synonyms/#an-introduction-to-synonyms-in-obo-ontologies","title":"An Introduction to Synonyms in OBO Ontologies","text":"<p>Authors:</p> <ul> <li>Nicole Vasilevsky</li> <li>Nicolas Matentzoglu</li> </ul> <p>Last update: 25.04.2024</p> <p></p> <p>In this lesson, we will discuss:</p> <ul> <li>Definitions and basic concepts around synonyms</li> <li>The importance of accurate curation of synonyms</li> <li>The OBO model for synonym representation</li> <li>A brief outlook on validation</li> </ul>"},{"location":"lesson/synonyms/#overview","title":"Overview","text":"<p>Over time, different scientific communities have developed widely divergent terminologies for describing the same things. For example, \"Varicella\" and \"Chickenpox\" both refer to the same real-world condition. Or magroglossia can be described as enlarged tongue, megaglossia, etc.</p> <p></p> <p>To understand if different scientific resources such as a biological database, a scientific publication or an ontology talk about the same thing, we need to recognise if the expression they are using refers to the same real-world concept (have the same meaning).</p> <p>The primary role of an ontology is to provide an explicit conceptualisation of a domain. However, ontologies especially in the biomedical domain have become much more than just that: for example, they encode hierarchical structures (taxonomies), provide formal definitions for terms (terminologies) and provide synonyms for terminological disambiguation (thesaurus).</p> <p>Synonyms can have different levels of precision (e.g. exact, broad, related) and be used for different purposes (laypersons, acronyms, language translations). The variety of synonym types makes them difficult to curate and use.</p> <p>This lesson is aimed at ontology curators, data analysts and tool developers to understand how to accurately curate and use synonyms provided by OBO ontologies.</p>"},{"location":"lesson/synonyms/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of ontologies.</li> <li>Basic understanding of structured data.</li> </ul>"},{"location":"lesson/synonyms/#preparation","title":"Preparation","text":"<p>None.</p>"},{"location":"lesson/synonyms/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<ul> <li>A basic understanding of the core concepts around synonyms.</li> <li>A clear understanding of synonym properties and synonym types.</li> <li>Rules to curate and use synonym properties and synonym types correctly.</li> </ul>"},{"location":"lesson/synonyms/#table-of-contents","title":"Table of contents","text":"<ul> <li>Introduction and examples</li> <li>The OBO model for synonyms</li> <li>Basic overview of validation</li> </ul>"},{"location":"lesson/synonyms/#introduction-and-examples","title":"Introduction and examples","text":"<p>Duration: 10 min</p> <ul> <li>What is a synonym?</li> <li>Are all synonyms exact?</li> <li>How are synonyms usually shared and curated?</li> </ul> <p></p>"},{"location":"lesson/synonyms/#what-is-a-synonym","title":"What is a synonym?","text":"<p>Definition of synonym, Merrian Webster</p> <p>Synonym: one of two or more words or expressions of the same language that have the same or nearly the same meaning in some or all senses (https://www.merriam-webster.com/dictionary/synonym)</p> <p>Synonyms provide linguistic variety and help clarify meaning by offering alternative expressions with similar or identical meanings.</p> <p>In the medical or clinical domain, synonyms are particularly valuable for ensuring that different terminologies used by various healthcare professionals and patients refer to the same conditions or treatments. This is crucial for effective communication, accurate diagnosis, and comprehensive patient care.</p> <ol> <li> <p>Acute Myocardial Infarction:</p> <ul> <li>Synonyms: Heart Attack, MI</li> <li>Context: \"Acute Myocardial Infarction\" is the clinical term for what is commonly known as a \"heart attack.\" Using both terms can help ensure clarity in patient communications as well as in medical documentation.</li> <li>Note: Cardiac arrest, also known as sudden cardiac arrest, is when the heart stops beating suddenly. It's not the same as a heart attack. Reference</li> </ul> </li> <li> <p>Hypertension:</p> <ul> <li>Synonyms: High Blood Pressure</li> <li>Context: \"Hypertension\" is often used interchangeably with \"High Blood Pressure.\" While \"hypertension\" is commonly used in clinical settings, \"high blood pressure\" is more frequently used in patient education and public health communications.</li> </ul> </li> <li> <p>Cerebrovascular Accident:</p> <ul> <li>Synonyms: Stroke, Brain Attack</li> <li>Context: A \"Cerebrovascular Accident,\" often referred to simply as a \"stroke,\" is sometimes also called a \"brain attack\" in educational materials to emphasize its urgency and nature, similar to a heart attack.</li> </ul> </li> <li> <p>Varicella:</p> <ul> <li>Synonyms: Chickenpox</li> <li>Context: \"Varicella\" is the medical term for \"chickenpox,\" a common childhood illness. Using both terms helps in making literature accessible to both medical professionals and the general public.</li> </ul> </li> </ol> <p>These examples illustrate how synonyms in the clinical domain facilitate effective communication across different levels of medical expertise and lay understanding, enhancing the clarity and efficacy of healthcare delivery.</p> <p></p>"},{"location":"lesson/synonyms/#are-all-synonyms-exact-no-here-is-why","title":"Are all synonyms exact? No. Here is why.","text":"<p>Not all synonyms should be considered exact synonyms because subtle variations in meaning, context, and usage can lead to significant differences in interpretation, particularly in domains where precision is critical, such as in clinical or medical settings. Recognizing synonyms as related, broad, or narrow helps clarify their specific meanings and applications, ensuring more accurate communication and appropriate usage.</p> <ul> <li> <p>Precision in Diagnosis and Treatment: In the clinical domain, the precise understanding of terminology can directly affect diagnosis and treatment decisions. Misinterpretations due to synonym confusion can lead to incorrect treatment plans and potentially harmful outcomes.</p> </li> <li> <p>Effective Communication: Health professionals often communicate with a diverse team, including other clinicians, researchers, nurses, and patients. Clear distinctions among synonym types ensure that all parties have a correct understanding of the condition or treatment discussed.</p> </li> <li> <p>Medical Documentation and Coding: Medical records, billing, and coding rely heavily on precise terminology. Using the correct synonym in the right context is crucial for accurate record-keeping and insurance processing.</p> </li> </ul> <p>Let's look at some examples from the clinical domain:</p> <ol> <li> <p>Myocardial Infarction vs. Heart Attack:</p> <ul> <li>Myocardial Infarction (Exact): A medical term used to describe the interruption of blood supply to a part of the heart, causing heart cells to die. This term is used in clinical diagnostics and documentation.</li> <li>Heart Attack (Broad): Commonly used to describe any critical heart-related event by the general public. It is broader and might not always refer to myocardial infarction specifically. A heart attack is when blood flow to the heart is blocked. Sudden cardiac arrest is when the heart malfunctions and suddenly stops beating. A heart attack is a \u201ccirculation\u201d problem and sudden cardiac arrest is an \u201celectrical\u201d problem. Reference</li> </ul> </li> <li> <p>Osteoarthritis vs. Arthritis:</p> <ul> <li>Osteoarthritis (Narrow): A type of arthritis that occurs when flexible tissue at the ends of bones wears down.</li> <li>Arthritis (Broad): A general term used to refer to conditions involving pain and inflammation in joints. It includes over 100 different types of joint diseases, making it a broader term than osteoarthritis.</li> </ul> </li> <li> <p>Seizure vs. Epilepsy:</p> <ul> <li>Seizure (Related): A single medical event that involves abnormal electrical activity in the brain, which can manifest as convulsions, minor physical signs, thought disturbances, or a combination of symptoms.</li> <li>Epilepsy (Broad): A chronic disorder characterized by unprovoked, recurrent seizures. Epilepsy implies a condition with broader implications than a single seizure event.</li> </ul> </li> <li> <p>Tumor vs. Cancer:</p> <ul> <li>Tumor (Related): An abnormal lump or growth of tissue, which can be benign or malignant.</li> <li>Cancer (Broad): A broad term for diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body. Not all tumors are cancerous (e.g., benign tumors).</li> </ul> </li> </ol> <p>Cancer vs Malignant Tumor</p> <p>In Mondo, malignant tumor, malignant neoplasm, malignancy are synonyms for cancer. Mondo follows the National Cancer Institute Thesaurus (NCIT), which uses 'Malignant Neoplasm' as the primary label (and cancer is a synonym). this may differ in other disease terminologies.</p> <p>Distinguishing between exact, broad, narrow, and related synonyms in the clinical domain is essential for maintaining the high standards of precision and care required in medical practice. This nuanced understanding aids in better patient outcomes, more effective communication among healthcare providers, and precise medical documentation.</p> <p></p>"},{"location":"lesson/synonyms/#how-are-synonyms-usually-shared-and-curated","title":"How are synonyms usually shared and curated?","text":"<p>Synonyms are commonly shared in many different ways. Here are a few that are widely used:</p> <p>Thesauri</p> <p>Definition: A thesaurus is a structured vocabulary of controlled terms that includes synonyms and antonyms (opposites), and often, hierarchical (broader and narrower) relationships among the terms.</p> <p>Example: The Art &amp; Architecture Thesaurus (AAT) is widely used in cultural heritage and museum documentation to standardize terminology related to art, architecture, and material culture.</p> <p>Synonym Rings</p> <p>Definition: Synonym rings group terms that are considered synonymous in a particular context, without specifying a preferred term. This is useful in information retrieval systems where different users may use different terms to mean the same thing.</p> <p>Example: Many search engines internally use synonym rings to expand search queries to include various terms that users might use interchangeably.</p> <p>Ontologies</p> <p>Definition: Ontologies provide a formal representation of knowledge as a set of concepts within a domain, along with the relationships between those concepts. They can include synonym information to ensure that abstract concepts/classes can be understood and recognised across domains.</p> <p>Example: The Gene Ontology (GO) is an ontology widely used in the bioinformatics community to unify the representation of gene and gene product attributes across species and databases.</p> <p>Taxonomies</p> <p>Definition: Taxonomies classify information into ordered categories or classes. While not primarily focused on synonyms, they often include them to help map similar categories across different systems.</p> <p>Example: The Linnaean taxonomy used in biology classifies living organisms but also aligns various synonymous names that have been historically used for the same species.</p> <p>Controlled Vocabularies</p> <p>Definition: A controlled vocabulary is an organized arrangement of words and phrases used to index content and/or to retrieve content through browsing or searching. It typically includes synonyms to ensure that different terms that mean the same thing lead to the same content.</p> <p>Example: Medical Subject Headings (MeSH) is used in many biomedical indexes and databases. It includes a wide array of synonyms for medical terms to ensure comprehensive search coverage.</p> <p>Lexical Databases</p> <p>Definition: Lexical databases store information about words, including definitions, synonyms, antonyms, and relationships between words.</p> <p>Example: WordNet, a lexical database for the English language, provides sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by conceptual-semantic and lexical relations.</p>"},{"location":"lesson/synonyms/#why-are-synonyms-so-important","title":"Why are synonyms so important?","text":"<p>Synonyms are hugely important for many use cases in the information and data domains. Here are some examples:</p> <ol> <li> <p>Database curation:</p> <ul> <li>Use Case: Making it easier to identify the right ontology or vocabulary term during data curation.</li> <li>Benefit: Precise synonyms make it easier to recognise that a specific class in an ontology is \"the right one\" when curating a record in a database, especially if the term I most frequently use is one of the synonyms.</li> </ul> </li> <li> <p>Integration of community-level terminological preferences:</p> <ul> <li>Use Case: Ontology classes can have a at maximum a single preferred label. This can be frustrating when two different communities use different terminologies for the same concepts. Using synonyms we have the choice of favoring one community for the primary label, and another for the synonym; or using neutral terminology for the label, and making each community's preferred term a synonym. Optionally, the synonyms can be marked up using axioms to indicate they are preferred for that community, and some tools can then render that as the primary label in given contexts.</li> <li>Benefit: Scientific communities evolved across a spectrum of disciplines and geographical locations. Many have divergent terminological preferences, for example, ClinGen prefers, in some cases, to explicitly include gene symbols as part of a disease name, while other organisation prefer to use other forms of naming. Synonyms are a great way to record such community-level preferences linked to a common disease concept. This enables a pluralistic, more inclusive approach to ontology development involving many different communities.</li> </ul> </li> <li> <p>Natural Language Processing (NLP):</p> <ul> <li>Use Case: Improving the accuracy of speech recognition systems, chatbots, and AI assistants. Synonyms help these systems understand and process human language more effectively by mapping different words to the same meanings.</li> <li>Benefit: Enhances the user experience by allowing more flexible, conversational interactions with technology using varied vocabulary, thus making AI interactions seem more natural and less constrained by specific word choices. In our domain, this is particularly important when automatically extracting data and knowledge from unstructured sources such as scientific articles, for example into a knowledge graph.</li> </ul> </li> <li> <p>Database Search:</p> <ul> <li>Use Case: Enhancing the capability of database queries to return relevant results even when search terms use different terminology from that stored in the database. This is particularly important in fields like healthcare, legal, and academic databases where diverse terminologies are common.</li> <li>Benefit: Users can find relevant information even if they don't know the exact terminology used in the database, reducing search time and improving the usability of information systems. Many databases in the biomedical domain employ synonyms for more accurate discovery of information.</li> </ul> </li> <li> <p>Automated Ontology and Knowledge Graph Matching:</p> <ul> <li>Use Case: Frequently, we have to align different ontologies, KGs or controlled vocabularies. A new buzzword (promoted as far as we can see by a business that wants to sell the service) is \"entity resolved knowledge graphs\" - KGs in which all records/terms that mean the same thing resolve to the same graph entity. This is, for many of us of course, our daily bread and butter. Synonyms are critical features of terms needed for entity resolution and ontology matching.</li> <li>Benefits: By matching on exact synonyms, we can easily determine if two terms are equivalent.</li> </ul> </li> </ol> <p>Info</p> <p>The importance of explicitly curated synonyms may become a little less obvious in the age of LLMs, where synonyms are naturally represented in some magic place called \"latent space\", which makes it possible for chatbots to understand if two expressions refer to the same real world entity. However, LLMs are far from perfect at reflecting specialised scientific synonymy and are still prone to hallucination in 2024. For scenarios where accuracy and provenance (who says this is a synonym?) is important, structured sources for synonymy are key.</p> <p></p>"},{"location":"lesson/synonyms/#the-obo-model-for-synonyms","title":"The OBO model for synonyms","text":"<p>Duration: 25 min</p> <ul> <li>OBO modeling and synonyms</li> <li>Overview of synonym properties</li> <li>Overview of synonym types</li> </ul> <p></p>"},{"location":"lesson/synonyms/#obo-modeling-and-synonyms","title":"OBO modeling and synonyms","text":"<p>All standardised metadata properties promoted by the OBO Foundry included in the OBO Metadata Ontology (OMO).</p> <p>There are two hierarchies of synonym types of particular relevance to synonym representation:</p> <ul> <li>Synonym Properties</li> <li>Synonym Types</li> </ul> <p>For the main part of the course, please study the following to reference guides.</p> <ul> <li>Overview of synonym properties</li> <li>Overview of synonym types</li> </ul> <p></p>"},{"location":"lesson/synonyms/#basic-overview-of-validation","title":"Basic overview of validation","text":"<p>Duration: 5 min</p> <p>Goals of this section:</p> <ul> <li>Basic considerations for validation</li> <li>Materials<ul> <li>Basic validation with ROBOT <code>report</code></li> <li>Advanced validation with custom <code>SPARQL</code> queries</li> </ul> </li> </ul>"},{"location":"lesson/templates-for-obo/","title":"Templating systems for OBO ontologies: a deep dive","text":"<p>Ontologies are notoriously hard to edit. This makes it a very high burden to edit ontologies for anyone but a select few. However, many of the contents of ontologies are actually best edited by domain experts with often little or known ontological training - editing labels and synonyms, curating definitions, adding references to publications and many more. Furthermore, if we simply remove the burden of writing OWL axioms, editors with very little ontology training can actually curate even logical content: for example, if we want to describe that a class is restricted to a certain taxon (also known as taxon-restriction), the editor is often capable to select the appropriate taxon for a term (say, a \"mouse heart\" is restricted to the taxon of Mus musculus), but maybe they would not know how to \"add that restriction to the ontology\".</p> <p>Tables are great (for a deep dive into tables and triples see here). Scientists in particular love tables, and, even more importantly, can be trained easily to edit data in spreadsheet tools, such as Google Sheets or Microsoft Excel.</p> <p>Ontology templating systems, such as DOSDP templates, ROBOT templates and Reasonable Ontology Templates (OTTR) allow separating the raw data in the ontology (labels, synonyms, related ontological entities, descriptions, cross-references and other metadata) from the OWL language patterns that are used to manifest them in the ontology. There are three main ingredients to a templating system:</p> <ol> <li>A way to capture the data. In all the systems we care about, these are tables, usually manifested as spreadsheets in Excel or Google Sheets.</li> <li>A way to capture the template. In ROBOT templates the templates are captured in a header row of the same table that captures the data, in DOSDP templates the templates are captured in a separate YAML file and in OTTR typically the templates are serialised as and RDF-graph in a format like RDF/XML or Turtle.</li> <li>A toolkit that can combine the data and the template to generate OWL axioms and annotations. ROBOT templates can be compiled to OWL using ROBOT, DOSDP templates can be compiled using DOSDP tools and OTTR templates using Lutra.</li> </ol> <p>In OBO we are currently mostly concerned with ROBOT templates and DOSDP templates. Before moving on, we recommend to complete a basic tutorial in both:</p> <ul> <li>ROBOT template tutorial</li> <li>DOSDP template tutorial</li> </ul>"},{"location":"lesson/templates-for-obo/#robot-template-vs-dosdp-template","title":"ROBOT template vs DOSDP template","text":"<p>Ontologies, especially in the biomedical domain, are complex and, while growing in size, increasingly hard to manage for their curators. In this section, we will look at some of the key differences of two popular templating systems in the OBO domain: Dead Simple Ontology Design Patterns (DOSDPs) and ROBOT templates. We will not cover the rationale for templates in general in much depth (the interested reader should check ontology design patterns and Reasonable Ontology Templates (OTTR): Motivation and Overview, which pertains to a different system, but applies none-the-less in general), and focus on making it easier for developers to pick the right templating approach for their particular use case. We will first discuss in detail representational differences, before we go through the functional ones and delineate use cases.</p>"},{"location":"lesson/templates-for-obo/#structural-differences-formats-and-tools","title":"Structural differences, formats and tools","text":""},{"location":"lesson/templates-for-obo/#dosdp-templates-structure-and-format","title":"DOSDP templates: structure and format","text":"<p>DOSDP separates data and templates into two files: a yaml file which defines the template, and a TSV file which holds the data. Lets look at s example.</p> <p>The template: abnormalAnatomicalEntity</p> <pre><code>pattern_name: abnormalAnatomicalEntity\npattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml\ndescription: \"Any unspecified abnormality of an anatomical entity.\"\n\ncontributors:\n  - https://orcid.org/0000-0002-9900-7880\n  - https://orcid.org/0000-0001-9076-6015\n  - https://orcid.org/0000-0003-4148-4606\n  - https://orcid.org/0000-0002-3528-5267\n\nclasses:\n  quality: PATO:0000001\n  abnormal: PATO:0000460\n  anatomical entity: UBERON:0001062\n\nrelations:\n  inheres_in_part_of: RO:0002314\n  has_modifier: RO:0002573\n  has_part: BFO:0000051\n\nannotationProperties:\n  exact_synonym: oio:hasExactSynonym\n\nvars:\n  anatomical_entity: \"'anatomical entity'\"\n\nname:\n  text: \"abnormal %s\"\n  vars:\n   - anatomical_entity\n\nannotations:\n  - annotationProperty: exact_synonym\n    text: \"abnormality of %s\"\n    vars:\n     - anatomical_entity\n\ndef:\n  text: \"Abnormality of %s.\"\n  vars:\n    - anatomical_entity\n\nequivalentTo:\n  text: \"'has_part' some ('quality' and ('inheres_in_part_of' some %s) and ('has_modifier' some 'abnormal'))\"\n  vars:\n    - anatomical_entity\n</code></pre> <p>The data: abnormalAnatomicalEntity.tsv</p> defined_class defined_class_label anatomical_entity anatomical_entity_label HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit"},{"location":"lesson/templates-for-obo/#robot-templates-structure-and-format","title":"ROBOT templates: structure and format","text":"<p>ROBOT encodes both the template and the data in the same TSV; after the table header, the second row basically encodes the entire template logic, and the data follows in table row 3.</p> ID Label EQ Anatomy Label ID LABEL EC 'has_part' some ('quality' and ('inheres_in_part_of' some %) and ('has_modifier' some 'abnormal')) HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit <p>Note that for the <code>Anatomy Label</code> we deliberately left the second row empty, which instructs the ROBOT template tool to completely ignore this column.</p>"},{"location":"lesson/templates-for-obo/#a-discussion-on-the-main-differences","title":"A discussion on the main differences","text":""},{"location":"lesson/templates-for-obo/#ontology-engineering-perspective","title":"Ontology Engineering perspective","text":"<p>From an ontology engineering perspective, the essence of the difference between DOSDP and ROBOT templates could be captured as follows:</p> <pre><code>DOSDP templates are more about generating annotations and axioms, while ROBOT templates are more about curating annotations and axioms.\n</code></pre> <p><code>Curating annotations and axioms</code> means that an editor, or ontology curator, manually enters the labels, synonyms, definitions and so forth into the spreadsheet.</p> <p><code>Generating axioms</code> in the sense of this section means that we try to automatically generate labels, synonyms, definitions and so forth based on the related logical entities in the patterns. E.g., using the example template above, the label \"abnormal kidney\" would automatically be generated when the Uberon term for kidney is supplied.</p> <p>While both ROBOT and DOSDP can be used for \"curation\" of annotation of axioms, DOSDP seeks to apply generation rules to automatically generate synonyms, labels, definitions and so forth while for ROBOT template seeks to collect manually curated information in an easy-to-use table which is then compiled into OWL. In other words:</p> <ul> <li>the average DOSDP user will not write their own labels, definitions and synonyms - they will want those to be generated automatically from a set of simple rules;</li> <li>the average ROBOT template user will not want automatically generated definitions, labels and synonyms - they will want to capture their own.</li> </ul>"},{"location":"lesson/templates-for-obo/#sharing-and-re-use","title":"Sharing and Re-use","text":"<p>However, there is another dimension in which both approaches differ widely: sharing and re-use. DOSDPs by far the most important feature is that it allows a community of developers to rally around a modelling problem, debate and establish consensus; for example, a pattern can be used to say: this is how we model abnormal anatomical entities. Consensus can be made explicit by \"signing off\" on the pattern (e.g. by adding your ORCId to the list of contributors), and due to the template/data separation, the template can be simply imported using its IRI (for example http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml) and re-used by everyone. Furthermore, additional metadata fields including textual descriptions, and more recently \"examples\", make DOSDP template files comparatively easy to understand, even by a less technically inclined editor.</p> <p>ROBOT templates on the other hand do not lend themselves to community debates in the same way; first of all, they are typically supplied including all data merged in; secondly, they do not provide additional metadata fields that could, for example, conveniently be used to represent a sign off (you could, of course, add the ORCId's into a non-functional column, or as a pipe-separated string into a cell in the first or second row; but its obvious that this would be quite clunky) or a textual description. A yaml file is much easier for a human to read and understand then the header of a TSV file, especially when the template becomes quite large.</p> <p>However, there is a flipside to the strict separation of data and templates. One is that DOSDP templates are really hard to change. Once, for example, a particular variable name was chosen, renaming the variable will require an excessive community-wide action to rename columns in all associated spreadsheets - which requires them all to be known beforehand (which is not always the case). You don't have such a problem with ROBOT templates; if you change a column name, or a template string, everything will continue to work without any additional coordination.</p>"},{"location":"lesson/templates-for-obo/#summary","title":"Summary","text":"<p>Both ROBOT templates and DOSDP templates are widely used. The author of this page uses both in most of the projects he is involved in, because of their different strengths and capabilities. You can use the following rules of thumb to inform your choice:</p> <p>Consider ROBOT templates if your emphasis is on</p> <ol> <li>manually curating labels, definitions, synonyms and axioms or other annotations</li> <li>managing your templates in the spreadsheet itself is a concern for you (this is often the case, for example, when turning an existing data table into a ROBOT template ad hoc)</li> </ol> <p>Consider DOSDP templates if your emphasis is on</p> <ol> <li>re-use, community-wide implementation of the same templates and community discussion, you should consider DOSDP templates</li> <li>automatically generating labels, definitions, synonyms from rules in the pattern.</li> </ol>"},{"location":"lesson/templates-for-obo/#detour-concerns-with-managing-tables","title":"Detour: Concerns with Managing Tables","text":"<p>There is a nice debate going on which questions the use of tables in ontology curation altogether. There are many nuances in this debate, but I want to stylise it here as two schools of thoughts (there are probably hundreds in between, but this makes it easier to follow): The one school (let's call them Tablosceptics) claims that using tables introduces a certain degree of fragility into the development process due to a number of factors, including:</p> <ol> <li>losing the immediateness of QC feedback; Table-based development, so the Tablosceptics, encourages lazy editing (adding stuff to a template and then not reviewing the consequence properly, which we will discuss in more depth later).</li> <li>losing track of the ID space (in a multi-table world, it becomes increasingly hard to manage IDs, making sure they are not double used etc) and 3) encouraging bad design (relying more on assertion than inference).</li> </ol> <p>They prefer to use tools like Protege that show the curator immediately the consequences of their actions, like reasoning errors (unintended equivalent classes, unsatisfiable classes and other unintended inferences). The Tablophile school of thought responds to these accusations in essence with \"tools\"; they say that tables are essentially a convenient matrix to input the data (which in turns opens ontology curation to a much wider range of people), and it is up to the tools to ensure that QC is run, hierarchies are being presented for review and weird ID space clashes are flagged up. Furthermore, they say, having a controlled input matrix will actually decrease the number of faulty annotations or axioms (which is evidenced by the large number of wrongful annotation assertions across OBO foundry ontologies I see every day as part of my work). At first sight, both template systems are affected equally by the war of the Tablosceptics and the Tablophile. Indeed, in my on practice, the ID space issue is really problematic when we manage 100s and more templates, and so far, I have not seen a nice and clear solution that ensures that no ID used twice unless it is so intended and respects ID spaces which are often semi-formally assigned to individual curators of an ontology.</p> <p>Generally in this course we do not want to take a 100% stance. The author of this page believes that the advantage of using tables and involving many more people in the development process outweighs any concerns, but tooling is required that can provide more immediate feedback when such tables such as the ones presented here are curated at scale.</p>"},{"location":"lesson/using-disease-and-phenotype-ontologies/","title":"Finding and using Disease and Phenotype Ontologies","text":""},{"location":"lesson/using-disease-and-phenotype-ontologies/#prerequisites","title":"Prerequisites","text":"<ul> <li>None</li> </ul>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#preparation","title":"Preparation","text":"<ul> <li>Review tutorial on Ontology Term Use</li> </ul>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course?","text":"<p>Description: An introduction to the landscape of disease and phenotype terminologies and ontologies, and how they can be used to add value to your analysis.</p>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#learning-objectives","title":"Learning objectives","text":"<ul> <li>Become aware of the major disease and phenotype ontologies that are available</li> <li>Be able to decide which phenotype or disease ontology to use for different use cases</li> <li>Understand how to leverage disease and phenotype ontologies for advanced data analytics</li> <li>Have a basic understanding of how to integrate other data</li> </ul>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#tutorials","title":"Tutorials","text":"<ul> <li>Video from Disease and Phenotypes c-path lesson 2021-06-16</li> </ul>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#additional-materials-and-resources","title":"Additional materials and resources","text":""},{"location":"lesson/using-disease-and-phenotype-ontologies/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> <li>Nico Matentzoglu</li> </ul>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#major-disease-and-phenotype-ontologies-that-are-available","title":"Major disease and phenotype ontologies that are available","text":"<p>A landscape analysis of major disease and phenotype ontologies that are currently available is here (also available in Zenodo here).</p> <p></p>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#decide-which-phenotype-or-disease-ontology-to-use-for-different-use-cases","title":"Decide which phenotype or disease ontology to use for different use cases","text":"<p>Different ontologies are built for different purposes and were created for various reasons. For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis.</p> <p>The unified phenotype ontology (uPheno) aggregates species-specific phenotype ontologies into a unified resource. Several species-specific phenotype ontologies exist, such as the Human Phenotype Ontology, Mammalian Phenotype Ontology (http://www.informatics.jax.org/searches/MP_form.shtml), and many more.</p> <p>Similarly to the phenotype ontologies, there are many disease ontologies that exist that are specific to certain areas of diseases, such as infectious diseases (e.g. Infectious Disease Ontology), cancer (e.g. National Cancer Institute Thesaurus), rare diseases (e.g. Orphanet), etc.</p> <p>In addition, there are several more general disease ontologies, such as the Mondo Disease Ontology, the Human Disease Ontology (DO), SNOMED, etc.</p> <p>Different disease ontologies may be built for different purposes; for example, ontologies like Mondo and DO are intended to be used for classifying data, and downstream computational analyses. Some terminologies are used for indexing purposes, such as the International classification of Diseases (ICD). ICD-11 is intended for indexing medical encounters for the purposes of billing and coding. Some of the disease ontologies listed on the landscape contain terms that define diseases, such as Ontology for General Medical Sciences (OGMS) are upper-level ontologies and are intended for integration with other ontologies.</p> <p>When deciding on which phenotype or disease ontology to use, some things to consider:</p> <ul> <li>Do you need a more specific ontology, such as a species-specific ontology, or do you need a more general ontology that is cross-species or covers more aspects of diseases?</li> <li>Is the ontology open and free to use?</li> <li>Does the description of the ontology describe its intended use? For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis.</li> <li>Is the ontology actively maintained?</li> <li>Does the ontology contain the terms you need? If not, is there a mechanism to request changes or add new terms? Are the ontology developers responsive to change requests on their tracker?</li> <li>Is the ontology widely used by the community? You can check things like active contributors on GitHub, usages described on the OBO Foundry page (for example http://obofoundry.org/ontology/mondo.html), published papers, and/or citations.</li> </ul> <p></p>"},{"location":"lesson/using-disease-and-phenotype-ontologies/#understand-how-to-leverage-disease-and-phenotype-ontologies-for-advanced-data-analytics","title":"Understand how to leverage disease and phenotype ontologies for advanced data analytics","text":""},{"location":"lesson/using-disease-and-phenotype-ontologies/#how-to-integrate-other-data","title":"How to integrate other data","text":""},{"location":"lesson/vocabularies-for-bioinformatics/","title":"Vocabularies for bioinformatics","text":""},{"location":"lesson/vocabularies-for-bioinformatics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of biology is required for this course. No particular bioinformatics competencies are required to take this course.</li> </ul>"},{"location":"lesson/vocabularies-for-bioinformatics/#preparation","title":"Preparation","text":"<ul> <li>None</li> </ul>"},{"location":"lesson/vocabularies-for-bioinformatics/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: This is an e-learning course from SIB Swiss Institute of Bioinformatics.</p>"},{"location":"lesson/vocabularies-for-bioinformatics/#learning-objectives","title":"Learning objectives","text":""},{"location":"lesson/vocabularies-for-bioinformatics/#competencies","title":"Competencies","text":"<p>At the end of the course, participants should be able to:</p> <ul> <li>Describe different types of vocabularies in bioinformatics</li> <li>Understand why vocabularies are essential to data interoperability</li> </ul>"},{"location":"lesson/vocabularies-for-bioinformatics/#learning-outcomes","title":"Learning outcomes","text":"<p>At the end of the course, the participants are expected to:</p> <ul> <li>List the different types of vocabularies</li> <li>Give examples of commonly used vocabularies</li> <li>Describe uses of vocabularies</li> <li>Find vocabularies</li> </ul>"},{"location":"lesson/vocabularies-for-bioinformatics/#tutorials","title":"Tutorials","text":"<ul> <li>e-learning course materials are available here</li> </ul>"},{"location":"lesson/vocabularies-for-bioinformatics/#contributors","title":"Contributors","text":"<ul> <li>Monique Zahn, SIB</li> </ul>"},{"location":"lesson/vocabularies-for-bioinformatics/#additional-information","title":"Additional information","text":"<p>You are welcome to register to the SIB courses mailing list to be informed of all future courses and workshops, as well as all important deadlines using the form here.</p> <p>Please note that participation in SIB courses is subject to their general conditions.</p> <p>SIB abides by the ELIXIR Code of Conduct. Participants of SIB courses are also required to abide by the same code.</p> <p>For more information, please contact training@sib.swiss.</p>"},{"location":"pathways/ai-powered-semantic-engineer/","title":"AI-powered semantic engineering","text":"<p>AI-powered semantic engineering is a pathways to help using AI more effectively.</p> <p>This is a placeholder for a comprehensive Getting Started Guide at https://ai4curation.io/aidocs/ (follow issue here). https://ai4curation.io/aidocs/ complements OBO Academy and is developed by people deeply involved in both resources. We decided a separate resource was needed due to the large amount of new and experimental materials emerging in the era of AI.</p>"},{"location":"pathways/ai-powered-semantic-engineer/#training-materials","title":"Training materials","text":""},{"location":"pathways/ai-powered-semantic-engineer/#efficient-biocuration-and-bioinformatics-with-claude-code-part-1","title":"Efficient biocuration and bioinformatics with Claude Code: Part 1","text":"<p>October 14, 2025  Host: Christopher Tabone Recording: Efficient Biocuration and Bioinformatics with Claude Code: Part 1 Materials: https://oboacademy.github.io/obook/tutorial/claude-code-getting-started/</p> <ul> <li>Join us for Part 2 on December 9, 2025! Learn more about how to join here.</li> </ul>"},{"location":"pathways/ai-powered-semantic-engineer/#generating-sparql-queries-with-chatgpt","title":"Generating SPARQL queries with ChatGPT","text":"<p>Join us November 11, 2025 (8am PT, 11am ET, 5pm CET) for this session hosted by Trish Whetzel.  Learn more about how to join here.</p>"},{"location":"pathways/ai-powered-semantic-engineer/#using-ai-coding-tools-for-ontology-developers","title":"Using AI coding tools for ontology developers","text":"<p>June 9, 2025 Host: Chris Mungall Recording: Using AI Coding Apps for Ontology Development Slides: Using AI coding apps for ontology development GitHub repository:  https://github.com/ai4curation/obi-tutorial </p>"},{"location":"pathways/ai-powered-semantic-engineer/#introduction-to-developing-agentic-workflows-for-semantic-engineers","title":"Introduction to developing agentic workflows for semantic engineers","text":"<p>April 8, 2025 Host: Chris Mungall Recording: Introduction to developing agentic workflows for semantic engineers GitHub repository: https://github.com/ai4curation/agent-tutorial </p>"},{"location":"pathways/ai-powered-semantic-engineer/#ontogpt-extract-structured-information-from-text-with-llms-instruction-prompts-and-ontology-based-grounding","title":"OntoGPT: extract structured information from text with LLMs, instruction prompts, and ontology-based grounding","text":"<p>October 15, 2024 Host: Harry Caufield  Recording OntoGPT: Hands-on Workshop GitHub repository: https://github.com/monarch-initiative/ontogpt  Publication: http://dx.doi.org/10.1093/bioinformatics/btae104 </p>"},{"location":"pathways/ai-powered-semantic-engineer/#structures-and-instructors-applying-the-strengths-of-large-language-models-to-biomedical-informatics","title":"Structures and Instructors: applying the strengths of large language models to biomedical informatics","text":"<p>June 25, 2024 Host: Harry Caufield  Recording: Applying the Strengths of Large Language Models to Biomedical Informatics</p>"},{"location":"pathways/ai-powered-semantic-engineer/#ai-assisted-ontology-editing-workflows-generating-augmenting-and-validation","title":"AI-assisted ontology editing workflows: generating, augmenting, and validation","text":"<p>April 16, 2024 &amp; March 5, 2024 Host: Chris Mungall  Recordings:  AI-assisted ontology editing workflows, Part 1 AI-assisted ontology editing workflows Part 2: Validation Slides: AI-guided-curation-workflows </p>"},{"location":"pathways/early-career-data-scientist/","title":"Early Career Data Scientist","text":"<p>Description: A collection of videos, tutorials, training materials, and exercises targeted towards any entry-level, early-career trainee interested in learning basic skills in data science.</p> <p>Preparation: no advance preparation is required.</p>"},{"location":"pathways/early-career-data-scientist/#1-data-science-ethics","title":"1. Data Science Ethics","text":""},{"location":"pathways/early-career-data-scientist/#videos","title":"Videos","text":"<p>6 videos available here</p>"},{"location":"pathways/early-career-data-scientist/#2-overview-what-is-data-science","title":"2. Overview: What is Data Science","text":""},{"location":"pathways/early-career-data-scientist/#videos_1","title":"Videos","text":"<ol> <li>IBM OpenDS4All What is Data Science? with Yucen Wang - Part I </li> <li>IBM OpenDS4All What is Data Science? with Yucen Wang - Part II </li> </ol>"},{"location":"pathways/early-career-data-scientist/#3-understand-and-appreciate-open-and-fair-data","title":"3. Understand and Appreciate Open and FAIR Data","text":""},{"location":"pathways/early-career-data-scientist/#article-to-read","title":"Article to read","text":"<ol> <li>The FAIR Guiding Principles for scientific data management and stewardship</li> </ol>"},{"location":"pathways/early-career-data-scientist/#exercises","title":"Exercises","text":"<ol> <li>Create an ORCID</li> <li>Create wikidata entry about yourself and link to other projects if applicable</li> <li>Share past work on FigShare/Zenodo, etc</li> </ol>"},{"location":"pathways/early-career-data-scientist/#4-learn-github","title":"4. Learn GitHub","text":""},{"location":"pathways/early-career-data-scientist/#getting-started","title":"Getting started","text":"<ol> <li>Create a GitHub account, see https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-account</li> <li>Download and install GitHub Desktop</li> </ol>"},{"location":"pathways/early-career-data-scientist/#tutorials","title":"Tutorials","text":""},{"location":"pathways/early-career-data-scientist/#introduction-to-github","title":"Introduction to GitHub","text":"<ol> <li>GitHub getting started guide</li> <li>Git 101: Git and GitHub for Beginners</li> <li>GitHub fundamentals</li> </ol>"},{"location":"pathways/early-career-data-scientist/#github-issues","title":"GitHub Issues","text":"<ol> <li>Learn Markdown syntax</li> <li>GitHub issues</li> <li>About issues</li> <li>Intro to managing and tracking issues in GitHub</li> </ol>"},{"location":"pathways/early-career-data-scientist/#exercises_1","title":"Exercises","text":"<ol> <li>Help improve this pathway! Make edits to this OBO Academy page and make a pull request. (For example, find typos to fix, add or revise content to this document, etc.)</li> <li>Create a GitHub website by forking this repository: https://github.com/laderast/academic_site_workshop</li> </ol>"},{"location":"pathways/early-career-data-scientist/#5-learn-command-line","title":"5. Learn command line","text":""},{"location":"pathways/early-career-data-scientist/#tutorials_1","title":"Tutorials","text":"<p>Note: for the tutorials below PC users need to install ODK (instructions are linked from the tutorial)  Alternatively, PC users can download Git Bash</p> <ol> <li>Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 1</li> <li>Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 2</li> </ol>"},{"location":"pathways/early-career-data-scientist/#6-introduction-to-ontologies","title":"6. Introduction to Ontologies","text":""},{"location":"pathways/early-career-data-scientist/#articles-to-read","title":"Articles to read","text":"<ol> <li>Ontology 101 by D. McGuiness</li> <li>Ontological Annotation of Data</li> </ol>"},{"location":"pathways/early-career-data-scientist/#videos_2","title":"Videos","text":"<ol> <li>An Introduction to Ontologies by Mark Musen, Stanford University (~15 min)</li> <li>Introduction to Biomedical Ontologies #1:  What is an Ontology?, by Jennifer Smith, Rat Genome Database (~15 min)</li> <li>Using ontologies to standardize rare disease data collection, by Nicole Vasilevsky, C-Path (1 hr)</li> </ol>"},{"location":"pathways/early-career-data-scientist/#tutorials_2","title":"Tutorials","text":"<ol> <li>Introduction to ontologies</li> <li>Ontology fundamentals</li> <li>Contributing to ontologies</li> </ol>"},{"location":"pathways/early-career-data-scientist/#7-basic-data-management","title":"7. Basic Data Management","text":""},{"location":"pathways/early-career-data-scientist/#videos_3","title":"Videos","text":"<ol> <li>Data Preparation and Planning</li> <li>https://dmice.ohsu.edu/bd2k/demo/BDK12-2/presentation_html5.html</li> <li>https://dmice.ohsu.edu/bd2k/demo/BDK12-3/presentation_html5.html</li> <li>Data sharing snafu: Data Sharing and Management Snafu in 3 Short Acts</li> </ol>"},{"location":"pathways/early-career-data-scientist/#article-to-read_1","title":"Article to read","text":"<ol> <li>10 Simple Rules for the Care and Feeding of Scientific Data</li> <li>Big Data: The Future of Biocuration</li> <li>A primer on data sharing</li> <li>Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data</li> <li>Reproducible and reusable research: Are journal data sharing policies meeting the mark?</li> </ol>"},{"location":"pathways/early-career-data-scientist/#exercise","title":"Exercise","text":"<p>Data Management 101</p>"},{"location":"pathways/early-career-data-scientist/#8-preparing-your-cv-and-tracking-your-contributions","title":"8. Preparing your CV and Tracking Your Contributions","text":""},{"location":"pathways/early-career-data-scientist/#video","title":"Video","text":"<p>Workshop from Biocuration: Workshop - Careers In Biocuration</p>"},{"location":"pathways/early-career-data-scientist/#articles","title":"Articles","text":"<p>Is authorship sufficient for today\u2019s collaborative research? A call for contributor roles</p>"},{"location":"pathways/early-career-data-scientist/#9-effective-communication-in-data-science","title":"9. Effective Communication in Data Science","text":""},{"location":"pathways/early-career-data-scientist/#tutorials_3","title":"Tutorials","text":"<p>Survival strategies for team communication</p>"},{"location":"pathways/ontology-contributor/","title":"Ontology Contributor Pathway","text":"<p>Description: These guidelines are developed for anyone interested in contributing to ontologies to guide how to contribute to OBO Foundry ontologies.</p>"},{"location":"pathways/ontology-contributor/#why-should-you-contribute-to-ontology-development-efforts","title":"Why should you contribute to ontology development efforts?","text":"<p>Ontologies are routinely used for data standardization and in analytical analysis, but the ontologies themselves are under constant revisions and iterative development. Building ontologies is a community effort, and we need expertise from different areas:</p> <ul> <li>Technical expertise</li> <li>Domain expertise</li> <li>User experiences</li> </ul> <p>The OBO foundry ontologies are open, meaning anyone can access them and contribute to them. The types of contributions may include reporting issues, identifying bugs, making requests for new terms or changes, and you can also contribute directly to the ontology itself- if you are familiar with ontology editing workflows, you can download our ontologies and make edits on a branch and make a pull request in GitHub.</p>"},{"location":"pathways/ontology-contributor/#providing-feedback-to-an-ontology","title":"Providing Feedback to an Ontology","text":"<ul> <li>The preferred mechanism for feedback for almost all OBO Foundry ontologies is via the ontology's GitHub issue tracker.</li> <li>To find the correct GitHub issue tracker, go to the OBO Foundry website website and search for a particular ontology.</li> <li>For example, click on go (Gene Ontology), and you should see a link to the tracker (https://github.com/geneontology/go-ontology/issues/)</li> <li>The OBO Metadata also has a link to a contact but this is generally for OBO administrative purposes and should not be used for general questions, new term requests, or general feedback</li> <li>Some ontologies may have other means of engaging their use community, and these should all be listed in a standard way on the OBO page. This includes mailing lists, slack groups, and Twitter or other social media accounts.</li> </ul>"},{"location":"pathways/ontology-contributor/#getting-started-with-github","title":"Getting Started with GitHub","text":"<ol> <li>Open a free account at https://github.com/.</li> <li>GitHub Fundamentals for OBO Engineers. Provides an introduction to GitHub including:</li> <li>how to get started</li> <li>an overview of the organization of GitHub</li> <li>an introduction to Markdown (the simple markup language used in GitHub to format text, like bold or italics)</li> <li>types of content that can be added to GitHub (e.g., you can attach a screenshot to an issue)</li> <li>Intro to managing and tracking issues in GitHub. This tutorial walks you through creating issues in GitHub.</li> </ol>"},{"location":"pathways/ontology-contributor/#using-ontologies-and-ontology-terms","title":"Using Ontologies and Ontology Terms","text":"<ul> <li>Why ontologies are useful</li> <li>Finding good ontologies</li> <li>Ontology repositories</li> <li>Assessing ontologies for use</li> </ul>"},{"location":"pathways/ontology-contributor/#contributing-to-ontologies","title":"Contributing to Ontologies","text":"<p>Community feedback is welcome for all open OBO Foundry ontologies. Feedback is often provided in the form of:</p> <ul> <li>New terms requests</li> <li>Add/revise synonyms, definitions</li> <li>Reclassify a term</li> <li>Report a bug</li> <li>etc.</li> </ul>"},{"location":"pathways/ontology-contributor/#ways-to-provide-feedback","title":"Ways to provide feedback","text":"<ul> <li>Create a new issue on a GitHub issue tracker</li> <li>See the lesson on Identifying missing terms</li> <li>See the lesson on Making term requests to existing ontologies</li> <li>Join the discussion: Comment on tickets or the discussion board</li> <li>Join the conversation: Attend ontology calls (many ontology developer groups have recurring calls open to the community. Contact the ontology owner to request information about calls.)</li> <li>Edit the ontology file: make changes on a branch and do a pull request (more advanced)</li> <li>See the lesson on Contributing to OBO ontologies</li> </ul>"},{"location":"pathways/ontology-contributor/#relevant-presentations","title":"Relevant Presentations","text":"<ul> <li>Using ontologies to standardize rare disease data collection - By Nicole Vasilevsky, Presentation on June 15, 2022</li> <li>RDCA-DAP: Searchability and Standardized Ontologies- By Ramona Walls, Recording from October 13, 2020</li> </ul>"},{"location":"pathways/ontology-curator-go-style/","title":"Ontology Curator Pathway: GO-Style","text":"<p>Note: There is no one single accepted way of doing ontology curation in the OBO-World, see here. This guide reflects the practice of the GO-style ontology curation, as it is used by GO, Uberon, CL, PATO and others.</p> <p>Note: Work on this document is still in progress, items that are not linked are currently being worked on.</p>"},{"location":"pathways/ontology-curator-go-style/#getting-set-up","title":"Getting Set-up","text":"<ol> <li>Download and install GitHub Desktop</li> <li>Download and install Protege. See instructions on how to set up Protege here</li> <li>Install ELK reasoner in protege</li> <li>Setting up your ID range</li> <li>Setting up ODK</li> </ol>"},{"location":"pathways/ontology-curator-go-style/#learning","title":"Learning","text":""},{"location":"pathways/ontology-curator-go-style/#learning-about-ontologies","title":"Learning about Ontologies","text":"<ol> <li>Fundamentals of Ontologies</li> </ol>"},{"location":"pathways/ontology-curator-go-style/#learning-git-and-github","title":"Learning Git and GitHub","text":"<ol> <li>Fundamentals of GitHub</li> <li>Github issues</li> <li>Cloning a Repo</li> <li>Creating pull requests</li> <li>Introduction into the command line</li> </ol>"},{"location":"pathways/ontology-curator-go-style/#learning-protege","title":"Learning protege","text":"<ol> <li>Protege interface</li> <li>Browse and Search</li> <li>DL query</li> <li>Editing Terms</li> <li>Creating New Terms</li> <li>Adding disjointness</li> <li>Logical axiomatization of classes &amp; use of reasoning</li> <li>Ontology Relations</li> <li>Updating Imports with ODK</li> <li>OBO-style term annotation</li> <li>Obsoleting terms</li> <li>Merging terms</li> </ol>"},{"location":"pathways/ontology-curator-go-style/#general-mindset","title":"General mindset","text":"<ol> <li>How to be a team open science player</li> </ol>"},{"location":"pathways/ontology-curator-go-style/#ontology-curator-how-to-collection","title":"Ontology Curator How To Collection","text":"<p>This section is a non-ordered collection of how to documents that a curator might needs</p> <ul> <li>Adding taxon restrictions</li> <li>Changing files in pull requests</li> <li>Cloning a repo</li> <li>Creating new terms</li> <li>Daily Workflow</li> <li>Fixing Conflicts</li> <li>Creating a github fork</li> <li>Creating a github pull request</li> <li>Setting up your ID range</li> <li>Setting up ELK reasoner in protege</li> <li>Obsoleting terms</li> <li>Merging terms</li> <li>Creating slims</li> <li>Setting up ODK</li> <li>Switching ontologies</li> <li>Updating Imports with ODK</li> </ul>"},{"location":"pathways/ontology-curator-obi-style/","title":"Ontology Curator Pathway: OBI-style","text":"<p>Note: There is no one single accepted way of doing ontology curation in the OBO-World, see here. This guide reflects the practice of the OBI-style ontology curation, as it is used by OBI, IAO and others.</p>"},{"location":"pathways/ontology-curator-obi-style/#getting-set-up","title":"Getting Set-up","text":""},{"location":"pathways/ontology-curator-obi-style/#learning","title":"Learning","text":""},{"location":"pathways/ontology-curator-obi-style/#learning-git-and-github","title":"Learning Git and GitHub","text":"<ol> <li>Fundamentals of GitHub</li> <li>Github issues</li> <li>Cloning a Repo</li> <li>Creating pull requests</li> </ol>"},{"location":"pathways/ontology-curator-obi-style/#learning-how-to-edit-ontologies","title":"Learning how to edit ontologies","text":""},{"location":"pathways/ontology-curator-obi-style/#ontology-curator-how-to-collection","title":"Ontology Curator How To Collection","text":""},{"location":"pathways/ontology-curator/","title":"Ontology Curator Pathway","text":"<p>There is no one single accepted methodology for building ontologies in the OBO-World. We can distinguish at least two major schools of ontology curation</p> <ul> <li>GO-style ontology curation</li> <li>OBI-style ontology curation</li> </ul> <p>Note that there are many more variants, probably as many as there are ontologies. Both schools differ only in how they curate their ontologies - the final product is always an ontology in accordance with OBO Principles. These are some of the main differences of the two schools:</p> GO-style OBI-style Edit format Historically developed in OBO format Developed in an OWL format Annotation properties Many annotation properties from the oboInOwl namespace, for example for synonyms and provenance. Many annotation properties from the IAO namespace. Upper Ontology Hesitant alignment with BFO, often uncommitted. Strong alignment with BFO. Logic Tend do be simple existential restrictions (<code>some</code>), ontologies in OWL 2 EL. No class expression nesting. Simple logical definition patterns geared towards automating classification Tend to use a lot more expressive logic, including <code>only</code> and <code>not</code>. Class expression nesting can be more complex. Examples GO, Uberon, Mondo, HPO, PATO, CL, BSPO OBI, IAO, OGMS <p>There are a lot of processes happening that are bringing these schools together, sharing best practices (GitHub, documentation) and reconciling metadata conventions and annotation properties in the OBO Metadata Ontology (OMO). The Upper Level alignment is now done by members of both schools through the Core Ontology for Biology and Biomedicine (COB). While these processes are ongoing, we decided to curate separate pathways for both schools:</p> <ul> <li>Pathway for GO-style ontology curation</li> <li>Pathway for OBI-style ontology curation</li> </ul>"},{"location":"pathways/ontology-engineer/","title":"Ontology Engineer/Developer Pathway","text":""},{"location":"pathways/ontology-engineer/#getting-set-up","title":"Getting Set-up","text":"<ol> <li>Download and install GitHub Desktop</li> <li>Download and install Protege</li> <li>Install ELK reasoner in protege</li> <li>Setting up your ID range</li> <li>Setting up ODK</li> </ol>"},{"location":"pathways/ontology-engineer/#learning","title":"Learning","text":"<p>As a ontology engineer, it would be useful for you to know how curators work, as such, it would be useful to be familiar with all the concepts in the ontology curator pathways document. This pathways will however be focusing on the engineering side of things.</p>"},{"location":"pathways/ontology-engineer/#very-basics","title":"Very basics","text":"<ol> <li>Basic introduction to CLI 1</li> <li>Basic introduction to CLI 2</li> </ol>"},{"location":"pathways/ontology-engineer/#learning-git-and-github","title":"Learning Git and GitHub","text":"<ol> <li>Fundamentals of GitHub</li> <li>Github issues</li> <li>Cloning a Repo</li> <li>Creating pull requests</li> </ol>"},{"location":"pathways/ontology-engineer/#learning-ontology-engineering","title":"Learning ontology engineering","text":"<ol> <li>Setting up a ODK repository</li> <li>Developing an obo ontology</li> <li>Understanding product variants</li> <li>Dealing with large ontologies</li> <li>ROBOT tutorial pt. 1</li> <li>ROBOT tutorial pt. 2</li> <li>Templates</li> <li>Getting started with DOSDP templates</li> <li>DOSDP Templates Basic Tutorial</li> <li>Introduction to Managing DOSDP Templates in ODK</li> <li>Basics of SPARQL</li> <li>Setting up slims</li> </ol>"},{"location":"pathways/ontology-engineer/#ontology-engineer-how-to-collection","title":"Ontology Engineer How To Collection","text":"<p>This section is a non-ordered collection of how to documents that an engineer might need (this includes everything from the curators list as they may be pertinent knowledge to an engineer).</p> <ul> <li>Adding taxon restrictions</li> <li>Changing files in pull requests</li> <li>Cloning a repo</li> <li>Creating new terms</li> <li>Daily Curator Workflow</li> <li>Fixing Conflicts</li> <li>Creating a github fork</li> <li>Creating a github pull request</li> <li>Setting up your ID range</li> <li>Setting up ELK reasoner in protege</li> <li>Obsoleting terms</li> <li>Merging terms</li> <li>Creating slims</li> <li>Setting up ODK</li> <li>Switching ontologies</li> <li>Dealing with large ontologies</li> </ul>"},{"location":"pathways/pathways/","title":"Introduction to pathways","text":"<p>Pathways are materials from OBOOK in a linear fashion for the purpose of helping people in different roles finding the materials relevant to their work more easily. To browse through the pathways, look under the \"Pathways\" menu item.</p>"},{"location":"reference/base-specification/","title":"The Base File Specification (DRAFT)","text":"<p>The \"base file\" is a specific release flavour (variant) of an ontology. It reflects the intention of the ontology author for the official (publicly released) representation of the ontologies \"base entities\". \"Base entities\" are entities that are defined (\"owned\") by the ontology. The representation includes the intended public metadata (annotations), and classification (subClassOf hierarchy), including any statements where a base entity is the subject. The purpose of this document is to provide a detailed specification of what a base is, including a technical implementation which reflects best practice.</p> <p>Up until March 2023, the ontology \"base\" reflected the structure of the editors file with no instantiation of axioms inferred by a reasoner.</p> <p>The new base reflects the structure of the primary ontology release file (in the OBO world, the release variant that is available at <code>http://purl.obolibrary.org/obo/ontology.owl</code>), which often (but not always) corresponds to what is called the full release.</p>"},{"location":"reference/base-specification/#motivation","title":"Motivation","text":"<ul> <li>Dependency management</li> <li>Modular ontology development (refer to \"no-modification principle)\"</li> </ul> <p>The \"base file\" allows complete versions of ontologies to be released in a way that facilitates modular composition. This could be composition ahead of time (for example, building a KG or application ontology), or dynamic composition; a future version of OLS may allow seamless transition between ontologies, e.g. traversing from mondo to uberon to cl to go in a way that preserves the complete structure of each ontology.</p> <p>It is also built for safety - it is guaranteed that the intentions of contributing ontologies are respected, and hierarchies are not broken.</p>"},{"location":"reference/base-specification/#technical-specification","title":"Technical specification","text":"<pre><code>Note: The following section is _normative_.\n</code></pre> <p>As per design, a base file is not defined in terms of what it contains, but by some constraints on what it MUST NOT contain. A \"base file\" MUST NOT:</p> <ol> <li>contain any axiom where the subject is not a base entity. Positively formulated, it may only contain axioms where the subject is a base entity.</li> </ol> <pre><code>Note: The following section is _NOT normative_.\n</code></pre> <p>For practical purposes, we have defined a non-normative recommendation for how base files should be defined. Non-normative means: the ontology developer may decide to adopt, modify or alter this recommendation in any way they want, as long as the normative constraints above are not violated.</p> <p>A \"base file\" SHOULD:</p> <ul> <li>include all \"base entities\", including deprecated ones</li> <li>include all standard and any number of user defined annotations on \"base entities\". Standard annotations are defined by the Ontology Metatada Ontology (OMO) and include labels (rdfs:label), deprecation assertions (owl:deprecated) and definitions (IAO:0000115). User-defined annotations are all annotations that are non-standard and are intended for inclusion in the public release.</li> <li>include a non-redundant set of reasoner-inferred axioms, including</li> <li>inferred <code>rdfs:subClassOf</code> axioms between \"base entities\"</li> <li>inferred <code>rdfs:subClassOf</code> axioms from \"base entities\" to non-base entity parents</li> <li>inferred \"existential restrictions\", axioms of the form \"<code>:A rdfs:subClassOf :R some :B</code>\" where <code>:A</code> is a \"base entity\", and <code>:R</code> is an object property as defined by the ontology developer responsible for defining the release.</li> <li>\"non-redundant\" means that the <code>rdfs:subClassOf</code> sub-graph of the ontology corresponds to the \"transitive reduct\". In laypersons terms this means, roughly, that if a <code>rdfs:subClassOf</code> axiom is already implied by a path of <code>rdfs:subClassOf</code> axioms, that axiom is considered \"redundant\". (For example: given <code>:A rdfs:subClassOf :B</code>, <code>:B rdfs:subClassOf :C</code>, <code>:A rdfs:subClassOf :C</code>, the latter, <code>:A rdfs:subClassOf :C</code> is redundant because it is already implied by tge path through the former two axioms.)</li> <li>break down all <code>owl:equivalentClass</code> axioms into <code>rdfs:subClassOf</code> axioms where the subject is a \"base entity\"</li> </ul>"},{"location":"reference/base-specification/#implementation","title":"Implementation","text":"<p>This is the current implementation of the recommended base technical specification above. The full operational definition of the process for generating the \"base file\" is:</p> <pre><code># base: A version of the ontology that does not include any externally imported axioms.\n$(ONT)-base.owl:\n    robot merge -i $(SRC) -i $(OTHER_SRC) -i $(ALL_IMPORTS) \\\n    reason --reasoner ELK --equivalent-classes-allowed asserted-only --exclude-tautologies structural \\\n    materialize $(patsub %, --term %, $(OBJECT_PROPERTIES_OF_INTEREST)) \\\n    relax \\\n    reduce -r ELK \\\n    remove --base-iri $(URIBASE)/OBA --axioms external --preserve-structure false --trim false \\\n    {STANDARD_COMMANDS}\n    --output $@\n</code></pre> <p>This is a breakdown of the sub-processes:</p> <ul> <li><code>robot merge -i $(SRC) -i $(OTHER_SRC) -i $(ALL_IMPORTS)</code>:  Merge the ontology with all its dependencies together.</li> <li><code>reason --reasoner {REASONER} --equivalent-classes-allowed asserted-only --exclude-tautologies structural</code>: Add all inferred <code>rdfs:subClassOf</code> axioms.</li> <li><code>materialize $(patsub %, --term %, $(OBJECT_PROPERTIES_OF_INTEREST))</code>: Add all inferred subclass of R some B restrictions, where R is an \"object property of interest\".</li> <li><code>relax</code>: break down all <code>owl:equivalentClass</code> axioms into <code>rdfs:subClassOf</code> axioms where the subject is a \"base entity\"</li> <li><code>reduce -r {REASONER}</code>: remove all redundant <code>rdfs:subClassOf</code> axioms</li> <li><code>remove --base-iri {BASEIRI} --axioms external --preserve-structure false --trim false</code>: remove all axioms where the subject is not a \"base entity\"</li> </ul>"},{"location":"reference/base-specification/#how-to-determine-the-subject-of-an-axiom","title":"How to determine the \"subject\" of an axiom","text":"<p>Instead of exhaustively defining the idea of subject (extensional definition), we define it by inclusion (intentional definition):</p> <p>Given:</p> <ul> <li><code>?x</code> a base entity</li> <li><code>?y</code> an arbitrary expression (like a class or property name, or class expression)</li> <li><code>?property</code> an instance of <code>owl:AnnotationProperty</code>, <code>owl:ObjectProperty</code> or <code>owl:DataProperty</code></li> <li><code>?annotation_property</code> an instance of <code>owl:AnnotationProperty</code></li> <li><code>?object_property</code> an instance of <code>owl:ObjectProperty</code></li> <li><code>?data_property</code> an instance of <code>owl:DataProperty</code></li> </ul> <p>The following axioms are considered to having <code>?x</code> as a subject (aka \"base axioms\"):</p> <ul> <li>Assertions of the form <code>?x  ?property ?value</code> (includes Annotation, Object and DataProperty Assertions)</li> <li>SubClassOf axioms of the form <code>?x  rdfs:subClassOf ?y</code></li> <li>Equivalent class axioms of the form <code>?x  owl:equivalentClass ?y</code></li> <li>Equivalent class axioms of the form <code>?y  owl:equivalentClass ?x</code></li> <li>An assertion of a characteristic of the form <code>Characteristic(?x)</code>, like <code>TransitiveCharacteristic(?x)</code> etc.</li> <li>An assertion of the form <code>ObjectPropertyDomain(?x, ?y)</code></li> <li>An assertion of the form <code>ObjectPropertyRange(?x, ?y)</code></li> </ul> <p>Examples of axioms that do not have <code>?x</code> as a subject:</p> <ul> <li>SubClassOf axioms of the form <code>?y rdfs:subClassOf ?x</code></li> <li>An assertion of the form <code>ObjectPropertyDomain(?y, ?x)</code> (where <code>?x</code> is the domain of an object property <code>?y</code>)</li> <li>An assertion of the form <code>ObjectPropertyRange(?y, ?x)</code> (where <code>?x</code> is the range of an object property <code>?y</code>)</li> <li>A GCI axiom of the form <code>SomeValuesFrom(?object_property, ?x) rdfs:subClassOf ?y</code></li> <li>A GCI axiom of the form <code>?y rdfs:subClassOf SomeValuesFrom(?object_property, ?x)</code></li> <li>A GCI axiom of the form <code>SomeValuesFrom(?x, ?y) rdfs:subClassOf ?z</code></li> <li>IMPORTANT NOTE: Handling of GCIs is still work in progress. It is not straight forward to define which is the defining entity. For now, unless all entities on the left are in the ontology for which we are making a base, GCIs are never in the base. This is being reviewed and may change in the future.</li> </ul> <p></p>"},{"location":"reference/base-specification/#the-no-modification-principle","title":"The No-Modification Principle","text":"<p>We start with a proposed principle that applies to databases (triplestores and other databases) that expose one or more ontologies and that also applies to derived ontologies:</p> <p>Each ontology\u2019s released SubClassOf hierarchy (across the authority signature) should be preserved. Specifically, the original SubClassOf hierarchy SHOULD be preserved in the majority of cases. Modifications to the SubClassOf hierarchy MUST NOT be re-released.</p> <p>Here modifications means:</p> <ul> <li>Removing a SubClassOf axiom</li> <li>Adding a new SubClassOf axiom</li> </ul> <p>This includes is-a (SubClassOf between named classes) and SubClassOf where the parent is an expression (including existentials, aka relationships)</p> <p>Except under the following conditions:</p> <ul> <li>An ontology subset is being made</li> <li>This SHOULD be done by the main provider EXCEPT for import modules</li> <li>When an ontology subset is made it should be both complete and correct with respect to the original SubClassOf graph. Here correct means that every axiom in the subset must be entailed by the parent ontology. Complete means roughly that all axioms from the parent ontology must be in the subset, unless an entity in the signature of the axiom is not in the subset signature (including object properties). Thus if a subset signature has classes C1, \u2026, Cn and OP1, then any entailed Cx SubClassOf OP1 some Cy (where x and y are in 1..n) should be included, but Cx SubClassOf OP2 some Cy would be excluded</li> <li>Axioms may be added under conditions recognized where injections are allowed</li> <li>Any such exceptions MUST be to link to another ontology (bridging axioms)</li> <li>For example, a ssAO to a snAO</li> <li>The modifications are explicitly sanctioned by the source ontology providers</li> </ul> <p>Here \"original\" refers to the source ontology and all immediate products (for example, uberon and uberon-basic). Re-released means made available by alternate downloads or queries</p>"},{"location":"reference/catalog-xml/","title":"XML Catalogs demystified","text":"<p>The XML catalog is a file that tells tools like <code>robot</code> and Prot\u00e9g\u00e9 where to find locally-stored OWL files when processing imports. It is a relatively simple, but important, piece of infrastructure that is vital for managing ontology imports. </p> <p>Here is a sample line from an XML catalog:</p> <p><code>&lt;uri name=\"https://w3id.org/elmo/elmo/imports/ro_import.owl\" uri=\"imports/ro_import.owl\"/&gt;</code></p> <p>This tells the tool that if it sees the URI <code>https://w3id.org/elmo/elmo/imports/ro_import.owl</code>, replace that with <code>imports/ro_import.owl</code> which will access the local file. It is essentially redirecting internal requests from <code>https://w3id.org/elmo/elmo/imports/ro_import.owl</code> to the local file.</p> <p>An XML catalog is used in the ODK to manage imports so that the tool does not have to constantly re-download imported ontologies. </p> <p>Here are some additional resources about XML catalogs:</p> <ul> <li><code>robot</code> XML Catalogs documentation</li> <li>Prot\u00e9g\u00e9 and XML Catalogs</li> </ul>"},{"location":"reference/chatgpt-prompts-for-ontology-development/","title":"Leveraging ChatGPT for ontology curation","text":""},{"location":"reference/chatgpt-prompts-for-ontology-development/#effective-chatgpt-prompts-for-ontology-development","title":"Effective ChatGPT prompts for ontology development","text":"<p>For a basic tutorial on how to leverage ChatGPT for ontology development see here.</p>"},{"location":"reference/chatgpt-prompts-for-ontology-development/#act-as-a-mapping-api","title":"Act as a mapping API","text":"<p>I want you to act as a REST API, which takes natural language searches a an input and returns an SSSOM mapping in valid JSON in a codeblock, no comments,  no additional text. An example of a valid mapping is </p> <p>{       \"subject_id\": \"a:something\",       \"predicate_id\": \"rdfs:subClassOf\",       \"object_id\": \"b:something\",       \"mapping_justification\": \"semapv:LexicalMatching\",       \"subject_label\": \"XXXXX\",       \"subject_category\": \"biolink:AnatomicalEntity\",       \"object_label\": \"xxxxxx\",       \"object_category\": \"biolink:AnatomicalEntity\",       \"subject_source\": \"a:example\",       \"object_source\": \"b:example\",       \"mapping_tool\": \"rdf_matcher\",       \"confidence\": 0.8,       \"subject_match_field\": [         \"rdfs:label\"       ],       \"object_match_field\": [         \"rdfs:label\"       ],       \"match_string\": [         \"xxxxx\"       ],       \"comment\": \"mock data\"     }</p> <p>As a first task, I want you to return a suitable mapping for MONDO:0004975 in ICD 10 CM.</p>"},{"location":"reference/formatting-license/","title":"Formatting your ontology annotations correctly","text":"<p>The new OBO Foundry guidelines encourage the annotation of ontologies with an appropriately formatted description, title and license. Here are some examples that can be used as a guide to implement those in your ontology.</p> <p>Note: these examples purposefully do not include version information, this should not be manually added, instead it should be added by ROBOT as part of a pipeline. An ontology set up with the ODK will take care of all of this for you.</p>"},{"location":"reference/formatting-license/#rdfxml-example","title":"RDF/XML Example:","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;rdf:RDF xmlns=\"http://purl.obolibrary.org/obo/license.owl#\"\n     xml:base=\"http://purl.obolibrary.org/obo/license.owl\"\n     xmlns:dc=\"http://purl.org/dc/elements/1.1/\"\n     xmlns:owl=\"http://www.w3.org/2002/07/owl#\"\n     xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n     xmlns:xml=\"http://www.w3.org/XML/1998/namespace\"\n     xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n     xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"\n     xmlns:terms=\"http://purl.org/dc/terms/\"&gt;\n    &lt;owl:Ontology rdf:about=\"http://purl.obolibrary.org/obo/license.owl\"&gt;\n        &lt;dc:description rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;An integrated and fictional ontology for the description of abnormal tomato phenotypes.&lt;/dc:description&gt;\n        &lt;dc:title rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;Tomato Phenotype Ontology (TPO)&lt;/dc:title&gt;\n        &lt;terms:license rdf:resource=\"https://creativecommons.org/licenses/by/3.0/\"/&gt;\n    &lt;/owl:Ontology&gt;\n    &lt;owl:AnnotationProperty rdf:about=\"http://purl.org/dc/elements/1.1/description\"/&gt;\n    &lt;owl:AnnotationProperty rdf:about=\"http://purl.org/dc/elements/1.1/title\"/&gt;\n    &lt;owl:AnnotationProperty rdf:about=\"http://purl.org/dc/terms/license\"/&gt;\n&lt;/rdf:RDF&gt;\n</code></pre>"},{"location":"reference/formatting-license/#functional-syntax-example","title":"Functional Syntax Example:","text":"<pre><code>Prefix(:=&lt;http://purl.obolibrary.org/obo/license.owl#&gt;)\nPrefix(owl:=&lt;http://www.w3.org/2002/07/owl#&gt;)\nPrefix(rdf:=&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;)\nPrefix(xml:=&lt;http://www.w3.org/XML/1998/namespace&gt;)\nPrefix(xsd:=&lt;http://www.w3.org/2001/XMLSchema#&gt;)\nPrefix(rdfs:=&lt;http://www.w3.org/2000/01/rdf-schema#&gt;)\n\n\nOntology(&lt;http://purl.obolibrary.org/obo/license.owl&gt;\nAnnotation(&lt;http://purl.org/dc/elements/1.1/description&gt; \"An integrated and fictional ontology for the description of abnormal tomato phenotypes.\"^^xsd:string)\nAnnotation(&lt;http://purl.org/dc/elements/1.1/title&gt; \"Tomato Phenotype Ontology (TPO)\"^^xsd:string)\nAnnotation(&lt;http://purl.org/dc/terms/license&gt; &lt;https://creativecommons.org/licenses/by/3.0/&gt;)\n\n)\n</code></pre>"},{"location":"reference/formatting-license/#owlxml-example","title":"OWL/XML Example:","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;Ontology xmlns=\"http://www.w3.org/2002/07/owl#\"\n     xml:base=\"http://purl.obolibrary.org/obo/license.owl\"\n     xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n     xmlns:xml=\"http://www.w3.org/XML/1998/namespace\"\n     xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n     xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"\n     ontologyIRI=\"http://purl.obolibrary.org/obo/license.owl\"&gt;\n    &lt;Prefix name=\"\" IRI=\"http://purl.obolibrary.org/obo/license.owl#\"/&gt;\n    &lt;Prefix name=\"owl\" IRI=\"http://www.w3.org/2002/07/owl#\"/&gt;\n    &lt;Prefix name=\"rdf\" IRI=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"/&gt;\n    &lt;Prefix name=\"xml\" IRI=\"http://www.w3.org/XML/1998/namespace\"/&gt;\n    &lt;Prefix name=\"xsd\" IRI=\"http://www.w3.org/2001/XMLSchema#\"/&gt;\n    &lt;Prefix name=\"rdfs\" IRI=\"http://www.w3.org/2000/01/rdf-schema#\"/&gt;\n    &lt;Annotation&gt;\n        &lt;AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/description\"/&gt;\n        &lt;Literal&gt;An integrated and fictional ontology for the description of abnormal tomato phenotypes.&lt;/Literal&gt;\n    &lt;/Annotation&gt;\n    &lt;Annotation&gt;\n        &lt;AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/title\"/&gt;\n        &lt;Literal&gt;Tomato Phenotype Ontology (TPO)&lt;/Literal&gt;\n    &lt;/Annotation&gt;\n    &lt;Annotation&gt;\n        &lt;AnnotationProperty abbreviatedIRI=\"terms:license\"/&gt;\n        &lt;IRI&gt;https://creativecommons.org/licenses/by/3.0/&lt;/IRI&gt;\n    &lt;/Annotation&gt;\n    &lt;Declaration&gt;\n        &lt;AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/title\"/&gt;\n    &lt;/Declaration&gt;\n    &lt;Declaration&gt;\n        &lt;AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/description\"/&gt;\n    &lt;/Declaration&gt;\n    &lt;Declaration&gt;\n        &lt;AnnotationProperty IRI=\"http://purl.org/dc/terms/license\"/&gt;\n    &lt;/Declaration&gt;\n&lt;/Ontology&gt;\n</code></pre>"},{"location":"reference/formatting-license/#obo-example","title":"OBO Example:","text":"<pre><code>format-version: 1.2\nontology: license\nproperty_value: http://purl.org/dc/elements/1.1/description \"An integrated and fictional ontology for the description of abnormal tomato phenotypes.\" xsd:string\nproperty_value: http://purl.org/dc/elements/1.1/title \"Tomato Phenotype Ontology (TPO)\" xsd:string\nproperty_value: http://purl.org/dc/terms/license https://creativecommons.org/licenses/by/3.0/\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/","title":"Frequently used ODK commands","text":""},{"location":"reference/frequently-used-odk-commands/#updates-the-makefile-to-the-latest-odk","title":"Updates the Makefile to the latest ODK","text":"<pre><code>sh run.sh make update_repo\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#recreates-and-deploys-the-automated-documentation","title":"Recreates and deploys the automated documentation","text":"<pre><code>sh run.sh make update_docs\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#preparing-a-new-release","title":"Preparing a new release","text":"<pre><code>sh run.sh make prepare_release\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#refreshing-a-single-import","title":"Refreshing a single import","text":"<pre><code>sh run.sh make refresh-%\n</code></pre> <p>Example:</p> <pre><code>sh run.sh make refresh-chebi\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#refresh-all-imports","title":"Refresh all imports","text":"<pre><code>sh run.sh make refresh-imports\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#refresh-all-imports-excluding-large-ones","title":"Refresh all imports excluding large ones","text":"<pre><code>sh run.sh make refresh-imports-excluding-large\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#run-all-the-qc-checks","title":"Run all the QC checks","text":"<pre><code>sh run.sh make test\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#print-the-version-of-the-currently-installed-odk","title":"Print the version of the currently installed ODK","text":"<pre><code>sh run.sh make odkversion\n</code></pre>"},{"location":"reference/frequently-used-odk-commands/#checks-the-owl2-dl-profile-validity","title":"Checks the OWL2 DL profile validity","text":"<p>(of a specific file)</p> <pre><code>sh run.sh make validate_profile_%\n</code></pre> <p>Example:</p> <pre><code>sh run.sh make validate_profile_hp-edit.owl\n</code></pre>"},{"location":"reference/gh-actions-errors/","title":"Common Errors in GitHub actions","text":""},{"location":"reference/gh-actions-errors/#killed-running-out-of-memory","title":"<code>Killed</code>: Running out of memory","text":"<p>Running the same workflow several times simultaneously (e.g. if two PRs are submitted in a short time, and the second PR triggers the CI workflow while the CI workflow triggered by the first PR is still running) could lead to lack-of-memory situations because all concurrent workflows have to share a single memory limit.</p> <p>(Note: it isn't really clear with documentation of GitHub Actions on whether concurrent workflow runs share a single memory limit.)</p> <p>What could possibly be done is to forbid a given workflow from ever running as long as there is already a run of the same workflow ongoing, using the concurrency property.</p>"},{"location":"reference/git-faq/","title":"Git FAQs","text":"<p>This page aims to consolidate some tips and tricks that ontology editors have found useful in using <code>git</code>. It is not meant to be a tutorial of <code>git</code>, but rather as a page with tips that could help in certain specialised situations.</p>"},{"location":"reference/git-faq/#reverting-commits","title":"Reverting Commits","text":""},{"location":"reference/git-faq/#reverting-particular-files-back-to-master-version","title":"Reverting particular files back to master version","text":"<ol> <li>If you want to revert only certain files (eg import files), you can do it using Terminal. For this example, we will use uberon_import.owl as the file we want reverted back to the version in master branch, however, this can be done on any file.</li> <li>Assuming your directory is set to <code>src/ontology</code>, in terminal use: <code>git checkout master -- imports/uberon_import.owl</code>.</li> <li>Commit the change to the branch as normal.</li> </ol>"},{"location":"reference/git-faq/#reverting-particular-files-back-to-a-previous-version","title":"Reverting particular files back to a previous version","text":"<ol> <li>If you want to revert a file back to a previous version instead of master, you can use the commit ID.</li> <li>To do this, in Terminal use: <code>git log</code> to list out the previous commits and copy the commit code of the commit you would like to revert to (example: see yellow string of text in screenshot below).</li> </ol> <ol> <li>Press q on your keyboard to quit git log (or down arrow to scroll down to continue to find the commit ID you want to revert if it is further down).</li> <li>In terminal use: <code>git checkout ff18c9482035062bbbbb27aaeb50e658298fb635 -- imports/uberon_import.owl</code> using whichever commit code you want instead of the commit code in this example.</li> <li>commit the change to the branch as normal.</li> </ol>"},{"location":"reference/github-desktop/","title":"GitHub Desktop","text":"<p>For most of our training activities, we recommend using GitHub Desktop. It provides a very convenient way to push and pull changes, and inspect the \"diff\". It is, however, not mandatory if you are already familiar with other git workflows (such as command line, or Sourcetree).</p>"},{"location":"reference/github-intro/","title":"Git, GitHub and GitHub Desktop (version control)","text":"<p>A repository can consist of many files with several users simultaneously editing those files at any moment in time. In order to ensure conflicting edits between the users are not made and a history of the edits are tracked, software classified as a \"distributed version control system\" is used.</p> <p>All OBO repositories are managed by the Git version control system. This allows users to make their own local branch of the repository, i.e., making a mirror copy of the repository directories and files on their own computers, and make edits as desired. The edits can then be reviewed by other users before the changes are incorporated in the 'main' or 'master' branch of the repository. This process can be executed by running Git line commands and/or by using a web interface (Github.com) along with a desktop application (GitHub Desktop).</p> <p>Documentation, including an introduction to GitHub, can be found here: Hello World.</p>"},{"location":"reference/glossary-of-terms/","title":"Glossary of Terms","text":"<p>This document is a list of terms that you might encounter in the ontology world. It is not an exhaustive list and will continue to evolve. Please create a ticket if there is a term you find missing or a term you encounter that you do not understand, and we will do our best to add them. This list is not arranged in any particular order. Please use the search function to find terms.</p> <p>Acknowledgement: Many terms are taken directly from OAK documentation with the permission of Chris Mungall. Many descriptions are also taken from https://www.w3.org/TR/owl2-syntax/.</p>"},{"location":"reference/glossary-of-terms/#annotation","title":"Annotation","text":"<p>This term is frequently ambiguous. It can refer to Text Annotation, OWL Annotation, or Association.</p>"},{"location":"reference/glossary-of-terms/#annotationproperty","title":"AnnotationProperty","text":"<p>Annotation properties are OWL axioms that are used to place annotations on individuals, class names, property names, and ontology names. They do not affect the logical definition unless they are used as a \"shortcut\" that a pipeline expands to a logical axiom.</p>"},{"location":"reference/glossary-of-terms/#anonymous-ancestor","title":"Anonymous Ancestor","text":"<p>An accumulation of all of the superclasses from ancestors of a class.</p>"},{"location":"reference/glossary-of-terms/#anonymous-individual","title":"Anonymous Individual","text":"<p>If an individual is not expected to be used outside an ontology, one can use an anonymous individual, which is identified by a local node ID rather than a global IRI. Anonymous individuals are analogous to blank nodes in RDF.</p>"},{"location":"reference/glossary-of-terms/#api","title":"API","text":"<p>Application Programming Interface. An intermediary that allows two or more computer programs to communicate with each other. In ontologies, this usually means an Endpoint in which the ontology can be programmatically accessed.</p>"},{"location":"reference/glossary-of-terms/#application-ontology","title":"Application Ontology","text":"<p>Usually refers to a Project Ontology.</p>"},{"location":"reference/glossary-of-terms/#axiom","title":"Axiom","text":"<p>Axioms are statements that are asserted to be true in the domain being described. For example, using a subclass axiom, one can state that the class a:Student is a subclass of the class a:Person. (Note: in OWL, there are also annotation axioms which does not apply any logical descriptions)</p>"},{"location":"reference/glossary-of-terms/#bioportal","title":"Bioportal","text":"<p>An Ontology Repository that is a comprehensive collection of multiple biologically relevant ontologies.</p>"},{"location":"reference/glossary-of-terms/#controlled-vocabulary","title":"Controlled Vocabulary","text":"<p>Standardized and organized arrangements of words and phrases that provide a consistent way to describe data. A controlled vocabulary may or may not include definitions. Ontologies can be seen as a controlled vocabulary expressed in an ontological language which includes relations.</p>"},{"location":"reference/glossary-of-terms/#class","title":"Class","text":"<p>An OWL entity that formally represents something that can be instantiated. For example, the class \"heart\".</p>"},{"location":"reference/glossary-of-terms/#curie","title":"CURIE","text":"<p>A CURIE is a compact URI. For example, <code>CL:0000001</code> expands to http://purl.obolibrary.org/obo/CL_0000001. For more information, please see https://www.w3.org/TR/curie/. </p>"},{"location":"reference/glossary-of-terms/#curies","title":"<code>curies</code>","text":"<p><code>curies</code> is a Python package for working with prefix maps and  extended prefix maps and converting between CURIEs and URIs.</p>"},{"location":"reference/glossary-of-terms/#data-model","title":"Data Model","text":"<p>An abstract model that organizes elements of data and standardizes how they relate to one another.</p>"},{"location":"reference/glossary-of-terms/#dataproperty","title":"dataProperty","text":"<p>dataProperty relate OWL entities to literal data (e.g., strings, numbers, datetimes, etc.) as opposed to ObjectProperty which relate individuals to other OWL entities. Unlike AnnotationProperty, dataProperty axioms fall on the logical side of OWL and are hence useable by reasoners.</p>"},{"location":"reference/glossary-of-terms/#datatype","title":"Datatype","text":"<p>Datatypes are OWL entities that refer to sets of data values. Thus, datatypes are analogous to classes, the main difference being that the former contain data values such as strings and numbers, rather than individuals. Datatypes are a kind of data range, which allows them to be used in restrictions. For example, the datatype xsd:integer denotes the set of all integers, and can be used with the range of a dataProperty to state that the range of said dataProperty must be an integer. </p>"},{"location":"reference/glossary-of-terms/#description-logic","title":"Description Logic","text":"<p>Description Logics (DL) are a family of formal knowledge representation languages. It provides a logical formalism for ontologies and is what OWL is based on. DL querying can be used to query ontologies in Protege.</p>"},{"location":"reference/glossary-of-terms/#domain","title":"Domain","text":"<p>Domain, in reference to a dataProperty or ObjectProperty, refers to the restriction on the subject of a triple - if a given property has a given class in its domain this means that any individual that has a value for the property, will be inferred to be an instance of that domain class. For example, if <code>John hasParent Mary</code> and <code>Person</code> is listed in the domain of <code>hasParent</code>, then <code>John</code> will be inferred to be an instance of <code>Person</code>. </p>"},{"location":"reference/glossary-of-terms/#dosdp","title":"DOSDP","text":"<p>Dead Simple Ontology Design Patterns. A templating system for ontologies with well-documented patterns and templates.</p>"},{"location":"reference/glossary-of-terms/#edge","title":"Edge","text":"<p>A typed, directed link between Nodes in a knowledge graph. Translations of OWL into Knowledge graphs vary, but typically edges are generated for simple triples, relating two individuals or two classes via an AnnotationProperty or ObjectProperty and simple existential restrictions (A SubClassOf R some B), with the edge type corresponding to the property.</p>"},{"location":"reference/glossary-of-terms/#endpoint","title":"Endpoint","text":"<p>Where an API interfaces with the ontology.</p>"},{"location":"reference/glossary-of-terms/#existential-restriction","title":"Existential Restriction","text":"<p>A relationship between two classes, A R (some) B, that states that all individuals of class A stand in relation R to at least one individual of class B. For example, <code>neuron has_part some dendrite</code> states that all instances of neuron have at least one individual of type dentrite as a part. In Manchester syntax, the keyword 'some' is used to denote existential restrictions and is interpreted as \"there exists\", \"there is at least one\", or \"some\". See documentation on classifications for more details.</p>"},{"location":"reference/glossary-of-terms/#extended-prefix-map","title":"Extended Prefix Map","text":"<p>Extended prefix maps (EPMs) address the limitations in expressiveness of prefix maps by including explicit fields for CURIE prefix synonyms and URI prefix synonyms in addition to explicit fields for the preferred CURIE prefix and URI prefix. An abbreviated example (just containing an entry for ChEBI) looks like:</p> <pre><code>[\n    {\n        \"prefix\": \"CHEBI\",\n        \"uri_prefix\": \"http://purl.obolibrary.org/obo/CHEBI_\",\n        \"prefix_synonyms\": [\"chebi\"],\n        \"uri_prefix_synonyms\": [\n            \"https://identifiers.org/chebi:\"\n        ]\n    }\n]\n</code></pre>"},{"location":"reference/glossary-of-terms/#functional-syntax","title":"Functional Syntax","text":"<p>An official syntax of OWL (others are RDF-XML and OWL-XML) in which each line represents and axiom (although things get a little more complex with axiom annotations, and axioms use prefix syntax (order = relation (subject, object)). This is in contrast to in-fix syntax (e.g. Manchester syntax) (order = subject relation object). Functional syntax is the preferred syntax for editor files maintained on GitHub, because it can be safely diff'd and (somewhat) human readable.</p>"},{"location":"reference/glossary-of-terms/#graph","title":"Graph","text":"<p>Formally a graph is a data structure consisting of Nodes and Edges. There are different forms of graphs, but for our purposes an ontology graph has all Terms as nodes, and relationships connecting terms (is-a, part-of) as edges. Note the concept of an ontology graph and an RDF graph do not necessarily fully align - RDF graphs of OWL ontologies employ numerous blank nodes that obscure the ontology structure.</p>"},{"location":"reference/glossary-of-terms/#individual","title":"Individual","text":"<p>An OWL entity that represents an instance of a class. For example, the instance \"John\" or \"John's heart\". Note that instances are not commonly represented in ontologies. For instance, \"John\" (an instance of person) or \"John's heart\" (an instance of heart).</p>"},{"location":"reference/glossary-of-terms/#information-content","title":"Information Content","text":"<p>A measure of how informative an ontology concept is; broader concepts are less informative as they encompass many things, whereas more specific concepts are more unique. This is usually measured as <code>-log2(Pr(term))</code>. The method of calculating the probability varies, depending on which predicates are taken into account (for many ontologies, it makes sense to use part-of as well as is-a), and whether the probability is the probability of observing a descendant term, or of an entity annotated using that term.</p>"},{"location":"reference/glossary-of-terms/#interface","title":"Interface","text":"<p>A programmatic abstraction that allows us to focus on what something should do rather than how it is done.</p>"},{"location":"reference/glossary-of-terms/#jaccard-similarity","title":"Jaccard Similarity","text":"<p>A measures of the similarity between two sets of data to see which members are shared and distinct.</p>"},{"location":"reference/glossary-of-terms/#kgcl","title":"KGCL","text":"<p>Knowledge Graph Change Language (KGCL) is a data model for communicating desired changes to an ontology. It can also be used to communicate differences between two ontologies. See KGCL docs.</p>"},{"location":"reference/glossary-of-terms/#knowledge-graph","title":"Knowledge Graph","text":"<p>A network of real-world entities (i.e., objects, events, situations, and concepts) that illustrates the relationships between them. Knowledge graphs (in relation to ontologies) are thought of as real data built using an ontology as a framework.</p>"},{"location":"reference/glossary-of-terms/#label","title":"Label","text":"<p>Usually refers to a human-readable text string corresponding to the <code>rdfs:label</code> predicate. Labels are typically unique per ontology. In OBO Format and in the bio-ontology literature, labels are sometimes called Names. Sometimes in the machine learning literature, and in databases such as Neo4J, \"label\" actually refers to a Category.</p>"},{"location":"reference/glossary-of-terms/#lutra","title":"Lutra","text":"<p>Lutra is the open source reference implementation of the OTTR templating language.</p>"},{"location":"reference/glossary-of-terms/#mapping","title":"Mapping","text":"<p>A means of linking two resources (e.g. two ontologies, or an ontology and a database) together. Also see SSSOM</p>"},{"location":"reference/glossary-of-terms/#materialised","title":"Materialised","text":"<p>The process of making inferred axioms explicit by asserting them.</p>"},{"location":"reference/glossary-of-terms/#name","title":"Name","text":"<p>Usually synonymous with Label, but in the formal logic and OWL community, \"Name\" sometimes denotes an Identifier</p>"},{"location":"reference/glossary-of-terms/#named-individual","title":"Named Individual","text":"<p>An Individual that is given an explicit name that can be used in any ontology to refer to the same object; named individuals get IRIs whereas anonymous individuals do not.</p>"},{"location":"reference/glossary-of-terms/#nodes","title":"Nodes","text":"<p>Terms represented in a graph</p>"},{"location":"reference/glossary-of-terms/#object","title":"Object","text":"<p>The \"right\" side of a Triple.</p>"},{"location":"reference/glossary-of-terms/#objectproperty","title":"ObjectProperty","text":"<p>An owl entity that is used to related 2 individuals ('my left foot' part_of 'my left leg') or two classes ('foot' part_of some leg) or an individual and a class ('the neuron depicted in this image' (is) has_soma_location some 'primary motor cortex. More rarely it is used to define a class in terms of some individual (the class 'relatives of Shawn' related_to Value Shawn.</p>"},{"location":"reference/glossary-of-terms/#obo","title":"OBO","text":"<p>Open Biological and Biomedical Ontology. This could refer to the OBO Foundry (e.g. OBO ontologies = ontologies that follow the standards of the OBO Foundry) or OBO Format</p>"},{"location":"reference/glossary-of-terms/#obo-format","title":"OBO Format","text":"<p>A serialization format for ontologies designed for easy viewing, direct editing, and readable diffs. It is popular in bioinformatics, but not widely used or known outside the genomics sphere. OBO is mapped to OWL, but only expresses a subset, and provides some OWL abstractions in a more easy to understand fashion.</p>"},{"location":"reference/glossary-of-terms/#ols","title":"OLS","text":"<p>Ontology Lookup Service. An Ontology Repository that is a curated collection of multiple biologically relevant ontologies, many from OBO. OLS can be accessed with this link</p>"},{"location":"reference/glossary-of-terms/#ontology","title":"Ontology","text":"<p>A flexible concept loosely encompassing any collection of OWL entities and statements or relationships connecting them.</p>"},{"location":"reference/glossary-of-terms/#odk","title":"ODK","text":"<p>Ontology Development Kit. A toolkit and docker image for managing ontologies.</p>"},{"location":"reference/glossary-of-terms/#ontology-library","title":"Ontology Library","text":"<p>The systems or platform where various types of ontologies are stored from different sources and provide the ability to data providers and application developers to share and reuse the ontologies.</p>"},{"location":"reference/glossary-of-terms/#ontology-repository","title":"Ontology Repository","text":"<p>A curated collection of ontologies.</p>"},{"location":"reference/glossary-of-terms/#ottr","title":"OTTR","text":"<p>Reasonable Ontology Templates. A system for composable ontology templates and documentation.</p>"},{"location":"reference/glossary-of-terms/#owl","title":"OWL","text":"<p>Web Ontology Language. An ontology language that uses constructs from Description Logic. OWL is not itself an ontology format, it can be serialized through different formats such as Functional Syntax, and it can be mapped to :RDF and serialized via an RDF format.</p>"},{"location":"reference/glossary-of-terms/#owl-annotation","title":"OWL Annotation","text":"<p>In the context of OWL, the term Annotation means a piece of metadata that does not have a strict logical interpretation. Annotations can be on entities, for example, Label annotations, or annotations can be on Axioms.</p>"},{"location":"reference/glossary-of-terms/#owl-api","title":"OWL API","text":"<p>A java-based API to interact with OWL ontologies. Full documentation can be found at http://owlcs.github.io/owlapi/apidocs_5/index.html</p>"},{"location":"reference/glossary-of-terms/#owl-entity","title":"OWL Entity","text":"<p>OWL Entities, such as classes, properties, and individuals, are identified by IRIs. They form the primitive terms of an ontology and constitute the basic elements of an ontology. For example, a class a:Person can be used to represent the set of all people. Similarly, the object property a:parentOf can be used to represent the parent-child relationship. Finally, the individual a:Peter can be used to represent a particular person called \"Peter\".  The following is a complete list of types of OWL Entities:</p> <ul> <li>Class </li> <li>Individual</li> <li>ObjectProperty</li> <li>AnnotationProperty</li> <li>dataProperty </li> <li>Datatype</li> </ul>"},{"location":"reference/glossary-of-terms/#predicate","title":"Predicate","text":"<p>An OWL entity that represents the type of a Relationship. Typically corresponds to an ObjectProperty in OWL, but this is not always true; in particular, the is-a relationship type is a builtin construct <code>SubClassOf</code> in OWL Examples:</p> <ul> <li>is-a</li> <li>part-of (BFO:0000050)</li> </ul>"},{"location":"reference/glossary-of-terms/#prefix-map","title":"Prefix Map","text":"<p>A prefix map is a dictionary data structure where keys represent CURIE prefixes and their associated values represent URI prefixes. Ideally, these are constrained to be bijective (i.e., no duplicate keys, no duplicate values), but this is not always done in practice. Here\u2019s an abbreviated example prefix map describing OBO Foundry ontologies:</p> <pre><code>{\n  \"CHEBI\": \"http://purl.obolibrary.org/obo/CHEBI_\",\n  \"MONDO\": \"http://purl.obolibrary.org/obo/MONDO_\",\n  \"GO\": \"http://purl.obolibrary.org/obo/GO_\"\n}\n</code></pre>"},{"location":"reference/glossary-of-terms/#project-ontology","title":"Project Ontology","text":"<p>An ontology that is specific to a project and does not necessarily have interoperability with other ontologies in mind.</p>"},{"location":"reference/glossary-of-terms/#pronto","title":"Pronto","text":"<p>An Ontology Library for parsing obo and owl files.</p>"},{"location":"reference/glossary-of-terms/#property","title":"Property","text":"<p>An OWL entity that represents an attribute or a characteristic of an element. In OWL, properties are divided into disjoint categories:</p> <ul> <li>ObjectProperty</li> <li>AnnotationProperty</li> <li>dataProperty</li> </ul>"},{"location":"reference/glossary-of-terms/#protege","title":"Protege","text":"<p>A typical ontology development tool used by ontology developers in the OBO-sphere. Full documentation can be found at https://protege.stanford.edu/.</p>"},{"location":"reference/glossary-of-terms/#range","title":"Range","text":"<p>Range, in reference to a dataProperty or ObjectProperty, refers to the restriction on the object of a triple - if a given property has a given class in its domain this means that any individual that has a value for the property (i.e. is the subject of a relation along the property), will be inferred to be an instance of that domain class. For example, if <code>John hasParent Mary</code> and <code>Person</code> is listed in the domain of <code>hasParent</code>, then <code>John</code> will be inferred to be an instance of <code>Person</code>.</p>"},{"location":"reference/glossary-of-terms/#rdf","title":"RDF","text":"<p>A datamodel consisting of simple Subject predicate Object Triples organized into an RDF Graph.</p>"},{"location":"reference/glossary-of-terms/#rdflib","title":"rdflib","text":"<p>A python library to interact with RDF data. Full documentation can be found at https://rdflib.readthedocs.io/en/stable/.</p>"},{"location":"reference/glossary-of-terms/#reasoner","title":"Reasoner","text":"<p>An ontology tool that will perform inference over an ontology to yield new axioms (e.g. new Edges) or to determine if an ontology is logically coherent.</p>"},{"location":"reference/glossary-of-terms/#relationship","title":"Relationship","text":"<p>A Relationship is a type connection between two OWL entities. The first element is called the subject, and the second one the Object, with the type of connection being the Relationship Type. Sometimes Relationships are equated with Triples in RDF but this can be confusing, because some relationships map to multiple triples when following the OWL RDF serialization. An example is the relationship \"finger part-of hand\", which in OWL is represented using a Existential Restriction that maps to 4 triples.</p>"},{"location":"reference/glossary-of-terms/#relationship-type","title":"Relationship Type","text":"<p>See predicate</p>"},{"location":"reference/glossary-of-terms/#robot","title":"ROBOT","text":"<p>A toolkit for transforming and interacting with ontologies. Full documentation can be found at http://robot.obolibrary.org/</p>"},{"location":"reference/glossary-of-terms/#semantic-similarity","title":"Semantic Similarity","text":"<p>A means of measuring similarity between either pairs of ontology concepts, or between entities annotated using ontology concepts. There is a wide variety of different methods for calculating semantic similarity, for example Jaccard Similarity and Information Content based measures.</p>"},{"location":"reference/glossary-of-terms/#semantic-sql","title":"Semantic SQL","text":"<p>Semantic SQL is a proposed standardized schema for representing any RDF/OWL ontology, plus a set of tools for building a database conforming to this schema from RDF/OWL files. See Semantic-SQL</p>"},{"location":"reference/glossary-of-terms/#sparql","title":"SPARQL","text":"<p>The standard query language and protocol for Linked Open Data on the web or for RDF triplestores - used to query ontologies.</p>"},{"location":"reference/glossary-of-terms/#sssom","title":"SSSOM","text":"<p>Simple Standard for Sharing Ontological Mappings (https://github.com/mapping-commons/sssom).</p>"},{"location":"reference/glossary-of-terms/#subject","title":"Subject","text":"<p>The \"left\" side of a Triple.</p>"},{"location":"reference/glossary-of-terms/#subset","title":"Subset","text":"<p>A named collection of elements, typically grouped for some purpose. In the ODK/OBO world, there is a standard annotation property and pattern for this, for more information, see the subset documentation.</p>"},{"location":"reference/glossary-of-terms/#term","title":"Term","text":"<p>Usually used to mean Class and Individuals, however sometimes used to refer to wider OWL entities.</p>"},{"location":"reference/glossary-of-terms/#text-annotation","title":"Text Annotation","text":"<p>The process of annotating spans of texts within a text document with references to ontology terms, or the result of this process. This is frequently done automatically. The Bioportal implementation provides text annotation services.</p>"},{"location":"reference/glossary-of-terms/#triple","title":"Triple","text":"<p>A set of three entities that codifies a statement about semantic data in the form of Subject-predicate-Object expressions (e.g., \"Bob is 35\", or \"Bob knows John\"). Also see Relationship.</p>"},{"location":"reference/glossary-of-terms/#triplestore","title":"Triplestore","text":"<p>A purpose-built database for the storage and retrieval of triples through semantic queries. A triple is a data entity composed of subject\u2013predicate\u2013object, like \"Bob is 35\" or \"Bob knows Fred\".</p>"},{"location":"reference/glossary-of-terms/#ubergraph","title":"Ubergraph","text":"<p>An integrated OBO ontology Triplestore and a Ontology Repository, with merged set of mutually referential OBO ontologies (see the ubergraph github for list of ontologies included), that allows for SPARQL querying of integrated OBO ontologies.</p>"},{"location":"reference/glossary-of-terms/#uri","title":"URI","text":"<p>A Uniform Resource Indicator, a generalization of URL. Most people think of URLs as being solely for addresses for web pages (or APIs) but in semantic web technologies, URLs can serve as actual identifiers for entities like OWL entities. Data models like OWL and RDF use URIs as identifiers. In OAK, URIs are mapped to CURIE</p>"},{"location":"reference/glossary/","title":"Glossary for concepts in and around OBO","text":"<pre><code>IMPORTANT NOTE TO EDITORS, MERGE THIS WITH glossary.md.\n</code></pre> <p>New OBOOK Glossary</p>"},{"location":"reference/glossary/#tools","title":"Tools","text":"Term Definition Type Docs Ontology Development Kit (ODK) A toolkit and docker image for managing ontology releases. Tool docs ROBOT A toolkit for transforming and interacting with ontologies. Tool docs rdflib A python library to interact with RDF data Library docs OWL API A java-based API to interact with OWL ontologies Library docs Protege A typical ontology development tool used by ontology developers in the OBO-sphere Tool docs ROBOT templates A templating system based on tables, where the templates are integrated in the same table as the data Standard docs Dead Simple Ontology Design Patterns (DOSDP) A templating system for ontologies with well-documented patterns and templates. Standard docs DOSDP tools DOSDP is the open source reference implementation of the DOSDP templating language. Tool docs Reasonable Ontology Templates (OTTR) A system for composable ontology templates and documentation Standard docs Lutra Lutra is the open source reference implementation of the OTTR templating language. Tool docs"},{"location":"reference/go-style-annotation-property-practice/","title":"Recommended metadata properties to use in curating OBO ontologies (GO-style)","text":"<p>Note that while most of the practices documented here apply to all OBO ontologies this recommendation applies only to ontologies that are developed using GO-style curation workflows.</p> Type Property to use Required Number/Limit Description Format Annotation Reference/Comments Label rdfs:label Y Max 1 * Full name of the term, must be unique. Free text None * some ontologies have multiple labels for different languages, in which case, there should maximum be one label per language Definition IAO:0000115 Y Max 1 A textual definition of ther term. In most ontologies, must be unique. Free text database_cross_reference: reference materials used and contributors (in ORCID ID link format) See this document for guide on writing definitions Contributor dcterms:contributor N (though highly reccomended) No limit The ORCID ID of people who contributed to the creation of the term. ORCID ID (using full link) None Synonyms http://www.geneontology.org/formats/oboInOwl#hasExactSynonym, http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym, http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym, http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym N No limit Synonyms of the term. Free text database_cross_reference: reference material in which the synonymn is used See synonyms documentation for guide on using synonyms Comments rdfs:comment N Max 1 Comments about the term, extended descriptions that might be useful, notes on modelling choices, other misc notes. Free text database_cross_reference: reference material relating to the comment See documentation on comments for more information about comments Editor note IAO:0000116 N Max 1 A note that is not relevant to front users, but might be to editors Free text database_cross_reference: reference material relating to the note Subset http://www.geneontology.org/formats/oboInOwl#inSubset N No limit A tag that marks a term as being part of a subset annotation property that is a subproperty of subset_property (see guide on how to select this) None See Slim documentation for more information on subsets Database Cross Reference http://www.geneontology.org/formats/oboInOwl#hasDbXref N No limit Links out to external references. string and should* take the form {prefix}:{accession}; see db-xrefs yaml for prefixes None *Some ontologies allow full URLS in specific cases, but this is controversial Date created dcterms:created N Max 1 Date in which the term was created ISO-8601 format None Date last updated dcterms:date N Max 1 Date in which the term was last updated ISO-8601 format None Deprecation http://www.w3.org/2002/07/owl#deprecated N Max 1 A tag that marks a term as being obsolete/deprecated xsd:boolean (true/false) None See obsoletion guide for more details Replaced by IAO:0100001 N Max 1 Term that has replaced an obsoleted term IRI/ID (e.g. CL:0000001) None See obsoletion guide and merging terms guide for more details Consider oboInOwl:consider N No limit Term that can be considered from manual replacement of an obsoleted term IRI/ID (e.g. CL:0000001) None See obsoletion guide and merging terms guide for more details"},{"location":"reference/llms/","title":"LLMs for the Semantic Engineer","text":""},{"location":"reference/llms/#large-language-models-for-the-semantic-engineer","title":"Large Language Models for the Semantic Engineer","text":"<p>This page contains a collection of resources useful to review for Semantic Engineers (Ontology, KG and Semantic Software Engineers).</p> <p>Please suggest your own content here by making a PR!</p>"},{"location":"reference/llms/#resources","title":"Resources","text":""},{"location":"reference/llms/#introductory","title":"Introductory","text":"<ul> <li>OBO Academy training: Leveraging ChatGPT for ontology curation</li> <li>Video trainings<ul> <li>OBO Academy: Enhancing curation workflows with CurateGPT</li> <li>OBO Academy: AI-assisted ontology editing workflows, Part 1</li> <li>OBO Academy: AI-assisted ontology editing workflows Part 2: Validation</li> <li>OBO Academy: Applying the Strengths of Large Language Models to Biomedical Informatics</li> </ul> </li> <li>Berkelely Guide to LLMs: Evolving Google docs with links to many useful slide decks and presentations. Particularly useful for Curators and Semantic Software Engineers who wish to levarage Monarch LLM tools like DragonAI, CurateGPT and OntoGPT.</li> </ul>"},{"location":"reference/llms/#experienced","title":"Experienced","text":"<ul> <li>Applied LLM: A practical guide to building successful LLM products, covering the tactical, operational, and strategic. Very useful resource, please check out if you have not already done so!</li> <li>LLMs with OAK</li> </ul>"},{"location":"reference/managing-issues/","title":"Tools for Managing Issues","text":"<p>Based on Intro to GitHub (GO-Centric) with credit to Nomi Harris and Chris Mungall</p>"},{"location":"reference/managing-issues/#labels","title":"Labels","text":"<p>Labels are a useful tool to help group and organize issues, allowing people to filter issues by grouping. Note: Only project contributors can add/change labels</p>"},{"location":"reference/managing-issues/#best-practices-for-labels","title":"Best Practices for Labels","text":"<ul> <li>Make use of use GitHub's default labels: bug, question, enhancement, good first issue, etc.</li> <li>Define new labels as needed for project management</li> <li>Lightly coordinate labels across repos in an organization</li> <li>Labels are not ontologies; don\u2019t overload them. A small simple set consistently applied is better than overly specific inconsistently applied labels</li> </ul>"},{"location":"reference/managing-issues/#superissues","title":"Superissues","text":"<p>Superissues are issues that have checklists (added using -[] on items). These are useful as they show progress towards completion. These can be used for issues that require multiple steps to solve.</p> <p></p>"},{"location":"reference/managing-issues/#milestones","title":"Milestones","text":"<p>Milestones are used for issues with a specific date/deadline. Milestones contain issues and issues can be filtered by milestones. They are also useful for visualizing how many issues in it is completed.</p> <p></p>"},{"location":"reference/managing-issues/#project-boards","title":"Project Boards","text":"<p>Project boards are a useful tool to organise, as the name implies, projects. They can span multiple repos (though the repos need to be in the same organisation). Notes can also be added.</p> <p></p>"},{"location":"reference/medical-ontology-landscape/","title":"Medical Ontology landscape","text":""},{"location":"reference/medical-ontology-landscape/#the-landscape-of-disease-and-phenotype-ontologies","title":"The Landscape of Disease and Phenotype Ontologies","text":"<p>Compiled by Nicole Vasilevsky. Feel free to make pull requests to suggest edits. Note: This currently just provides an overview of disease and phenotype ontologies. Contributors are welcome to add more descriptions of other medical ontologies. This was last updated in 2021.</p>"},{"location":"reference/medical-ontology-landscape/#disease-ontologies-terminologies","title":"Disease Ontologies &amp; Terminologies","text":""},{"location":"reference/medical-ontology-landscape/#disease-summary-table","title":"Disease Summary Table","text":"Name Disease Area Artificial Intelligence Rheumatology Consultant System Ontology (AI-RHEUM) Rheumatic diseases Autism DSM-ADI-R Ontology (ADAR) Autism Autism Spectrum Disorder Phenotype Ontology (ASDPTO) Autism Brucellosis Ontology (IDOBRU) brucellosis Cardiovascular Disease Ontology (CVDO) Cardiovascular Chronic Kidney Disease Ontology (CKDO) Chronic kidney disease Chronic Obstructive Pulmonary Disease Ontology (COPDO) Chronic obstructive pulmonary disease (COPD) Coronavirus Infectious Disease Ontology (CIDO) Coronavirus infectious diseases Diagnostic and Statistical Manual of Mental Disorders (DSM) Mental disorders Dispedia Core Ontology (DCO) Rare diseases Experimental Factor Ontology (EFO) Broad disease coverage Fibrotic Interstitial Lung Disease Ontology (FILDO) Fibrotic interstitial lung disease Genetic and Rare Diseases Information Center (GARD) Rare diseases Holistic Ontology of Rare Diseases (HORD) Rare disease Human Dermatological Disease Ontology (DERMO) Dermatology (skin) Human Disease Ontology (DO) Human disease Infectious Disease Ontology (IDO) Infectious disease International Classification of Functioning, Disability and Health (ICF) Cross-discipline, focuses disabilities International Statistical Classification of Diseases and Related Health Problems (ICD-11) Broad coverage International Classification of Diseases for Oncology (ICD-O) Cancer Logical Observation Identifier Names and Codes (LOINC) Broad coverage Medical Subject Headings (MeSH) Broad coverage MedGen Human medical genetics Medical Dictionary for Regulatory Activities (MedDRA) Broad coverage Mental Disease Ontology (MDO) Mental functioning Mondo Disease Ontology (Mondo) Broad coverage, Cross species National Cancer Institute Thesaurus (NCIT) Humam cancer and neoplasms Neurological Disease Ontology (ND) Neurology Online Mendelian Inheritance in Man (OMIM) Mendelian, genetic diseases. Ontology of Cardiovascular Drug Adverse Events (OCVDAE) Cardiovascular Ontology for General Medical Science (OGMS) Broad coverage Ontology for Genetic Susceptibility Factor (OGSF) Genetic disease Ontology of Glucose Metabolism Disorder (OGMD) Metabolic disorders Ontology of Language Disorder in Autism (LDA) Austism The Oral Health and Disease Ontology (OHD) Oral health and disease Orphanet (ORDO) Rare diseases Parkinson Disease Ontology (PDO) Parkinson disease Pathogenic Disease Ontology (PDO) Pathogenic diseases PolyCystic Ovary Syndrome Knowledgebase (PCOSKB) Polycystic ovary syndrome Rat Disease Ontology (RDO) Broad coverage Removable Partial Denture Ontology (RPDO) Oral health Resource of Asian Primary Immunodeficiency Diseases (RPO) Immunodeficiencies Sickle Cell Disease Ontology (SCDO) Sickle Cell Disease SNOMED Clinical Terminology (SNOMED CT) Broad disease representation for human diseases. Symptom Ontology Human diseases Unified Medical Language System Broad coverage"},{"location":"reference/medical-ontology-landscape/#artificial-intelligence-rheumatology-consultant-system-ontology-ai-rheum","title":"Artificial Intelligence Rheumatology Consultant System ontology (AI-RHEUM)","text":"<p>Description: Contains findings, such as clinical signs, symptoms, laboratory test results, radiologic observations, tissue biopsy results, and intermediate diagnosis hypotheses, for the diagnosis of rheumatic diseases. Disease area: Rheumatic diseases Use Cases: Used by clinicians and informatics researchers. Website: https://bioportal.bioontology.org/ontologies/AI-RHEUM Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#autism-dsm-adi-r-ontology-adar","title":"Autism DSM-ADI-R Ontology (ADAR)","text":"<p>Description: An ontology of autism spectrum disorder (ASD) and related neurodevelopmental disorders. Disease area: Autism Use Cases: It extends an existing autism ontology to allow automatic inference of ASD phenotypes and Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria based on subjects\u2019 Autism Diagnostic Interview\u2013Revised (ADI-R) assessment data. Website: https://bioportal.bioontology.org/ontologies/ADAR Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#autism-spectrum-disorder-phenotype-ontology-asdpto","title":"Autism Spectrum Disorder Phenotype Ontology (ASDPTO)","text":"<p>Description: Encapsulates the ASD behavioral phenotype, informed by the standard ASD assessment instruments and the currently known characteristics of this disorder. Disease area: Autism Use Cases: Intended for use in research settings where extensive phenotypic data have been collected, allowing a concept-based approach to identifying behavioral features of importance and for correlating these with genotypic data. Website: https://bioportal.bioontology.org/ontologies/ASDPTO Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#brucellosis-ontology-idobru","title":"Brucellosis Ontology (IDOBRU)","text":"<p>Description: Describes the most common zoonotic disease, brucellosis, which is caused by Brucella, a type of facultative intracellular bacteria. Disease area: brucellosis bacteria Use Cases: An extension ontology of the core Infectious Disease Ontology (IDO-core). This project appears to be inactive. Website: https://github.com/biomedontology/idobru Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#cardiovascular-disease-ontology-cvdo","title":"Cardiovascular Disease Ontology (CVDO)","text":"<p>Description: An ontology to describe entities related to cardiovascular diseases. Disease area: Cardiovascular Use Cases: Describes entities related to cardiovascular diseases including the diseases themselves, the underlying disorders, and the related pathological processes. Imports upper level terms from OGMS and imports some terms from Disease Ontology (DO). GitHub repo: https://github.com/OpenLHS/CVDO/ Website: https://github.com/OpenLHS/CVDO OBO Foundry webpage: http://obofoundry.org/ontology/cvdo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#chronic-kidney-disease-ontology-ckdo","title":"Chronic Kidney Disease Ontology (CKDO)","text":"<p>Description: An ontology of chronic kidney disease in primary care. Disease area: Chronic kidney disease Use Cases: CKDDO was developed to assist routine data studies and case identification of CKD in primary care. Website: http://purl.bioontology.org/ontology/CKDO Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#chronic-obstructive-pulmonary-disease-ontology-copdo","title":"Chronic Obstructive Pulmonary Disease Ontology (COPDO)","text":"<p>Description: Models concepts associated with chronic obstructive pulmonary disease in routine clinical databases. Disease area: Chronic obstructive pulmonary disease (COPD) Use Cases: Clinical use. Website: https://bioportal.bioontology.org/ontologies/COPDO Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#coronavirus-infectious-disease-ontology-cido","title":"Coronavirus Infectious Disease Ontology (CIDO)","text":"<p>Description: Aims to ontologically represent and standardize various aspects of coronavirus infectious diseases, including their etiology, transmission, epidemiology, pathogenesis, diagnosis, prevention, and treatment. Disease area: Coronavirus infectious diseases, including COVID-19, SARS, MERS; covers etiology, transmission, epidemiology, pathogenesis, diagnosis, prevention, and treatment. Use Cases: Used for disease annotations related to coronavirus infections. GitHub repo: https://github.com/cido-ontology/cido OBO Foundry webpage: http://obofoundry.org/ontology/cido.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#diagnostic-and-statistical-manual-of-mental-disorders-dsm","title":"Diagnostic and Statistical Manual of Mental Disorders (DSM)","text":"<p>Description: Authoritative source to define and classify mental disorders to improve diagnoses, treatment, and research. Disease area: Mental disorders Use Cases: Used in clinical healthcare and research by pyschiatrists and psychologists. Website: https://www.psychiatry.org/psychiatrists/practice/dsm Open: No, must be purchased</p>"},{"location":"reference/medical-ontology-landscape/#dispedia-core-ontology-dco","title":"Dispedia Core Ontology (DCO)","text":"<p>Description: A schema for information brokering and knowledge management in the complex field of rare diseases. DCO describes patients affected by rare diseases and records expertise about diseases in machine-readable form. Disease area: Rare disease Use Cases: DCO was initially created with amyotrophic lateral sclerosis as a use case. Website: http://purl.bioontology.org/ontology/DCO Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#experimental-factor-ontology-efo","title":"Experimental Factor Ontology (EFO)","text":"<p>Description: Provides a systematic description of many experimental variables available in EBI databases, and for projects such as the GWAS catalog. Disease area: Broad disease coverage, integrates the Mondo disease ontology. Use Cases: Application ontology build for European Bioinformatics (EBI) tools and databases and Open Targets Genetics Portal. Website: https://www.ebi.ac.uk/efo/ Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#fibrotic-interstitial-lung-disease-ontology-fildo","title":"Fibrotic Interstitial Lung Disease Ontology (FILDO)","text":"<p>Description: An in-progress, four-tiered ontology proposed to standardize the diagnostic classification of patients with fibrotic interstitial lung disease. Disease area: Fibrotic interstitial lung disease Use Cases: Goal is to standardize the diagnostic classification of patients with fibrotic ILD. A paper was published in 2017 and an ontology is not publicly available. Publication: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5803648/ Open: No</p>"},{"location":"reference/medical-ontology-landscape/#genetic-and-rare-diseases-information-center-gard","title":"Genetic and Rare Diseases Information Center (GARD)","text":"<p>Description: NIH resource that provides the public with access to current, reliable, and easy-to-understand information about rare or genetic diseases in English or Spanish. Disease area: Rare diseases Use Cases: Patient portal. Integrates defintions and synonyms from Orphanet, maps to HPO phenotypes, and is integrated by Mondo. Website: https://rarediseases.info.nih.gov/ Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#holistic-ontology-of-rare-diseases-hord","title":"Holistic Ontology of Rare Diseases (HORD)","text":"<p>Description: Describes the biopsychosocial state (i.e., disease, psychological, social, and environmental state) of persons with rare diseases in a holistic way. Disease area: Rare disease Use Cases: Rehabilita, Disruptive Technologies for the Rehabilitation of the Future, a project that aims to enhance rehabilitation transforming it to a more personalized, ubiquitous and evidence-based rehabilitation. Website: http://purl.bioontology.org/ontology/HORD Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#human-dermatological-disease-ontology-dermo","title":"Human Dermatological Disease Ontology (DERMO)","text":"<p>Description: The most comprehensive dermatological disease ontology available, with over 3,500 classes available. There are 20 upper-level disease entities, with features such as anatomical location, heritability, and affected cell or tissue type. Disease area: Dermatology (skin) Use Cases: DermO can be used to extract data from patient electronic health records using text mining, or to translate existing variable-granularity coding such as ICD-10 to allow capture and standardization of patient/disease annotations. Website: https://bioportal.bioontology.org/ontologies/DERMO Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#human-disease-ontology-do","title":"Human Disease Ontology (DO)","text":"<p>Description: An ontology for describing the classification of human diseases organized by etiology. Disease area: Human disease terms, phenotype characteristics and related medical vocabulary disease concepts. Use Cases: Used by Model Organism Databases (MOD), such as Mouse Genome Informatics disease model for diseae annotations, and Alliance for Genome Resources for disease annotations. In 2018, DO tracked over 300 DO project citations suggesting wide adoption and usage for disease annotations. GitHub repo: https://github.com/DiseaseOntology/HumanDiseaseOntology/ Website: http://www.disease-ontology.org/ OBO Foundry webpage: http://obofoundry.org/ontology/doid.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#infectious-disease-ontology-ido","title":"Infectious Disease Ontology (IDO)","text":"<p>Description: A set of interoperable ontologies that will together provide coverage of the infectious disease domain. IDO core is the upper-level ontology that hosts terms of general relevance across the domain, while extension ontologies host terms to specific to a particular part of the domain. Disease area: Infectious disease features, such as acute, primary, secondary infection, and chronic, hospital acquired and local infection. Use Cases: Does not seem active, has not been released since 2017. GitHub repo: https://github.com/infectious-disease-ontology/infectious-disease-ontology/ Website: http://www.bioontology.org/wiki/index.php/Infectious_Disease_Ontology OBO Foundry webpage: http://obofoundry.org/ontology/ido.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#international-classification-of-functioning-disability-and-health-icf","title":"International Classification of Functioning, Disability and Health (ICF)","text":"<p>Description: Represents diseases and provides a conceptual basis for the definition and measurement of health and disability as organized by patient-oriented outcomes of function and disability. ICF considers environmental factors as well as the relevance of associated health conditions in recognizing major models of disability. Disease area: Cross-discipline, focuses on health and disability Use Cases: ICF is the World Health Organization (WHO) framework for measuring health and disability at both individual and population levels. ICF was officially endorsed by the WHO as the international standard to describe and measure health and disability. Website: https://www.who.int/standards/classifications/international-classification-of-functioning-disability-and-health Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#international-statistical-classification-of-diseases-and-related-health-problems-icd-11","title":"International Statistical Classification of Diseases and Related Health Problems (ICD-11)","text":"<p>Description: A medical classification list by the World Health Organization (WHO) that contains codes for diseases, signs and symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or diseases. Disease area: Broad coverage of human disease features, such as disease of anatomical systems, infectious diseases, injuries, external causes of morbidity and mortality. Use Cases: The main purpose of ICD-11 is for clinical care, billing and coding for insurance companies. Website: https://www.who.int/standards/classifications/classification-of-diseases Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#international-classification-of-diseases-for-oncology-icd-o","title":"International Classification of Diseases for Oncology (ICD-O)","text":"<p>Description: A domain-specific extension of the International Statistical Classification of Diseases and Related Health Problems for tumor diseases. Disease area: A multi-axial classification of the site, morphology, behaviour, and grading of neoplasms. Use Cases: Used principally in tumour or cancer registries for coding the site (topography) and the histology (morphology) of neoplasms, usually obtained from a pathology report. Website: https://www.who.int/standards/classifications/other-classifications/international-classification-of-diseases-for-oncology Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#logical-observation-identifier-names-and-codes-loinc","title":"Logical Observation Identifier Names and Codes (LOINC)","text":"<p>Description: Identifies medical laboratory observations. Disease area: Broad coverage Use Cases: The Regenstrief Institute first developed LOINC in 1994 in response to the demand for an electronic database for clinical care and management. LOINC is publicly available at no cost and is endorsed by the American Clinical Laboratory Association and the College of American Pathologists. Since its inception, LOINC has expanded to include not just medical laboratory code names but also nursing diagnoses, nursing interventions, outcome classifications, and patient care data sets. Website: https://loinc.org/ Open: Yes, registration is required.</p>"},{"location":"reference/medical-ontology-landscape/#medical-subject-headings-mesh","title":"Medical Subject Headings (MeSH)","text":"<p>Description: Medical Subject Headings (MeSH) thesaurus is a controlled and hierarchically-organized vocabulary produced by the National Library of Medicine. Disease area: Broad coverage Use Cases: It is used for indexing, cataloging, and searching of biomedical and health-related information. Integrated into Mondo. Website: https://meshb.nlm.nih.gov/search Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#medgen","title":"MedGen","text":"<p>Description: Organizes information related to human medical genetics, such as attributes of conditions and phenotypes of genetic contributions. Disease area: Human medical genetics Use Cases: MedGen is NCBI's portal to information about conditions and phenotypes related to Medical Genetics. Terms from the NIH Genetic Testing Registry (GTR), UMLS, HPO, Orphanet, ClinVar and other sources are aggregated into concepts, each of which is assigned a unique identifier and a preferred name and symbol. The core content of the record may include names, identifiers used by other databases, mode of inheritance, clinical features, and map location of the loci affecting the disorder. The concept identifier (CUI) is used to aggregate information about that concept, similar to the way NCBI Gene serves as a gateway to gene-related information. Website: https://www.ncbi.nlm.nih.gov/medgen/ Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#medical-dictionary-for-regulatory-activities-meddra","title":"Medical Dictionary for Regulatory Activities (MedDRA)","text":"<p>Description: Provides a standardized international medical terminology to be used for regulatory communication and evaluation of data about medicinal products for human use. Disease area: Broad coverage Use Cases: Mainly targeted towards industry and regulatory users. Website: https://www.meddra.org/ Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#mental-disease-ontology-mdo","title":"Mental Disease Ontology (MDO)","text":"<p>Description: An ontology to describe and classify mental diseases such as schizophrenia, annotated with DSM-IV and ICD codes where applicable. Disease area: Mental functioning, including mental processes such as cognition and traits such as intelligence. Use Cases: The ontology has been partially aligned with the related projects Cognitive Atlas, knowledge base on cognitive science and the Cognitive Paradigm Ontology, which is used in the Brainmap, a database of neuroimaging experiments. GitHub repo: https://github.com/jannahastings/mental-functioning-ontology OBO Foundry webpage: http://obofoundry.org/ontology/mfomd.html Open: yes</p>"},{"location":"reference/medical-ontology-landscape/#mondo-disease-ontology-mondo","title":"Mondo Disease Ontology (Mondo)","text":"<p>Description: An integrated disease ontology that provides precise mappings between source ontologies that comprehensively covers cross-species diseases, from common to rare diseases. Disease area: Cross species, intended to cover all areas of diseases, integrating source ontologies that cover Mendelian diseases (OMIM), rare diseases (Orphanet), neoplasms (NCIt), human diseases (DO), and others. See all sources here. Use Cases: Mondo was developed for usage in the Monarch Initiative, a discovery system that allows navigation of similarities between phenotypes, organisms, and human diseases across many data sources and organisms. Mondo is also used by ClinGen for disease curations, the Kids First Data Resource Portal for disease annotations and others, see an extensive list here. GitHub repo: https://github.com/monarch-initiative/mondo Website: https://mondo.monarchinitiative.org/ OBO Foundry webpage: http://obofoundry.org/ontology/mondo.html Open: yes</p>"},{"location":"reference/medical-ontology-landscape/#national-cancer-institute-thesaurus-ncit","title":"National Cancer Institute Thesaurus (NCIT)","text":"<p>Description: NCI Thesaurus (NCIt)is a reference terminology that includes broad coverage of the cancer domain, including cancer related diseases, findings and abnormalities. The NCIt OBO Edition aims to increase integration of the NCIt with OBO Library ontologies. NCIt OBO Edition releases should be considered experimental. Disease area: Cancer and neoplasms Use Cases: NCI Thesaurus (NCIt) provides reference terminology for many National Cancer Institute and other systems. It is used by the Clinical Data Interchange Standards Consortium Terminology (CDISC), the U.S. Food and Drug Administration (FDA), the Federal Medication Terminologies (FMT), and the National Council for Prescription Drug Programs (NCPDP). It provides extensive coverage of neoplasms and cancers. GitHub repo: https://github.com/NCI-Thesaurus/thesaurus-obo-edition/issues Website: https://ncithesaurus.nci.nih.gov/ncitbrowser/pages/home.jsf?version=20.11e OBO Foundry webpage: http://obofoundry.org/ontology/ncit.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#neurological-disease-ontology-nd","title":"Neurological Disease Ontology (ND)","text":"<p>Description: A framework for the representation of key aspects of neurological disease. Disease area: Neurology Use Cases: Goal is to provide a framework to enable representation of aspects of neurological diseases that are relevant to their treatment and study. This project may be inactive, the last commit to GitHub was in 2016. GitHub repo: https://github.com/addiehl/neurological-disease-ontology Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#online-mendelian-inheritance-in-man-omim","title":"Online Mendelian Inheritance in Man (OMIM)","text":"<p>Description: a comprehensive, authoritative compendium of human genes and genetic phenotypes that is freely available and updated daily. Disease area: Mendelian, genetic diseases. Use Cases: Integrated into the disease ontology, used by the Human Phenotype Ontology for disease annotations, patients and researchers. Website: https://omim.org/ Open: yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-of-cardiovascular-drug-adverse-events-ocvdae","title":"Ontology of Cardiovascular Drug Adverse Events (OCVDAE)","text":"<p>Description: A biomedical ontology of cardiovascular drug\u2013associated adverse events. Disease area: Cardiovascular Use Cases: One novel study of the OCVDAE project is the development of the PCR method. Specifically, an AE-specific drug class effect is defined to exist when all the drugs (drug chemical ingredients or drug products) in a drug class are associated with an AE, which is formulated as a proportional class level ratio (\u201cPCR\u201d)\u2009=\u20091. See more information in the paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5653862/. This project may be inactive, the last GitHub commit was in 2019. GitHub repo: https://github.com/OCVDAE/OCVDAE Website: https://bioportal.bioontology.org/ontologies/OCVDAE Open: yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-for-general-medical-science-ogms","title":"Ontology for General Medical Science (OGMS)","text":"<p>Description: An ontology of entities involved in a clinical encounter. Use Cases: Provides a formal theory of disease that can be further elaborated by specific disease ontologies. It is intended to be used as a upper level ontology for other disease ontologies. Used by Cardiovascular Disease Ontology. GitHub repo: https://github.com/OGMS/ogms OBO Foundry webpage: http://obofoundry.org/ontology/ogms.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-for-genetic-susceptibility-factor-ogsf","title":"Ontology for Genetic Susceptibility Factor (OGSF)","text":"<p>Description: An application ontology to represent genetic susceptibility to a specific disease, adverse event, or a pathological process. Use Cases: Modeling genetic susceptibility to vaccine adverse events. GitHub repo: https://github.com/linikujp/OGSF OBO Foundry webpage: http://obofoundry.org/ontology/ogsf.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-of-glucose-metabolism-disorder-ogmd","title":"Ontology of Glucose Metabolism Disorder (OGMD)","text":"<p>Description: Represents glucose metabolism disorder and diabetes disease names, phenotypes, and their classifications. Disease area: Metabolic disorders Use Cases: Still under development (last verssion released in BioPortal was in 2021) but there is little information about its usage online. Website: https://bioportal.bioontology.org/ontologies/OGMD Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-of-language-disorder-in-autism-lda","title":"Ontology of Language Disorder in Autism (LDA)","text":"<p>Description: An ontology assembled from a set of language terms mined from the autism literature. Disease area: Austism Use Cases: This has not been released since 2008 and looks like it is inactive. Website: https://bioportal.bioontology.org/ontologies/LDA Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#the-oral-health-and-disease-ontology-ohd","title":"The Oral Health and Disease Ontology (OHD)","text":"<p>Description: Represents the content of dental practice health records and is intended to be further developed for use in translational medicine. OHD is structured using BFO (Basic Formal Ontology) and uses terms from many ontologies, NCBITaxon, and a subset of terms from the CDT (Current Dental Terminology). Disease area: Oral health and disease Use Cases: Used to represent the content of dental practice health records and is intended to be further developed for use in translation medicine. Appears to be inactive. OBO Foundry webpage: http://www.obofoundry.org/ontology/ohd.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#orphanet-ordo","title":"Orphanet (ORDO)","text":"<p>Description: The portal for rare diseases and orphan drugs. Contains a structured vocabulary for rare diseases capturing relationships between diseases, genes, and other relevant features, jointly developed by Orphanet and the EBI. It contains information on nearly 10,000 cancers and related diseases, 8,000 single agents and combination therapies, and a wide range of other topics related to cancer and biomedical research. Disease area: Rare diseases Use Cases: Used by rare disease research and clinical community. Integrated into the Mondo disease ontology, aligned with OMIM. Website: https://www.orpha.net/consor/cgi-bin/index.php Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#parkinson-disease-ontology-pdo","title":"Parkinson Disease ontology (PDO)","text":"<p>Description: A comprehensive semantic framework with a subclass-based taxonomic hierarchy, covering the whole breadth of the Parkinson disease knowledge domain from major biomedical concepts to different views on disease features held by molecular biologists, clinicians, and drug developers. Disease area: Parkinson disease Use Cases: This resource has been created for use in the IMI-funded AETIONOMY project. Last release was in 2015, may be inactive. Website: https://bioportal.bioontology.org/ontologies/PDON Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#pathogenic-disease-ontology-pdo","title":"Pathogenic Disease Ontology (PDO)","text":"<p>Description: Provides information on infectious diseases, disease synonyms, transmission pathways, disease agents, affected populations, and disease properties. Diseases are grouped into syndromic disease categories, organisms are structured hierarchically, and both disease transmission and relevant disease properties are searchable. Disease area: human infectious diseases caused by microbes and the diseases that is related to microbial infection. Use Cases: Has not been released since 2016 and may be inactive. Website: https://bioportal.bioontology.org/ontologies/PDO Open: Yes.</p>"},{"location":"reference/medical-ontology-landscape/#polycystic-ovary-syndrome-knowledgebase-pcoskb","title":"PolyCystic Ovary Syndrome Knowledgebase (PCOSKB)","text":"<p>Description: Comprises genes, single nucleotide polymorphisms, diseases, gene ontology terms, and biochemical pathways associated with polycystic ovary syndrome, a major cause of female subfertility worldwide. Disease area: polycystic ovary syndrome Use Cases: Ontology underlying the Polycystic Ovary Syndrome Knowledgebase, a manually curated knowledgebase on PCOS. Website: http://pcoskb.bicnirrh.res.in/go_d.php Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#rat-disease-ontology-rdo","title":"Rat Disease Ontology (RDO)","text":"<p>Description: Provides the foundation for ten comprehensive disease area\u2013related data sets at the Rat Genome Database Disease Portals. Disease area: Broad coverage including animal diseases, infectious diseases, chemically-induced disorders, occupational diseases, wounds and injuries and more. Use Cases: Developed for use with the Rat Genome Database Disease Portals. Website: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=DOID:4 Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#removable-partial-denture-ontology-rpdo","title":"Removable Partial Denture Ontology (RPDO)","text":"<p>Description: Represents knowledge of a patient\u2019s oral conditions and denture component parts, originally developed to create a clinician decision support model. Disease area: Oral health and dentures Use Cases: A paper was published on this in 2016 but it does not appear any other information is available about this ontology on the website, presumably it is an inactive project. Publication: https://www.nature.com/articles/srep27855 Open: No</p>"},{"location":"reference/medical-ontology-landscape/#resource-of-asian-primary-immunodeficiency-diseases-rpo","title":"Resource of Asian Primary Immunodeficiency Diseases (RPO)","text":"<p>Description: Represents observed phenotypic terms, sequence variations, and messenger RNA and protein expression levels of all genes involved in primary immunodeficiency diseases. Disease area: Primary immunodeficiency diseases Use Cases: This terminology is used in a freely accessible, dynamic and integrated database for primary immunodeficiency diseases (PID) called Resource of Asian Primary Immunodeficiency Diseases (RAPID), which is available here. Publication: https://academic.oup.com/nar/article/37/suppl_1/D863/1004993 Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#sickle-cell-disease-ontology-scdo","title":"Sickle Cell Disease Ontology (SCDO)","text":"<p>Description: SCDO establishes (a) community-standardized sickle cell disease terms and descriptions, (b) canonical and hierarchical representation of knowledge on sickle cell disease, and (c) links to other ontologies and bodies of work. Disease area: Sickle Cell Disease (SCD). Use Cases: SCDO is intended to be a comprehensive collection of knowledge on SCD, facilitate exploration of new scientific questions and ideas, facilitate seamless data sharing and collaborations including meta-analysis within the SCD community, support the building of databasing and clinical informatics in SCD. GitHub repo: https://github.com/scdodev/scdo-ontology/issues Website: https://scdontology.h3abionet.org/ OBO Foundry webpage: http://obofoundry.org/ontology/scdo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#snomed-clinical-terminology-snomed-ct","title":"SNOMED Clinical Terminology (SNOMED CT)","text":"<p>Description: A comprehensive clinical terminology/ontology used in healthcare settings. Disease area: Broad disease representation for human diseases. Use Cases: Main coding system used in Electronic Health Records (EHRs). Website: https://browser.ihtsdotools.org/? Open: No, requires a license for usage.</p>"},{"location":"reference/medical-ontology-landscape/#symptom-ontology","title":"Symptom Ontology","text":"<p>Description: An ontology of disease symptoms, with symptoms encompasing perceived changes in function, sensations or appearance reported by a patient indicative of a disease. Disease area: Human diseases Use Cases: Developed by the Disease Ontology (DO) team and used for describing symptoms of human diseases in the DO. Website: http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page OBO Foundry webpage: http://obofoundry.org/ontology/symp.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#unified-medical-language-system","title":"Unified Medical Language System","text":"<p>Description: The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services. Disease area: Broad coverage Use Cases: Healthcare settings including electronic health records and HL7. Website: https://www.nlm.nih.gov/research/umls/index.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#phenotype-ontologies","title":"Phenotype ontologies","text":""},{"location":"reference/medical-ontology-landscape/#phenotype-summary-table","title":"Phenotype Summary Table","text":"Name Species Area Ascomycete phenotype ontology (APO) Ascomycota C. elegans phenotype (wbphenotype) C elegans Dictyostelium discoideum phenotype ontology (ddpheno) Dictyostelium discoideum Drosophila Phenotype Ontology (DPO) Drosophila Flora Phenotype Ontology (FLOPO) Viridiplantae Fission Yeast Phenotype Ontology (FYPO) S. pombe Human Phenotype Ontology (HPO) Human HPO - ORDO Ontological Module (HOOM) Human Mammalian Phenotype Ontology (MP) Mammals Ontology of Microbial Phenotypes (OMP) Microbe Ontology of Prokaryotic Phenotypic and Metabolic Characters Prokaryotes Pathogen Host Interaction Phenotype Ontology pathogens Planarian Phenotype Ontology (PLANP) Schmidtea mediterranea Plant Trait Ontology (TO) Viridiplantae Plant Phenology Ontology Plants Unified Phenotype Ontology (uPheno) Cross-species coverage Xenopus Phenotype Ontology (XPO) Xenopus Zebrafish Phenotype Ontology (ZP) Zebrafish"},{"location":"reference/medical-ontology-landscape/#ascomycete-phenotype-ontology-apo","title":"Ascomycete phenotype ontology (APO)","text":"<p>Description: A structured controlled vocabulary for the phenotypes of Ascomycete fungi. Species: Ascomycota GitHub repo: https://github.com/obophenotype/ascomycete-phenotype-ontology/ Webpage: http://www.yeastgenome.org/ OBO Foundry webpage: http://obofoundry.org/ontology/wbphenotype.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#c-elegans-phenotype-wbphenotype","title":"C. elegans phenotype (wbphenotype)","text":"<p>Description: A structured controlled vocabulary of Caenorhabditis elegans phenotypes. Species: C elegans GitHub repo: https://github.com/obophenotype/c-elegans-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/wbphenotype.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#dictyostelium-discoideum-phenotype-ontology-ddpheno","title":"Dictyostelium discoideum phenotype ontology (ddpheno)","text":"<p>Description: A structured controlled vocabulary of phenotypes of the slime-mould Dictyostelium discoideum. Species: Dictyostelium discoideum GitHub repo: https://github.com/obophenotype/dicty-phenotype-ontology/issues Webpage: http://dictybase.org/ OBO Foundry webpage: http://obofoundry.org/ontology/ddpheno.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#drosophila-phenotype-ontology-dpo","title":"Drosophila Phenotype Ontology (DPO)","text":"<p>Description: An ontology of commonly encountered and/or high level Drosophila phenotypes. Species: Drosophila GitHub repo: https://github.com/obophenotype/c-elegans-phenotype-ontology Webpage: http://purl.obolibrary.org/obo/fbcv OBO Foundry webpage: http://obofoundry.org/ontology/dpo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#flora-phenotype-ontology-flopo","title":"Flora Phenotype Ontology (FLOPO)","text":"<p>Description: Traits and phenotypes of flowering plants occurring in digitized Floras. Species: Viridiplantae GitHub repo: https://github.com/flora-phenotype-ontology/flopoontology/ OBO Foundry webpage: http://obofoundry.org/ontology/flopo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#fission-yeast-phenotype-ontology-fypo","title":"Fission Yeast Phenotype Ontology (FYPO)","text":"<p>Description: FYPO is a formal ontology of phenotypes observed in fission yeast. Species: S. pombe GitHub repo: https://github.com/pombase/fypo OBO Foundry webpage: http://obofoundry.org/ontology/fypo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#human-phenotype-ontology-hpo","title":"Human Phenotype Ontology (HPO)","text":"<p>Description: HPO provides a standardized vocabulary of phenotypic abnormalities encountered in human disease. Each term in the HPO describes a phenotypic abnormality. Species: Human GitHub repo: https://github.com/obophenotype/human-phenotype-ontology Website: https://hpo.jax.org/app/ OBO Foundry webpage: http://obofoundry.org/ontology/hp.html Open: yes</p>"},{"location":"reference/medical-ontology-landscape/#hpo-ordo-ontological-module-hoom","title":"HPO - ORDO Ontological Module (HOOM)","text":"<p>Description: Orphanet provides phenotypic annotations of the rare diseases in the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion. In ORDO a clinical entity is either a group of rare disorders, a rare disorder or a subtype of disorder. The phenomes branch of ORDO has been refactored as a logical import of HPO, and the HPO-ORDO phenotype disease-annotations have been provided in a series of triples in OBAN format in which associations, frequency and provenance are modeled. HOOM is provided as an OWL (Ontologies Web Languages) file, using OBAN, the Orphanet Rare Disease Ontology (ORDO), and HPO ontological models. HOOM provides extra possibilities for researchers, pharmaceutical companies and others wishing to co-analyse rare and common disease phenotype associations, or re-use the integrated ontologies in genomic variants repositories or match-making tools. Species: Human Website: http://www.orphadata.org/cgi-bin/img/PDF/WhatIsHOOM.pdf BioPortal: https://bioportal.bioontology.org/ontologies/HOOM Open: yes</p>"},{"location":"reference/medical-ontology-landscape/#mammalian-phenotype-ontology-mp","title":"Mammalian Phenotype Ontology (MP)","text":"<p>Description: Standard terms for annotating mammalian phenotypic data. Species: Mammals (main focus is on mouse and rodents) GitHub repo: https://github.com/obophenotype/mammalian-phenotype-ontology Website: http://www.informatics.jax.org/searches/MP_form.shtml OBO Foundry webpage: http://obofoundry.org/ontology/mp.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-of-microbial-phenotypes-omp","title":"Ontology of Microbial Phenotypes (OMP)","text":"<p>Description: An ontology of phenotypes covering microbes. Species: microbes GitHub repo: https://github.com/microbialphenotypes/OMP-ontology Website: http://microbialphenotypes.org OBO Foundry webpage: http://obofoundry.org/ontology/omp.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#ontology-of-prokaryotic-phenotypic-and-metabolic-characters","title":"Ontology of Prokaryotic Phenotypic and Metabolic Characters","text":"<p>Description: An ontology of phenotypes covering microbes. Species: Prokaryotes GitHub repo: https://github.com/microbialphenotypes/OMP-ontology/issues Website: http://microbialphenotypes.org/ OBO Foundry webpage: http://obofoundry.org/ontology/omp.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#pathogen-host-interaction-phenotype-ontology","title":"Pathogen Host Interaction Phenotype Ontology","text":"<p>Description: PHIPO is a formal ontology of species-neutral phenotypes observed in pathogen-host interactions. Species: pathogens GitHub repo: https://github.com/PHI-base/phipo Website: http://www.phi-base.org OBO Foundry webpage: http://obofoundry.org/ontology/phipo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#planarian-phenotype-ontology-planp","title":"Planarian Phenotype Ontology (PLANP)","text":"<p>Description: Planarian Phenotype Ontology is an ontology of phenotypes observed in the planarian Schmidtea mediterranea. Species: Schmidtea mediterranea GitHub repo: https://github.com/obophenotype/planarian-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/planp.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#plant-trait-ontology-to","title":"Plant Trait Ontology (TO)","text":"<p>Description: A controlled vocabulary of describe phenotypic traits in plants. Species: Viridiplantae GitHub repo: https://github.com/Planteome/plant-trait-ontology/ OBO Foundry webpage: http://obofoundry.org/ontology/to.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#plant-phenology-ontology","title":"Plant Phenology Ontology","text":"<p>Description: An ontology for describing the phenology of individual plants and populations of plants, and for integrating plant phenological data across sources and scales. Species: Plants GitHub repo: https://github.com/PlantPhenoOntology/PPO OBO Foundry webpage: http://obofoundry.org/ontology/ppo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#unified-phenotype-ontology-upheno","title":"Unified Phenotype Ontology (uPheno)","text":"<p>Description: The uPheno ontology integrates multiple phenotype ontologies into a unified cross-species phenotype ontology. Species: Cross-species coverage GitHub repo: https://github.com/obophenotype/upheno OBO Foundry webpage: http://obofoundry.org/ontology/upheno.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#xenopus-phenotype-ontology-xpo","title":"Xenopus Phenotype Ontology (XPO)","text":"<p>Description: XPO represents anatomical, cellular, and gene function phenotypes occurring throughout the development of the African frogs Xenopus laevis and tropicalis. Species: Xenopus GitHub repo: https://github.com/obophenotype/xenopus-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/xpo.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#zebrafish-phenotype-ontology-zp","title":"Zebrafish Phenotype Ontology (ZP)","text":"<p>Description: The Zebrafish Phenotype Ontology formally defines all phenotypes of the Zebrafish model organism. Species: Zebrafish GitHub repo: https://github.com/obophenotype/zebrafish-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/zp.html Open: Yes</p>"},{"location":"reference/medical-ontology-landscape/#references","title":"References","text":"<ul> <li>A Census of Disease Ontologies Melissa A. Haendel, Julie A. McMurry, Rose Relevo, Christopher J. Mungall, Peter N. Robinson, Christopher G. Chute   Annual Review of Biomedical Data Science 2018 1:1, 305-331</li> <li>OMOP2OBO repository</li> </ul>"},{"location":"reference/mungall-blog-radar/","title":"Monkeying around with OWL","text":"<p>An index page to find some of our favourite articles on Chris' blog. These are not all articles, but I selection we found useful during our every work.</p>"},{"location":"reference/mungall-blog-radar/#ontology-development-and-modelling","title":"Ontology development and modelling","text":"<ul> <li> <p>OntoTips Series. Must read series for the beginning ontology developer.</p> </li> <li> <p>Warning about complex modelling. Chris is generally big on Occam's Razor solutions: given two solutions that solve a use case, the simpler is better.</p> </li> <li> <p>OntoTip: Don\u2019t over-specify OWL definitions. From the above OntoTip series.</p> </li> <li> <p>How to deal with unintentional equivalent classes</p> </li> </ul>"},{"location":"reference/mungall-blog-radar/#ontology-curation","title":"Ontology curation","text":"<ul> <li>OntoTip: Write simple, concise, clear, operational textual definitions. One of our favourite blog posts of Chris. Must read!</li> </ul>"},{"location":"reference/obook-maturity-indicator/","title":"OBOOK Maturity Indicator","text":"<p>Some resources on OBOOK are less well developed than others. We use the OBOOK Maturity Indicator to document this (discussion).</p> <ul> <li>: This page is under development and not yet suitable for self-study or teaching.</li> <li>: This page is still under development (may be rough around the edges) but complete and suitable for self-study or teaching.</li> <li>: This page is developed to an extent it can be considered stable (although nothing is ever really stable) and therefore suitable for self-study or teaching.</li> </ul> <p>To add a status badge onto a site, simply paste a badge like this right under the title:</p> <pre><code>&lt;a href=\"https://oboacademy.github.io/obook/reference/obook-maturity-indicator/\"&gt;&lt;img src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2FOBOAcademy%2Fobook%2Fmaster%2Fdocs%2Fresources%2Fobook-badge-final.json\" /&gt;&lt;/a&gt;\n</code></pre>"},{"location":"reference/odk/","title":"Ontology Development Kit (ODK) Reference","text":"<p>The ODK is essentially two things:</p> <ol> <li>A toolbox. All frequently used tools for managing the ontology life cycle are bundled together into a Docker image: ROBOT, owltools, fastobo-validator, dosdp-tools, riot and many, many more.</li> <li>A system, you could have say \"methodology\" for managing the ontology life cycle from continious integration and quality control to imports and release management.</li> </ol>"},{"location":"reference/odk/#the-toolbox","title":"The Toolbox","text":"<p>The ODK bundles a lot of tools together, such as ROBOT, owltools, fastobo-validator and dosdp-tools. To get a better idea, its best to simply read the Dockerfile specifications of the ODK image:</p> <ul> <li>ODK Lite Image. This contains the most essentials tools related to ODK development. Most of the day to day activities of ontology developers with ROBOT are well covered by odklite.</li> <li>ODK Full Image. Extends the ODK Lite image with a further round of powerful tools. It contains for example Apache Jena, the OBO Dashboard, the Konclude reasoner and a large array of command line tools.</li> </ul>"},{"location":"reference/odk/#the-system-for-ontology-life-cycle-management","title":"The system for ontology life cycle management","text":"<p>One of the tools in the toolbox, the \"seed my repo\" function, allows us to generate a complete GitHub repository with everything needed to manage an OBO ontology according to OBO best practices. The two central components are</p> <ol> <li>A Makefile that encodes the rules by which ontology release files should be derived from the source of truth (the edit file).</li> <li>A support for CI such as GitHub actions or Travis for running continuous integration checks.</li> </ol>"},{"location":"reference/odk/#odk-project-configuration-schema","title":"ODK Project Configuration Schema","text":"<p>Schema can be found in ODK documentation here</p>"},{"location":"reference/ontology-curator/","title":"A Day in the Life of an Ontology Curator","text":"<ol> <li>Review issues on the issue tracker.</li> <li>Tickets by organized by assigning labels (such as new term requests) and milestones</li> <li>Can also sort tickets on Project boards<ul> <li>Can search on labels, milestones, assignee, etc.</li> </ul> </li> <li>In Mondo, we set priorities based on user requests, size of the ticket (ie amount of work required), if it is blocking something else, etc.</li> <li>Edits to the Mondo ontology are made on Branches and via Pull Requests on the mondo-edit.obo file.</li> <li>Example: work on an open ticket to add a new term using Protege.</li> <li>Detailed instructions on how to add a new term are here.</li> <li>Example: User request to add 50+ subtyps of acute myeloid leukemia. We used a ROBOT template.</li> <li>Do you want to contribute? See tickets labeled good first issue.</li> <li>Documentation and more instructions are available in the Mondo editors guide.</li> </ol>"},{"location":"reference/ontology-user/","title":"Ontology User References","text":"<p>Description: This reference guide includes resources on ontology use, such as how to use ontologies when annotating data, how to use ontologies for search and data analysis, and specific use cases.</p> <p>Preparation: A basic understanding of what ontologies are is helpful. Some introductory resources are below:</p> <ul> <li>OBO Academy Introduction to Ontologies</li> <li>Introduction to ontology semantics and reasoning by Henriette Harmse</li> </ul>"},{"location":"reference/ontology-user/#general-ontology-use-guidelines","title":"General Ontology Use Guidelines","text":"<ul> <li>OBO Academy Using Ontologies and Ontology Terms</li> <li>Elixir's Selecting terminologies and ontologies</li> <li>Elixir's Ontology mapping with Ontology Xref Service (OxO)</li> <li>Biological ontology use guidelines by Henriette Harmse</li> </ul>"},{"location":"reference/ontology-user/#gene-ontology-curation","title":"Gene Ontology curation","text":"<ul> <li>Curating and browsing with Gene Ontology (GO)</li> <li>Gene Ontology handbook</li> </ul>"},{"location":"reference/ontology-user/#disease-ontology-specific-applications","title":"Disease Ontology Specific Applications","text":""},{"location":"reference/ontology-user/#clinical-applications-of-the-human-disease-ontology-do","title":"Clinical Applications of the Human Disease Ontology (DO)","text":"<p>A video library is available that covers:</p> <ul> <li>Clinical applications of the Human Disease Ontology</li> <li>How is the Human Disease Ontology FAIR?</li> <li>Searching the Human Disease Ontology website</li> <li>What is an ontology?</li> <li>Mining disease information via imports: Connecting disease-related information</li> <li>How the Human Disease Ontology is used for drug studies</li> <li>Cancer resources and tools utilizing the Human Disease Ontology</li> <li>Advanced searches of the DO website using relation axioms</li> </ul>"},{"location":"reference/ontology-user/#using-the-mondo-disease-ontology-for-disease-data-curation","title":"Using the Mondo Disease Ontology for Disease Data Curation","text":"<ul> <li>Slides on Using the Mondo Disease Ontology for Disease Data Curation</li> <li>User guide for the Mondo Disease ontology</li> </ul>"},{"location":"reference/ontology-user/#using-ontologies-to-standardize-rare-disease-data-collection","title":"Using ontologies to standardize rare disease data collection","text":"<p>Video of webinar: Using ontologies to standardize rare disease data collection</p>"},{"location":"reference/ontology-user/#project-specific-applications","title":"Project Specific Applications","text":"<p>Ontology application and use at the ENCODE DCC</p>"},{"location":"reference/ontology-user/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> <li>Sue Bello</li> <li>Sierra Moxon</li> <li>Shawn Tan</li> <li>Sabrina Toro</li> <li>Danielle Welter</li> <li>N\u00faria Queralt Rosinach</li> <li>Marc Robinson-Rechavi</li> </ul>"},{"location":"reference/other-resources/","title":"Other Resources","text":"<p>Here's a collection of links about the Open Biological and Biomedical Ontologies (OBO), and related topics.</p> <p>If you're completely new to OBO, I suggest starting with Ontologies 101:</p> <ul> <li>Unit 1: Controlled Vocabularies, Ontologies, and Data Linking (PowerPoint Slides)</li> <li>Unit 2: An Introduction to OWL (Powerpoint Slides)</li> <li>Unit 3: Ontology Community (Powerpoint Slides)</li> <li>BDK14 Ontologies 101 repository</li> </ul> <p>If you're new to scientific computing more generally, then I strongly recommend Software Carpentry, which provides a set of very pragmatic introductions to the Unix command line, git, Python, Make, and other tools widely used by OBO developers.</p>"},{"location":"reference/other-resources/#open-biological-and-biomedical-ontologies","title":"Open Biological and Biomedical Ontologies","text":"<p>OBO is a community of people collaborating on open source ontologies for science. We have a set of shared principles and best practises to help people and data work together effectively.</p> <ul> <li>OBO Foundry Homepage</li> <li>The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration (journal article)</li> <li>OBO Discuss mailing list</li> </ul>"},{"location":"reference/other-resources/#services","title":"Services","text":"<p>Here is a very incomplete list of some excellent services to help you find an use OBO terms and ontologies.</p> <ul> <li>EMBL-EBI</li> <li>OLS: Ontology Lookup Service     is an excellent ontology browser and search service</li> <li>OxO     shows mappings between ontologies and terms</li> <li>Zooma     for mapping free text to ontology terms</li> <li>Onto-Animals</li> <li>Ontobee     is an ontology browser</li> <li>Ontofox     is an ontology extraction tool</li> <li>Bioportal   provides ontology browsing, search, mapping, etc.</li> <li>Bioregistry   provides metadata for ontologies and other semantic resources</li> </ul>"},{"location":"reference/other-resources/#tools","title":"Tools","text":"<p>This is the suite of open source software that most OBO developers use.</p> <ul> <li>OBO Tools mailing list</li> <li>GitHub   is where most OBO projects are hosted   and what we use to manage code, issues, etc.</li> <li>GitHub tutorial</li> <li>Prot\u00e9g\u00e9   is a graphical user interface for editing OWL ontologies. (Java)</li> <li>ROBOT   is a command-line tool for automating ontology tasks. (Java)</li> <li>ROBOT tutorial</li> <li>ROBOT: A Tool for Automating Ontology Workflows (journal article)</li> <li>ENVO ROBOT Template and Merge Workflow</li> <li>DOS-DP   is a command-line tool for working with ontology design patterns. (Python)</li> <li>ODK: Ontology Development Kit   is a collection of tools for building and maintaining an OBO project. (Docker)</li> <li>OBO Tools and Workflows (Google Slides)     A good overview of technical and advanced topics of OBO practises,     including the Ontology Development Kit.</li> <li>OWLAPI   is a Java library for working with ontologies,   and is the foundation for Prot\u00e9g\u00e9 and ROBOT.</li> <li><code>curies</code> is a Python library for working with prefix maps   and extended prefix maps and converting between CURIEs and URIs.</li> <li>OBO PURL System   is used to redirect OBO terms from their IRIs to the right resource</li> <li>String of PURLs \u2013 frugal migration and maintenance of persistent identifiers (journal article)</li> </ul>"},{"location":"reference/other-resources/#technical","title":"Technical","text":"<p>This section is for technical reference, not beginners.</p> <p>OBO projects use Semantic Web and Linked Data technologies:</p> <ul> <li>W3C Semantic Web overview</li> <li>Search for W3C data standards</li> <li>W3C Data on the Web Best Practices</li> </ul> <p>These standards form layers:</p> <ol> <li>IRI: Internationalized Resource Identifiers    are a superset of the familiar URLs used to locate resources on the web.    Every ontology term has a globally unique IRI.</li> <li>RDF: Resource Description Format    is a standard for combining IRIs into subject-predicate-object \"triples\"    that make a statement about some thing.    Sets of triples form a graph (i.e. network),    and graphs can easily be merged to form larger graphs.    SPARQL is the language for querying RDF graphs.</li> <li>RDF 1.1 Primer</li> <li>SPARQL 1.1 Overview</li> <li>RDFS: RDF Schema 1.1    extends RDF with classes, hierarchies, and other features.</li> <li>XSD: W3C XML Schema Definition Language (XSD) 1.1 Part 2: Datatypes    is the common standard for datatypes in RDF</li> <li>OWL: Web Ontology Language    extends RDF and RDFS to provide more powerful logic</li> <li>OWL 2 Web Ontology Language Primer (Second Edition)</li> </ol> <p>Other useful resources on technical topics:</p> <ul> <li>Monkeying around with OWL   Chris Mungall's blog, mostly on technical topics for ontologies.</li> </ul>"},{"location":"reference/outreach/","title":"OBO Academy Outreach","text":""},{"location":"reference/outreach/#cite-us","title":"Cite us","text":"<p>Nicole Vasilevsky, James Overton, Rebecca Jackson, Sabrina Toro, Shawn Tan, Bradley Varner, David Osumi-Sutherland, &amp; Nicolas Matentzoglu. (2022, August 3). OBO Academy: Training materials for bio-ontologists. 2022 ISMB Bio-Ontologies Community, Madison, WI. https://doi.org/10.5281/zenodo.6955490</p>"},{"location":"reference/outreach/#generic-obo-academy-slide-deck","title":"Generic OBO Academy slide deck","text":"<p>Available here. Please feel free to use this slide deck to promote the OBO Academy.</p>"},{"location":"reference/outreach/#presentations","title":"Presentations","text":"<ul> <li>Nicole Vasilevsky, James Overton, Rebecca Jackson, Sabrina Toro, Shawn Tan, David Osumi-Sutherland, Nicolas Matentzoglu. \"OBO Academy:  Training materials for Bio-ontologists\". Presentation at OntoCommons, Virtual. June 14, 2023.</li> <li>Nicole Vasilevsky and Nico Matentzoglu. Panel discussion at Knowledge Graph Conference 2023. May 08, 2023.</li> <li>Nicole Vasilevsky, James Overton, Rebecca Jackson, Sabrina Toro, Shawn Tan, Bradley Varner, David Osumi-Sutherland, Nicolas Matentzoglu. \"OBO Academy: Training materials for Bio-ontologists.\" [Poster presentation] Biocuration 2022, Virtual. October 04, 2022.</li> <li>James Overton, Rebecca Jackson, Sabrina Toro, Shawn Tan, Bradley Varner, David Osumi-Sutherland, Nicolas Matentzoglu, Nicole Vasilevsky. \"OBO Academy: Training materials for Bio-ontologists.\" [Lightning talk and Poster presentation] ICBO, Ann Arbor, MI. September 27, 2022. https://zenodo.org/record/7116623</li> <li>Nicole Vasilevsky, James Overton, Rebecca Jackson, Sabrina Toro, Shawn Tan, Bradley Varner, David Osumi-Sutherland, &amp; Nicolas Matentzoglu. (2022, August 3). \"OBO Academy: Training materials for bio-ontologists.\" Presentation at 2022 ISMB Bio-Ontologies Community, Madison, WI, July 2022 </li> </ul>"},{"location":"reference/protege-faq/","title":"Proteg\u00e9 FAQs","text":""},{"location":"reference/protege-faq/#how-to-escape-characters-in-the-class-expression-editor","title":"How to escape characters in the class expression editor","text":"<p>To add an ontology term (such as a GO term) that contains <code>'</code> in its name (e.g. <code>RNA-directed 5'-3' RNA polymerase activity</code>) in the class expression editor, you need to escape the <code>'</code> characters. In Proteg\u00e9 5.5 this is not automatically handled when you auto-complete with tab. To escape the character append <code>\\</code> before the <code>'</code> -&gt; <code>RNA-directed 5\\'-3\\' RNA polymerase activity</code>. You won't be able to add the annotation otherwise.</p> <p>As in Proteg\u00e9 5.5, the <code>\\</code> characters will show up in the description window, and when hovering over the term, you won't be able to click on it with a link. However, when you save the file, the relationship is saved correctly. You can double-check by going to the ontology text file and see that the term is correctly mentioned in the relationship.</p>"},{"location":"reference/protege-interface/","title":"Reference document for protege interface","text":"<p>For this reference, we will use the cell ontology to highlight the key information on the user interface in Protege</p>"},{"location":"reference/protege-interface/#general-interface-buttons","title":"General interface buttons","text":"<p>'+' button (not shown above) = add '?' button = explain axiom '@' button = annotate 'x' button = remove 'o' button = edit</p>"},{"location":"reference/protege-interface/#active-ontology-tab","title":"Active Ontology tab","text":""},{"location":"reference/protege-interface/#overview","title":"Overview","text":"<p>When you open the ontology on protege, you should land on the Active ontology tab, alternatively, it is available on the top as one of your tabs.</p> <p></p>"},{"location":"reference/protege-interface/#ontology-level-annotations","title":"Ontology Level Annotations","text":"<p>Annotations on the active ontology tab are ontology level annotations and contain metadata about the ontology. This includes:</p> <ol> <li>title (name of the ontology)</li> <li>description</li> <li>license</li> <li>contributors (ideally this should be in ORCID but many ontologies use names instead)</li> <li>references (under rdfs:comment)</li> <li>preferred_root (this allows certain browsers to know which root to display the ontology from)</li> </ol>"},{"location":"reference/protege-interface/#entities-tab","title":"Entities tab","text":"<p>Entities are where your \"entries\" in the ontology live and where you can add terms etc.</p> <p></p>"},{"location":"reference/reasoning/","title":"Why do we need reasoning?","text":"<p>A quick personal perspective up-front. When I was finishing my undergrad, I barely had heard the term Semantic Web. What I had heard vaguely intrigued me, so I decided that for my final project, I would try to combine something Semantic Web related with my other major, Law and build a tool that could automatically infer the applicability of a law (written in OWL) given a legal case. Super naively, I just went went ahead, read a few papers about legal ontologies, build a simple one, loaded it into my application and somehow got it to work, with reasoning and all, without even having heard of Description Logic.</p> <p>In my PhD, I worked on actual reasoning algorithms, which meant, no more avoiding logic. But - I did not get it. Up until this point in my life, I could just study harder and harder, and in the end I was confident with what I learned, but First Order Logic, in particular model theory and proofs, caused me anxiety until the date of my viva. In the end, a very basic understanding of model theory and Tableau did help me with charactering the algorithms I was working with (I was studying the effect of modularity, cutting out logically connected subsets of an ontology, on reasoning performance) but I can confidently say today: I never really, like deeply, understood logical proofs. I still cant read them - and I have a PhD in Reasoning (albeit from an empirical angle).</p> <p>If you followed the Open HPI courses on logic, and you are anything like me, your head will hurt and you will want to hide under your blankets. Most students feel like that. For a complete education in Semantic Web technologies, going through this part once is essential: it tells you something about how difficult some stuff is under the hood, and how much work has been done to make something like OWL work for knowledge representation. You should have gained some appreciation of the domain, which is no less complex than Machine Learning or Stochastic Processes. But, in my experience, some of the most effective ontology engineers barely understand reasoning - definitely have no idea how it works - and still do amazing work. In that spirit, I would like to invite you at this stage to put logic and reasoning behind you (unless it made you curious of course) - you won't need to know much of that for being an effective Semantic Engineer. In the following, I will summarise some of the key take-aways that I find useful to keep in mind.</p> <ul> <li>Semantics define how to interpret an ontology. For example, in OWL, the statement <code>Human SubClassOf: Mammal</code> means that all instances of the <code>Human</code> class, like me, are also instances of the <code>Mammal</code> class. Or, in other words, from the statements:</li> </ul> <pre><code>Human SubClassOf: Mammal\nNico type: Human\n</code></pre> <p>Semantics allow as to deduce that <code>Nico:Mammal</code>. What are semantics practically? Show me your semantics? Look at something like the OWL semantics. In there, you will find language statements (syntax) like <code>X SubClassOf: Y</code> and a bunch of formulae from model theory that describe how to interpret it - no easy read, and not really important for you now.</p> <ul> <li>OWL has a number of profiles, basically sub-languages where you can say less things. Why would we want to restrict our \"expressivity\"? Because their is a trade-off. An important slide I remember from when I learned about ontology languages was the triangle of complexity (here only paraphrased from memory):</li> </ul> <p></p> <ol> <li>When expressivity goes up, cognitive complexity and computational complexity go up.</li> <li>When we want to decrease cognitive complexity (make it easier to build ontologies), expressivity goes down.</li> <li> <p>When we want reasoners to be faster at making inferences (computational complexity), we need to decrease expressivity.    So we need to find a way to balance.</p> </li> <li> <p>What are the most important practical applications of reasoning? There are many, and there will be many opinions, but in the OBO world, by far (95%) of all uses of reasoners pertain to the following:</p> </li> <li>Classification. Most, if not all, of our ontologies are conceptually hierarchies of classes we use reasoners to automatically infer hierarchies. Look for example at the Xenopus Phenotype Ontology - the class hierarchy is entirely build with a reasoner - no Human intervention!</li> <li>Debugging. There are two major threats to ontologies. In the worst case, they can be <code>inconsistent</code> - which means, totally broken. A slightly less bad, but still undesirable situation is that some of the classes in your ontologies break (in parlance, become unsatisfiable). This happens when you say some contradictory things about them. Reasoners help you find these unsatisfiable classes, and there is a special reasoning algorithm that can generate an explanation for you - to help fixing your problem.</li> <li> <p>So in general, what is reasoning? There are probably a dozen or more official characterisations in the scientific literature, but from the perspective of biomedical ontologies, the question can be roughly split like this:</p> </li> <li> <p>How can we capture what we know? This is the (research-) area of knowledge representation, logical formalisms, such as First Order Logic, Description Logic, etc. It is concerned with how we write down what we now:</p> </li> </ol> <pre><code>All cars have four wheels\nIf you are a human, you are also a mammal\nIf you are a bird, you can fly (unless you are a penguin)\n</code></pre> <ol> <li>How can we uncover implicit knowledge efficiently? This is the area of reasoning, and while being closely related to the formalisms above, it makes sense to think of them in a distinct manner, as the problems are very different in practice. It can typically be grouped into the following two categories:</li> <li>deductive reasoning infers by</li> </ol> <p>Lets think about a naive approach: using a fact-, or data-, base.</p>"},{"location":"reference/release-artefacts/","title":"Release artefacts","text":"<p>For explanation of different release artefacts, please see discussion documentation on owl format variants</p> <p>We made a first stab add defining release artefacts that should cover all use cases community-wide. We need to (1) agree they are all that is needed and (2) they are defined correctly in terms of ROBOT commands. This functionality replaces what was previously done using OORT.</p>"},{"location":"reference/release-artefacts/#terminology","title":"Terminology:","text":"<p>The source ontology is the ontology we are talking about. A release artefact is a version of the ontology modified in some specific way, intended for public use. An import is a module of an external ontology which contains all the axioms necessary for the source ontology. A component is a file containing axioms that belong to the source ontology (but are for one reason or another, like definitions.owl, managed in a separate file). An axiom is said to be foreign if it 'belongs' to a different ontology, and native if it belongs to the source ontology. For example, the source ontology might have, for one reason or another, been physically asserted (rather than imported) the axiom TransitiveObjectProperty(BFO:000005). If the source ontology does not 'own' the BFO namespace, this axiom will be considered foreign.</p> <p>There are currently 6 release defined in the ODK:</p> <ul> <li>base (required)</li> <li>full (required)</li> <li>non-classified (optional)</li> <li>simple (optional)</li> <li>basic (optional)</li> <li>simple-non-classified (optional, transient)</li> </ul> <p>We discuss all of them here in detail.</p>"},{"location":"reference/release-artefacts/#release-artefact-1-base-required","title":"Release artefact 1: base (required)","text":"<p>Base specification defined here.</p>"},{"location":"reference/release-artefacts/#release-artefact-2-full-required","title":"Release artefact 2: full (required)","text":"<p>The full release artefact contains all logical axioms, including inferred subsumptions. Redundancy stripping (i.e. redundant subclass of axioms) and typical relaxation operations are performed. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. Every OBO ontology will contain a mandatory full release.</p> <p>The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies</p> <pre><code>$(ONT)-full.owl: $(SRC) $(OTHER_SRC)\n    $(ROBOT) merge --input $&lt; \\\n        reason --reasoner ELK \\\n        relax \\\n        reduce -r ELK \\\n        annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@\n</code></pre>"},{"location":"reference/release-artefacts/#release-artefact-3-non-classified-optional","title":"Release artefact 3: non-classified (optional)","text":"<p>The non-classified release artefact reflects the 'unmodified state' of the editors file at release time. No operations are performed that modify the axioms in any way, in particular no redundancy stripping. As opposed to the base artefact, both component and imported ontologies are merged into the non-classified release.</p> <p>The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies</p> <pre><code>$(ONT)-non-classified.owl: $(SRC) $(OTHER_SRC)\n    $(ROBOT) merge --input $&lt; \\\n        annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@\n</code></pre>"},{"location":"reference/release-artefacts/#release-artefact-4-simple-optional","title":"Release artefact 4: simple (optional)","text":"<p>Many users want a release that can be treated as a simple existential graph of the terms defined in an ontology. This corresponds to the state of OBO ontologies before logical definitions and imports. For example, the only logical axioms in -simple release of CL will contain be of the form <code>CL1 subClassOf CL2</code> or <code>CL1 subClassOf R some CL3</code> where R is any objectProperty and CLn is a CL class. This role has be fulfilled by the -simple artefact, which up to now has been supported by OORT.</p> <p>To construct this, we first need to assert inferred classifications, relax equivalentClass axioms to sets of subClassOf axioms and then strip all axioms referencing foreign (imported) classes. As ontologies occasionally end up with forieign classes and axioms merged into the editors file, we achieve this will a filter based on obo-namespace. (e.g. finding all terms with iri matching http://purl.obolibrary.org/obo/CL_{\\d}7).</p> <p>The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(SIMPLESEED): all terms that 'belong' to the ontology</p> <pre><code>$(ROBOT) merge --input $&lt; $(patsubst %, -i %, $(OTHER_SRC)) \\\n    reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} \\\n    relax \\\n    remove --axioms equivalent \\\n    relax \\\n    filter --term-file $(SIMPLESEED) --select \"annotations ontology anonymous self\" --trim true --signature true \\\n    reduce -r {{ project.reasoner }} \\\n    annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@\n</code></pre> <p>NOTES: This requires $(ONTOLOGYTERMS) to include all ObjectProperties usesd. <code>--select parents</code> is required for logical axioms to be retained, but results in a few upper-level classes bleeding through. We hope this will be fixed by further improvments to Monarch.</p>"},{"location":"reference/release-artefacts/#release-artefact-5-basic","title":"Release artefact 5: basic","text":"<p>Some legacy users (e.g. MGI) require an OBO DAG version of -simple. OBO files derived from OWL are not guarenteed to be acyclic, but acyclic graphs can be achieved using judicious filtering of relationships (simple existential restrictions) by objectProperty. The -basic release artefact has historically fulfilled this function as part of OORT driven ontology releases. The default -basic version corresponds to the -simple artefact with only 'part of' relationships (BFO:0000050), but others may be added where ontology editors judge these to be useful and safe to add without adding cycles. We generate by taking the simple release and filtering it</p> <p>The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(KEEPRELATIONS): all relations that should be preserved. $(SIMPLESEED): all terms that 'belong' to the ontology</p> <pre><code>$(ROBOT) merge --input $&lt; $(patsubst %, -i %, $(OTHER_SRC)) \\\n    reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} \\\n    relax \\\n    remove --axioms equivalent \\\n    remove --axioms disjoint \\\n    remove --term-file $(KEEPRELATIONS) --select complement --select object-properties --trim true \\\n    relax \\\n    filter --term-file $(SIMPLESEED) --select \"annotations ontology anonymous self\" --trim true --signature true \\\n    reduce -r {{ project.reasoner }} \\\n    annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@\n</code></pre>"},{"location":"reference/release-artefacts/#release-artefact-6-simple-non-classified-optional","title":"Release artefact 6: simple-non-classified (optional)","text":"<p>This artefact caters to the very special and hopefully transient case of some ontologies that do not yet trust reasoning (MP, HP). The simple-non-classified artefact corresponds to the simple artefact, just without the reasoning step.</p> <p>$(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONTOLOGYTERMS): all terms that 'belong' to the ontology</p> <pre><code>$(ONT)-simple-non-classified.owl: $(SRC) $(OTHER_SRC) $(ONTOLOGYTERMS)\n    $(ROBOT) remove --input $&lt; --select imports \\\n        merge  $(patsubst %, -i %, $(OTHER_SRC))  \\\n        relax \\\n        reduce -r ELK \\\n        filter --term-file $(ONTOLOGYTERMS) --trim true \\\n        annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@\n</code></pre>"},{"location":"reference/semantic-engineering-toolbox/","title":"The Semantic OBO Engineer's Toolbox","text":"<p>Essentials</p> <ul> <li>Prot\u00e9g\u00e9</li> <li>DL Query Tab</li> <li>ROBOT</li> <li>OBO Dashboard: OBO-wide quality control monitor for OBO ontologies.</li> </ul> <p>Automation</p> <ul> <li>GNU Make</li> <li>Ontology Development Kit (ODK)</li> <li>DROID: DROID is a web-based interface for working with <code>make</code>, managed by <code>git</code>.</li> </ul> <p>Text editors:</p> <ul> <li>Kakoune text/code editor</li> <li>Sublime</li> <li>Atom</li> </ul> <p>Ontology access and query tools:</p> <ul> <li>Ontology Access Kit (OAK)</li> <li>PyOBO</li> <li>Yasgui</li> <li>ROBOT query</li> </ul> <p>SPARQL endpoints</p> <ul> <li>Ontobee SPARQL endpoint: Useful to run queries across all OBO Foundry ontologies</li> <li>Ubergraph SPARQL endpoint: Many key OBO ontologies are loaded here with lots of materialised inferences (docs).</li> </ul> <p>Templating systems</p> <ul> <li>DOSDP with DOSDP Tools</li> <li>ROBOT template</li> </ul> <p>Ontology Mappings</p> <ul> <li>SSSOM and sssom-py: Toolkit and framework for managing mappings across and beyond ontologies.</li> <li>AgreementMakerLight (AML): Matching tool for ontologies</li> <li>RDF Matcher: Experimental SSSOM based matching tool</li> <li>LogMap: Matching tool for ontologies</li> </ul> <p>Where to find ontologies and terms: Term browsers and ontology repositories</p> <ul> <li>OLS: The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work.</li> <li>Ontobee: The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. In practice, there is no particular reason why you would favour Ontobee over OLS for example - I just sometimes prefer the way Ontobee presents annotations and \"uses\" by other ontologies, so I use both.</li> <li>AberOWL: Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS.</li> <li>identifiers.org: A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here, and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with.</li> <li>OBO Foundry Ontology Library. The OBO Foundry works with other repositories and term browsers such as OLS, Ontobee and BioPortal. For example, OLS directly reads the OBO Foundry registry metadata, and automatically loads new ontologies added to the OBO Foundry Ontology Library.</li> <li>BioPortal</li> <li>CPT Story. The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes.</li> <li>AgroPortal: Like BioPortal, but focussed on the Agronomy domain.</li> <li>Linked Open Data Vocabularies (LOV): Lists the most important vocabularies in the Linked Data space, such as Dublin Core, SKOS and Friend-of-a-Friend (FOAF).</li> </ul> <p>Ontology visualisation</p> <ul> <li>OBO Graphviz: Library to visualise ontologies in beautifully readable graphics based on <code>Dot</code>.</li> </ul>"},{"location":"reference/semantic-engineering-toolbox/#nicos-top-10-tools-for-the-semantic-obo-engineers-toolbox","title":"Nico's top 10 tools for the Semantic OBO Engineer's Toolbox","text":"<ol> <li>ROBOT: Manipulating and transforming ontologies: the undeniable Swiss Army Knife for Ontology Engineers. </li> <li>Ontology Access Kit (OAK): Accessing ontologies, ontology databases, ontology mapping, text annotation - everything you will ever need for using ontologies.</li> <li>Prot\u00e9g\u00e9</li> <li>Term browsers (OLS, Ontobee)</li> <li>Ontology Development Kit (ODK)</li> <li>GNU Make</li> <li>Text editor workflows (i.e. Visual Studio Code, Atom, Sublime, VIM), plus a bit of regex. Select your text editor well!</li> <li>Basic Shell scripting and pipelining</li> <li>LinkML: Building semantic datamodels for your data - using ontologies!</li> <li>SPARQL (e.g. ROBOT query and Yasgui)</li> <li>From tables to ontologies: DOSDP templates and ROBOT templates</li> </ol> <p>Other tools in my toolbox</p> <p>These are a bit less essential than the above, but I consider them still tremendously useful.</p> <ul> <li>Cogs (experimental) for automatically synchronising your spreadsheets with Google Sheets.</li> <li>Basic Dockerfile development: This can help you automate processes that go beyond usual ODK day-to-day business, such as automated mapping tools, graph machine learning, NLP etc.</li> <li>GitHub community management and git version control: Learn how to effectively manage your contributors, issue requests and code reviews. Also get your git commands straight - these can be life savers!</li> <li>Basics in python scripting: This is always useful, and python is our go-to language for most of automation nowadays - this used to be Java. Most of the Java heavy lifting is done in ROBOT now!</li> <li>SSSOM and sssom-py: Toolkit and framework for managing mappings between ontologies.</li> <li>DROID: DROID is a web-based interface for working with <code>make</code>, managed by <code>git</code>.</li> <li>OBO Dashboard: OBO-wide quality control monitor for OBO ontologies.</li> </ul>"},{"location":"reference/semantic-etl/","title":"The 3 phases of Semantic Data Engineering / ETL","text":"<p>Semantic Data Engineering or Semantic Extract-Transform-Load (ETL) is an engineering discipline that is concerned with extracting information from a variety of sources, linking it together into a knowledge graph and enabling a range of semantic analyses for downstream users such as data scientists or researchers.</p> <ol> <li>Getting Data</li> <li>Information Extraction from text</li> <li>Obtaining data from external sources</li> <li>In-house biocuration</li> <li>Integrating data</li> <li>Entity Resolution: Make sure that if your sources talk about the same things, they use the same ontologies to reference those things.</li> <li>Knowledge merging: Combine the resolved sources into a coherent whole, for example a knowledge graph.</li> <li>Analysis: Query the integrated data and run advanced analyses using Semantic Technologies (next week).</li> </ol>"},{"location":"reference/semantic-etl/#glossary","title":"Glossary:","text":"<p>The following glossary only says how we use the terms we are defining, not how they are defined by some higher authority.</p> Term Definition Example Entity An entity is a thing in the world, like a molecule, or something more complex, like a disease. Entities do not have to be material, they can be processes as well, like cell proliferation. Marfan syndrome, H2O molecule, Ring finger, Phone Term A term is a sequence of characters (string) that refers to an entity in a precise way. SMOKER (referring to the role of being a smoker), HP:0004934 (see explanations below) Relation A link between two (or more) entities that signifies some kind of interaction. <code>:A :loves :B</code>, <code>:smoking :causes :cancer</code> Property A type of relation. The <code>:causes</code> in <code>:smoking :causes :cancer</code>"},{"location":"reference/semantic-etl/#getting-the-data","title":"Getting the data","text":"<p>As a Semantic Engineer, you typically coordinate the data collection from three largely separate sources: 1. Unstructured text, for example a corpus of scientific literature 2. External biological databases, such as STRING, a database of Protein-Protein Interaction Networks. 3. Manual in-house bio-curation efforts, i.e. the manual translation and integration of information relevant to biology (or medicine) into a database.</p> <p>Here, we are mostly concerned with the automated approaches of Semantic ETL, so we briefly touch on these and provide pointers to the others.</p>"},{"location":"reference/semantic-etl/#information-extraction-from-text","title":"Information Extraction from text","text":"<p>The task of information extraction is concerned with extracting information from unstructured textual sources to enable identifying entities, like diseases, phenotypes and chemicals, as well as classifying them and storing them in a structured format.</p> <p>The discipline that is concerned with techniques for extracting information from text is called Natural Language Processing (NLP).</p> <p>NLP is a super exciting and vast engineering discipline which goes beyond the scope of this course. NLP is concerned with many problems such as document classification, speech recognition and language translation. In the context of information extraction, we are particularly interested in Named Entity Recognition (NER), and Relationship Extraction (ER).</p>"},{"location":"reference/semantic-etl/#named-entity-recognition","title":"Named Entity Recognition","text":"<p>Named Entity Recognition (NER) is the task of identifying and categorising entities in text. NER tooling provides functionality to first isolate parts of sentence that correspond to things in the world, and then assigning them to categories (e.g. Drug, Disease, Publication).</p> <p>For example, consider this sentence:</p> <pre><code>As in the X-linked Nettleship-Falls form of ocular albinism (300500), the patients showed reduced visual acuity, photophobia, nystagmus, translucent irides, strabismus, hypermetropic refractive errors, and albinotic fundus with foveal hypoplasia.\n</code></pre> <p>An NER tool would first identify the relevant sentence parts that belong together:</p> <pre><code>As in the [X-linked] [Nettleship-Falls] form of [ocular albinism] (300500), the patients showed [reduced visual acuity], [photophobia], [nystagmus], [translucent irides], [strabismus], [hypermetropic refractive errors], and [albinotic fundus] with [foveal hypoplasia].\n</code></pre> <p>And then categorise them according to some predefined categories:</p> <pre><code>As in the Phenotype[X-linked] [Nettleship-Falls] form of Disease[ocular albinism] (300500), the patients showed Phenotype[reduced visual acuity], Phenotype[photophobia], Phenotype[nystagmus], Phenotype[translucent irides], Phenotype[strabismus], Phenotype[hypermetropic refractive errors], and Phenotype[albinotic fundus] with Phenotype[foveal hypoplasia].\n</code></pre> <p>See the NER with Gilda tutorial.</p> <p>Interesting sources for further reading:</p> <ul> <li>Using Uberon for text mining</li> <li>Named Entity Recognition with NLTK and SpaCy</li> <li>NLP Sandbox</li> </ul>"},{"location":"reference/semantic-etl/#relationship-extraction","title":"Relationship extraction","text":"<p>Relationship extraction (RE) is the task of extracting semantic relationships from text.  RE is an important component for the construction of Knowledge Graphs from the Scientific Literature, a task that many Semantic Data Engineering projects pursue to  augment or inform their manual curation processes. </p> <p>Interesting sources for further reading:</p> <ul> <li>http://nlpprogress.com/english/relationship_extraction.html</li> <li>https://github.com/roomylee/awesome-relation-extraction</li> <li>https://www.indra.bio/</li> </ul>"},{"location":"reference/semantic-etl/#other-data-sources-and-in-house-curation-efforts","title":"Other data sources and in-house curation efforts","text":"<ul> <li>Scientific data sources relevant to work around genes, phenotypes and diseases are plentiful. See here for an overview of the data sources used by the Monarch Initiative. All of the sources listed are part of a Semantic ETL pipeline involving the extraction of the data from data dumps (like published spreadsheets) or Web APIs, the transformation into a common format (including mapping to ontologies) and its subsequent load into the Monarch Knowledge Graph. More comprehensive lists are being produced in the academic literature, for example here.</li> <li>In-house biocuration. Biocuration is the task of manual translation and integration of information relevant to biology (or medicine) into some kind of database form. Biocuration can take many forms:</li> <li>The curation of scientific literature, i.e. extracting structured metadata from scientific papers to increase discoverability of relevant knowledge. The object of the curation is usually a particular publication, which goes through a triage process (Is the paper relevant to my problem? Is it good enough?), an initial metadata extraction phase (titles, authors, etc), and eventually to extracting the scientific knowledge (not unlike what the Named Entity Recongnition and Relation Extraction procedures described above) do.</li> <li>The focussed curation of specific scientific entities, such as diseases. For example, you may be interested in discovering all therapeutic interventions / drugs used for treating a specific disease.</li> </ul>"},{"location":"reference/semantic-etl/#integrating-data","title":"Integrating data","text":"<p>There is a huge amount of literature and tutorials on the topic of integrating data, the practice of consolidating data from disparate sources into a single dataset. We want to emphasise here two aspects of data integration, which are of particular importance to the Semantic Data engineer.</p> <ol> <li>Entity Resolution: Make sure that if your sources talk about the same things, they use the same ontologies to reference those things.</li> <li>Knowledge merging: Combine the resolved sources into a coherent whole, for example a knowledge graph.</li> </ol>"},{"location":"reference/semantic-etl/#entity-resolution-er","title":"Entity Resolution (ER):","text":"<p>Entity resolution (ER), sometimes called \"record linking\", \"grounding\", or named entity normalization (NEN), is the task of disambiguating records that correspond to real world entities across and within datasets. This task as many dimensions, but for us, the most important one is mapping a string, for example the one that was matched by our Named Entity Recognition pipeline, to ontology terms.</p> <p>Given our example:</p> <pre><code>As in the Phenotype[X-linked] Nettleship-Falls form of Phenotype[ocular albinism] (300500), the patients showed Phenotype[reduced visual acuity], Phenotype[photophobia], Phenotype[nystagmus], Phenotype[translucent irides], Phenotype[strabismus], Phenotype[hypermetropic refractive errors], and Phenotype[albinotic fundus] with Phenotype[foveal hypoplasia].\n</code></pre> <p>We could end up, for example, resolving ocular albinism to HP:0001107.</p> <p>See the NEN with Gilda tutorial.</p> <p>There are a lot of materials about Entity Resolution in general: - https://www.districtdatalabs.com/basics-of-entity-resolution - https://www.sciencedirect.com/topics/computer-science/entity-resolution - https://github.com/gyorilab/gilda</p> <p>In effect the term Ontology Mapping, which is the focus of this lesson, is Entity Resolution for ontologies - usually we don't have problem to use the two terms synonymously, although you may find that the literature typically favours one or the other.</p>"},{"location":"reference/semantic-etl/#knowledge-graph-ontology-merging","title":"Knowledge Graph / Ontology merging","text":"<p>Knowledge, Knowledge Graph or Ontology Merging are the disciplines concerned with combining all your data sources into a semantically coherent whole. This is a very complex research area, in particular to do this in a way that is semantically consistent. There are essentially two separate problems to be solved to achieve semantic merging: 1. The entities aligned during the entity resolution process must be aligned in the semantically correct way: if you you use logical equivalence to align them (<code>owl:equivalentClasses</code>) the classes must mean absolutely the same thing, or else you may run into the hairball problem, in essence faulty equivalence cliques. In cases of close, narrow or broadly matching classes, the respective specialised semantically correct relationships need to be used in the merging process. 2. The axioms of the merged ontologies must be logically consistent. For example, one ontology may say: a disease is a material entity. Another: a disease is a process. A background, or upper, ontology such as the ubiquitous Basic Formal Ontology (BFO) furthermore says that a process is not a material entity and vice versa. Merging this two ontologies would cause logical inconsistency.</p> <p>Unfortunately, the literature on ontology and knowledge graph merging is still sparse and very technical. You are probably best off checking out the OpenHPI course on Ontology Alignment, which is closely related.</p>"},{"location":"reference/sparql-basics/","title":"Basic SPARQL commands useful for OBO Engineers","text":""},{"location":"reference/sparql-basics/#basic-select-query","title":"Basic SELECT query","text":"<p>A basic SELECT query contains a set of prefixes, a SELECT clause and a WHERE clause.</p> <pre><code>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT ?term ?value\nWHERE {\n  ?term rdfs:label ?value .\n}\n</code></pre>"},{"location":"reference/sparql-basics/#prefixes","title":"Prefixes","text":"<p>Prefixes allow you to specify shortcuts. For example, instead of using the prefixes above, you could have simply said:</p> <pre><code>SELECT ?term ?value\nWHERE {\n  ?term &lt;http://www.w3.org/2000/01/rdf-schema#label&gt; ?value .\n}\n</code></pre> <p>Without the prefix. It means the exact same thing. But it looks nicer. Some people even go as far as adding entire entities into the prefix header:</p> <pre><code>PREFIX label: &lt;http://www.w3.org/2000/01/rdf-schema#label&gt;\n\nSELECT ?term ?value\nWHERE {\n  ?term label: ?value .\n}\n</code></pre> <p>This query is, again, the same as the ones above, but even more concise.</p>"},{"location":"reference/sparql-basics/#select-clause","title":"SELECT clause","text":"<p>The SELECT clause defines what you part of you query you want to show, for example, as a table.</p> <pre><code>SELECT ?term ?value\n</code></pre> <p>means: \"return\" or \"show\" whatever you find for the variable <code>?term</code> and the variable <code>?value</code>.</p> <p>There are other cool things you can do in the SELECT clause:</p> <ul> <li>Maths. You can count.</li> </ul>"},{"location":"reference/sparql-reference/","title":"Reference templates for SPARQL queries","text":"<p>This document contains template SPARQL queries that can be adapted. Comments are added in-code with <code>#</code> above each step to explain them so that queries can be spliced together</p>"},{"location":"reference/sparql-reference/#checksreport-generation","title":"Checks/Report generation","text":""},{"location":"reference/sparql-reference/#all-terms-native-to-ontology","title":"All terms native to ontology","text":"<p>note: we assume that all native terms here have the same namespace - that of the ontology</p> <pre><code># select unique instances of the variable\nSELECT DISTINCT ?term\nWHERE {\n  # selecting where the variable term is either used as a subject or object\n  { ?s1 ?p1 ?term . }\n  UNION\n  { ?term ?p2 ?o2 . }\n  # filtering out only terms that have the MONDO namespace (assumed to be native terms)\n  FILTER(isIRI(?term) &amp;&amp; (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/MONDO_\")))\n}\n</code></pre>"},{"location":"reference/sparql-reference/#report-of-terms-with-labels-containing-certain-strings-in-ubergraph","title":"Report of terms with labels containing certain strings in ubergraph","text":"<pre><code># adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix BFO: &lt;http://purl.obolibrary.org/obo/BFO_&gt;\n\n# selecting only unique instances of the three variables\nSELECT DISTINCT ?entity ?label WHERE\n{\n  # the variable label is a rdfs:label\n  VALUES ?property {\n    rdfs:label\n  }\n\n  # only look for uberon terms. note: this is only used in ubergraph, use filter for local ontology instead.\n  ?entity rdfs:isDefinedBy &lt;http://purl.obolibrary.org/obo/uberon.owl&gt; .\n\n  # defining the order of variables in the triple\n  ?entity ?property ?label .\n  # entity must be material\n  ?entity rdfs:subClassOf BFO:0000040\n  # filtering out triples where the variable label has sulcus or incisure, or fissure in it\n  FILTER(contains(STR(?label), \"sulcus\")||contains(STR(?label), \"incisure\")||contains(STR(?label), \"fissure\"))\n\n}\n# arrange report by entity variable\nORDER BY ?entity\n</code></pre>"},{"location":"reference/sparql-reference/#report-of-labels-and-definitions-of-terms-with-certain-namespace","title":"Report of labels and definitions of terms with certain namespace","text":"<pre><code>prefix label: &lt;http://www.w3.org/2000/01/rdf-schema#label&gt;\nprefix oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nprefix definition: &lt;http://purl.obolibrary.org/obo/IAO_0000115&gt;\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\n# select a report with 3 variables\nSELECT DISTINCT ?term ?label ?def\n\n# defining the properties to be used\n    WHERE {\n        VALUES ?defproperty {\n        definition:\n        }\n        VALUES ?labelproperty {\n        label:\n        }\n\n# defining the order of the triples\n      ?term ?defproperty ?def .\n      ?term ?labelproperty ?label .\n\n# selects entities that are in a certain namespace\n  FILTER(isIRI(?term) &amp;&amp; (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/CP_\")))\n}\n\n# arrange report by term variable\nORDER BY ?term\n</code></pre>"},{"location":"reference/sparql-reference/#definition-lacks-xref","title":"Definition lacks xref","text":"<p>adaptable for lacking particular annotation</p> <pre><code># adding prefixes used\nprefix oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nprefix definition: &lt;http://purl.obolibrary.org/obo/IAO_0000115&gt;\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\nSELECT ?entity ?property ?value WHERE\n{\n  # the variable property has to be defintion (IAO:0000115)\n  VALUES ?property {\n    definition:\n  }\n  # defining the order of variables in the triple\n  ?entity ?property ?value .\n\n  # selecting annotation on definition\n  ?def_anno a owl:Axiom ;\n  owl:annotatedSource ?entity ;\n  owl:annotatedProperty definition: ;\n  owl:annotatedTarget ?value .\n\n  # filters out definitions which do not have a dbxref annotiton\n  FILTER NOT EXISTS {\n    ?def_anno oboInOwl:hasDbXref ?x .\n  }\n\n  # removes triples where entity is blank\n  FILTER (!isBlank(?entity))\n  # selects entities that are native to ontology (in this case MONDO)\n  FILTER (isIRI(?entity) &amp;&amp; STRSTARTS(str(?entity), \"http://purl.obolibrary.org/obo/MONDO_\"))\n\n}\n# arrange report by entity variable\nORDER BY ?entity\n</code></pre>"},{"location":"reference/sparql-reference/#checks-wether-definitions-contain-underscore-characters","title":"Checks wether definitions contain underscore characters","text":"<p>adaptable for checking if there is particular character in annotation</p> <pre><code># adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix IAO: &lt;http://purl.obolibrary.org/obo/IAO_&gt;\nprefix definition: &lt;http://purl.obolibrary.org/obo/IAO_0000115&gt;\n\n# selecting only unique instances of the three variables\nSELECT DISTINCT ?entity ?property ?value WHERE\n{\n  # the variable property has to be definition (IAO:0000115)\n  VALUES ?property {\n    definition:\n  }\n  # defining the order of variables in the triple\n  ?entity ?property ?value .\n  # filtering out triples where the variable value has _ in it\n  FILTER( regex(STR(?value), \"_\"))\n  # removes triples where entity is blank\n  FILTER (!isBlank(?entity))\n}\n# arrange report by entity variable\nORDER BY ?entity\n</code></pre>"},{"location":"reference/sparql-reference/#only-allowing-a-fix-set-of-annotation-properties","title":"Only allowing a fix set of annotation properties","text":"<pre><code># adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nprefix oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nprefix IAO: &lt;http://purl.obolibrary.org/obo/IAO_&gt;\nprefix RO: &lt;http://purl.obolibrary.org/obo/RO_&gt;\nprefix mondo: &lt;http://purl.obolibrary.org/obo/mondo#&gt;\nprefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\nprefix dce: &lt;http://purl.org/dc/elements/1.1/&gt;\nprefix dcterms: &lt;http://purl.org/dc/terms/&gt;\n\n# selecting only unique instances of the three variables\nSELECT DISTINCT ?term ?property ?value WHERE\n{\n  # order of the variables in the triple\n    ?term ?property ?value .\n    # the variable property is an annotation property\n    ?property a owl:AnnotationProperty .\n  # selects entities that are native to ontology (in this case MONDO)\n    FILTER (isIRI(?term) &amp;&amp; regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\"))\n    # removes triples where the variable value is blank\n    FILTER(!isBlank(?value))\n  # listing the allowed annotation properties\n  FILTER (?property NOT IN (dce:creator, dce:date, IAO:0000115, IAO:0000231, IAO:0100001, mondo:excluded_subClassOf, mondo:excluded_from_qc_check, mondo:excluded_synonym, mondo:pathogenesis, mondo:related, mondo:confidence, dcterms:conformsTo, mondo:should_conform_to, oboInOwl:consider, oboInOwl:created_by, oboInOwl:creation_date, oboInOwl:hasAlternativeId, oboInOwl:hasBroadSynonym, oboInOwl:hasDbXref, oboInOwl:hasExactSynonym, oboInOwl:hasNarrowSynonym, oboInOwl:hasRelatedSynonym, oboInOwl:id, oboInOwl:inSubset, owl:deprecated, rdfs:comment, rdfs:isDefinedBy, rdfs:label, rdfs:seeAlso, RO:0002161, skos:broadMatch, skos:closeMatch, skos:exactMatch, skos:narrowMatch))\n}\n</code></pre>"},{"location":"reference/sparql-reference/#checking-for-misused-replaced_by","title":"Checking for misused replaced_by","text":"<p>adaptable for checking that a property is used in a certain way</p> <pre><code># adding prefixes used\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nPREFIX replacedBy: &lt;http://purl.obolibrary.org/obo/IAO_0100001&gt;\n\n# selecting only unique instances of the three variables\nSELECT DISTINCT ?entity ?property ?value WHERE {\n # the variable property is IAO_0100001 (item replaced by)\n VALUES ?property { replacedBy: }\n\n # order of the variables in the triple\n ?entity ?property ?value .\n # removing entities that have either owl:deprecated true or oboInOwl:ObsoleteClass (these entities are the only ones that should have replaced_by)\n FILTER NOT EXISTS { ?entity owl:deprecated true }\n FILTER (?entity != oboInOwl:ObsoleteClass)\n}\n# arrange report by entity variable\nORDER BY ?entity\n</code></pre>"},{"location":"reference/sparql-reference/#count","title":"Count","text":""},{"location":"reference/sparql-reference/#count-class-by-prefixes","title":"Count class by prefixes","text":"<pre><code># this query counts the number of classes you have with each prefix (eg number of MONDO terms, CL terms, etc.)\n\n# adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix obo: &lt;http://purl.obolibrary.org/obo/&gt;\n\n# selecting 2 variables, prefix and numberOfClasses, where number of classes is a count of distinct cls\nSELECT ?prefix (COUNT(DISTINCT ?cls) AS ?numberOfClasses) WHERE\n{\n  # the variable cls is a class\n  ?cls a owl:Class .\n  # removes any cases where the variable cls is blank\n  FILTER (!isBlank(?cls))\n  # Binds the variable prefix as the prefix of the class (eg. MONDO, CL, etc.). classes that do not have obo purls will come out as blank in the report.\n  BIND( STRBEFORE(STRAFTER(str(?cls),\"http://purl.obolibrary.org/obo/\"), \"_\") AS ?prefix)\n}\n# grouping the count by prefix\nGROUP BY ?prefix\n</code></pre>"},{"location":"reference/sparql-reference/#counting-subclasses-in-a-namespace","title":"Counting subclasses in a namespace","text":"<pre><code># this query counts the number of classes that are subclass of CL:0000003 (native cell) that are in the pcl namespace\n\n# adding prefixes used\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX CL: &lt;http://purl.obolibrary.org/obo/CL_&gt;\nPREFIX PCL: &lt;http://purl.obolibrary.org/obo/PCL_&gt;\n\n# count the number of unique term\nSELECT (COUNT (DISTINCT ?term) as ?pclcells)\nWHERE {\n    # the variable term is a class\n    ?term a owl:Class .\n    # the variable term has to be a subclass of CL:0000003, including those that are subclassof by property path\n    ?term rdfs:subClassOf* CL:0000003\n  # only count the term if it is in the pcl namespace\n  FILTER(isIRI(?term) &amp;&amp; (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/PCL_\")))\n}\n</code></pre>"},{"location":"reference/sparql-reference/#removing","title":"Removing","text":""},{"location":"reference/sparql-reference/#removes-all-ro-terms","title":"Removes all RO terms","text":"<p>adaptable for removing all terms of a particular namespace</p> <pre><code># adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\n# removing triples\nDELETE {\n  ?s ?p ?o\n}\nWHERE\n{\n  {\n    # the variable p must be a rdfs:label\n    VALUES ?p {\n      rdfs:label\n    }\n  # the variable s is an object property\n  ?s a owl:ObjectProperty ;\n  # the other variables can be anything else (note the above value restriction of p)\n  ?p ?o\n    # filter out triples where ?s starts with \"http://purl.obolibrary.org/obo/RO_\"\n    FILTER (isIRI(?s) &amp;&amp; STRSTARTS(str(?s), \"http://purl.obolibrary.org/obo/RO_\"))\n  }\n}\n</code></pre>"},{"location":"reference/sparql-reference/#deleting-axiom-annotations-by-prefix","title":"Deleting axiom annotations by prefix","text":"<pre><code># adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\n# delete triples\nDELETE {\n  ?anno ?property ?value .\n}\nWHERE {\n  # the variable property is either synonym_type: or source:\n  VALUES ?property { synonym_type: source: }\n  # structure of variable value and variable anno\n  ?anno a owl:Axiom ;\n         owl:annotatedSource ?s ;\n         owl:annotatedProperty ?p ;\n         owl:annotatedTarget ?o ;\n         ?property ?value .\n  # filter out the variable value which start with \"ICD10EXP:\"\n  FILTER(STRSTARTS(STR(?value),\"ICD10EXP:\"))\n}\n</code></pre>"},{"location":"reference/sparql-reference/#replacing","title":"Replacing","text":""},{"location":"reference/sparql-reference/#replace-oboinowlsource-with-oboinowlhasdbxref-in-synonyms-annotations","title":"Replace oboInOwl:source with oboInOwl:hasDbXref in synonyms annotations","text":"<p>adaptable for replacing annotations properties on particular axioms</p> <pre><code># adding prefixes used\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nprefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n\n# delete triples where the relation is oboInOwl:source\nDELETE {\n    ?ax oboInOwl:source ?source .\n}\n# insert triples where the variables ax and source defined above are used, but using oboInOwl:hasDbXref instead\nINSERT {\n    ?ax oboInOwl:hasDbXref ?source .\n}\nWHERE\n{\n  # restricting to triples where the property variable is in this list\n  VALUES ?property { oboInOwl:hasExactSynonym oboInOwl:hasNarrowSynonym  oboInOwl:hasBroadSynonym oboInOwl:hasCloseSynonym oboInOwl:hasRelatedSynonym } .\n  # order of the variables in the triple\n  ?entity ?property ?value .\n  # structure on which the variable ax and source applies\n  ?ax rdf:type owl:Axiom ;\n    owl:annotatedSource ?entity ;\n    owl:annotatedTarget ?value ;\n    owl:annotatedProperty ?property ;\n    oboInOwl:source ?source .\n  # filtering out triples where entity is an IRI\n  FILTER (isIRI(?entity))\n}\n</code></pre>"},{"location":"reference/synonym-validation/","title":"Synonym Validation","text":""},{"location":"reference/synonym-validation/#synonym-validation","title":"Synonym validation","text":""},{"location":"reference/synonym-validation/#related-materials","title":"Related materials","text":"<ul> <li>Overview of synonym properties</li> <li>Synonym types</li> <li>Lesson on synonyms</li> </ul>"},{"location":"reference/synonym-validation/#basic-validation","title":"Basic validation","text":""},{"location":"reference/synonym-validation/#the-same-synonym-cannot-be-an-exact-synonym-of-two-distinct-concepts","title":"The same synonym cannot be an exact synonym of two distinct concepts","text":"<ul> <li>Implemented in the form of the duplicate_exact_synonym check implemented in ROBOT report.</li> <li>The most fundamental of all synonym checks: if the synonym is exact, the assumption is that no two terms can have the same synonym.</li> </ul> <p>Warning</p> <p>Some exact synonyms are not globally unique. For example, the acronym \"ASD\" is an exact synonym of the concept representing \"Atrial septal defect\" and \"Autism Spectrum Disorder\".</p>"},{"location":"reference/synonym-validation/#the-same-synonym-cannot-be-duplicated-with-a-different-scope","title":"The same synonym cannot be duplicated with a different scope","text":"<ul> <li>An entity has duplicate synonyms (the same exact string value, like \"Depression\") with different properties (e.g. broad and related). This causes ambiguity and considerable confusion among downstream users.</li> <li>This test is implemented in the form of Duplicate Scoped Synonyms check in the ROBOT report.</li> <li>Unfortunately, quasi scope-duplicated synonyms cannot always we recognised easily. In the below example, you can see 3 synonyms that almost seem like they are the same, but have different scopes. Better tools are needed to recognise such cases.</li> </ul>"},{"location":"reference/synonym-validation/#the-same-synonym-cannot-be-an-exact-synonym-and-a-label-at-the-same-time","title":"The same synonym cannot be an exact synonym and a label at the same time","text":"<ul> <li>Implemented in the form of the duplicate_label_synonym check implemented in ROBOT report.</li> <li>This check has a long history of controversy and confusion. In general, we cannot expect this assumption to hold in all cases for various reasons:<ul> <li>For historical reasons, many ontologies avoid attaching synonym metadata and provenance to the primary label of a class. For example, the Mondo ontology captures the preferred labels of various major nomenclature organisations. Instead of capturing which organisations prefer the label on the primary label, they are captured as \"exact syonyms\", even though the two often co-incide.</li> <li>It is often considered more convenient to be able to expect all exact synonyms to be available via <code>oboInOwl:hasExactSynonym</code>, and not requiring downstream users to know that exact synonyms are scattered across multiple properties (such as <code>rdfs:label</code>).</li> <li>No matter whether you agree or disagree with the above, as a ontology user you should not assume</li> </ul> </li> </ul>"},{"location":"reference/synonym-validation/#synonym-types-must-be-a-child-of-synonym-type-property","title":"Synonym types must be a child of Synonym Type Property","text":"<ul> <li>A synonym type is used in an annotation, but is not properly declared as a child of oboInOwl:SynonymTypeProperty. This can cause problems with conversions to OBO format.</li> <li>For example, if you add your own synonym type, like <code>hp:abbreviation</code>, it has to be child of oboInOwl:SynonymTypeProperty to be correctly interpreted by ROBOT and generally OWL API related tooling.</li> <li>Implemented in the form of Missing Synonym Type Declaration in the ROBOT report.</li> </ul>"},{"location":"reference/synonym-validation/#advanced-validation","title":"Advanced validation","text":""},{"location":"reference/synonym-validation/#duplicate-exact-synonym-check-that-excludes-abbreviations","title":"Duplicate exact synonym check that excludes abbreviations","text":"<ul> <li>In Mondo, this SPARQL query checks for duplicate exact synonyms between terms but excludes any abbreviations.</li> <li>For example, \"SMS\" is an abbreviation for MONDO:0008491 stiff-person syndrome and MONDO:0008434 Smith-Magenis syndrome and this is acceptable.</li> <li>Implemented as qc-duplicate-exact-synonym-no-abbrev.sparql in Mondo (see below).</li> </ul> <p>??? Query qc-duplicate-exact-synonym-no-abbrev.sparql</p> <pre><code>```\nPREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT DISTINCT ?entity ?property ?value WHERE {\n  VALUES ?property1 {\n    obo:IAO_0000118\n    oboInOwl:hasExactSynonym\n    rdfs:label\n  }\n  VALUES ?property2 {\n    obo:IAO_0000118\n    oboInOwl:hasExactSynonym\n    rdfs:label\n  }\n  ?entity1 ?property1 ?value.\n  ?entity2 ?property2 ?value .\n\n  FILTER NOT EXISTS {\n    ?axiom owl:annotatedSource ?entity1 ;\n         owl:annotatedProperty ?property1 ;\n         owl:annotatedTarget ?value ;\n         oboInOwl:hasSynonymType &lt;http://purl.obolibrary.org/obo/mondo#ABBREVIATION&gt; .\n  }\n\n  FILTER NOT EXISTS {\n    ?axiom owl:annotatedSource ?entity2 ;\n         owl:annotatedProperty ?property2 ;\n         owl:annotatedTarget ?value ;\n         oboInOwl:hasSynonymType &lt;http://purl.obolibrary.org/obo/mondo#ABBREVIATION&gt; .\n  }\n\n  FILTER NOT EXISTS { ?entity owl:deprecated true }\n  FILTER NOT EXISTS { ?entity2 owl:deprecated true }\n  FILTER (?entity1 != ?entity2)\n  FILTER (!isBlank(?entity1))\n  FILTER (!isBlank(?entity2))\n  BIND(CONCAT(CONCAT(REPLACE(str(?entity1),\"http://purl.obolibrary.org/obo/MONDO_\",\"MONDO:\"),\"-\"), REPLACE(str(?entity2),\"http://purl.obolibrary.org/obo/MONDO_\",\"MONDO:\")) as ?entity)\n  BIND(CONCAT(CONCAT(REPLACE(REPLACE(str(?property1),\"http://www.w3.org/2000/01/rdf-schema#\",\"rdfs:\"),\"http://www.geneontology.org/formats/oboInOwl#\",\"oboInOwl:\"),\"-\"), REPLACE(REPLACE(str(?property1),\"http://www.w3.org/2000/01/rdf-schema#\",\"rdfs:\"),\"http://www.geneontology.org/formats/oboInOwl#\",\"oboInOwl:\")) as ?property)\n}\nORDER BY DESC(UCASE(str(?value)))\n```\n</code></pre>"},{"location":"reference/synonym-validation/#exact-synonymsnon-exact-mappings","title":"Exact Synonyms/Non-exact Mappings","text":"<ul> <li>In Mondo, this SPARQL query checks for an exact synonym and a database cross-reference (dbxref) that is not exact. If the dbxef is equivalent to the Mondo term, the synonyms from that term should be added as exact synonyms.</li> <li>This is a very specific use case to Mondo, as dbxrefs in Mondo have equivalence mappings (in the form of MONDO:equivalentTo). The issue here was, in a merger, DOID:5603 was added as an equivalent dbxref, but the synonyms 'T-cell acute lymphoblastic leukemia' and 'precursor T lymphoblastic leukemia' were related synonyms. They were changed to exact and the QC check passed.</li> <li>Implemented as qc-exact-synonyms-non-exact-mappings.sparql in Mondo.</li> <li> <p>See the Pull Request here where the QC check failed.</p> <p></p> </li> </ul> <p>??? Query qc-exact-synonyms-non-exact-mappings.sparql</p> <pre><code>PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nPREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\nPREFIX MONDO: &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\n\nSELECT DISTINCT ?entity ?label ?xref ?synonym ?code\nWHERE {\n\n  VALUES ?code {\n    \"MONDO:relatedTo\"^^xsd:string\n    \"MONDO:mondoIsNarrowerThanSource\"^^xsd:string\n    \"MONDO:directSiblingOf\"^^xsd:string\n    \"MONDO:mondoIsBroaderThanSource\"^^xsd:string\n  }\n\n  ?entity rdfs:subClassOf* MONDO:0000001 .\n  ?entity rdfs:label ?label .\n\n  ?entity oboInOwl:hasDbXref ?xref .\n    [ \n      owl:annotatedSource ?entity ;\n      owl:annotatedProperty oboInOwl:hasDbXref ;\n      owl:annotatedTarget ?xref ;\n      oboInOwl:source ?code \n    ] .\n\n  ?entity oboInOwl:hasExactSynonym ?synonym .\n    [ \n      owl:annotatedSource ?entity ;\n      owl:annotatedProperty oboInOwl:hasExactSynonym ;\n      owl:annotatedTarget ?synonym ;\n      oboInOwl:hasDbXref ?xref \n    ] .\n\n}\n</code></pre>"},{"location":"reference/synonyms-properties/","title":"Synonym Properties","text":""},{"location":"reference/synonyms-properties/#synonym-properties-obo","title":"Synonym Properties OBO","text":""},{"location":"reference/synonyms-properties/#related-materials","title":"Related materials","text":"<ul> <li>Lesson on synonyms</li> <li>Synonym validation</li> <li>Overview of synonym types</li> </ul>"},{"location":"reference/synonyms-properties/#overview","title":"Overview","text":"<p>A synonym indicates an alternative name for a term.  Terms can have multiple synonyms of of different types and using different properties. For example, the Mondo term MONDO:0005992 cancer, has many exact synonyms and a related synonym:</p> <p></p>"},{"location":"reference/synonyms-properties/#list-of-widely-used-synonym-properties","title":"List of widely used synonym properties","text":"Label ID Definition Comment has exact synonym oboInOwl:hasExactSynomym The definition of the synonym is exactly the same as primary term definition. This is used when the same class can have more than one name. has related synonym oboInOwl:hasRelatedSynomym An alternative label for a class or property that has been used synonymously with the primary term name, but the usage is not strictly correct. has broad synonym oboInOwl:hasBroadSynomym An alternative label for a class or property which has a more general meaning than the preferred name/primary label. has narrow synonym oboInOwl:hasNarrowSynomym An alternative label for a class or property which has a more specific meaning than the preferred name/primary label. has alternative label IAO:0000118 A label for a class or property that can be used to refer to the class or property instead of the preferred rdfs:label. Alternative labels should be used to indicate community- or context-specific labels, abbreviations, shorthand forms and the like. alternative label skos:altLabel An alternative lexical label for a resource. Not recommended by OBO. hidden label skos:hiddenLabel A lexical label for a resource that should be hidden when generating visual displays of the resource, but should still be accessible to free text search operations. Not recommended by OBO. alternate name schema:alternateName An alias for the item. Not recommended by OBO."},{"location":"reference/synonyms-properties/#additional-information-about-synonym-properties","title":"Additional information about synonym properties","text":""},{"location":"reference/synonyms-properties/#oboinowlhasexactsynomym","title":"<code>oboInOwl:hasExactSynomym</code>","text":"<p>The definition of the synonym is exactly the same as primary term definition. This is used when the same class can have more than one name.</p> <p>For example, hereditary Wilms' tumor has the exact synonoym familial Wilms' tumor.</p> <p>Additionally, translations into other languages are listed as exact synonyms. For example, the Plant Ontology list both Spanish and Japanese translations as exact synonyms; e.g. anther wall has exact synonym \u2018pared de la antera\u2019 (Spanish) and \u2018\u846f\u58c1 \u2018(Japanese).</p>"},{"location":"reference/synonyms-properties/#oboinowlhasnarrowsynomym","title":"<code>oboInOwl:hasNarrowSynomym</code>","text":"<p>The definition of the synonym is the same as the primary definition, but has additional qualifiers.</p> <p>For example, pod is a narrow synonym of fruit.</p> <p>Note - when adding a narrow synonym, please first consider whether a new subclass should be added instead of a narrow synonym. If there is any uncertainty, start a discussion on the GitHub issue tracker.</p>"},{"location":"reference/synonyms-properties/#oboinowlhasbroadsynomym","title":"<code>oboInOwl:hasBroadSynomym</code>","text":"<p>The primary definition accurately describes the synonym, but the definition of the synonym may encompass other structures as well. In some cases where a broad synonym is given, it will be a broad synonym for more than one ontology term.</p> <p>For example, Cyst of eyelid has the broad synonym Lesion of the eyelid.</p> <p>Note - when adding a broad synonym, please first consider whether a new superclass should be added instead of a broad synonym. If there is any uncertainty, start a discussion on the GitHub issue tracker.</p>"},{"location":"reference/synonyms-properties/#oboinowlhasrelatedsynomym","title":"<code>oboInOwl:hasRelatedSynomym</code>","text":"<p>This scope is applied when a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct. That is, the synonym in fact has a slightly different meaning than the primary term name. Since users may not be aware that the synonym was being used incorrectly when searching for a term, related synonyms are included.</p> <p>For example, Autistic behavior has the related synonym Autism spectrum disorder.</p> <p>"},{"location":"reference/synonyms-properties/#provenance-on-synonyms","title":"Provenance on synonyms","text":"<p>Ontologies developed using the GO-family ontology development pattern use the oboInOwl:hasDbXref property to represent \"provenance\" in general. This could be anything:</p> <ol> <li>An ORCiD, meaning \"this person asserted/verified this synonym\"</li> <li>An ontology term, meaning \"the synonym was source from this ontology concept\"</li> <li>A Pubmed ID, to indicate the publication that used the synonym. References to PubMed IDs should be in the format PMID:XXXXXXX (no space). See example below.</li> </ol> <p></p> <p>Some other ontologies have started using dc:contributor and rdfs:seeAlso for more fine-grained provenance, but this pattern is not widely adopted.</p> <p>Info</p> <p>We warmly recommend to use to be generous with provenance when curating synonyms. At the very least, we recommend to capture the ORCiD of the curator that captured the synonym, or the Research Organization Registry (ROR) identifier of the organisation that promotes the term. Ideally, however, you furthermore capture the source of the synonym, which could be a PubMed ID (PMID), a term from an ontology or a Digital Object Identifier (DOI).</p>"},{"location":"reference/synonyms-types/","title":"Synonym Types","text":""},{"location":"reference/synonyms-types/#synonym-types","title":"Synonym Types","text":""},{"location":"reference/synonyms-types/#related-materials","title":"Related materials","text":"<ul> <li>Overview of synonym properties</li> <li>Lesson on synonyms</li> <li>Synonym validation</li> </ul>"},{"location":"reference/synonyms-types/#overview","title":"Overview","text":"<p>In contrast to synonym properties, which encode the semantic precision of a specific synonym such as \"exact\" or \"broad, synonym types encode the purpose of the synonym. In the following, we will describe some basic synonym types and give examples of their usage.</p> <p>Definition synonym type</p> <p>A synonym type represents the function of a specific synonym, such as \"acronym\", \"layperson\", or \"language translation\".</p> <p>Warning</p> <p>While synonym properties MUST be included in OMO to recognise a valid synonym, synonym types are often defined by the ontologies themselves. This can be pretty confusing for tool developers. For example, at the time of this writing, a number of OBO ontologies define their own properties for \"layperson\" or \"plural form\".</p>"},{"location":"reference/synonyms-types/#typical-synonym-types","title":"Typical synonym types","text":"<p>Synonyms can also be classified by types. The default is no type. The synonym types vary in each ontology, but some commonly used synonym types include:</p> <ul> <li>abbreviation - to indicate the synonym is an abbreviation. Note the scope for an acronym should be determined on a case-by-case basis. Not all acronyms are necessarily exact.</li> <li>ambiguous - to indicate the synonym is open to more than one interpretation; may have a double meaning</li> <li>dubious synonym - to indicate the synonym may be suspect</li> <li>layperson term - to indicate the synonym is common language (used by the Human Phenotype Ontology)</li> <li>plural form - indicating the form of the term that means more than one</li> <li>UK spelling - the english language spelling that is used in the United Kingdom (UK) but not in the United States (US)</li> <li>Other possible synonym types which are under discussion are synonyms that are considered harmful.</li> <li>A very different class of synonym types are \"language translations\". Most people would not consider translations as synonyms at all, and it makes sense to separate the two because the way they are represented in the ontology are very different from \"normal\" synonyms. For details see below.</li> </ul> <p>Additional standardized synonym types can be found in the OBO Metadata Ontology as sub-properties of <code>oboInOwl:SynonymType</code>.</p>"},{"location":"reference/synonyms-types/#common-synonym-types","title":"Common Synonym Types","text":"<p>Synonyms can also be classified by types. The default is no type. The synonym types vary in each ontology, but some commonly used synonym types include:</p>"},{"location":"reference/synonyms-types/#representation","title":"Representation","text":"Synonym type Description Example abbreviation A shortened form of a word or phrase. Warning Many abbreviations you will find in the wild are actually acronyms. etc for et cetera acronym A synonym type for describing abbreviations that are a part of the full name's words, such as initialisms or alphabetisms. Note the scope for an acronym should be determined on a case-by-case basis. Not all acronyms are necessarily exact. MONDO:0000190 ventricular fibrillation synonym: VF dubious synonym A synonym type to indicate the synonym may be suspect or used wrongly in practice. MONDO:0024309 neuropathy, hereditary sensory and autonomic, type 2A, synonym: Morvan disease layperson term to indicate the synonym is a common language (used by the Human Phenotype Ontology) HP:0001028 Hemangioma, synonym: Strawberry mark plural form indicating the form of the term that means more than one HP:0010819 Atonic seizure, synonym: Atonic seizures UK spelling the english language spelling that is used in the United Kingdom (UK) but not in the United States (US) MONDO:0005059 leukemia, synonym: leukaemia (disease) ambiguous to indicate the synonym is open to more than one interpretation; may have a double meaning MONDO:0019781 astrocytoma (excluding glioblastoma) synonym: \"astrocytoma\" (see associated GitHub ticket <p>Additional standardized synonym types can be found in the OBO Metadata Ontology as sub-properties of oboInOwl:SynonymType.</p>"},{"location":"reference/synonyms-types/#additional-interesting-synonym-type-examples-from-mondo","title":"Additional (interesting) Synonym Type Examples from Mondo","text":"Synonym type Description Example 'A synonym that is historic and discouraged' (DEPRECATED) Mondo marks synonyms with DEPRECATED that are historic and no longer appropriate to use, e.g. all occurrences of \u201cmental retardation\u201d should be \u201cintellectual disability\u201d. They try to avoid including things in this list: https://en.wikipedia.org/wiki/List_of_medical_eponyms_with_Nazi_associations but if it\u2019s established (e.g. Wegener granulomatosis), it may be included as a synonym and mark DEPRECATED MONDO:0007113 Angelman syndrome, synonym: happy puppet syndrome 'Synonym to be removed from public release but maintained in edit version as record of external usage' (EXCLUDE) Some synonyms are annotated with EXCLUDE, e.g. \u201cNOS\u201d (not otherwise specified) synonyms. It is useful to have these in the edit version, but these are filtered on release. MONDO:0007667 subependymoma, synonym: subependymal astrocytoma NOS ClinGen label (CLINGEN_LABEL) Added to gene-based names/synonyms (or other labels) that were requested by an external user, ClinGen, and other terms that are the preferred terms for ClinGen. MONDO:0010015 anterior segment dysgenesis 7, synonym: PXDN-related ocular dysgenesis"},{"location":"reference/synonyms-types/#example-synonym-types-in-protege-from-hpo-are-shown-below","title":"Example synonym types in Protege from HPO are shown below","text":""},{"location":"reference/synonyms-types/#example-synonym-types-in-protege-from-mondo-are-shown-below","title":"Example synonym types in Protege from Mondo are shown below","text":"Owl <pre><code>&lt;owl:Axiom&gt;\n    &lt;owl:annotatedSource rdf:resource=\"http://purl.obolibrary.org/obo/MONDO_0016971\"/&gt;\n    &lt;owl:annotatedProperty rdf:resource=\"http://www.geneontology.org/formats/oboInOwl#hasExactSynonym\"/&gt;\n    &lt;owl:annotatedTarget rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;limb-girdle muscular dystrophy&lt;/owl:annotatedTarget&gt;\n    &lt;oboInOwl:hasSynonymType rdf:resource=\"http://purl.obolibrary.org/obo/mondo#CLINGEN_LABEL\"/&gt;\n&lt;/owl:Axiom&gt;\n&lt;owl:Axiom&gt;\n    &lt;owl:annotatedSource rdf:resource=\"http://purl.obolibrary.org/obo/MONDO_0016989\"/&gt;\n    &lt;owl:annotatedProperty rdf:resource=\"http://www.geneontology.org/formats/oboInOwl#hasExactSynonym\"/&gt;\n    &lt;owl:annotatedTarget rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;FHI&lt;/owl:annotatedTarget&gt;\n    &lt;oboInOwl:hasDbXref rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\"&gt;Orphanet:263479&lt;/oboInOwl:hasDbXref&gt;\n    &lt;oboInOwl:hasSynonymType rdf:resource=\"http://purl.obolibrary.org/obo/OMO_0003012\"/&gt;\n&lt;/owl:Axiom&gt;\n</code></pre> <p>In the above OWL example, you can see how an acronym and a custom synonym type called \"CLINGEN_LABEL\" would look like in OWL Format. Note that the acronym property was changed here for illustration purposes, as Mondo has not yet caught up with implementing all the new standard OMO synonym type properties.</p> Obo <pre><code>[Term]\nid: MONDO:0016971\nname: limb-girdle muscular dystrophy\nsynonym: \"LGMD\" EXACT OMO:0003012 [Orphanet:263]\nsynonym: \"limb-girdle muscular dystrophy\" EXACT CLINGEN_LABEL []\n</code></pre> <p>In the above OBO example, you can see how an acronym and a custom synonym type called \"CLINGEN_LABEL\" would look like in OBO Format.</p>"},{"location":"reference/synonyms-types/#how-to-add-synonym-types-in-protege-v561","title":"How to add Synonym Types in Protege v5.6.1","text":"<ol> <li> <p>Click on the annotation button next to the synonym </p> </li> <li> <p>Click the annotation button in the box </p> </li> <li> <p>Select has_synonym_type on the left, then Entity IRI and start searching for the synonym type in the search box, then select the annotation you wish to add. Click OK on each box. </p> </li> <li> <p>You will see your annotation on your synonym. </p> </li> </ol>"},{"location":"reference/tables-and-triples/","title":"Tables and Triples","text":"<p>Tables and triples seem very different. Tables are familiar and predictable. Triples are weird and floppy. SQL is normal, SPARQL is bizarre, at least at first.</p> <p>Tables are great, and they're the right tool for a lot of jobs, but they have their limitations. Triples shine when it comes to merging heterogeneous data. But it turns out that there's a clear path from tables to triples, which should help make RDF make more sense.</p>"},{"location":"reference/tables-and-triples/#tables","title":"Tables","text":"<p>Tables are great! Here's a table!</p> first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo <p>You won't be surprised to find out that tables have rows and columns. Often each row corresponds to some thing that we want to talk about, such as a fictional character from Star Wars. Each column usually corresponds to some sort of property that those things might have. Then the cells contain the values of those properties for their respective row. We take some sort of complex information about the world, and we break it down along two dimensions: the things (rows) and their properties (columns).</p>"},{"location":"reference/tables-and-triples/#primary-keys","title":"Primary Keys","text":"<p>Tables are great! We can add another name to our table:</p> first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo Anakin Skywalker <p>Hmm. That's a perfectly good table, but it's not capturing the information that we wanted. It turns out (Spoiler Alert!) that Anakin Skywalker is Darth Vader! We might have thought that the rows of our table were describing individual people, but it turns out that they're just describing individual names. A person can change their name or have more than one name.</p> <p>We want some sort of identifier that lets us pick out the same person, and distinguish them from all the other people. Sometimes there's a \"natural key\" that we can use for this purpose: some bit of information that uniquely identifies a thing. When we don't have a natural key, we can generate an \"artificial key\". Random strings and number can be good artificial keys, but sometimes a simple incrementing integer is good enough.</p> <p>The main problem with artificial keys is that it's our job to maintain the link between the thing and the identifier that we gave it. We prefer natural keys because we just have to inspect that thing (in some way) to figure out what to call it. Even when it's possible, sometimes that's too much work. Maybe we could use a DNA sequence as a natural key for a person, but it probably isn't practical. We do use fingerprints and facial recognition, for similar things, though.</p> <p>(Do people in Star Wars even have DNA? Or just midichlorions?)</p> <p>Let's add a column with an artificial key to our table:</p> sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker <p>This is our table of names, allowing a given person to have multiple names. But what we thought we wanted was a person table with one row for each person, like this:</p> sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo <p>In SQL we could assert that the \"sw_id\" column of the person table is a PRIMARY KEY. This means it must be unique. (It probably shouldn't be NULL either!)</p> <p>The names in the person table could be the primary names that we use in our Star Wars database system, and we could have another alternative_name table:</p> sw_id first_name last_name 3 Anakin Skywalker"},{"location":"reference/tables-and-triples/#holes","title":"Holes","text":"<p>Tables are great! We can add more columns to our person table:</p> sw_id first_name last_name occupation 1 Luke Skywalker Jedi 2 Leia Organa princess 3 Darth Vader 4 Han Solo scoundrel <p>The 2D pattern of a table is a strong one. It not only provides a \"slot\" (cell) for every combination of row and column, it also makes it very obvious when one of those slots is empty. What does it mean for a slot to be empty? It could mean many things.</p> <p>For example, in the previous table in the row for Darth Vader, the cell for the \"occupation\" column is empty. This could mean that:</p> <ul> <li>we don't know whether he has an occupation</li> <li>we know that he has an occupation, but we don't know which occupation it is.</li> <li>we might know, but we haven't bothered to write it down yet</li> <li>we might know, but it doesn't fit nicely   into the New Republic Standard Registry of Occupations;   in other words, we know what his occupation is,   but including it here would violate a constraint on our database</li> <li>we specifically know that he doesn't have an occupation;   we triple-checked</li> <li>we know more generally (Spoiler Alert!!) that he's dead,   and dead people can't have an occupation.</li> </ul> <p>I'm sure I haven't captured all the possibilities. The point is that there's lot of possible reasons why a cell would be blank. So what can we do about it?</p> <p>If our table is stored in a SQL database, then we have the option of putting a NULL value in the cell. NULL is pretty strange. It isn't TRUE and it isn't FALSE. Usually NULL values are excluded from SQL query results unless you are careful to ask for them.</p> <p>The way that NULL works in SQL eliminates some of the possibilities above. SQL uses the \"closed-world assumption\", which is the assumption that if a statement is true then it's known to be true, and conversely that if it's not known to be true then it's false. So if Anakin's occupation is NULL in a SQL database, then as far as SQL is concerned, we must know that he doesn't have an occupation. That might not be what you were expecting!</p> <p>The Software Carpentry module on Missing Data has more information.</p>"},{"location":"reference/tables-and-triples/#multiple-values","title":"Multiple Values","text":"<p>Tables are great! Let's add even more information to our table:</p> sw_id first_name last_name occupation enemy 1 Luke Skywalker Jedi 3 2 Leia Organa princess 3 3 Darth Vader 1,2,4 4 Han Solo scoundrel 3 <p>We're trying to say that Darth Vader is the enemy of everybody else in our table. We're using the primary key of the person in the enemy column, which is good, but we've ended up with multiple values in the \"enemy\" column for Darth Vader.</p> <p>In any table or SQL database you could make the \"enemy\" column a string, pick a delimiter such as the comma, and concatenate your values into a comma-separated list. This works, but not very well.</p> <p>In some SQL databases, such as Postgres, you could given the \"enemy\" column an array type, so it can contain multiple values. You get special operators for querying inside arrays. This can work pretty well.</p> <p>The usual advice is to break this \"one to many\" information into a new \"enemy\" table:</p> sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 <p>Then you can JOIN the person table to the enemy table as needed.</p>"},{"location":"reference/tables-and-triples/#sparse-tables","title":"Sparse Tables","text":"<p>Tables are great! Let's add even more information to our table:</p> sw_id first_name last_name occupation father lightsaber_color ship 1 Luke Skywalker Jedi 3 green 2 Leia Organa princess 3 3 Darth Vader red 4 Han Solo scoundrel Millennium Falcon <p>A bunch of these columns only apply to a few rows. Now we've got a lot more NULLs to deal with. As the number of columns increases, this can become a problem.</p>"},{"location":"reference/tables-and-triples/#property-tables","title":"Property Tables","text":"<p>Tables are great! If sparse tables are a problem, then let's try to apply the same solution that worked for the \"many to one\" problem in the previous section.</p> <p>name table:</p> sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker <p>occupation table:</p> sw_id occupation 1 Jedi 2 princess 4 scoundrel <p>enemy table:</p> sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 <p>father table:</p> sw_id father 1 3 2 3 <p>lightsaber_color table:</p> sw_id lightsaber_color 1 green 3 red <p>ship table:</p> sw_id ship 4 Millennium Falcon <p>Hmm. Yeah, that will work. But every query we write will need some JOINs. It feels like we've lost something.</p>"},{"location":"reference/tables-and-triples/#entity-attribute-value","title":"Entity, Attribute, Value","text":"<p>Tables are great! But there's such a thing as too many tables. We started out with a table with a bunch of rows and a bunch of columns, and ended up with a bunch of tables with a bunch of rows but just a few columns.</p> <p>I have a brilliant idea! Let's combine all these property tables into just one table, by adding a \"property\" column!</p> sw_id property value 1 first_name Luke 2 first_name Leia 3 first_name Darth 4 first_name Han 5 first_name Anakin 1 last_name Skywalker 2 last_name Skywalker 3 last_name Vader 4 last_name Solo 5 last_name Skywalker 1 occupation Jedi 2 occupation princess 4 occupation scoundrel 1 enemy 3 2 enemy 3 3 enemy 1 3 enemy 2 3 enemy 4 4 enemy 1 1 father 3 2 father 3 1 lightsaber_color green 3 lightsaber_color red 4 ship Millenium Falcon <p>It turns out that I'm not the first one to think of this idea. People call it \"Entity, Attribute, Value\" or \"EAV\". People also call it an \"anti-pattern\", in other words: a clear sign that you've made a terrible mistake.</p> <p>There are lots of circumstances in which one big, extremely generic table is a bad idea. First of all, you can't do very much with the datatypes for the property and value columns. They kind of have to be strings. It's potentially difficult to index. And tables like this are miserable to query, because you end up with all sorts of self-joins to handle.</p> <p>But there's at least one use case where it turns out to work quite well...</p>"},{"location":"reference/tables-and-triples/#merging-tables","title":"Merging Tables","text":"<p>Tables are great! Until they're not.</p> <p>The strong row and column structure of tables makes them great for lots of things, but not so great for merging data from different sources. Before you can merge two tables you need to know all about:</p> <ol> <li>how the columns are structured</li> <li>what the rows mean</li> <li>what the cells mean</li> </ol> <p>So you need to know the schemas of the two tables before you can start merging them together. But if you happen to have two EAV tables then, as luck would have it, they already have the same schema!</p> <p>You also need to know that you're talking about the same things: the rows have to be about the same things, you need to be using the same property names for the same things, and the cell values also need to line up. If only there was an open standard for specifying globally unique identifiers...</p> <p>Yes, you guessed it: URLs (and URNs and URIs and IRIs)! Let's assume that we use the same URLs for the same things across the two tables. Since we're a close-knit community, we've come to an agreement on a Star Wars data vocabulary.</p> <p>URLs are annoyingly long to use in databases, so let's use standard \"sw\" prefix to shorten them. Now we have table 1:</p> sw_id property value sw:1 sw:first_name Luke sw:2 sw:first_name Leia sw:3 sw:first_name Darth sw:4 sw:first_name Han sw:5 sw:first_name Anakin sw:1 sw:last_name Skywalker sw:2 sw:last_name Skywalker sw:3 sw:last_name Vader sw:4 sw:last_name Solo sw:5 sw:last_name Skywalker sw:1 sw:occupation sw:Jedi sw:2 sw:occupation sw:princess sw:4 sw:occupation sw:scoundrel <p>and table 2:</p> sw_id property value sw:1 sw:enemy sw:3 sw:2 sw:enemy sw:3 sw:3 sw:enemy sw:1 sw:3 sw:enemy sw:2 sw:3 sw:enemy sw:4 sw:4 sw:enemy sw:1 sw:1 sw:father sw:3 sw:2 sw:father sw:3 sw:1 sw:lightsaber_color green sw:3 sw:lightsaber_color red sw:4 sw:ship Millenium Falcon <p>To merge these two tables, we simple concatenate them. It couldn't be simpler.</p> <p>Wait, this looks kinda familiar...</p>"},{"location":"reference/tables-and-triples/#rdf","title":"RDF","text":"<p>These tables are pretty much in RDF format. You just have to squint a little!</p> <ul> <li>sw_id == subject</li> <li>property == predicate</li> <li>value == object</li> </ul> <p>Each row of the table is a subject-predicate-object triple. Our subjects, predicates, and some objects are URLs. We also have some literal objects. We could turn this table directly into Turtle format with a little SQL magic (basically just concatenating strings):</p> <pre><code>SELECT \"@prefix sw: &lt;http://example.com/sw_&gt; .\"\nUNION ALL\nSELECT \"\"\nUNION ALL\nSELECT\n   sw_id\n|| \" \"\n|| property\n|| \" \"\n|| IF(\n     INSTR(value, \":\"),\n     value,                -- CURIE\n     \"\"\"\" || value || \"\"\"\" -- literal\n   )\n|| \" .\"\nFROM triple_table;\n</code></pre> <p>The first few lines will look like this:</p> <pre><code>@prefix sw: &lt;http://example.com/sw_&gt; .\n\nsw:1 sw:first_name \"Luke\" .\nsw:2 sw:first_name \"Leia\" .\nsw:3 sw:first_name \"Darth\" .\nsw:4 sw:first_name \"Han\" .\n</code></pre> <p>Two things we're missing from RDF are language tagged literals and typed literals. We also haven't used any blank nodes in our triple table. These are easy enough to add.</p> <p>The biggest thing that's different about RDF is that it uses the \"open-world assumption\", so something may be true even though we don't have a triple asserting that it's true. The open-world assumption is a better fit than the closed-world assumption when we're integrating data on the Web.</p>"},{"location":"reference/tables-and-triples/#conclusion","title":"Conclusion","text":"<p>Tables are great! We use them all the time, they're strong and rigid, and we're comfortable with them.</p> <p>RDF, on the other hand, looks strange at first. For most common data processing, RDF is too flexible. But sometimes flexiblity is the most important thing.</p> <p>The greatest strength of tables is their rigid structure, but that's also their greatest weakness. We saw a number of problems with tables, and how they could be overcome by breaking tables apart into smaller tables, until we got down to the most basic pattern: subject-predicate-object. Step by step, we were pushed toward RDF.</p> <p>Merging tables is particularly painful. When working with data on the Web, merging is one of the most common and important operations, and so it makes sense to use RDF for these tasks. If self-joins with SQL is the worst problem for EAV tables, then SPARQL solves it.</p> <p>These examples show that it's not really very hard to convert tables to triples. And once you've seen SPARQL, the RDF query language, you've seen one good way to convert triples to tables: SPARQL SELECT results are just tables!</p> <p>Since it's straightforward to convert tables to triples and back again, make sure to use the right tool for the right job. When you need to merge heterogeneous data, reach for triples. For most other data processing tasks, use tables. They're great!</p>"},{"location":"reference/troublehooting-robot/","title":"Lessons learned from troubleshooting ROBOT","text":""},{"location":"reference/troublehooting-robot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Review tutorial on Ontology pipelines with ROBOT and SPARQL</li> </ul>"},{"location":"reference/troublehooting-robot/#learning-objectives","title":"Learning objectives","text":"<p>Learn common mistakes when using ROBOT and how to troubleshoot and fix them.</p>"},{"location":"reference/troublehooting-robot/#lessons-learned","title":"Lessons learned","text":""},{"location":"reference/troublehooting-robot/#copying-pasting-especially-in-google-docs-can-introduce-unexpected-format-changes-in-row-2-of-the-template","title":"Copying-pasting (especially in google docs) can introduce unexpected format changes in row 2 of the template:","text":"<ul> <li>Note that these format changes are not always visible.</li> <li>The most common typos are:</li> <li>introduction of space in cells</li> <li>single quotes are changed into apostrophes</li> <li>These errors are most commonly reported as \"MANCHESTER PARSE ERROR\"</li> </ul>"},{"location":"reference/troublehooting-robot/#restrictions-for-the-first-2-rows-of-a-robot-template","title":"Restrictions for the first 2 rows of a ROBOT template:","text":"<ul> <li>In the same column, it is OK to have a header string (row #1) with no template string (row #2).</li> <li>the information in the column is useful to curators (e.g. term labels) but will be ignored by ROBOT.</li> </ul> <ul> <li>In the same column, if there is a template string (row #2), there MUST be a header string (row #1)</li> <li>if the row #1 is missing, the error will be reported as: COLUMN MISMATCH ERROR the template string in column 1 must have a corresponding header in table \"tmp/merge_template.tsv\u201d</li> </ul>"},{"location":"reference/troublehooting-robot/#the-content-of-the-template-break-some-obo-or-protege-rules","title":"The content of the template break some OBO or Protege rules","text":"<ul> <li>for example, Protege only allows one comment on a class. If you are adding new comments to terms via ROBOT, you will get an error if a comment already exists on a term.</li> <li>error will be reported as: OBO STRUCTURE ERROR Ontology does not conform to OBO structure rules: multiple comment tags not allowed.</li> <li>Note: If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch. The error with the optional \u201cnull\u201d is when the mondo-edit file is empty. <code>Optional.get() cannot be called on an absent value</code> <code>Use the -vvv option to show the stack trace.</code> <code>Use the --help option to see usage information</code> <code>make: *** [mondo.Makefile:454: merge_template] Error 1</code></li> </ul>"},{"location":"reference/troublehooting-robot/#new-id-prefix","title":"New ID prefix:","text":"<ul> <li>ROBOT template can be used to add axioms containing terms (and IDs) from other ontologies which were recently imported</li> <li>The ID prefix is not recognized by ROBOT, and the error is reported as MANCHESTER PARSE ERROR</li> <li>Resolution: the ontology Makefile should be updated to include the prefix in the merge_template.</li> <li>Note: If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch.</li> </ul>"},{"location":"reference/troublehooting-robot/#example-templates","title":"Example templates","text":"<ul> <li>Example templates from Mondo are available here</li> <li>Example templates from OBI are available here</li> </ul>"},{"location":"reference/troublehooting-robot/#contributors","title":"Contributors","text":"<ul> <li>Sabrina Toro (ORCID)</li> <li>Nicole Vasilevsky (ORCID)</li> </ul>"},{"location":"reference/wikidata/","title":"A reference guide to Wikidata","text":""},{"location":"reference/wikidata/#a-reference-guide-to-wikidata-and-its-connection-to-obo","title":"A reference guide to Wikidata and its connection to OBO","text":""},{"location":"reference/wikidata/#introduction-to-wikidata","title":"Introduction to Wikidata","text":"<ul> <li>Wikidata in one page</li> <li>Wikidata</li> <li>Wikidata Query Service (UI)</li> <li>Wikidata Query Serivce (Machine readable)</li> <li>Wikidata API</li> </ul>"},{"location":"reference/wikidata/#obo-in-wikidata","title":"OBO in Wikidata","text":"<ul> <li>Related wikidata OBO properties</li> <li>Wikidata OBO properties and curation catalogs</li> <li>Related wikidata OBO items</li> <li>A mapping from OBO Foundry prefixes to Wikidata properties</li> </ul>"},{"location":"reference/wikidata/#licenses","title":"Licenses","text":"<p>On Wikidata the following licenses applies:</p> <p>\"All structured data from the main, Property, Lexeme, and EntitySchema namespaces is available under the Creative Commons CC0 License; text in the other namespaces is available under the Creative Commons Attribution-ShareAlike License\" </p> <p>Adding non-CC0 licensed OBO ontologies in full might be problematic due to  * License stacking</p> <p>IANL, but my understanding is that as long as only URI mappings are created to OBO ontology terms no licenses are breached (even if the ontology is not CC0)</p>"},{"location":"reference/wikidata/#why-map-obo-uris-to-wikidata","title":"Why map OBO uris to Wikidata?","text":"<ul> <li>Wikidata is a hub in the Linked Open Data cloud; it is, thus, a good place to crowdsource database cross references (e.g. between the Cell Ontology and FMA).</li> <li>Wikidata provides direct links between items and Wikipedia, as well as a proxy for how many different Wikipedia languages are available for each concept. These can be acessed via SPARQL queries  (e.g https://w.wiki/6Tpd). Wikipedia links are useful for adding explainability to applications, and language count can be a proxy for popularity of concepts. <ul> <li>E.g. The top popular concepts with a CL ID are \"Cell\", \"Red Blood Cell\" and \"Neuron\" with over 100 Wikipedia languages each (https://w.wiki/6TyY)</li> </ul> </li> <li>Wikidata is multilingual and most (if not all) OBO ontologies are English-only. Wikidata provides infrastructure to record preferred labels accross 200+ languages (not sure the current number).</li> </ul>"},{"location":"reference/wikidata/#notable-differences","title":"Notable differences","text":"<ul> <li>Wikidata's model is definition-free. The meaning of Wikidata terms by a combination of the label, description, aliases and statements.</li> <li>Wikidata does not support reasoning, as supporting inconsistencies are a feature (not a bug). It is so to handle knowledge diversity. </li> </ul>"},{"location":"reference/wikidata/#literature","title":"Literature","text":"<ul> <li>Wikidata as a semantic framework for the Gene Wiki initiative</li> <li>A protocol for adding knowledge to Wikidata: aligning resources on human coronaviruses</li> </ul>"},{"location":"reference/wikidata/#tools","title":"Tools","text":"<ul> <li>Scholia</li> <li>Chlambase</li> <li>Wikigenomes</li> <li>Science Stories</li> <li>Entity Explosion</li> </ul>"},{"location":"teaching/case-studies/","title":"Learning Outcomes for Critical Path Tutorial","text":"<ol> <li>Understand the value of URIs as global identifiers and the potential shortcomings.</li> <li>Having a basic picture of the flagship efforts of the Semantic Web.</li> <li>Being aware of some of the central Semantic Web applications in the biomedical domain.</li> <li>Having a cursory understanding of how linked data can help to power your Critical Path data analysis problems.</li> </ol>"},{"location":"teaching/case-studies/#interesting-case-studies-to-talk-about","title":"Interesting Case Studies to talk about:","text":"<ol> <li>The Experimental Factor Ontology: from controlled vocabulary to integrated application ontology driving drug target identification.</li> <li>From barely structured data via data dictionaries to semantic data integration:</li> <li>International HundredK+ Cohorts Consortium (IHCC) data harmonization case study: How to get from messy, individual data dictionaries for COHORT data to an integrated resource for browsing and grouping.</li> <li>The EJPRD story:<ul> <li>Registry level integration using a semantic metadata model</li> <li>Common data elements in rare disease registration.</li> </ul> </li> </ol>"},{"location":"teaching/case-studies/#efo-case-study","title":"EFO case study","text":"<ol> <li>Build controlled vocabulary</li> <li>Look a bit at the anatomy of a term</li> <li>So what happens now?<ol> <li>The story of scientific database curation</li> <li>The integrator hub with the killer use case comes along</li> <li>Now the vocabulary is getting \u201cforced\u201d onto other databases that want to be part (and have to be part)</li> <li>The number of terms needed shoot up exponentially - external ontologies need two be integrated<ol> <li>Uberon</li> <li>Mondo<ol> <li>Why Mondo and not DO?</li> </ol> </li> <li>Finally: better, more specialised hierarchies</li> <li>Its hard to re-use. (Measurement story)</li> </ol> </li> <li>Output data of integrator hub can now be integrated even higher (e.g. disease to gene networks)</li> <li>Individual sources can also be integrated individually</li> </ol> </li> <li>Stories like this happen all the time: The SCDO story<ol> <li>First started building a vocab</li> <li>Then using ROBOT</li> <li>Then linking OBO terms</li> <li>Then applying for OBO membership</li> <li>Then using OBO purls and re-using OBO terms</li> <li>More to come</li> </ol> </li> </ol>"},{"location":"teaching/case-studies/#ihcc-story","title":"IHCC story","text":"<ol> <li>Cohort data are scattered and there is no easy way to group data across cohorts</li> <li>Even just finding the right cohort can be difficult</li> <li>Data dictionaries are often just spreadsheets on someones computer</li> <li>Data dictionaries do not have rich metadata (you dont know data dictionary category or value pertains to a disease)</li> <li>What to do:</li> <li>Build controlled vocabulary</li> <li>Map data dictionaries to a controlled vocabulary</li> <li>Build ontological model from controlled terms rich enough to group the data for the use cases at hand</li> <li>Design a process that makes the above scalable</li> <li>Show examples</li> <li>So now, we want enable the discovery of data across these cohorts.</li> <li>Build GECKO</li> <li>Assign data dictionary elements to IDs and publish as \"Linked Data\" (browse here)</li> <li>Build mapping pipeline<ol> <li>Check example google sheet</li> <li>Link IDs to ontology terms</li> </ol> </li> <li>These links can now be used to group the metadata for identifying cohorts</li> </ol>"},{"location":"teaching/case-studies/#ejprd-story","title":"EJPRD story","text":"<ol> <li>Rare disease registries are scattered across the web and there is no easy way to search across all</li> <li>EJPRD is developing two metadata schemas:</li> <li>On Registry level, they are building the metadata model which is reusing some standard vocabularies such as dcat. There is not that much \"semantics\" here - it really is a metadata model</li> <li>On Record level, they are building the Clinical Data Elements (CDE) Semantic Model, see for example the core model.</li> <li>The idea is that registries publish their metadata (and eventually data) as linked data that can be easily queried using the above models. One of the most major problems is the size of the project and competing voices (\"If its not RDF its not FAIR\"), but also the sheer scale of the technical issue: many of the so called registries are essentially excel spreadsheets on an FTP server.</li> </ol>"},{"location":"tutorial/Delphi-method-ontology/","title":"Using the Delphi method to bring domain knowledge into ontologies","text":"<p>Author: Tim Alamenciak</p>"},{"location":"tutorial/Delphi-method-ontology/#overview","title":"Overview","text":"<p>Ontologies are meant to be underpinned by domain knowledge, but there are huge cognitive and technical barriers to contributing directly to ontologies. Many domain experts do not have knowledge of logical axioms, OWL, URIs or Github, yet still have much to contribute to ontology development.</p> <p>Enter the Delphi method - a widely-used social science methodology designed to help experts reach consensus. Broadly, the method involves soliciting predictions or knowledge from groups of experts, synthesizing that, then presenting it back to those same experts in an anonymized form and conducting revisions. Rinse and repeat for as long as necessary.</p> <p>The process works a little differently when applied to ontologies.</p> <p>Typically ontology researchers will ask participants to review an ontology based on design criteria using a survey and rate terms on a Likert scale. These surveys can be simple, such as asking the experts to rate their level of agreement with the term and its definition, and provide suggestions to update inadequate terms (Hassan and Mat Nayan 2021). </p> <p>The process can also handle more complex input. For example, Holsapple and Joshi (2002) asked participants to rate the comprehensiveness, correctness, conciseness, clarity and utility of an ontology and to provide suggestions for improvement where they felt it fell short of those criteria. The improvements were then integrated into the ontology and a revised version was created and presented again to the experts (called a \"round\" in the Delphi process).</p> <p>This guide will walk through the steps of conducting a Delphi method review of an ontology or its parts starting with developing an anchor ontology, followed by selecting a sample, determining your criteria, and executing the study.</p> <p>We will talk through this process using the example of an ontology of ecological interventions being developed as part of the Environment Ontology (ENVO).</p>"},{"location":"tutorial/Delphi-method-ontology/#study-preparation","title":"Study preparation","text":""},{"location":"tutorial/Delphi-method-ontology/#anchor-ontology","title":"Anchor ontology","text":"<p>Unless you're paying experts, the survey needs to be scoped to ensure that it doesn't take participants too much time to review the ontology. This can be tricky since many ontologies have thousands of terms. </p> <p>You might be developing a project ontology or a modification of a domain ontology. In the case of a project ontology, it may be small enough that the whole ontology can be the \"anchor ontology.\" </p> <p>The basic idea here is that you select a subset of terms on which you want feedback. Technically speaking, this can be extracted from the ontology using ROBOT or RDFLib.</p> <p>For example: We wish to solicit feedback from domain experts specifically on our ecological intervention terms. Those all are subclasses of active ecosystem management process (<code>ENVO:01001170</code>), so we can consider that to be our anchor ontology.</p>"},{"location":"tutorial/Delphi-method-ontology/#sample-selection","title":"Sample selection","text":"<p>\"Purposive sampling\" is typically used in the Delphi process, which means that you find a group of experts who have the relevant knowledge to comment on the terms. This contrasts with random sampling, which attempts to get a representative sample of a population. </p> <p>Typical sample sizes in the published research range from 4 to 79 experts, with an average of 24.</p> <p>For example: We created a list of academics (n = 15) and practitioners (n = 15) with expertise in restoration ecology. We also used snowball sampling to find participants not part of our professional networks.</p>"},{"location":"tutorial/Delphi-method-ontology/#survey-design","title":"Survey design","text":"<p>Once you have your anchor ontology, it's time to solicit feedback and do revisions. The Delphi process is an iterative one. Each round should involve changes to the anchor ontology which are then subsequently presented back to the experts.</p> <p>Ontology terms and definitions may be a bit obtuse for domain experts not familiar with Aristotelian definitions, axioms, and so on. It may be beneficial to design an \"interpretation layer\" where the definitions are made more readable. You could also consider clustering the terms (e.g. we include \"planting process\" and all of its subclasses as one \"term\" in our survey). </p> <p>Typically you would use a Likert scale to allow people to rate each term according to certain criteria. The criteria you select are crucial. Raad and Cruz (2015) present a good list of criteria in their open access paper. Here are the criteria that may be relevant to ask domain experts about:</p> <ul> <li>Accuracy - Are the definitions, descriptions and properties correct?</li> <li>Conciseness - Does the ontology include extraneous elements?</li> <li>Clarity - How clear are the ontology term labels and definitions?</li> <li>Completeness - Is the domain of interest covered by this ontology?</li> </ul> <p>Each item should have a Likert rating and an open textbox for respondents to suggest changes to the ontology.</p> <p>There are three additional criteria that may best be evaluated by the ontology mechanic responsible for the project:</p> <ul> <li>Adaptability - Can the ontology fulfill its anticipated tasks?</li> <li>Computational efficiency - How well does the reasoner work with the ontology?</li> <li>Consistency - Does the ontology have any contradictions?</li> </ul> <p>Each iteration of the ontology should also run the reasoner and quality control tools (e.g. ROBOT report; OntOlogy Pitfall Scanner!)</p> <p>For example: Here is one section of our survey that shows the cluster of terms under \"mechanical vegetation removal process\" and the rating + textbox:</p> <p>![[../images/delphi_racoon_example.jpg]]</p>"},{"location":"tutorial/Delphi-method-ontology/#automated-survey-creation","title":"Automated survey creation","text":"<p>It is entirely possible to automate the survey creation process by piping your OWL file into a Python script which will then feed it into a jinja2 template and spit out a CSV that can be imported into survey software. </p> <p>There is an example of this approach implemented in Python that creates surveys for REDCap in this repository. There are a number of options for automating this step, including using the open source survey platform LimeSurvey or combining Airtables with Fillout.</p>"},{"location":"tutorial/Delphi-method-ontology/#revision-and-review-process-rounds","title":"Revision and review process (rounds)","text":"<p>The survey above should change for each round of the Delphi process. Typically the Delphi process continues until consensus is reached. Consensus may mean that there are no changes to be made, or that people agree within a certain range.</p> <p>You as the researcher and ontology mechanic get to decide where that cut-off is. </p>"},{"location":"tutorial/Delphi-method-ontology/#final-ontology","title":"Final ontology","text":"<p>After you are satisfied with the consensus received, you can release the ontology in the wild. You may also choose to indicate that specific terms have gone through this Delphi process. There is not a standard annotation property for this, but it can be included as a comment on the term.</p>"},{"location":"tutorial/basic-dl-query/","title":"DL query","text":"<p>This tutorial is based off https://ontology101tutorial.readthedocs.io/en/latest/DL_QueryTab.html Created by: Melissa Haendel, Chris Mungall, David Osumi-Sutherland, Matt Yoder, Carlo Torniai, and Simon Jupp</p>"},{"location":"tutorial/basic-dl-query/#dl-query-tab","title":"DL query tab","text":"<p>The DL query tab shown below provides an interface for querying and searching an ontology. The ontology must be classified by a reasoner before it can be queried in the DL query tab.</p> <p>For this tutorial, we will be using cc.owl which can be found here.</p> <p>Open cc.owl in Protege (use Open from URL and enter the <code>https://raw.githubusercontent.com/OHSUBD2K/BDK14-Ontologies-101/master/BDK14_exercises/basic-dl-query/cc.owl</code>). Run the reasoner. Navigate to the DL Query tab.</p> <p></p> <p>Type <code>organelle</code> into the box, and make sure <code>subclasses</code> and <code>direct subclasses</code> are ticked.</p> <p></p> <p>You can type any valid OWL class expression into the DL query tab. For example, to find all classes whose members are part_of a membrane, type <code>part_of some membrane</code> and click <code>execute</code>. Note the linking underscore for this relation in this ontology. Some ontologies do not use underscores for relations, whereby you'd need single quotes (i.e. <code>part of</code>).</p> <p></p> <p>The OWL keyword <code>and</code> can be used to make a class expression that is the intersection of two class expressions. For example, to find the classes in the red area below, we want to find subclasses of the intersection of the class <code>organelle</code> and the class <code>endoplasmic reticulum part</code></p> <p></p> <p></p> <p>Note that we do not need to use the <code>part</code> grouping classes in the gene ontology (GO). The same results can be obtained by querying for the intersection of the class <code>organelle</code> and the restriction <code>part_of some ER</code> \u2013 try this and see.</p> <p></p> <p>We can also ask for superclasses by ticking the boxes as below:</p> <p></p> <p>The <code>or</code> keyword is to used to create a class expression that is the union of two class expressions. For example: (WARNING: <code>or</code> is not supported by ELK reasoner)</p> <p></p> <p>This is illustrated by the red area in the following Venn diagram:</p> <p></p> <p>For further exercises, please see https://ontology101tutorial.readthedocs.io/en/latest/EXERCISE_BasicDL_Queries.html</p>"},{"location":"tutorial/biomappings/","title":"Curating Semantic Mappings with Biomappings","text":"<p>There are many partially overlapping ontologies and controlled vocabularies. Many ontologies curate semantic mappings such as database cross-references to support downstream users who might need to merge information, but these are often missing or incomplete.</p> <p>Curating these mappings by hand is time-consuming, so this tutorial presents a semi-automated semantic mappings curation workflow using Biomappings.</p> <p></p>"},{"location":"tutorial/biomappings/#installing-and-running-biomappings","title":"Installing and Running Biomappings","text":"<ol> <li>Fork the upstream Biomappings repository</li> <li>Clone your fork, make a branch, and install it.</li> </ol> <p><pre><code>$ git clone git+https://github.com/biopragmatics/biomappings.git\n$ cd biomappings\n$ git checkout -b your-branch-name\n$ pip install -e .[web]\n</code></pre> 3. Run the curation interface with</p> <pre><code>biomappings web\n</code></pre> <p>It can be accessed by navigating to http://localhost:5000/ in your browser. After you do some curations, the web    application takes care of interacting with the git repository from which you installed biomappings via the \"commit    and push\" button.</p>"},{"location":"tutorial/biomappings/#using-the-curation-interface","title":"Using the Curation Interface","text":"<p>Biomappings implements a simple web-based interface. This tutorial covers the most important feature to get you curating. It already has predictions built in - each has provenance on how it was generated and is assigned a numerical confidence (though the meaning varies based on how it was generated).</p>"},{"location":"tutorial/biomappings/#navigating-and-curating","title":"Navigating and Curating","text":"<p>The curation interface shows a set of mappings. There are buttons to curate each as:</p> <ol> <li>Exact</li> <li>Narrow</li> <li>Broad</li> <li>Incorrect</li> <li>Unsure</li> </ol> <p></p> <p>If you click the \"summarize\" button, it gives a breakdown of the source/target prefix pairs. This can be helpful for focusing curation.</p> <p></p> <p>The search bar can be used to search the CURIE or name for either the subject or target. In the following example, \"gene\" was searched.</p> <p></p> <p>After you do some curations, a \"Commit and Push\" button will appear. This automates interacting with Git in the background.</p> <p></p> <p>Warning You're also free to interact with Git on your own, but make sure that you stay consistent with how you interact with the files. Biomappings keeps track of line numbers, so if they change externally, be sure to restart the curation web application.</p> <p>When you're done, you can close the web application with control + c.</p>"},{"location":"tutorial/biomappings/#contributing-upstream","title":"Contributing Upstream","text":"<p>After curating, committing, and pushing, you can make a pull request back to the upstream Biomappings repository. All content in Biomappings is licensed under CC0, meaning that it can be freely reused. Further, several summaries and exports are automatically generated following all pull requests containing curation:</p> <p>Prominently, this includes the following exports in the Simple Standard for Sharing Ontological Mappings (SSSOM) format:</p> Artifact Description PURL TSV Mappings (predicted and curated, positive and negative) https://w3id.org/biopragmatics/biomappings/sssom/biomappings.sssom.tsv Metadata YML Metadata (extended prefix map, license, etc) for TSV https://w3id.org/biopragmatics/biomappings/sssom/biomappings.sssom.yml JSON Mappings in JSON https://w3id.org/biopragmatics/biomappings/sssom/biomappings.sssom.json OWL Mappings as an ontology https://w3id.org/biopragmatics/biomappings/sssom/biomappings.sssom.owl <p>These files can be directly pulled by the Ontology Development Kit (ODK) to be incorporated in ontologies. Similarly, Biomappings includes some experimental scripts for contributing them directly into upstream ontology files. For example, to contribute UBERON mappings upstream, do the following:</p> <pre><code>python -m biomappings.contribute.obo --prefix uberon --path ~/dev/uberon/src/ontology/uberon-edit.obo\n</code></pre> <p>Note that this assumes you have UBERON cloned in a certain place. You can edit this to your needs, then ultimately make a commit.</p>"},{"location":"tutorial/biomappings/#predicting-mappings","title":"Predicting Mappings","text":"<p>The Biomappings workflow is generic and allows for any mapping prediction workflow to be used. In this section, we will predict lexical mappings using Gilda. This approach is fast, simple, and interpretable since it relies on the labels and synonyms for concepts appearing in ontologies and other controlled vocabularies (e.g., MeSH). It takes the following steps:</p> <ol> <li>Index labels and synonyms from entities in various controlled vocabularies (e.g., MeSH, ChEBI)</li> <li>Filter out concepts that have mappings in primary source or third-party mappings in Biomappings</li> <li>Perform all-by-all comparison of indexes for each controlled vocabulary</li> <li>Filter out mappings that have been previously marked as incorrect (e.g.,    avoid zombie mappings)</li> </ol>"},{"location":"tutorial/biomappings/#clone-the-repository","title":"Clone the Repository","text":"<ol> <li>Fork the upstream Biomappings repository</li> <li>Clone your fork, make a branch, and install it. Note that we're including the <code>web</code> and <code>gilda</code> extras, so we can run    the curation interface locally as well as get all the tools we need for generating predictions.</li> </ol> <p><pre><code>git clone https://github.com/&lt;your namespace&gt;/biomappings\ncd biomappings\ngit checkout -b tutorial\npython -m pip install -e .[web,gilda]\n</code></pre> 3. Go into the <code>scripts/</code> directory</p> <p><pre><code>cd scripts/\n</code></pre> 4. Make a Python file for predictions. In this example, we're going to generate mappings between the ChEBI ontology and    Medical Subject Headings (MeSH).</p> <pre><code>touch generate_chebi_mesh_example.py\n</code></pre>"},{"location":"tutorial/biomappings/#preparing-the-mapping-script","title":"Preparing the Mapping Script","text":"<p>Biomappings has a lot of first-party support for Gilda prediction workflows, so generating mappings can be quite easy using a pre-defined workflow. Open your newly created <code>generate_chebi_mesh_example.py</code> in your favorite editor and add the following four lines:</p> <pre><code># generate_chebi_mesh_example.py\n\nfrom biomappings.gilda_utils import append_gilda_predictions\nfrom biomappings.utils import get_script_url\n\nprovenance = get_script_url(__file__)\nappend_gilda_predictions(\"chebi\", \"mesh\", provenance=provenance)\n</code></pre> <p>All generated mappings in Biomapping should point to the script that generated them. <code>provenance = get_script_url(__file__)</code> is a sneaky function that uses <code>__file__</code> to get the name of the current file and <code>get_script_url()</code> to generate a URI, assuming that this is in the <code>scripts/</code> directory of the Biomappings repository.</p> <p>The hard work is done by <code>append_gilda_predictions(\"chebi\", \"mesh\", provenance=provenance)</code>. Under the hood, this does the following:</p> <ol> <li>Looks up the names and synonyms for concepts in ChEBI and MeSH using PyOBO, a    unified interface for accessing ontologies and non-ontology controlled vocabularies (such as MeSH)</li> <li>Runs the algorithm described above</li> <li>Appends the predictions on to the local predictions TSV file</li> </ol>"},{"location":"tutorial/biomappings/#finishing-up","title":"Finishing Up","text":"<p>Execute the script from your command line and the predictions will be added to your local Biomappings cache.</p> <pre><code>python generate_chebi_mesh_example.py\n</code></pre> <p>This is a good time to review the changes and make a commit using</p> <pre><code>git add src/biomappings/resources/predictions.tsv\ngit commit -m \"Add predictions from ChEBI to MeSH\"\ngit push\n</code></pre> <p>Finally, you can run the web curation interface like normal and search for your new predictions to curate!</p> <pre><code>biomappings web\n</code></pre>"},{"location":"tutorial/biomappings/#custom-biomappings","title":"Custom Biomappings","text":"<p>If you're a company that wants to use Biomappings internally, it's possible to specify the path to the predictions and curations files used by the web interface. See the options below:</p> <pre><code>$ biomappings web --help\nUsage: biomappings web [OPTIONS]\n\n  Run the biomappings web app.\n\nOptions:\n  --predictions-path PATH  A predictions TSV file path\n  --positives-path PATH    A positives curation TSV file path\n  --negatives-path PATH    A negatives curation TSV file path\n  --unsure-path PATH       An unsure curation TSV file path\n  --help                   Show this message and exit.\n</code></pre>"},{"location":"tutorial/claude-code-getting-started/","title":"Getting Started with Claude Code","text":"<p>Claude Code is Anthropic's official CLI tool that transforms Claude into a development environment assistant. Unlike web-based interfaces, Claude Code runs locally on your machine with direct access to your filesystem, terminal commands, and development tools.</p> <p>Official Documentation: Claude Code Overview</p> <p>This document provides general background and information about Claude Code to help you understand its core concepts and capabilities. For hands-on lessons applying Claude Code to biocuration workflows, see the other parts of this tutorial: Exploring LinkML Models with Claude Code, Creating Slash Commands for API Lookups, and Using MCPs for Ontology Lookups. Please review this document to familiarize yourself with Claude Code before moving on to those practical lessons.</p>"},{"location":"tutorial/claude-code-getting-started/#obo-academy-training-session-part-1","title":"OBO Academy Training Session Part 1","text":""},{"location":"tutorial/claude-code-getting-started/#what-is-claude-code","title":"What is Claude Code?","text":"<p>Claude Code is an interactive command-line interface that provides:</p> <ul> <li>Direct file operations - Read, write, and edit files conversationally</li> <li>Command execution - Run terminal commands with natural language</li> <li>Tool integration - Grep, glob, git operations, and more</li> <li>Project awareness - Understands your project structure and git status</li> <li>Permission model - You control what Claude can execute</li> </ul> <p>Key difference from web interfaces: - Web Claude: Upload files manually, copy/paste code, no direct execution - Claude Code: Direct filesystem access, automatic tool selection, command execution</p>"},{"location":"tutorial/claude-code-getting-started/#installation","title":"Installation","text":"<p>Claude Code requires Node.js 18+ and is available via npm:</p> <pre><code>npm install -g @anthropic-ai/claude-code\n</code></pre> <p>Verify installation: <pre><code>claude --version\n</code></pre></p>"},{"location":"tutorial/claude-code-getting-started/#launching-claude-code","title":"Launching Claude Code","text":"<p>Start Claude Code in any directory:</p> <pre><code>claude\n</code></pre> <p>You'll see: <pre><code>Claude Code vX.X.X\nType /help for commands or just start chatting!\n\nWorking directory: /your/current/path\nGit status: Clean (on branch main)\n</code></pre></p> <p>Claude Code automatically detects: - Current directory and file structure - Git repository status - Available tools and permissions</p>"},{"location":"tutorial/claude-code-getting-started/#tool-overview","title":"Tool Overview","text":"<p>Claude Code has access to several built-in tools:</p> Tool Purpose Example Use Read Read file contents Schema files, code, documentation Write Create new files Scripts, config files, data Edit Modify existing files Fix typos, update code, refactor Bash Execute commands Git operations, build commands Grep Search file contents Find patterns, search codebase Glob Find files by pattern List all .yaml files WebSearch Search the web Look up API documentation <p>Claude automatically selects the appropriate tool based on your request.</p>"},{"location":"tutorial/claude-code-getting-started/#permission-model","title":"Permission Model","text":"<p>Claude Code will ask permission before: - \u274c Writing or modifying files - \u274c Executing shell commands - \u274c Deleting anything</p> <p>Permission dialog example: <pre><code>Claude wants to run: `git status`\nAllow? (yes/no/always):\n</code></pre></p> <p>Options: - <code>yes</code> - Allow this once - <code>no</code> - Deny this action - <code>always</code> - Auto-approve this type of action for this session</p> <p>Security tip: Start with individual approvals (<code>yes</code>) until you understand what Claude is doing!</p>"},{"location":"tutorial/claude-code-getting-started/#essential-commands","title":"Essential Commands","text":"<p>Claude Code has special commands starting with <code>/</code>:</p> Command Purpose <code>/help</code> Show available commands <code>/clear</code> Clear conversation history <code>/exit</code> Exit Claude Code <code>/reset</code> Reset the session"},{"location":"tutorial/claude-code-getting-started/#common-workflow-patterns","title":"Common Workflow Patterns","text":"<p>Below are some examples of how Claude Code works in different biocuration scenarios. These patterns demonstrate Claude Code's capabilities working with local files, databases, and APIs. Don't worry if some of the terminology seems unfamiliar\u2014you'll learn about LinkML schemas, database queries, and API interactions in detail in the later parts of this tutorial. For now, focus on understanding the general workflow patterns and how Claude Code can assist at each step.</p>"},{"location":"tutorial/claude-code-getting-started/#pattern-1-explore-understand-modify","title":"Pattern 1: Explore \u2192 Understand \u2192 Modify","text":"<pre><code>1. \"What LinkML files are in this directory?\"\n2. \"Read the CellLine class definition\"\n3. \"Add a new slot called 'tissue_origin' with type string\"\n</code></pre>"},{"location":"tutorial/claude-code-getting-started/#pattern-2-search-analyze-report","title":"Pattern 2: Search \u2192 Analyze \u2192 Report","text":"<pre><code>1. \"Find all files that reference BRCA1\"\n2. \"Summarize how BRCA1 is used across these files\"\n3. \"Create a markdown report of your findings\"\n</code></pre>"},{"location":"tutorial/claude-code-getting-started/#pattern-3-query-transform-export","title":"Pattern 3: Query \u2192 Transform \u2192 Export","text":"<pre><code>1. \"Query the database for all human genes\"\n2. \"Convert the results to TSV format\"\n3. \"Save it as human_genes.tsv\"\n</code></pre>"},{"location":"tutorial/claude-code-getting-started/#best-practices","title":"Best Practices","text":""},{"location":"tutorial/claude-code-getting-started/#do","title":"\u2705 DO:","text":"<ul> <li>Be specific - \"Read lines 10-50 of schema.yaml\" vs \"Read the file\"</li> <li>Iterate - Build complexity step-by-step</li> <li>Verify - Ask Claude to show what changed</li> <li>Use natural language - No special syntax required</li> </ul>"},{"location":"tutorial/claude-code-getting-started/#dont","title":"\u274c DON'T:","text":"<ul> <li>Blindly approve all permissions without reading</li> <li>Give vague instructions like \"fix it\"</li> <li>Assume Claude remembers files from previous sessions</li> <li>Skip verifying changes before committing</li> </ul>"},{"location":"tutorial/claude-code-getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/claude-code-getting-started/#claude-cant-find-the-file","title":"\"Claude can't find the file\"","text":"<p>Check your current directory: <pre><code>What directory am I in? List all files here.\n</code></pre></p>"},{"location":"tutorial/claude-code-getting-started/#permission-denied","title":"\"Permission denied\"","text":"<p>Grant the necessary permissions when Claude asks. Claude explains what it needs and why.</p>"},{"location":"tutorial/claude-code-getting-started/#unexpected-behavior","title":"\"Unexpected behavior\"","text":"<p>Ask Claude to explain: <pre><code>Can you explain what you just did step-by-step?\n</code></pre></p>"},{"location":"tutorial/claude-code-getting-started/#key-advantages-for-biocuration","title":"Key Advantages for Biocuration","text":"<ol> <li>Direct Database Access - Query production databases conversationally</li> <li>LinkML Integration - Read, understand, and modify schema files</li> <li>Batch Operations - Process multiple files or records at once</li> <li>Git Integration - Automatic git status, commit assistance</li> <li>Script Generation - Create data validation or transformation scripts</li> <li>API Integration - Query external APIs (UniProt, NCBI, etc.)</li> </ol>"},{"location":"tutorial/claude-code-getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand Claude Code basics, explore more advanced features:</p> <ul> <li>Exploring LinkML Models with Claude Code - Learn to navigate and extend AGR LinkML schemas</li> <li>Creating Slash Commands for API Lookups - Build reusable workflows for ChEBI, GO, and other API lookups</li> <li>Using MCPs for Ontology Lookups - Connect to EBI's Ontology Lookup Service for integrated ontology queries</li> </ul>"},{"location":"tutorial/claude-code-getting-started/#resources","title":"Resources","text":"<ul> <li>Claude Code Documentation: https://docs.claude.com/en/docs/claude-code</li> <li>GitHub Repository: https://github.com/anthropics/claude-code</li> <li>Installation Guide: https://docs.claude.com/en/docs/claude-code/installation</li> <li>MCP Servers: https://github.com/modelcontextprotocol/servers</li> </ul> <p>Tip: Claude Code shines for biocuration tasks that involve repetitive file operations, data validation, schema exploration, and database queries. Start with simple tasks and build up to complex workflows!</p> <p>Note on Content Generation: This tutorial was developed with the assistance of Claude Code (Anthropic's AI-powered CLI tool) for content generation, organization, and technical accuracy verification. All examples and workflows have been tested and validated for educational purposes.</p>"},{"location":"tutorial/claude-code-linkml/","title":"Exploring LinkML Models with Claude Code","text":"<p>This tutorial demonstrates how to use Claude Code to explore, understand, and extend LinkML schemas. We'll work with the Alliance Genome Resources (AGR) LinkML schema and create a new CellLine model based on real FlyBase database data.</p>"},{"location":"tutorial/claude-code-linkml/#prerequisites","title":"Prerequisites","text":"<ul> <li>Claude Code installed (Getting Started guide)</li> <li>Basic understanding of ontologies and database schemas</li> <li>PostgreSQL client (optional, for database exploration)</li> </ul>"},{"location":"tutorial/claude-code-linkml/#what-youll-learn","title":"What You'll Learn","text":"<ol> <li>Cloning and navigating the AGR LinkML schema repository</li> <li>Understanding LinkML class inheritance and patterns</li> <li>Exploring the FlyBase public database</li> <li>Creating a new LinkML model from scratch</li> <li>Connecting LinkML models to database schemas</li> </ol>"},{"location":"tutorial/claude-code-linkml/#background-alliance-linkml-schema","title":"Background: Alliance LinkML Schema","text":"<p>The Alliance Genome Resources uses LinkML to standardize genomic data across seven model organism databases: - FlyBase (Drosophila) - WormBase (C. elegans) - SGD (Yeast) - MGI (Mouse) - RGD (Rat) - ZFIN (Zebrafish) - XenBase (Xenopus)</p> <p>LinkML (Linked Data Modeling Language) is a YAML-based schema definition language that generates: - JSON schemas - SQL DDL (Data Definition Language) - Python data classes - Documentation</p> <p>Repository: https://github.com/alliance-genome/agr_curation_schema</p>"},{"location":"tutorial/claude-code-linkml/#part-1-clone-and-explore-the-agr-schema","title":"Part 1: Clone and Explore the AGR Schema","text":""},{"location":"tutorial/claude-code-linkml/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Ask Claude Code to clone the schema:</p> <pre><code>Clone the Alliance LinkML schema repository:\nhttps://github.com/alliance-genome/agr_curation_schema\n\nPut it in a folder called agr_curation_schema\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#step-2-explore-the-structure","title":"Step 2: Explore the Structure","text":"<pre><code>What's in the model/schema/ directory?\nShow me the main files and explain what they contain.\n</code></pre> <p>Key schema files: - <code>allianceModel.yaml</code> - Main entry point, imports all other schemas - <code>core.yaml</code> - Base classes (AuditedObject, CurieObject, SubmittedObject) - <code>gene.yaml</code> - Gene model - <code>allele.yaml</code> - Allele and variant model - <code>disease.yaml</code> - Disease annotations - <code>reagent.yaml</code> - Antibodies and constructs - <code>reference.yaml</code> - Publications and cross-references</p>"},{"location":"tutorial/claude-code-linkml/#step-3-understanding-linkml-basics","title":"Step 3: Understanding LinkML Basics","text":"<p>Before we dive deeper, let's understand the key LinkML concepts you'll encounter:</p> <pre><code>Read core.yaml and explain what these LinkML concepts mean:\n1. What is a \"class\" in LinkML?\n2. What is a \"slot\"?\n3. What does \"is_a\" mean (inheritance)?\n</code></pre> <p>LinkML Fundamentals: - Classes = Data types that represent entities (like Gene, Allele, CellLine) - Slots = Properties or fields that classes have (like name, taxon, references) - is_a = Inheritance - a class inherits all slots from its parent - slot_usage = Override or refine how a slot behaves in a specific class</p> <p>Example: If CellLine <code>is_a: GenomicEntity</code>, then CellLine automatically gets all the slots that GenomicEntity has (like taxon, curie), plus any additional slots it defines.</p>"},{"location":"tutorial/claude-code-linkml/#step-4-understand-the-inheritance-hierarchy","title":"Step 4: Understand the Inheritance Hierarchy","text":"<p>Now that you know what inheritance means, let's explore how AGR organizes its base classes:</p> <pre><code>In core.yaml, show me the inheritance chain.\nWhat is the relationship between AuditedObject, CurieObject, and SubmittedObject?\n</code></pre> <p>Inheritance chain: <pre><code>AuditedObject (audit fields: created_by, date_created, etc.)\n  \u2514\u2500&gt; CurieObject (adds curie field)\n      \u2514\u2500&gt; SubmittedObject (adds internal flag)\n          \u2514\u2500&gt; GenomicEntity (adds taxon)\n              \u2514\u2500&gt; Gene, Allele, etc.\n</code></pre></p> <p>Each class inherits all the slots from its parent and adds new ones. This means a Gene has audit fields (from AuditedObject), a curie (from CurieObject), an internal flag (from SubmittedObject), AND a taxon (from GenomicEntity), plus gene-specific slots.</p>"},{"location":"tutorial/claude-code-linkml/#part-2-explore-flybase-public-database","title":"Part 2: Explore FlyBase Public Database","text":"<p>FlyBase provides a public PostgreSQL database that anyone can query!</p>"},{"location":"tutorial/claude-code-linkml/#connection-details","title":"Connection Details","text":"<pre><code>Host:     chado.flybase.org\nUser:     flybase\nPassword: (none)\nDatabase: flybase\nPort:     5432\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#connect-and-explore","title":"Connect and Explore","text":"<p>Ask Claude to connect:</p> <pre><code>Connect to the FlyBase public database:\nHost: chado.flybase.org\nUser: flybase\nDatabase: flybase\nPort: 5432\n\nShow me all tables that contain \"cell_line\"\n</code></pre> <p>Cell line tables in FlyBase: - <code>cell_line</code> (341 records) - Main table - <code>cell_line_synonym</code> - Alternative names - <code>cell_line_pub</code> - Publication references - <code>cell_line_dbxref</code> - External database links - <code>cell_line_feature</code> - Gene/feature associations - <code>cell_line_relationship</code> - Parent/child relationships</p>"},{"location":"tutorial/claude-code-linkml/#examine-sample-data","title":"Examine Sample Data","text":"<pre><code>Describe the cell_line table structure.\nShow me a few example records.\n</code></pre> <p>Example cell lines: - FBtc0000001 - Kc167 (famous Drosophila cell line) - FBtc0000049 - ML-DmBG1 - FBtc0000051 - ML-DmBG2</p> <pre><code>For the Kc167 cell line, show me:\n1. Its synonyms\n2. Associated publications\n3. External database references\n</code></pre> <p>Discovery: Cell line data is rich and well-curated: - ~6,000 total synonyms - ~7,300 publication associations - ~1,250 external database references</p>"},{"location":"tutorial/claude-code-linkml/#part-3-plan-the-cellline-model","title":"Part 3: Plan the CellLine Model","text":""},{"location":"tutorial/claude-code-linkml/#check-existing-models","title":"Check Existing Models","text":"<pre><code>Search the AGR schema for any existing CellLine class.\nDoes it already exist?\n</code></pre> <p>Result: No CellLine model exists in AGR yet! Perfect opportunity to create one.</p>"},{"location":"tutorial/claude-code-linkml/#find-a-similar-model-to-use-as-a-template","title":"Find a Similar Model to Use as a Template","text":"<p>Now that we need to create a CellLine model, let's ask Claude for guidance on where to start:</p> <pre><code>I'm interested in modeling cell lines in LinkML for the AGR schema.\nCell lines are biological reagents used in research.\nBased on what exists in this schema, which class would be a good template or starting point?\n</code></pre> <p>Claude will likely suggest looking at the Antibody class in <code>reagent.yaml</code> because: - Antibodies are also biological reagents - They have similar requirements (taxon, references, cross-references) - They follow AGR patterns we can learn from</p>"},{"location":"tutorial/claude-code-linkml/#explore-the-suggested-model","title":"Explore the Suggested Model","text":"<p>Now let's examine the Antibody class to understand its structure:</p> <pre><code>Read the Antibody class in reagent.yaml.\nShow me:\n1. What does it inherit from?\n2. What slots does it define?\n3. How are references and cross-references handled?\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#design-decisions","title":"Design Decisions","text":"<p>Based on our exploration:</p> <p>Parent class: <code>GenomicEntity</code> - Provides: taxon, curie, audit fields - Appropriate for biological specimens</p> <p>Required fields: - <code>curie</code> (unique ID, e.g., FBtc0000001) - <code>name</code> (primary name, e.g., Kc167) - <code>taxon</code> (species of origin)</p> <p>Optional/multivalued: - <code>cell_line_synonyms</code> (alternative names) - <code>references</code> (publications) - <code>cross_references</code> (external database IDs) - <code>originating_lab</code> (source laboratory)</p>"},{"location":"tutorial/claude-code-linkml/#part-4-create-the-cellline-model","title":"Part 4: Create the CellLine Model","text":""},{"location":"tutorial/claude-code-linkml/#step-1-create-a-git-branch","title":"Step 1: Create a Git Branch","text":"<p>Before making changes to the schema, create a new branch (best practice!):</p> <pre><code>Create a new git branch called 'add-cell-line-model'\n</code></pre> <p>Claude will run <code>git checkout -b add-cell-line-model</code> for you.</p>"},{"location":"tutorial/claude-code-linkml/#step-2-create-the-basic-cellline-class","title":"Step 2: Create the Basic CellLine Class","text":"<p>Now, instead of micromanaging every detail, ask Claude to create the class and let it figure out the structure:</p> <pre><code>I want to create a bare bones CellLine class for the AGR schema.\nPlease base it on the Antibody model we looked at earlier.\nA cell line is a cultured cell population used in research.\n\nCreate the basic class structure and then tell me:\n1. What you've added\n2. Where you put it\n3. What else you think would be good to add\n</code></pre> <p>What Claude will do: - Decide whether to create a new <code>cell_line.yaml</code> file or add to an existing file - Create the basic CellLine class inheriting from GenomicEntity - Include essential fields like curie, name, taxon - Explain its decisions and suggest next steps</p> <p>Example of what Claude might create:</p> <pre><code>classes:\n  CellLine:\n    is_a: GenomicEntity\n    description: &gt;-\n      A cultured cell population derived from a multicellular organism,\n      maintained under controlled conditions for scientific research.\n    slots:\n      - name\n      - cell_line_synonyms\n      - references\n      - cross_references\n    slot_usage:\n      curie:\n        required: true\n      name:\n        required: true\n      taxon:\n        required: true\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#step-3-compare-to-existing-classes-and-validate-with-real-data","title":"Step 3: Compare to Existing Classes and Validate with Real Data","text":"<p>Now let's see how our model compares to other classes and whether it can actually represent the data we explored:</p> <pre><code>Great! Now can you:\n1. Compare our CellLine model to a few other existing AGR classes (like Antibody, Construct, or Gene)\n2. Tell me how the FlyBase data we looked at earlier (Kc167 cell line with its synonyms, publications, and cross-references) would fit into this LinkML model\n\nI want to make sure we've captured the right structure.\n</code></pre> <p>What this does: - Claude will analyze how CellLine compares to similar classes - It will map the real Kc167 data to the LinkML slots - You'll see if anything is missing (like synonym handling, publication references) - This is exactly how a real biocurator validates a new model!</p> <p>Example response you might get: - \"Kc167's name goes in the <code>name</code> slot\" - \"Its synonyms (Kc-167, KC167) go in <code>cell_line_synonyms</code>\" - \"Publications would use the <code>references</code> slot\" - \"FlyBase ID (FBtc0000001) is the <code>curie</code>\" - \"But we might need to add support for external database cross-references...\"</p>"},{"location":"tutorial/claude-code-linkml/#step-4-validate-conventions","title":"Step 4: Validate Conventions","text":"<pre><code>Review the CellLine model and check:\n1. Does it follow AGR naming conventions?\n2. Are all referenced types defined?\n3. Is anything missing compared to the Antibody pattern?\n</code></pre> <p>Claude will check the model against AGR patterns and let you know if anything needs adjustment.</p>"},{"location":"tutorial/claude-code-linkml/#step-5-add-the-dto-class-if-needed","title":"Step 5: Add the DTO Class (if needed)","text":"<p>LinkML uses DTO (Data Transfer Object) classes for data ingest. Claude may have already created one, but if not:</p> <pre><code>Does our CellLine model have a corresponding DTO class?\nIf not, can you add a CellLineDTO class following the AGR pattern?\n</code></pre> <p>What a DTO does: - Handles data import/ingest - Uses specific naming (cell_line_curie instead of just curie) - Includes _dtos suffix for related objects (synonym_dtos, cross_reference_dtos)</p>"},{"location":"tutorial/claude-code-linkml/#part-5-generate-test-data","title":"Part 5: Generate Test Data","text":"<p>Now let's validate our model by generating test data using the real FlyBase data we explored earlier!</p>"},{"location":"tutorial/claude-code-linkml/#create-sample-cellline-data","title":"Create Sample CellLine Data","text":"<p>Ask Claude to generate test data based on the FlyBase records:</p> <pre><code>Can you generate test data for our CellLine model using the Kc167 cell line data we explored from FlyBase?\n\nIf you need to, query the FlyBase database again to get:\n- The cell line name and ID\n- A few synonyms\n- Some publication references\n- Cross-references to external databases\n\nCreate the test data in the format that our LinkML model expects.\n</code></pre> <p>What Claude will do: - Query FlyBase for Kc167 data (if needed) - Format it according to our CellLine LinkML structure - Show you example data in YAML or JSON format</p> <p>Example test data you might get:</p> <pre><code>- curie: \"FB:FBtc0000001\"\n  name: \"Kc167\"\n  taxon: \"NCBITaxon:7227\"  # Drosophila melanogaster\n  cell_line_synonyms:\n    - synonym_sgml: \"Kc-167\"\n      synonym_type: \"alternate_name\"\n    - synonym_sgml: \"KC167\"\n      synonym_type: \"alternate_name\"\n  cross_references:\n    - referenced_curie: \"ATCC:CRL-1963\"\n    - referenced_curie: \"Cellosaurus:CVCL_0526\"\n  references:\n    - reference_curie: \"FB:FBrf0123456\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#generate-dto-test-data-optional","title":"Generate DTO Test Data (Optional)","text":"<p>If your model has a CellLineDTO class, you can also generate test ingest data:</p> <pre><code>Can you also create a sample CellLineDTO object that shows how this data would be ingested?\nUse the DTO naming conventions (cell_line_curie, synonym_dtos, etc.)\n</code></pre> <p>Why this matters: - Validates your model works with real data - Shows the difference between the model and DTO representations - Provides examples for data loading scripts - Tests that all required fields are present</p>"},{"location":"tutorial/claude-code-linkml/#key-claude-code-patterns-for-linkml","title":"Key Claude Code Patterns for LinkML","text":""},{"location":"tutorial/claude-code-linkml/#navigation","title":"Navigation","text":"<pre><code>\"Show me the gene.yaml file\"\n\"Find where the 'taxon' slot is defined\"\n\"Trace the inheritance chain for Allele\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#understanding","title":"Understanding","text":"<pre><code>\"Explain what the 'range' attribute means in this slot\"\n\"What's the difference between Gene and GeneDTO?\"\n\"Why does this class use slot_usage?\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#creation","title":"Creation","text":"<pre><code>\"Create a new LinkML class for [entity type]\"\n\"Add a new slot called [name] to [class]\"\n\"Generate SQL from this LinkML model\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#validation","title":"Validation","text":"<pre><code>\"Does this follow AGR naming conventions?\"\n\"Are all required slots defined?\"\n\"Is this inheritance chain correct?\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#best-practices","title":"Best Practices","text":""},{"location":"tutorial/claude-code-linkml/#do","title":"\u2705 DO:","text":"<p>Start with exploration - Understand existing patterns before creating new ones - Study similar classes (Antibody for CellLine) - Check what's already defined</p> <p>Explore real data first - Query production databases when possible - Understand actual data relationships - Discover edge cases early</p> <p>Work incrementally - Basic class structure first - Add complexity step-by-step - Validate at each stage</p> <p>Use Claude Code's strengths - File navigation and search - Pattern recognition - Inheritance tracing</p>"},{"location":"tutorial/claude-code-linkml/#dont","title":"\u274c DON'T:","text":"<p>Assume knowledge - Explain domain terms to Claude - Provide context about biological concepts - Don't assume Claude knows your abbreviations</p> <p>Skip validation - Always check conventions - Verify required fields - Test generated SQL</p> <p>Work in isolation - Don't create models without checking real data - Don't ignore existing patterns - Don't forget about downstream users</p>"},{"location":"tutorial/claude-code-linkml/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/claude-code-linkml/#cant-find-where-a-slot-is-defined","title":"\"Can't find where a slot is defined\"","text":"<p>Slots might be defined in imported files:</p> <pre><code>\"Trace the 'references' slot through all imports. Where is it actually defined?\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#model-validation-fails","title":"\"Model validation fails\"","text":"<p>Check inheritance chain:</p> <pre><code>\"What slots does GenomicEntity provide? Am I duplicating any?\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#generated-sql-doesnt-match-database","title":"\"Generated SQL doesn't match database\"","text":"<p>LinkML generates normalized schemas; databases may denormalize:</p> <pre><code>\"Show me how the FlyBase cell_line_synonym table relates to the main cell_line table\"\n</code></pre>"},{"location":"tutorial/claude-code-linkml/#resources","title":"Resources","text":""},{"location":"tutorial/claude-code-linkml/#databases","title":"Databases","text":"<ul> <li>FlyBase Public Database: chado.flybase.org</li> <li>User: flybase (no password)</li> <li>Database: flybase</li> <li>Port: 5432</li> <li>Test: <code>psql -h chado.flybase.org -U flybase -d flybase</code></li> </ul>"},{"location":"tutorial/claude-code-linkml/#documentation","title":"Documentation","text":"<ul> <li>AGR Schema: https://github.com/alliance-genome/agr_curation_schema</li> <li>AGR Schema Docs: https://alliance-genome.github.io/agr_curation_schema/</li> <li>LinkML Documentation: https://linkml.io/linkml/</li> <li>LinkML Tutorial: https://linkml.io/linkml/intro/tutorial.html</li> </ul>"},{"location":"tutorial/claude-code-linkml/#tools","title":"Tools","text":"<ul> <li>LinkML Validator: <code>linkml-validate</code></li> <li>SQL Generator: <code>gen-sql-ddl</code></li> <li>Python Classes: <code>gen-python</code></li> <li>JSON Schema: <code>gen-json-schema</code></li> </ul>"},{"location":"tutorial/claude-code-linkml/#next-steps","title":"Next Steps","text":"<p>You've learned how to explore and extend LinkML schemas with Claude Code. Continue with:</p> <ul> <li>Creating Slash Commands for API Lookups - Build reusable workflows for ontology lookups</li> <li>Using MCPs for Ontology Lookups - Connect to EBI's Ontology Lookup Service for integrated ontology queries</li> </ul>"},{"location":"tutorial/claude-code-linkml/#summary","title":"Summary","text":"<p>In this tutorial, you:</p> <p>\u2705 Cloned and explored the AGR LinkML schema repository \u2705 Connected to FlyBase's public database \u2705 Explored real cell line data (341 records) \u2705 Created a new CellLine LinkML model from scratch \u2705 Generated SQL schema from LinkML \u2705 Queried real biological data</p> <p>This is exactly how professional biocuration modeling works - understanding existing data, designing appropriate schemas, and validating with real-world requirements.</p> <p>Claude Code accelerates this process by helping you navigate complex schemas, understand inheritance patterns, and validate your models against established conventions.</p> <p>Note on Content Generation: This tutorial was developed with the assistance of Claude Code (Anthropic's AI-powered CLI tool) for content generation, organization, and technical accuracy verification. All examples and workflows have been tested and validated for educational purposes.</p>"},{"location":"tutorial/claude-code-mcps/","title":"Using MCPs for Ontology Lookups","text":"<p>Model Context Protocol (MCP) servers extend Claude Code with powerful integrations for bioinformatics workflows. This tutorial demonstrates how to use the EBI Ontology Lookup Service (OLS) MCP to query multiple ontologies through a single integrated interface.</p>"},{"location":"tutorial/claude-code-mcps/#prerequisites","title":"Prerequisites","text":"<ul> <li>Claude Code installed (Getting Started guide)</li> <li>Python 3.10+ with uv package manager</li> <li>Understanding of slash commands</li> <li>Completion of LinkML tutorial recommended</li> </ul>"},{"location":"tutorial/claude-code-mcps/#what-youll-learn","title":"What You'll Learn","text":"<ol> <li>What MCPs are and why they're powerful</li> <li>Installing the OLS MCP server</li> <li>Discovering OLS MCP capabilities through conversation</li> <li>Querying multiple ontologies (GO, ChEBI, HP, MONDO)</li> <li>Real-world ontology lookup workflows</li> <li>MCP vs slash command decision criteria</li> </ol>"},{"location":"tutorial/claude-code-mcps/#what-is-an-mcp","title":"What is an MCP?","text":"<p>MCP = Model Context Protocol</p> <p>MCPs are plugins that give Claude Code new capabilities through persistent connections to external services.</p>"},{"location":"tutorial/claude-code-mcps/#mcp-vs-slash-commands-vs-built-in-tools","title":"MCP vs Slash Commands vs Built-in Tools","text":"Feature Built-in Tools Slash Commands MCP Servers File operations \u2705 \u274c \u274c Simple APIs Via bash \u2705 \u274c Complex APIs \u274c Limited \u2705 Multiple ontologies \u274c One at a time \u2705 Persistent connections \u274c \u274c \u2705 Related tools grouped \u274c \u274c \u2705 Rate limiting \u274c \u274c \u2705"},{"location":"tutorial/claude-code-mcps/#when-to-use-each","title":"When to Use Each","text":"<p>Use built-in tools for: - Reading/writing files - Running commands - File system navigation</p> <p>Use slash commands for: - Simple API calls (ChEBI, GO lookups from Tutorial 3) - One-off queries - No authentication needed - Single ontology lookups</p> <p>Use MCPs for: - Querying multiple ontologies from one interface - Complex API integrations - Persistent database connections - Multiple related operations</p>"},{"location":"tutorial/claude-code-mcps/#the-ols-mcp-server","title":"The OLS MCP Server","text":"<p>The Ontology Lookup Service (OLS) from the European Bioinformatics Institute (EBI) provides access to hundreds of biomedical ontologies through a single API. The OLS MCP server brings this power directly into Claude Code.</p> <p>Note: The OLS MCP server is listed in the official MCP servers registry, a curated collection of community-maintained MCP servers for various use cases. The registry is a great place to discover other MCP servers for different workflows.</p>"},{"location":"tutorial/claude-code-mcps/#what-it-provides","title":"What It Provides","text":"<p>7 tools for ontology exploration:</p> <ol> <li>search_terms - Search across ontologies</li> <li>get_term_info - Get detailed term information</li> <li>get_ontology_info - Get ontology metadata</li> <li>search_ontologies - Find available ontologies</li> <li>get_term_children - Get child terms</li> <li>get_term_ancestors - Get parent/ancestor terms</li> <li>find_similar_terms - Find related terms using embeddings</li> </ol>"},{"location":"tutorial/claude-code-mcps/#ontologies-available","title":"Ontologies Available","text":"<p>OLS provides access to hundreds of ontologies, including:</p> <ul> <li>GO (Gene Ontology) - Biological processes, molecular functions, cellular components</li> <li>ChEBI (Chemical Entities of Biological Interest) - Small molecules and biochemistry</li> <li>HP (Human Phenotype Ontology) - Human phenotypic abnormalities</li> <li>MONDO (Monarch Disease Ontology) - Diseases and medical conditions</li> <li>UBERON - Anatomical structures across species</li> <li>CL (Cell Ontology) - Cell types</li> <li>And 200+ more ontologies</li> </ul>"},{"location":"tutorial/claude-code-mcps/#benefits","title":"Benefits","text":"<p>Compared to Tutorial 3's slash commands: - Multiple ontologies - Query GO, ChEBI, HP, MONDO from one place - No manual curl - Claude handles everything - Structured data - Automatic JSON parsing - Advanced features - Term relationships, hierarchies, similarity search - Official EBI service - Maintained by European Bioinformatics Institute</p>"},{"location":"tutorial/claude-code-mcps/#installing-the-ols-mcp-server","title":"Installing the OLS MCP Server","text":""},{"location":"tutorial/claude-code-mcps/#step-1-install-with-claude-mcp-add","title":"Step 1: Install with Claude MCP Add","text":"<p>The OLS MCP server is published on PyPI and can be installed using Claude Code's built-in MCP manager:</p> <pre><code>claude mcp add --transport stdio ols-mcp-server -- ols-mcp-server\n</code></pre> <p>What this does: - Installs the OLS MCP server using Python's uv package manager - Configures it in your <code>~/.claude/.claude.json</code> file - Sets up stdio transport (communication via stdin/stdout) - Makes 7 OLS tools available to Claude Code</p>"},{"location":"tutorial/claude-code-mcps/#step-2-restart-claude-code","title":"Step 2: Restart Claude Code","text":"<p>After installation, restart Claude Code to load the MCP:</p> <pre><code># Exit current session\nexit\n\n# Start new session\nclaude\n\n# You should see:\n\u2713 Loaded MCP: ols-mcp-server (7 tools)\n</code></pre> <p>Verification: - Look for \"ols-mcp-server\" in the startup messages - You should see \"7 tools\" listed - If you don't see this, check the troubleshooting section below</p>"},{"location":"tutorial/claude-code-mcps/#github-repository","title":"GitHub Repository","text":"<p>The OLS MCP server is open source: - Repository: https://github.com/seandavi/ols-mcp-server - Developer: Sean Davis - License: MIT</p>"},{"location":"tutorial/claude-code-mcps/#part-1-understanding-what-the-ols-mcp-can-do","title":"Part 1: Understanding What the OLS MCP Can Do","text":"<p>Before diving into specific examples, let's explore what the OLS MCP server is capable of. The best way to learn is to ask Claude!</p>"},{"location":"tutorial/claude-code-mcps/#discovering-available-capabilities","title":"Discovering Available Capabilities","text":"<p>Now that you've installed the OLS MCP, you can ask Claude Code to explain what it can do. Try these conversational queries:</p> <p>Ask Claude: <pre><code>What can I do with the OLS MCP server? What tools does it provide?\n</code></pre></p> <p>Claude will explain: - The 7 tools available (search_terms, get_term_info, etc.) - What each tool does - When you might use each one</p> <p>Ask Claude: <pre><code>What ontologies can I query using the OLS MCP?\nShow me some examples of what's available.\n</code></pre></p> <p>Claude will tell you: - Major ontologies: GO, ChEBI, HP, MONDO, UBERON, CL - What each ontology covers - How to specify which ontology to query</p> <p>Ask Claude: <pre><code>How do I search for terms across different ontologies?\nCan you show me the difference between searching GO vs ChEBI?\n</code></pre></p> <p>Claude will demonstrate: - How to search specific ontologies - The differences between biological ontologies (GO) and chemical ontologies (ChEBI) - How to interpret search results</p>"},{"location":"tutorial/claude-code-mcps/#understanding-term-information","title":"Understanding Term Information","text":"<p>Ask Claude: <pre><code>What kind of information can I get about a specific ontology term?\nWhat's the difference between searching for a term and getting detailed info about it?\n</code></pre></p> <p>Claude will explain: - <code>search_terms</code> finds terms matching your query - <code>get_term_info</code> retrieves complete details for a specific term ID - What metadata is available (definitions, synonyms, relationships, cross-references)</p>"},{"location":"tutorial/claude-code-mcps/#exploring-hierarchies","title":"Exploring Hierarchies","text":"<p>Ask Claude: <pre><code>Can I see how terms are related to each other?\nHow do I explore parent/child relationships in ontologies?\n</code></pre></p> <p>Claude will show you: - <code>get_term_children</code> for finding subtypes - <code>get_term_ancestors</code> for finding parent terms - How hierarchies help understand biological classifications</p>"},{"location":"tutorial/claude-code-mcps/#why-this-matters","title":"Why This Matters","text":"<p>By asking Claude these exploratory questions, you learn: - What's possible with the OLS MCP - How to formulate effective queries - Which tool to use for different tasks - How ontologies are structured</p> <p>This exploratory approach is exactly how you'll work with MCPs in practice - asking questions, discovering capabilities, and building up your understanding naturally.</p> <p>Now let's apply this knowledge to real examples!</p>"},{"location":"tutorial/claude-code-mcps/#part-2-exploring-gene-ontology-go","title":"Part 2: Exploring Gene Ontology (GO)","text":"<p>Let's start by exploring GO terms, which describe biological processes, molecular functions, and cellular components.</p>"},{"location":"tutorial/claude-code-mcps/#search-for-go-terms","title":"Search for GO Terms","text":"<p>Ask Claude: <pre><code>Using the OLS MCP, search for GO terms related to \"apoptosis\".\nShow me the top 5 results with their descriptions.\n</code></pre></p> <p>What happens: - Claude uses the <code>search_terms</code> tool - Specifies ontology \"go\" (Gene Ontology) - Returns structured results with term IDs and descriptions</p> <p>Example results: <pre><code>Found 369 apoptosis-related terms in GO:\n\n1. GO:0006915 - apoptotic process\n   A programmed cell death process which begins when a cell\n   receives an internal or external signal\n\n2. GO:0097194 - execution phase of apoptosis\n   The specific phase of apoptosis characterized by\n   cell morphological changes\n\n3. GO:0043065 - positive regulation of apoptotic process\n   Any process that activates or increases the frequency\n   of cell death by apoptosis\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#get-detailed-term-information","title":"Get Detailed Term Information","text":"<p>Ask Claude: <pre><code>Using OLS, get detailed information about GO:0006915.\nShow me the full definition, synonyms, and relationships.\n</code></pre></p> <p>What happens: - Uses <code>get_term_info</code> tool - Returns complete term metadata - Shows synonyms, relationships, cross-references</p> <p>Example results: <pre><code>GO:0006915 - apoptotic process\n\nDefinition:\nA programmed cell death process which begins when a cell\nreceives an internal or external signal\n\nSynonyms:\n- apoptosis\n- programmed cell death\n- cell suicide\n- type I programmed cell death\n\nRelationships:\n- is_a: GO:0012501 (programmed cell death)\n- part_of: GO:0008219 (cell death)\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#connection-to-tutorial-3","title":"Connection to Tutorial 3","text":"<p>Remember the <code>/go</code> slash command from Tutorial 3? The OLS MCP provides similar functionality but with access to all OLS ontologies through one interface instead of querying individual APIs.</p>"},{"location":"tutorial/claude-code-mcps/#part-3-exploring-chemical-entities-chebi","title":"Part 3: Exploring Chemical Entities (ChEBI)","text":"<p>ChEBI provides structured information about small molecules and their biological roles.</p>"},{"location":"tutorial/claude-code-mcps/#search-for-chemical-compounds","title":"Search for Chemical Compounds","text":"<p>Ask Claude: <pre><code>Search ChEBI via OLS for terms related to \"glucose\".\nShow molecular formulas where available.\n</code></pre></p> <p>Example results: <pre><code>Found 977 glucose-related terms in ChEBI:\n\n1. CHEBI:17234 - glucose\n   An aldohexose used as a source of energy and metabolic intermediate\n   Formula: C6H12O6\n\n2. CHEBI:4167 - D-glucose\n   The D-enantiomer of glucose\n   Formula: C6H12O6\n\n3. CHEBI:15903 - glucose-6-phosphate\n   A compound central to carbohydrate metabolism\n   Formula: C6H13O9P\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#compare-to-tutorial-3","title":"Compare to Tutorial 3","text":"<p>In Tutorial 3, you created a <code>/chebi</code> slash command that queried a single ChEBI ID. The OLS MCP allows you to: - Search across all ChEBI terms - Access related ontology information - Explore term hierarchies - Find similar compounds</p>"},{"location":"tutorial/claude-code-mcps/#part-4-exploring-human-phenotype-ontology-hp","title":"Part 4: Exploring Human Phenotype Ontology (HP)","text":"<p>HP describes phenotypic abnormalities in human disease.</p>"},{"location":"tutorial/claude-code-mcps/#search-for-phenotypes","title":"Search for Phenotypes","text":"<p>Ask Claude: <pre><code>Search the Human Phenotype Ontology via OLS for \"diabetes\".\nShow the definitions.\n</code></pre></p> <p>Example results: <pre><code>Found 35 diabetes-related terms in HP:\n\n1. HP:0000819 - Diabetes mellitus\n   A group of abnormalities characterized by hyperglycemia\n   and glucose intolerance\n\n2. HP:0005978 - Type 2 diabetes mellitus\n   A multifactorial disorder of glucose homeostasis caused\n   by a lack of sensitivity to insulin\n\n3. HP:0100560 - Transient neonatal diabetes mellitus\n   A form of diabetes presenting in the neonatal period\n   that is transient\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#get-term-hierarchy","title":"Get Term Hierarchy","text":"<p>Ask Claude: <pre><code>Using OLS, show me the child terms (subtypes) of HP:0000819\n(Diabetes mellitus).\n</code></pre></p> <p>What happens: - Uses <code>get_term_children</code> tool - Shows the hierarchical structure of disease terms - Helps understand disease classification</p>"},{"location":"tutorial/claude-code-mcps/#part-5-cross-ontology-workflows","title":"Part 5: Cross-Ontology Workflows","text":"<p>One of MCP's greatest strengths is combining information from multiple sources.</p>"},{"location":"tutorial/claude-code-mcps/#example-gene-to-disease-to-phenotype","title":"Example: Gene to Disease to Phenotype","text":"<p>Ask Claude: <pre><code>Let's explore the connections between biological processes and disease:\n\n1. Search GO for \"insulin secretion\"\n2. Search MONDO for \"diabetes\" diseases\n3. Search HP for diabetes phenotypes\n4. Help me understand how these connect\n</code></pre></p> <p>What happens: - Claude queries three different ontologies - Identifies connections between biological processes, diseases, and phenotypes - Provides integrated view of the biology</p> <p>This demonstrates: - Why MCPs are more powerful than individual slash commands - How integrated access accelerates research - The value of querying multiple ontologies together</p>"},{"location":"tutorial/claude-code-mcps/#real-world-workflow-comprehensive-term-analysis","title":"Real-World Workflow: Comprehensive Term Analysis","text":"<p>Let's analyze a biological process using OLS.</p>"},{"location":"tutorial/claude-code-mcps/#step-1-find-relevant-terms","title":"Step 1: Find Relevant Terms","text":"<pre><code>Search OLS for GO terms related to \"DNA repair\".\nShow me the most relevant 5 terms.\n</code></pre>"},{"location":"tutorial/claude-code-mcps/#step-2-get-detailed-information","title":"Step 2: Get Detailed Information","text":"<pre><code>For the top result, get the complete term information\nincluding relationships and cross-references.\n</code></pre>"},{"location":"tutorial/claude-code-mcps/#step-3-explore-hierarchy","title":"Step 3: Explore Hierarchy","text":"<pre><code>Show me the parent terms (ancestors) to understand where\nthis process fits in the GO hierarchy.\n</code></pre>"},{"location":"tutorial/claude-code-mcps/#step-4-find-related-chemistry","title":"Step 4: Find Related Chemistry","text":"<pre><code>Search ChEBI for compounds involved in DNA repair.\nLook for terms like \"nucleotide\" or \"base excision\".\n</code></pre>"},{"location":"tutorial/claude-code-mcps/#step-5-connect-to-disease","title":"Step 5: Connect to Disease","text":"<pre><code>Search MONDO for diseases related to DNA repair defects.\nShow how these connect to the GO terms we found.\n</code></pre> <p>Result: Comprehensive understanding of DNA repair combining: - GO: Biological processes - ChEBI: Chemical compounds involved - MONDO: Associated diseases - HP: Clinical phenotypes</p>"},{"location":"tutorial/claude-code-mcps/#why-mcps-are-powerful-for-biocuration","title":"Why MCPs Are Powerful for Biocuration","text":""},{"location":"tutorial/claude-code-mcps/#integrated-access","title":"Integrated Access","text":"<p>Instead of: <pre><code>/go apoptosis          # Query GO\n/chebi glucose         # Query ChEBI\n/mondo diabetes        # Query MONDO\n</code></pre></p> <p>With OLS MCP: <pre><code>Search OLS across GO, ChEBI, and MONDO for related terms\nabout insulin signaling and diabetes\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#persistent-connection","title":"Persistent Connection","text":"<ul> <li>MCP maintains connection to OLS</li> <li>No repeated authentication</li> <li>Faster queries</li> <li>Rate limiting handled automatically</li> </ul>"},{"location":"tutorial/claude-code-mcps/#structured-workflows","title":"Structured Workflows","text":"<p>MCPs enable multi-step research: 1. Find biological process in GO 2. Identify related compounds in ChEBI 3. Connect to diseases in MONDO 4. Explore phenotypes in HP 5. Generate comprehensive report</p>"},{"location":"tutorial/claude-code-mcps/#mcp-best-practices","title":"MCP Best Practices","text":""},{"location":"tutorial/claude-code-mcps/#when-to-use-mcps-vs-slash-commands","title":"When to Use MCPs vs Slash Commands","text":"<p>Use MCPs when: - Querying multiple ontologies - Need term relationships and hierarchies - Building complex research workflows - Want integrated cross-ontology search</p> <p>Use Slash Commands when: - Simple one-off lookup (from Tutorial 3) - Know exact term ID - Don't need relationships - Simpler is better for the task</p> <p>Real example: - Quick lookup: <code>/chebi CHEBI:15377</code> (slash command) - Research workflow: \"Search ChEBI and GO for glucose metabolism terms\" (MCP)</p>"},{"location":"tutorial/claude-code-mcps/#performance","title":"Performance","text":"<p>\u2705 DO: - Use MCPs for exploratory research - Query multiple ontologies together - Let Claude handle the integration</p> <p>\u274c DON'T: - Install unnecessary MCPs (slower startup) - Use MCP for single-term lookups when slash command is faster - Query massive result sets without limits</p>"},{"location":"tutorial/claude-code-mcps/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/claude-code-mcps/#mcp-failed-to-load","title":"\"MCP failed to load\"","text":"<p>Check: 1. Is Python 3.10+ installed? (<code>python3 --version</code>) 2. Is uv package manager available? (<code>uv --version</code>) 3. Check logs: <code>~/.claude/logs/mcp.log</code></p> <p>Common fixes: <pre><code># Reinstall OLS MCP\nclaude mcp remove ols-mcp-server\nclaude mcp add --transport stdio ols-mcp-server -- ols-mcp-server\n\n# Verify installation\nclaude mcp list\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#cant-find-termontology","title":"\"Can't find term/ontology\"","text":"<p>Try: - Use different search terms (symbol vs full name) - Check spelling and formatting - Try partial matches - Verify ontology abbreviation (GO, ChEBI, HP, not go, chebi, hp)</p> <p>Example: <pre><code># Might not work:\n\"Find apoptosis\"\n\n# Better:\n\"Search GO ontology for apoptosis\"\n\"Find apoptosis terms in Gene Ontology via OLS\"\n</code></pre></p>"},{"location":"tutorial/claude-code-mcps/#no-results-returned","title":"\"No results returned\"","text":"<p>Check: - Is ontology abbreviation correct? (GO, ChEBI, HP, MONDO) - Try broader search terms - Verify ontology is available in OLS - Check if term exists in that specific ontology</p>"},{"location":"tutorial/claude-code-mcps/#complete-tutorial-arc","title":"Complete Tutorial Arc","text":""},{"location":"tutorial/claude-code-mcps/#the-full-journey","title":"The Full Journey","text":"<pre><code>Tutorial 1: Learn Claude Code basics\n    \u2193\nTutorial 2: Explore LinkML schemas (data structure)\n    \u2193\nTutorial 3: Create API lookup slash commands (simple queries)\n    \u2193\nTutorial 4: Install OLS MCP for integrated ontology access!\n</code></pre> <p>You can now: - \u2705 Navigate and modify LinkML schemas - \u2705 Query ontologies with simple slash commands - \u2705 Use professional bioinformatics APIs via MCPs - \u2705 Perform integrated multi-ontology research - \u2705 Build comprehensive biological knowledge workflows</p>"},{"location":"tutorial/claude-code-mcps/#connection-to-previous-tutorials","title":"Connection to Previous Tutorials","text":""},{"location":"tutorial/claude-code-mcps/#tutorial-2-linkml-schemas","title":"Tutorial 2: LinkML Schemas","text":"<ul> <li>Tutorial 2: You explored LinkML models (structure of data)</li> <li>Tutorial 4: OLS MCP queries the actual ontology data those models reference</li> </ul>"},{"location":"tutorial/claude-code-mcps/#tutorial-3-slash-commands","title":"Tutorial 3: Slash Commands","text":"<ul> <li>Tutorial 3: Created <code>/chebi</code> and <code>/go</code> for individual lookups</li> <li>Tutorial 4: OLS MCP provides integrated access to all ontologies</li> </ul> <p>Evolution: - Simple lookups \u2192 Slash commands - Complex workflows \u2192 MCP servers - Both have their place in your toolkit!</p>"},{"location":"tutorial/claude-code-mcps/#key-advantages-of-mcps","title":"Key Advantages of MCPs","text":"<p>Integrated Ontology Access - Multiple ontologies (GO, ChEBI, HP, MONDO) through one interface - Cross-ontology search and comparison - Hierarchical relationships and term ancestry</p> <p>Connects Everything Together - Tutorial 2: LinkML schemas \u2192 Tutorial 4: Query data those schemas describe - Tutorial 3: Slash commands for simple APIs \u2192 Tutorial 4: MCPs for complex workflows - Combines multiple ontologies for comprehensive biological understanding</p> <p>Real-World Research Workflows - Search across ontologies simultaneously - Explore term relationships and hierarchies - Connect biological processes to diseases to phenotypes - Build comprehensive knowledge graphs</p> <p>Official EBI Service - Maintained by European Bioinformatics Institute - Access to 200+ biomedical ontologies - Production-grade reliability - Regularly updated</p>"},{"location":"tutorial/claude-code-mcps/#resources","title":"Resources","text":""},{"location":"tutorial/claude-code-mcps/#ols-mcp-server","title":"OLS MCP Server","text":"<p>GitHub Repository: - Repository: https://github.com/seandavi/ols-mcp-server - Developer: Sean Davis - Installation: <code>claude mcp add --transport stdio ols-mcp-server -- ols-mcp-server</code></p> <p>EBI Ontology Lookup Service: - Website: https://www.ebi.ac.uk/ols4/ - Documentation: https://www.ebi.ac.uk/ols4/help - Available ontologies: https://www.ebi.ac.uk/ols4/ontologies</p>"},{"location":"tutorial/claude-code-mcps/#official-mcp-resources","title":"Official MCP Resources","text":"<ul> <li>MCP Specification: https://modelcontextprotocol.io/ - Official protocol specification and documentation</li> <li>MCP Server Registry: https://github.com/modelcontextprotocol/servers - Curated collection of community-maintained MCP servers covering databases, APIs, file systems, development tools, and more. Browse here to discover MCP servers for your workflows beyond ontology lookups.</li> <li>Python SDK: https://github.com/modelcontextprotocol/python-sdk - Build your own MCP servers in Python</li> <li>TypeScript SDK: https://github.com/modelcontextprotocol/typescript-sdk - Build your own MCP servers in TypeScript</li> </ul>"},{"location":"tutorial/claude-code-mcps/#ontology-resources","title":"Ontology Resources","text":"<ul> <li>Gene Ontology: http://geneontology.org/</li> <li>ChEBI: https://www.ebi.ac.uk/chebi/</li> <li>Human Phenotype Ontology: https://hpo.jax.org/</li> <li>MONDO: https://mondo.monarchinitiative.org/</li> <li>OBO Foundry: http://obofoundry.org/</li> </ul>"},{"location":"tutorial/claude-code-mcps/#configuration","title":"Configuration","text":"<ul> <li>CLI Config: <code>~/.claude/.claude.json</code></li> <li>Desktop Config: <code>~/.config/claude/claude_desktop_config.json</code></li> <li>Logs: <code>~/.claude/logs/</code></li> </ul>"},{"location":"tutorial/claude-code-mcps/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <p>\u2705 What MCPs are and why they're powerful for bioinformatics \u2705 How to install the OLS MCP server with one command \u2705 Querying multiple ontologies (GO, ChEBI, HP, MONDO) \u2705 Building integrated cross-ontology research workflows \u2705 When to use MCPs vs slash commands</p> <p>MCPs provide integrated access to biomedical ontologies directly in your development environment. You now have:</p> <ul> <li>Access to 200+ ontologies through one interface</li> <li>Cross-ontology search and exploration capabilities</li> <li>Term relationships and hierarchies</li> <li>Integration with your existing slash commands from Tutorial 3</li> </ul> <p>This is how modern biocuration research works - combining data from multiple ontologies (GO, ChEBI, HP, MONDO) to gain comprehensive insights into genes, diseases, phenotypes, and biological processes.</p> <p>Congratulations! You've completed all four Claude Code tutorials for biocuration!</p> <p>You now have the skills to: - Navigate complex schemas and databases - Automate repetitive biocuration tasks - Query professional bioinformatics APIs - Build reproducible research workflows</p> <p>Keep exploring, building, and sharing your workflows with the OBO community!</p> <p>Note on Content Generation: This tutorial was developed with the assistance of Claude Code (Anthropic's AI-powered CLI tool) for content generation, organization, and technical accuracy verification. All examples and workflows have been tested and validated for educational purposes.</p>"},{"location":"tutorial/claude-code-slash-commands/","title":"Creating Slash Commands for API Lookups","text":"<p>Slash commands are reusable workflows stored as markdown files that Claude Code can execute instantly. This tutorial shows you how to create custom commands for common biocuration tasks like ontology term lookups and database queries.</p>"},{"location":"tutorial/claude-code-slash-commands/#prerequisites","title":"Prerequisites","text":"<ul> <li>Claude Code installed (Getting Started guide)</li> <li>Basic understanding of REST APIs</li> <li>Familiarity with command line tools (curl)</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#what-youll-learn","title":"What You'll Learn","text":"<ol> <li>What slash commands are and why they're useful</li> <li>Creating a ChEBI compound lookup command</li> <li>Building a Gene Ontology (GO) term command</li> <li>Command best practices and error handling</li> <li>Sharing commands with your team</li> </ol>"},{"location":"tutorial/claude-code-slash-commands/#what-are-slash-commands","title":"What Are Slash Commands?","text":"<p>Slash commands are saved workflows that Claude Code can execute with simple <code>/command</code> syntax.</p>"},{"location":"tutorial/claude-code-slash-commands/#without-slash-commands","title":"Without Slash Commands","text":"<pre><code>You: \"Query the ChEBI API for compound CHEBI:15377 and format the results nicely\"\nClaude: *writes curl command, executes, parses JSON, formats output*\n\n... later ...\n\nYou: \"Query the ChEBI API for compound CHEBI:16761 and format the results nicely\"\nClaude: *does everything from scratch again*\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#with-slash-commands","title":"With Slash Commands","text":"<pre><code>You: /chebi CHEBI:15377\nClaude: *instant results*\n\nYou: /chebi CHEBI:16761\nClaude: *instant results*\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#benefits","title":"Benefits","text":"<ul> <li>Faster - No re-explaining the task</li> <li>Consistent - Same format every time</li> <li>Reusable - Save and share with team</li> <li>Efficient - Instructions stored locally, not in conversation</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#command-structure","title":"Command Structure","text":"<p>Slash commands are markdown files stored in <code>.claude/commands/</code>:</p> <pre><code>---\ndescription: Brief description of what this command does\nallowed_tools:\n  - bash\n---\n\nInstructions for Claude go here.\n\nUse $ARGUMENTS for command parameters.\n</code></pre> <p>File location: - Project-specific: <code>.claude/commands/command-name.md</code> - Global: <code>~/.claude/commands/command-name.md</code></p> <p>Usage: <code>/command-name</code> (filename without <code>.md</code>)</p>"},{"location":"tutorial/claude-code-slash-commands/#part-1-chebi-compound-lookup","title":"Part 1: ChEBI Compound Lookup","text":""},{"location":"tutorial/claude-code-slash-commands/#what-is-chebi","title":"What is ChEBI?","text":"<p>ChEBI (Chemical Entities of Biological Interest) is a database of molecular entities focused on \"small\" chemical compounds.</p>"},{"location":"tutorial/claude-code-slash-commands/#step-1-discover-whats-available","title":"Step 1: Discover What's Available","text":"<p>Before creating a command, we need to understand what the ChEBI API offers. Ask Claude:</p> <pre><code>I want to create a slash command to look up ChEBI compounds.\nCan you help me explore what's available in the ChEBI API?\n\nPlease go to the ChEBI API documentation:\nhttps://www.ebi.ac.uk/chebi/backend/api/docs/\n\nTell me:\n1. What endpoints are available?\n2. What data can I get back for a compound?\n3. What would be useful to display in a slash command?\n</code></pre> <p>Claude will help you discover: - Available endpoints (compound lookup, search, structure, etc.) - What data fields are returned (name, formula, mass, synonyms, definitions, structures, etc.) - Which fields would be most useful for biocuration</p>"},{"location":"tutorial/claude-code-slash-commands/#step-2-get-the-openapi-specification-optional","title":"Step 2: Get the OpenAPI Specification (Optional)","text":"<p>For more detail, you can have Claude analyze the complete API specification:</p> <pre><code>Can you fetch the OpenAPI specification from:\nhttps://www.ebi.ac.uk/chebi/backend/api/schema/\n\nHelp me understand what I can learn about compounds from this API.\nWhat information is available for each compound?\n</code></pre> <p>This helps you: - See all the data ChEBI tracks for compounds - Understand what biological/chemical information is available - Make informed decisions about what to include in your command</p>"},{"location":"tutorial/claude-code-slash-commands/#step-3-decide-what-to-display","title":"Step 3: Decide What to Display","text":"<p>Based on what you discovered, ask Claude:</p> <pre><code>Based on the ChEBI API, I want to create a /chebi command.\nFor a biocurator, what would be the most useful information to display?\n\nI'm thinking:\n- Compound name\n- Chemical formula\n- Definition\n\nWhat else should I include?\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#step-4-create-the-command","title":"Step 4: Create the Command","text":"<p>Now that you know what's available, create the command:</p> <pre><code>Create a slash command file: .claude/commands/chebi.md\n\nThe command should:\n- Accept a ChEBI ID as $ARGUMENTS (e.g., CHEBI:15377)\n- Query the ChEBI REST API endpoint: /chebi/backend/api/public/compound/$ARGUMENTS/\n- Display the most useful fields we discussed\n- Format output in a readable table\n- Handle errors gracefully\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#example-implementation","title":"Example Implementation","text":"<p>Claude Code will create something like:</p> <pre><code>---\ndescription: Look up ChEBI chemical compound information by ID\nallowed_tools:\n  - bash\n---\n\nQuery ChEBI for compound: $ARGUMENTS\n\nSteps:\n1. Validate the ChEBI ID format (should be CHEBI:XXXXX)\n2. Use curl to fetch JSON from:\n   https://www.ebi.ac.uk/chebi/backend/api/public/compound/$ARGUMENTS/\n3. Parse the JSON response\n4. Display the key fields (name, formula, mass, definition, synonyms)\n5. Format as a readable table\n6. If ID is invalid or network fails, show helpful error message\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#test-it","title":"Test It","text":"<pre><code>/chebi CHEBI:15377\n</code></pre> <p>Expected output:</p> <pre><code>ChEBI Compound: CHEBI:15377\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nName:       Water\nFormula:    H\u2082O\nMW:         18.015 g/mol\nDefinition: An oxygen hydride consisting of an oxygen atom\n            that is covalently bonded to two hydrogen atoms.\nSynonyms:   H2O, HOH, oxidane, dihydrogen oxide\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#part-2-gene-ontology-go-term-lookup","title":"Part 2: Gene Ontology (GO) Term Lookup","text":""},{"location":"tutorial/claude-code-slash-commands/#what-is-go","title":"What is GO?","text":"<p>The Gene Ontology provides structured vocabularies for: - Biological Process (BP) - Molecular Function (MF) - Cellular Component (CC)</p> <p>Two APIs are available: - QuickGO (https://www.ebi.ac.uk/QuickGO/) - Great for searching GO terms - Gene Ontology API (https://api.geneontology.org/) - Detailed term information and relationships</p>"},{"location":"tutorial/claude-code-slash-commands/#step-1-explore-quickgo-api-no-openapi-spec","title":"Step 1: Explore QuickGO API (No OpenAPI Spec)","text":"<p>QuickGO doesn't have an OpenAPI specification, but we can still explore it! When there's no spec, just copy-paste the documentation.</p> <p>Go to: https://www.ebi.ac.uk/QuickGO/api/index.html</p> <p>Copy the \"Gene Ontology\" section (the list of endpoints) and ask Claude:</p> <pre><code>I want to create a /go command for searching and looking up GO terms.\nHere's the QuickGO API documentation I copied:\n\n[Paste the endpoint list here]\n\nHelp me understand:\n1. Which endpoint would be best for searching GO terms by name?\n2. Which endpoint would give me detailed information about a specific GO ID?\n3. What information can I get back about each term?\n</code></pre> <p>Key insight: When there's no OpenAPI spec, copy-paste documentation and Claude will parse it for you!</p>"},{"location":"tutorial/claude-code-slash-commands/#step-2-explore-gene-ontology-api","title":"Step 2: Explore Gene Ontology API","text":"<p>Now check what the Gene Ontology API offers:</p> <pre><code>Can you also explore the Gene Ontology API at:\nhttps://api.geneontology.org/\n\nTell me:\n1. What endpoints are available?\n2. How is this different from QuickGO?\n3. Which API should I use for what purpose?\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#step-3-decide-your-approach","title":"Step 3: Decide Your Approach","text":"<p>Based on what you learned, ask Claude to help you decide:</p> <pre><code>Based on both APIs, I want to create a /go command that:\n- Can search for GO terms by name (e.g., \"apoptosis\")\n- Can look up a specific GO ID (e.g., GO:0008150)\n- Shows useful information for biocuration\n\nWhich API should I use for search vs lookup?\nWhat information should I display?\n</code></pre> <p>Typical decision: - Use QuickGO <code>/ontology/go/search</code> for finding terms - Use Gene Ontology API or QuickGO <code>/ontology/go/terms/{ids}</code> for detailed lookups</p>"},{"location":"tutorial/claude-code-slash-commands/#step-4-create-the-command_1","title":"Step 4: Create the Command","text":"<p>Now create the command with Claude's help:</p> <pre><code>Create a slash command: .claude/commands/go.md\n\nBased on what we discussed:\n- If $ARGUMENTS looks like a GO ID (GO:XXXXXXX), do a lookup\n- If $ARGUMENTS is text, do a search\n- Display the most useful information for biocurators\n- Handle errors for invalid inputs\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#example-implementation_1","title":"Example Implementation","text":"<p>Claude might create a smart command that handles both search and lookup:</p> <pre><code>---\ndescription: Search or look up Gene Ontology terms\nallowed_tools:\n  - bash\n---\n\nProcess GO query: $ARGUMENTS\n\nSteps:\n1. Check if $ARGUMENTS is a GO ID (GO:XXXXXXX) or search term\n2. For GO ID:\n   - Use QuickGO or GO API to get term details\n   - Display: name, namespace, definition, relationships\n3. For search term:\n   - Use QuickGO search endpoint\n   - Display: top 5 matching terms with IDs\n4. Format output clearly\n5. Handle invalid inputs or API errors\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#test-it_1","title":"Test It","text":"<p>Search by name: <pre><code>/go apoptosis\n</code></pre></p> <p>Expected output: <pre><code>GO Term Search: \"apoptosis\"\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n1. GO:0006915 - apoptotic process\n   Definition: A programmed cell death process...\n\n2. GO:0097190 - apoptotic signaling pathway\n   Definition: A series of molecular signals...\n\n3. GO:0043065 - positive regulation of apoptosis\n   Definition: Any process that activates...\n</code></pre></p> <p>Lookup by ID: <pre><code>/go GO:0008150\n</code></pre></p> <p>Expected output: <pre><code>Gene Ontology Term: GO:0008150\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nName:       biological_process\nNamespace:  biological_process\nDefinition: A biological process represents a specific\n            objective that the organism is genetically\n            programmed to achieve.\n\nParent Terms:\n  (This is a root term)\n</code></pre></p>"},{"location":"tutorial/claude-code-slash-commands/#best-practices","title":"Best Practices","text":""},{"location":"tutorial/claude-code-slash-commands/#do-make-commands-self-documenting","title":"\u2705 DO: Make Commands Self-Documenting","text":"<p>Include clear descriptions and examples:</p> <pre><code>---\ndescription: Look up ChEBI compound by ID (e.g., /chebi CHEBI:15377)\n---\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#do-handle-errors","title":"\u2705 DO: Handle Errors","text":"<p>Always validate input and handle failures:</p> <pre><code>1. Check if $ARGUMENTS is provided\n2. Validate ID format (CHEBI:XXXXX or GO:XXXXXXX)\n3. Handle network failures (curl timeout)\n4. Handle invalid IDs (404 responses)\n5. Show helpful error messages\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#do-format-output-consistently","title":"\u2705 DO: Format Output Consistently","text":"<p>Use tables or structured formats for readability:</p> <pre><code>Term Name:    value\nDefinition:   value\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#dont-hardcode-values","title":"\u274c DON'T: Hardcode Values","text":"<p>Bad: <pre><code>Query https://api.example.com/term/GO:0008150\n</code></pre></p> <p>Good: <pre><code>Query https://api.example.com/term/$ARGUMENTS\n</code></pre></p>"},{"location":"tutorial/claude-code-slash-commands/#advanced-command-features","title":"Advanced Command Features","text":""},{"location":"tutorial/claude-code-slash-commands/#multiple-arguments","title":"Multiple Arguments","text":"<p>Commands can accept multiple parameters:</p> <pre><code>---\ndescription: Compare two GO terms\n---\n\nCompare GO terms: $ARGUMENTS\n\nParse $ARGUMENTS as two GO IDs:\n1. First GO ID (e.g., GO:0008150)\n2. Second GO ID (e.g., GO:0009987)\n\nShow hierarchical relationship and shared annotations.\n</code></pre> <p>Usage: <code>/compare-go GO:0008150 GO:0009987</code></p>"},{"location":"tutorial/claude-code-slash-commands/#command-options","title":"Command Options","text":"<p>Support flags for different output formats:</p> <pre><code>/search-chebi \"glucose\" --limit 5 --format table\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#chaining-commands","title":"Chaining Commands","text":"<p>Use command outputs in conversation:</p> <pre><code>/go GO:0008150\n\nBased on that GO term, what ChEBI compounds are related?\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#sharing-commands-with-your-team","title":"Sharing Commands with Your Team","text":""},{"location":"tutorial/claude-code-slash-commands/#version-control","title":"Version Control","text":"<p>Commands are just markdown files - commit them to git:</p> <pre><code>git add .claude/commands/chebi.md\ngit add .claude/commands/go.md\ngit commit -m \"Add ontology lookup commands\"\ngit push\n</code></pre> <p>Team members who clone the repo will have the commands automatically!</p>"},{"location":"tutorial/claude-code-slash-commands/#global-vs-project-commands","title":"Global vs Project Commands","text":"<p>Project-specific (<code>.claude/commands/</code>): - Specific to this repository - Shared via git - Available when working in this project</p> <p>Global (<code>~/.claude/commands/</code>): - Available in all projects - Personal commands - Not shared via git</p>"},{"location":"tutorial/claude-code-slash-commands/#common-use-cases-for-biocuration","title":"Common Use Cases for Biocuration","text":""},{"location":"tutorial/claude-code-slash-commands/#ontology-term-lookups","title":"Ontology Term Lookups","text":"<ul> <li><code>/chebi</code> - Chemical compounds</li> <li><code>/go</code> - Gene Ontology terms</li> <li><code>/mondo</code> - Disease Ontology (MONDO)</li> <li><code>/hp</code> - Human Phenotype Ontology</li> <li><code>/uberon</code> - Anatomical structures</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#database-queries","title":"Database Queries","text":"<ul> <li><code>/uniprot</code> - Protein information</li> <li><code>/ncbi-gene</code> - Gene records</li> <li><code>/pubmed</code> - Literature search</li> <li><code>/omim</code> - Genetic disorders</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#data-validation","title":"Data Validation","text":"<ul> <li><code>/validate-gene-symbols</code> - Check gene symbol formats</li> <li><code>/check-taxon</code> - Validate NCBI taxon IDs</li> <li><code>/verify-refs</code> - Check publication references</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#format-conversion","title":"Format Conversion","text":"<ul> <li><code>/gff-to-json</code> - Convert GFF3 to JSON</li> <li><code>/fasta-stats</code> - Analyze FASTA files</li> <li><code>/tsv-to-linkml</code> - Generate LinkML from TSV</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/claude-code-slash-commands/#command-not-found","title":"\"Command not found\"","text":"<p>Check the filename matches: - <code>.claude/commands/chebi.md</code> \u2192 <code>/chebi</code> - <code>.claude/commands/go.md</code> \u2192 <code>/go</code></p>"},{"location":"tutorial/claude-code-slash-commands/#api-returns-html-instead-of-json","title":"\"API returns HTML instead of JSON\"","text":"<p>Add proper headers to curl:</p> <pre><code>curl -H \"Accept: application/json\" https://api.example.com/...\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#command-is-slow","title":"\"Command is slow\"","text":"<ul> <li>Add result limits: <code>--limit 10</code></li> <li>Cache frequent queries</li> <li>Consider using an MCP server for complex integrations</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#hands-on-exercise","title":"Hands-On Exercise","text":""},{"location":"tutorial/claude-code-slash-commands/#challenge-create-a-uniprot-lookup-command","title":"Challenge: Create a UniProt Lookup Command","text":"<p>Task: Create a <code>/uniprot</code> command that looks up protein information.</p> <p>Requirements: - Accept a UniProt ID (e.g., P04637) - Query UniProt API: <code>https://www.uniprot.org/uniprot/$ID.xml</code> - Display: protein name, organism, function, sequence length - Show associated GO terms</p> <p>API documentation: https://www.uniprot.org/help/api</p> <p>Hints: 1. Start with just the protein name 2. Add more fields incrementally 3. Test with real IDs:    - P04637 = Human p53    - P12345 = Mouse p53</p> <p>Solution approach:</p> <p>Ask Claude: <pre><code>Create a /uniprot command that:\n- Accepts a UniProt ID\n- Fetches XML from https://www.uniprot.org/uniprot/$ID.xml\n- Parses and displays protein name, organism, function, length\n- Shows associated GO terms\n- Handles invalid IDs gracefully\n</code></pre></p>"},{"location":"tutorial/claude-code-slash-commands/#common-api-endpoints-for-biocuration","title":"Common API Endpoints for Biocuration","text":"<pre><code>ChEBI:     https://www.ebi.ac.uk/chebi/\nGO:        http://api.geneontology.org/api/\nUniProt:   https://www.uniprot.org/uniprot/\nNCBI Gene: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\nPubMed:    https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\nPubChem:   https://pubchem.ncbi.nlm.nih.gov/rest/pug/\nMONDO:     http://purl.obolibrary.org/obo/mondo.owl\nHPO:       https://hpo.jax.org/api/\n</code></pre>"},{"location":"tutorial/claude-code-slash-commands/#resources","title":"Resources","text":""},{"location":"tutorial/claude-code-slash-commands/#official-documentation","title":"Official Documentation","text":"<ul> <li>Claude Code Commands: https://docs.claude.com/en/docs/claude-code/slash-commands</li> <li>ChEBI Web Services: https://www.ebi.ac.uk/chebi/webServices.do</li> <li>GO API: http://api.geneontology.org/</li> <li>UniProt API: https://www.uniprot.org/help/api</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#example-command-libraries","title":"Example Command Libraries","text":"<ul> <li>OBO Academy Commands: (Coming soon - contribute your commands!)</li> <li>Alliance Genome Commands: Repository-specific ontology lookups</li> <li>FlyBase Curator Commands: Drosophila-specific validations</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#next-steps","title":"Next Steps","text":"<p>You've learned how to create reusable slash commands for common biocuration tasks. For even more powerful integrations, learn about:</p> <ul> <li>Using MCPs for Ontology Lookups - Connect to EBI's Ontology Lookup Service for integrated queries across GO, ChEBI, HP, MONDO, and 200+ other ontologies through a single interface</li> </ul>"},{"location":"tutorial/claude-code-slash-commands/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <p>\u2705 What slash commands are and why they're useful \u2705 How to create ChEBI and GO lookup commands \u2705 Best practices for error handling and formatting \u2705 How to share commands with your team via git \u2705 Common biocuration use cases for commands</p> <p>Slash commands are perfect for: - Repetitive API lookups - Standardized formatting - Team workflow standardization - Quick ontology term reference</p> <p>For more complex integrations (persistent connections, multiple related tools, authentication), consider using MCP servers covered in the next tutorial.</p> <p>Pro tip: Start building a library of commands for your most common tasks. After a few weeks, you'll have a powerful toolkit that makes biocuration significantly faster!</p> <p>Note on Content Generation: This tutorial was developed with the assistance of Claude Code (Anthropic's AI-powered CLI tool) for content generation, organization, and technical accuracy verification. All examples and workflows have been tested and validated for educational purposes.</p>"},{"location":"tutorial/curies/","title":"Standardizing Prefixes, CURIEs, and URIs with the <code>curies</code> package","text":"<p>Uniform resource identifiers (URIs) and compact URIs (CURIEs) have become the predominant syntaxes for identifying concepts in linked data applications. Therefore, efficient, faultless, and idiomatic conversion between them is a crucial low-level utility whose need is ubiquitous across many codebases.</p> <p>The quick version is 1) instantiate a \"converter\" object and 2) use it to expand and compress URIs like in</p> <pre><code>import curies\n\n# Get a converter\nconverter = curies.get_obofoundry_converter()\n\n&gt;&gt; &gt; converter.compress(\"http://purl.obolibrary.org/obo/CHEBI_1\")\n'CHEBI:1'\n\n&gt;&gt; &gt; converter.expand(\"CHEBI:1\")\n'http://purl.obolibrary.org/obo/CHEBI_1'\n</code></pre>"},{"location":"tutorial/curies/#data-structures","title":"Data Structures","text":""},{"location":"tutorial/curies/#prefix-maps","title":"Prefix Maps","text":"<p>A prefix map is a bijective mapping (i.e., no duplicate keys, no duplicate values) between CURIE prefixes and URI prefixes Found in semantic web applications like XML, RDF, SPARQL, and OWL</p> <p>Example Prefix Map:</p> <pre><code>{\n  \"CHEBI\": \"http://purl.obolibrary.org/obo/CHEBI_\",\n  \"MONDO\": \"http://purl.obolibrary.org/obo/MONDO_\",\n  \"GO\": \"http://purl.obolibrary.org/obo/GO_\"\n}\n</code></pre>"},{"location":"tutorial/curies/#extended-prefix-maps","title":"Extended Prefix Maps","text":"<p>Standard prefix maps and JSON-LD context have several shortcomings because they don't handle synonyms for CURIE prefixes nor URI prefixes. Therefore, we need a new format that can:</p> <ul> <li>include an arbitrary set of prefixes for CURIE prefixes and URI prefixes</li> <li>denote what's the preferred CURIE prefix and URI prefix</li> <li>use a simple format that can be encoded in JSON, YAML, etc.</li> </ul> <p>The extended prefix map (EPM) is a new format to support this. Here's a short example that only has one record:</p> <pre><code>[\n  {\n    \"prefix\": \"CHEBI\",\n    \"uri_prefix\": \"http://purl.obolibrary.org/obo/CHEBI_\",\n    \"prefix_synonyms\": [\n      \"chebi\",\n      ...\n    ],\n    \"uri_prefix_synonyms\": [\n      \"https://identifiers.org/chebi:\"\n    ]\n  }\n]\n</code></pre>"},{"location":"tutorial/curies/#loading-a-context","title":"Loading a Context","text":"<p>There are several ways to load a converter:</p> <ol> <li>From a pre-defined context, like the OBO Foundry, Bioregistry, Monarch, or others</li> <li> <p>From a prefix map</p> <pre><code>import curies\n\nprefix_map = {\n    \"CHEBI\": \"http://purl.obolibrary.org/obo/CHEBI_\",\n}\nconverter = curies.load_prefix_map(prefix_map)\n</code></pre> </li> <li> <p>From an extended prefix map. This is the preferred format since it supports synonyms for CURIE prefixes and URI    prefixes. See here.</p> </li> <li>From a JSON-LD context</li> </ol> <p>A more details on each of these can be found here.</p>"},{"location":"tutorial/curies/#chaining-and-merging","title":"Chaining and Merging","text":"<p><code>curies</code> implements a faultless chain operation curies.chain() that is configurable for case sensitivity and fully considers all synonyms.</p> <p><code>curies.chain()</code> prioritizes based on the order given. Therefore, if two prefix maps having the same prefix but different URI prefixes are given, the first is retained. The second is retained as a synonym</p> <pre><code>import curies\n\nc1 = curies.load_prefix_map({\"GO\": \"http://purl.obolibrary.org/obo/GO_\"})\nc2 = curies.load_prefix_map({\"GO\": \"https://identifiers.org/go:\"})\nconverter = curies.chain([c1, c2])\n\n&gt;&gt; &gt; converter.expand(\"GO:1234567\")\n'http://purl.obolibrary.org/obo/GO_1234567'\n&gt;&gt; &gt; converter.compress(\"http://purl.obolibrary.org/obo/GO_1234567\")\n'GO:1234567'\n&gt;&gt; &gt; converter.compress(\"https://identifiers.org/go:1234567\")\n'GO:1234567'\n</code></pre> <p>Chain is the perfect tool if you want to override parts of an existing extended prefix map. For example, if you want to use most of the Bioregistry, but you would like to specify a custom URI prefix (e.g., using Identifiers.org), you can do the following</p> <pre><code>import curies\n\noverrides = curies.load_prefix_map({\"pubmed\": \"https://identifiers.org/pubmed:\"})\nbioregistry_converter = curies.get_bioregistry_converter()\nconverter = curies.chain([overrides, bioregistry_converter])\n\n&gt;&gt; &gt; converter.expand(\"pubmed:1234\")\n'https://identifiers.org/pubmed:1234'\n</code></pre> <p>More information can be found here.</p>"},{"location":"tutorial/curies/#reconciliation","title":"Reconciliation","text":"<p>Reconciliation is the high-level process of modifying an (extended) prefix map with domain-specific rules. This is important as it allows for building on existing (extended) prefix maps without having to start from scratch. Further, storing the rules to transform an existing prefix map allows for high-level discussion about the differences and their reasons.</p> <p>As a specific example, the Bioregistry uses <code>snomedct</code> as a preferred prefix for the Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT). The OBO Foundry community prefers to use <code>SCTID</code> as the preferred prefix for this resource. Rather than maintaining a different extended prefix map than the Bioregistry, the OBO Foundry community could enumerate its preferred modifications to the base (extended) prefix map, then create its prefix map by transforming the Bioregistry\u2019s.</p> <p>Similarly, a consumer of the OBO Foundry prefix map who\u2019s implementing a resolver might want to override the URI prefix associated with the Ontology of Vaccine Adverse Events (OVAE) to point towards the Ontology Lookup Service instead of the default OntoBee.</p> <p>There are two operations that are useful for transforming an existing (extended) prefix map:</p> <ol> <li>Remapping is when a given CURIE prefix or URI prefix is replaced with another. See <code>curies.remap_curie_prefixes()</code>    and <code>curies.remap_uri_prefixes()</code>.</li> <li>Rewiring is when the correspondence between a CURIE prefix and URI prefix is updated. See <code>curies.rewire()</code>.</li> </ol>"},{"location":"tutorial/curies/#simple-curie-remapping","title":"Simple CURIE Remapping","text":"<p>If we start with the following extended prefix map,</p> <pre><code>[\n  {\n    \"prefix\": \"a\",\n    \"uri_prefix\": \"https://example.org/a/\",\n    \"prefix_synonyms\": [\n      \"a1\"\n    ]\n  },\n  {\n    \"prefix\": \"b\",\n    \"uri_prefix\": \"https://example.org/b/\"\n  }\n]\n</code></pre> <p>we can apply the remapping <code>{\"a\": \"a1\"}</code> to get:</p> <pre><code>[\n  {\n    \"prefix\": \"a1\",\n    \"uri_prefix\": \"https://example.org/a/\",\n    \"prefix_synonyms\": [\n      \"a\"\n    ]\n  },\n  {\n    \"prefix\": \"b\",\n    \"uri_prefix\": \"https://example.org/b/\"\n  }\n]\n</code></pre> <p>Notice that the old prefix is retained as a synonym.</p>"},{"location":"tutorial/curies/#transitive-curie-remapping","title":"Transitive CURIE REmapping","text":"<p>There\u2019s a special case of CURIE prefix remapping where one prefix is supposed to overwrite another. For example, in the Bioregistry, the Gene Expression Omnibus is given the prefix geo and the Geographical Entity Ontology is given the prefix <code>geogeo</code>. OBO Foundry users will want to rename the Gene Expression Omnibus record to something else like <code>ncbi.geo</code>and rename <code>geogeo</code> to <code>geo</code>. Taken by themselves, these two operations would not accomplish the desired results:</p> <ol> <li>Remapping with <code>{\"geo\": \"ncbi.geo\"}</code> would retain geo as a CURIE prefix synonym</li> <li>Remapping with <code>{\"geogeo\": \"geo\"}</code> would not change the mapping as geo is already part of a different record.</li> </ol> <p>The curies.remap_curie_prefixes() implements special logic to identify scenarios where two (or more) remappings are dependent (we\u2019re calling these transitive remappings) and apply them in the expected way. Therefore, we see the following:</p> <pre><code>from curies import Converter, Record, remap_curie_prefixes\n\nconverter = Converter([\n    Record(prefix=\"geo\", uri_prefix=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=\"),\n    Record(prefix=\"geogeo\", uri_prefix=\"http://purl.obolibrary.org/obo/GEO_\"),\n])\nremapping = {\"geo\": \"ncbi.geo\", \"geogeo\": \"geo\"}\nconverter = remap_curie_prefixes(converter, curie_remapping)\n\n&gt;&gt; &gt; converter.records\n[\n    Record(\n        prefix=\"geo\",\n        prefix_synonyms=[\"geogeo\"],\n        uri_prefix=\"http://purl.obolibrary.org/obo/GEO_\",\n    ),\n    Record(\n        prefix=\"ncbi.geo\",\n        uri_prefix=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=\",\n    ),\n]\n</code></pre> <p><code>geogeo</code> is maintained as a CURIE prefix synonym for the Geographical Entity Ontology\u2019s record. Synonyms of Gene Expression Omnibus would also be retained.</p> <p>More information about reconciliation can be found here.</p>"},{"location":"tutorial/curies/#further-reading","title":"Further Reading","text":"<p>A full tutorial for the <code>curies</code> package lives here.</p>"},{"location":"tutorial/custom-qc/","title":"Tutorial: How to add custom quality checks with ODK","text":"<p>This tutorial explains adding quality checks not included in the ROBOT Report.</p>"},{"location":"tutorial/custom-qc/#prerequisites","title":"Prerequisites","text":"<p>You have completed the tutorials:</p> <ol> <li>Getting started with your repo</li> <li>20 minute complete ODK walk-through</li> </ol>"},{"location":"tutorial/custom-qc/#custom-quality-checks","title":"Custom Quality Checks","text":"<ol> <li>Identify a quality issue in your ontology. For the sake of this tutorial, we've added the annotation <code>dcterms:date</code> to the <code>root_node</code> in the CAT Ontology.</li> </ol> <ol> <li>Write the SPARQL query to detect the error you want to check. For example, check the value type for the annotation <code>dcterms:date</code>. It will return the class with the annotation if it's not of type <code>xsd:dateTime</code>.</li> </ol> <pre><code>PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\nPREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\n\nSELECT ?cls WHERE\n{\n    ?cls dcterms:date ?date .\n  FILTER(DATATYPE(?date) != xsd:dateTime)\n}\n</code></pre> <ol> <li> <p>Save the SPARQL query in the <code>src/sparql</code> folder and name it <code>[violation name]-violation.sparql</code>. In the case of the tutorial, <code>date-as-string-violation.sparql</code></p> </li> <li> <p>Add the check to the ODK config file. In the previous tutorial, this is located at <code>~/cato/src/ontology/cato-odk.yaml</code>. Inside <code>robot_report</code>, add <code>custom_sparql_checks</code></p> </li> </ol> <pre><code>robot_report:\n  use_labels: TRUE\n  fail_on: ERROR\n  report_on:\n    - edit\n  custom_sparql_checks:\n    - date-as-string\n</code></pre> <ol> <li>Update the repository. After adding the custom SPARQL check, you need to update your pipeline to take this check when testing the ontology.</li> </ol> <pre><code>sh run.sh make update_repo\n</code></pre> <ol> <li>Test the check. You can run the checks and verify the expected result.</li> </ol> <p><pre><code>sh run.sh make sparql_test\nFAIL Rule ../sparql/date-as-string-violation.sparql: 1 violation(s)\ncls\nhttp://purl.obolibrary.org/obo/CATO_0000000\n</code></pre> To fix this issue, we need to change the annotation value to <code>xsd:dateTime</code>, and run the test again to certify everything is good this time.</p> <p></p> <pre><code>sh run.sh make sparql_test\nPASS Rule ../sparql/date-as-string-violation.sparql: 0 violation(s)\n</code></pre> <p>Push the changes to your repository, and the custom checks will run whenever creating a new Pull Request, as detailed here.</p>"},{"location":"tutorial/custom-qc/#custom-checks-available-in-odk","title":"Custom checks available in ODK","text":"<p>There are several checks already available in the ODK. If you'd like to add them, add the validation name in your ODK config file.</p> <ol> <li><code>owldef-self-reference</code>: verify if the term uses its term as equivalent</li> <li><code>redundant-subClassOf</code>: verify if there are redundant subclasses between three classes</li> <li><code>taxon-range</code>: verify if the annotations <code>present_in_taxon</code> or <code>never_in_taxon</code> always use classes from NCBITaxon</li> <li><code>iri-range</code>: verify if the value for the annotations <code>never_in_taxon</code>, <code>present_in_taxon</code>, <code>foaf:depicted_by</code>, <code>oboInOwl:inSubset</code> and <code>dcterms:contributor</code> are not an IRI</li> <li><code>iri-range-advanced</code>: same as <code>iri-range</code> plus check for <code>rdfs:seeAlso</code> annotation</li> <li><code>label-with-iri</code>: verify if there is IRI in the label</li> <li><code>multiple-replaced_by</code>: verify if an obsolete term has multiple <code>replaced_by</code> terms</li> <li><code>term-tracker-uri</code>: verify if the value for the annotation term_tracker_item is not URI</li> <li><code>illegal-date</code>: verify if the value for the annotations <code>dcterms:date</code>, <code>dcterms:issued</code> and <code>dcterms:created</code> are of type <code>xds:date</code> and use the pattern <code>YYYY-MM-DD</code></li> </ol>"},{"location":"tutorial/custom-qc/#custom-robot-report-in-odk","title":"Custom ROBOT Report in ODK","text":"<p>ROBOT report can also have custom quality checks. </p> <ol> <li>First, you need to add <code>custom_profile: TRUE</code>, in the ODK config file. </li> </ol> <p><pre><code>robot_report:\n  use_labels: TRUE\n  fail_on: ERROR\n  custom_profile: TRUE\n  report_on:\n    - edit\n  custom_sparql_checks:\n    - date-as-string\n</code></pre> 2. Create a SPARQL query with your quality check and save it at <code>src/sparql</code>. There isn't a restriction on the file name. However, it should return the variables <code>?entity ?property ?value</code>.</p> <pre><code>SELECT DISTINCT ?entity ?property ?value \nWHERE {\n  ...\n}\n</code></pre> <ol> <li>Add the path to the SPARQL query in the <code>src/ontology/profile.txt</code> file.</li> </ol> <p><pre><code>ERROR   file:../sparql/&lt;file name&gt;.sparql\n</code></pre> For more detail on the profile file, see here.</p> <ol> <li>Test your check. You'll find the failed cases on the same report for the ROBOT report at <code>src/ontology/reports/cato-edit.owl-obo-report.tsv</code>. The Rule Name will be the SPARQL file name.</li> </ol> <pre><code>sh run.sh make test\n</code></pre>"},{"location":"tutorial/custom-qc/#how-to-choose-between-custom-sparql-or-custom-robot-report","title":"How to choose between Custom SPARQL or Custom ROBOT report","text":"<ul> <li>If your test can return the exact three variables <code>entity</code>, <code>property</code> and <code>value</code> -&gt; ROBOT report</li> <li>If you need to return more detailed information -&gt; Custom SPARQL</li> <li>If you want the results of your custom tests in the ROBOT report file -&gt; ROBOT report</li> </ul> <p>Keep in mind that after changing the <code>profile.txt</code>, you won't get any upcoming updates, and you need to update manually.</p>"},{"location":"tutorial/disjointness/","title":"Disjointness","text":"<p>This tutorial is based off https://ontology101tutorial.readthedocs.io/en/latest/Disjointness.html Created by: Melissa Haendel, Chris Mungall, David Osumi-Sutherland, Matt Yoder, Carlo Torniai, and Simon Jupp</p> <p>For this excercise, we will be using chromosome-parts-interim.owl file that can be found here</p>"},{"location":"tutorial/disjointness/#disjointness_1","title":"Disjointness","text":"<p>In the chromosome-parts-interim.owl file, at the top of our class hierarchy we have cell, cell part, chromosomal part, intracellular part, organelle and organelle part. By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom.</p> <p>If you do not already have it open, load your previous ontology that was derived from the 'interim file'. Note: you can open a recent file by going to File-&gt; Open Recent</p> <p>We want to assert that <code>organelle</code> and <code>organelle part</code> are disjoint. To do this first select the <code>organelle</code> class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the hierarchy panel, you can use CTRL to select multiple classes. Select 'organelle part' as disjoint with organelle.</p> <p></p> <p>Note that the directionality is irrelevant. Prove this to yourself by deleting the disjoint axiom, and adding it back from <code>organelle part</code>.</p>"},{"location":"tutorial/disjointness/#reasoning-and-inconsistency-checking","title":"Reasoning and inconsistency checking","text":"<p>We have introduced a deliberate mistake into the ontology. We previously asserted that <code>intracellular organelle part</code> is a subclass of both <code>organelle part</code> and <code>organelle</code>. We have now added an axiom stating that <code>organelle</code> and <code>organelle part</code> are disjoint. We can use the reasoner to check the consistency of our ontology. The reasoner should detect our contradiction.</p> <p>Prot\u00e9g\u00e9 comes with several reasoners, and more can be installed via the plugins mechanism (see plugins chapter). Select a reasoner from the Reasoner menu (Elk, HermiT, Pellet, or Fact++ will work - we mostly use ELK). Once a reasoner is highlighted, select 'Start reasoner' from the menu. Note: you may get several pop-boxes/warnings, ignore those.</p> <p>The <code>intracellular organelle part</code> class will have changed to red indicating that the class is now unsatisfiable.</p> <p></p> <p>You can also see unsatisfiable classes by switching to the inferred view.</p> <p></p> <p>Here you will a special class called <code>Nothing</code>. When we previously said that all OWL classes are subclasses of OWL Thing. OWL <code>Nothing</code> is a leaf class or bottom class of your ontology. Any classes that are deemed unsatisfiable by the reasoner are shown as subclasses or equivalent to OWL Nothing. The inferred view will show you all subclasses of Nothing.</p> <p></p> <p>Once the ontology is classified, inferred statements or axioms are shown in the various panels with a light-yellow shading. The class description for <code>intracellular organelle part</code> should look something like the screen shot below. You will see that the class has been asserted equivalent to the <code>Nothing</code> class. Inside this statement, a small question mark icon appears, clicking this will get an explanation from the reasoner for this inconsistency.</p> <p></p> <p>Select the (?) icon to get an explanation for this inconsistency. The explanation shows the axioms involved. We see the disjoint class axiom alongside the two subclass axioms are causing the inconsistency. We can simply repair this ontology by removing the <code>intracellular organelle part</code> subClassOf <code>organelle</code> axiom.</p> <p></p> <p>Remove the Disjoint with axiom (click the (x) beside <code>organelle</code> in the Description pane for <code>intracellular organelle part</code>), and resynchronise the reasoner from the reasoner menu.</p>"},{"location":"tutorial/dosdp-odk/","title":"Using DOSDP templates in ODK Workflows","text":""},{"location":"tutorial/dosdp-odk/#preparation","title":"Preparation","text":"<ul> <li>You are set up for executing ODK workflows</li> <li>We assume you have a modern ODK-based repository (ODK version &gt;= 1.2.32) set up. For a tutorial on creating a new ontology repo from scratch see here.</li> <li>We assume you have completed at least one of the general DOSDP tutorials.</li> </ul>"},{"location":"tutorial/dosdp-odk/#tutorial","title":"Tutorial","text":"<ul> <li>Activate DOSDP in ODK</li> <li>Adding a first DOSDP template</li> </ul>"},{"location":"tutorial/dosdp-odk/#video","title":"Video","text":"<p>This is a very unprofessional video below recorded as part of one of our trainings. It walks you through this tutorial here, with some additional examples being given and a bit of Q&amp;A.</p>"},{"location":"tutorial/dosdp-odk/#glossary","title":"Glossary","text":"<ul> <li>Template: A document with template strings that include variables which can be instantiated my a tool. For example, a ROBOT template may contain the template string <code>SC 'part of' some %</code> which can be instantiated by <code>ROBOT</code> to be transformed into an OWL axiom: <code>SubClassOf(CATO:001 ObjectSomeValuesFrom(BFO:0000051 UBERON:123))</code>. Similarly, DOSDP YAML files are often referred to as \"templates\" (which is appropriate). Unfortunately, we often refer to them as \"patterns\" which is not strictly the right way to name them: they are templates that encode patterns (and that only to a limited extend). We recommend to refer to the DOSDP YAML files as \"templates\".</li> <li>Template string: See above: a single string with one or more slots for variables that can be instantiated and transformed into something else. The most important template string in DOSDP is the <code>equivalentTo</code> or <code>subClassOf</code> field: It tells DOSDP tools how to generate an OWL axiom, with which variable slots (<code>vars</code>).</li> <li>Pattern: See above, often misused to mean Template. In fact, a pattern provides a general modelling solution to a problem. Patterns can often be encoded in templates, but this is often incomplete. For example, a template typically does not care about the semantics of the properties it refers to, while this is critical for a pattern. (Avoid using this when talking about DOSDP YAML files).</li> <li>DOSDP template table: The spreadsheet (typically TSV or CSV) that contains the DOSDP variable data. The DOSDP template table is applied to the template string in the template to generate a set of OWL axioms and annotation assertions.</li> </ul>"},{"location":"tutorial/dosdp-odk/#preparation_1","title":"Preparation","text":"<p>This tutorial assumes you have set up an ODK repo with this config:</p> <pre><code>id: cato\ntitle: \"Cat Anatomy Ontology\"\ngithub_org: obophenotype\ngit_main_branch: main\nrepo: cat_anatomy_ontology\nrelease_artefacts:\n  - base\n  - full\n  - simple\nprimary_release: full\nexport_formats:\n  - owl\n  - obo\n  - json\nimport_group:\n  products:\n    - id: ro\n    - id: pato\n    - id: omo\nrobot_java_args: '-Xmx8G'\n</code></pre> <p></p>"},{"location":"tutorial/dosdp-odk/#activate-dosdp-in-odk","title":"Activate DOSDP in ODK","text":"<p>In your <code>src/ontology/{yourontology}-odk.yaml</code> file, simply add the following:</p> <pre><code>use_dosdps: true\n</code></pre> <p>This flag activates DOSDP in ODK - without it, none of the DOSDP workflows in ODK can be used. Technically, this flag tells ODK the following things:</p> <ol> <li>The <code>src/ontology/Makefile</code> is extended as follows:</li> <li>A set of <code>pipelines</code>, or workflows, for processing patterns, e.g. <code>pattern_schema_checks</code> for validating all DOSDP templates,<code>patterns</code> to regenerate all patterns.</li> <li>A new directory, <code>src/patterns</code>, is created with the following files:</li> <li><code>src/patterns/pattern.owl</code>: This is an ontology of your own patterns. This can be used to browse the your pattern in the form of a class hierarchy, which can help greatly to understand how they relate logically. There are some flaws in this system, like occasional unintended equivalencies between patterns, but for most uses, it is doing ok.</li> <li><code>src/patterns/definitions.owl</code>: This is the merged ontology of all your DOSDP generated classes. Basically, if you manage your classes across multiple DOSDP patterns and tables, their generated OWL axioms will all be added to this file.</li> <li><code>src/patterns/external.txt</code>: This file can be used to import external patterns. Just add the (p)URL to a pattern to the file, and the DOSDP pipeline will import it when you run it. We use this a lot when sharing DOSDP templates across ontologies.</li> <li>Two README files: one in the directory of the default DOSDP data pipeline (<code>src/patterns/data/default/</code>) and one in the <code>src/patterns</code> directory. The former points you to the place where you should put, by default, any DOSDP data tables. More about that in the next sections.</li> </ol> <p>To fully activate DOSDP in your ontology, please run:</p> <pre><code>sh run.sh make update_repo\n</code></pre> <p>This will:</p> <ol> <li>Update your ontology repository to whatever ODK you have installed in docker (<code>v1.3</code>, for example)</li> <li>Apply any changes to your configuration file. For example, the fact that you have activated the DOSDP pipeline in your config file will lead to the ODK extending your <code>Makefile</code> in certain ways</li> </ol> <p></p>"},{"location":"tutorial/dosdp-odk/#adding-a-first-simple-template","title":"Adding a first, simple template","text":"<p>(1) Create a new file <code>src/patterns/dosdp-patterns/haircoat_colour_pattern.yaml</code> and paste the following content:</p> <pre><code>pattern_name: haircoat_colour_pattern\npattern_iri: http://purl.obolibrary.org/obo/obo-academy/patterns/haircoat_colour_pattern.yaml\n\ndescription: \"\n  Captures the multicoloured characteristic of the fur, i.e. spotted, dotted, motley etc.\"\n\nclasses:\n  colour_pattern: PATO:0001533\n  coat_of_hair: UBERON:0010166\n\nrelations:\n  has_characteristic: RO:0000053\n\nvars:\n  colour_pattern: \"'colour_pattern'\"\n\nname:\n  text: \"%s coat of hair\"\n  vars:\n    - colour_pattern\n\ndef:\n  text: \"A coat of hair with a %s colour pattern.\"\n  vars:\n    - colour_pattern\n\nequivalentTo:\n  text: \"'coat_of_hair' and 'has_characteristic' some %s\"\n  vars:\n    - colour_pattern\n</code></pre> <p>(2) Let's also create a simple template table to capture traits for our ontology.</p> <p>Note: the filename of the DOSDP template file (<code>haircoat_colour_pattern.yaml</code>) excluding the extension must be identical to the filename of the template table (<code>haircoat_colour_pattern.tsv</code>) excluding the extension.</p> <p>Let's create the new file at <code>src/patterns/data/default/haircoat_colour_pattern.tsv</code>.</p> <pre><code>defined_class   colour_pattern\nCATO:0000001    PATO:0000333\n</code></pre> <p>We are creating a minimal table here with just two columns:</p> <ul> <li><code>defined_class</code> refers to the ID for the term that is being modelled by the template (mandatory for all DOSDP templates)</li> <li><code>colour_pattern</code> refers to the variable of the same name specified in the <code>vars:</code> section of the DOSDP template YAML file.</li> </ul>"},{"location":"tutorial/dosdp-odk/#dosdp-generate-turning-the-template-tables-into-owl-axioms","title":"DOSDP generate: Turning the template tables into OWL axioms","text":"<p>Next, we will get a bit used to various commands that help us with DOSDP-based ontology development.</p> <p>Lets first try to transform the simple table above to OWL using the ODK pipeline (we always use <code>IMP=false</code> to skip refreshing imports, which can be a lengthy process):</p> <pre><code>sh run.sh make ../patterns/definitions.owl -B IMP=false\n</code></pre> <p>This process will will create the <code>../patterns/definitions.owl</code> file, which is the file that contains all axioms generated by all templates you have configured. In our simple scenario, this means a simple single pattern. Let us look at definitions.owl in your favourite text editor first.</p> <pre><code>Tip: Remember, the `-B` tells `make` to run the make command no matter what - one of the advantages of `make` is that it only runs a command again if something changed, for example, you have added something to a DOSDP template table.\n</code></pre> <pre><code>Tip: Looking at ontologies in text editors can be very useful, both to reviewing files and making changes! Do not be afraid, the ODK will ensure you wont break anything.\n</code></pre> <p>Let us look in particular at the following section of the definitions.owl file:</p> <pre><code># Class: &lt;http://purl.obolibrary.org/obo/CATO_0000001&gt; (http://purl.obolibrary.org/obo/PATO_0000333 coat of hair)\n\nAnnotationAssertion(&lt;http://purl.obolibrary.org/obo/IAO_0000115&gt; &lt;http://purl.obolibrary.org/obo/CATO_0000001&gt; \"A coat of hair with a http://purl.obolibrary.org/obo/PATO_0000333 colour pattern.\"^^xsd:string)\nAnnotationAssertion(rdfs:label &lt;http://purl.obolibrary.org/obo/CATO_0000001&gt; \"http://purl.obolibrary.org/obo/PATO_0000333 coat of hair\"^^xsd:string)\nEquivalentClasses(&lt;http://purl.obolibrary.org/obo/CATO_0000001&gt; ObjectIntersectionOf(&lt;http://purl.obolibrary.org/obo/UBERON_0010166&gt; ObjectSomeValuesFrom(&lt;http://purl.obolibrary.org/obo/RO_0000053&gt; &lt;http://purl.obolibrary.org/obo/PATO_0000333&gt;)))\n</code></pre> <p>These are the three axioms / annotation assertions that were created by the DOSDP pipeline. The first annotation is a simple automatically generated definition. What is odd at first glance, is that the definition reads <code>\"A coat of hair with a http://purl.obolibrary.org/obo/PATO_0000333 colour pattern.\"</code> - what does the <code>PATO:0000333</code> IRI do in the middle of our definition? Understanding this is fundamental to the DODSP pattern workflow, because it is likely that you will have to fix cases like this from time to time.</p> <p>The DOSDP workflow is about generating axioms automatically from existing terms. For example, in this tutorial we are trying to generate terms for different kinds of hair coats for our cats, using the <code>colour pattern</code> (PATO:0001533) hierarchy in the PATO ontology as a basis. The only one term we have added so far is <code>spotted</code> (PATO:0000333). The problem is though, that <code>dosdp-tools</code>, the tool which is part of the ODK and responsible for the DOSDP workflows, does not know anything about PATO:0000333 unless it is already imported into the ontology. In order to remedy this situation, lets import the term:</p> <pre><code>sh run.sh make refresh-pato\n</code></pre> <p>ODK will automatically see that you have used PATO:0000333 in your ontology, and import it for you. Next, let us make sure that the our edit file has the correct import configured. Open your ontology in a text editor, and make sure you can find the following import statement:</p> <pre><code>Import(&lt;http://purl.obolibrary.org/obo/cato/patterns/definitions.owl&gt;)\n</code></pre> <p>Replace <code>cato</code> in the PURL with whatever is the ID of your own ontology. Also, do not forget to update <code>src/ontology/catalog-v001.xml</code>, by adding this line:</p> <pre><code>&lt;group id=\"Folder Repository, directory=, recursive=false, Auto-Update=false, version=2\" prefer=\"public\" xml:base=\"\"&gt;\n...\n&lt;uri name=\"http://purl.obolibrary.org/obo/cato/patterns/definitions.owl\" uri=\"../patterns/definitions.owl\"/&gt;\n...\n&lt;/group&gt;\n</code></pre> <p>Important: Remember that we have not yet told dosdp-tools about the freshly imported PATO:0000333 term. To do that, lets run the DOSDP pipeline again:</p> <pre><code>sh run.sh make ../patterns/definitions.owl -B IMP=false\n</code></pre> <p>A quick look at <code>src/patterns/definitions.owl</code> would now reveal your correctly formatted definitions:</p> <pre><code>AnnotationAssertion(&lt;http://purl.obolibrary.org/obo/IAO_0000115&gt; &lt;http://purl.obolibrary.org/obo/CATO_0000001&gt; \"A coat of hair with a spotted colour pattern.\"^^xsd:string)\n</code></pre> <p>Now, we are ready to view our ontology (the edit file, i.e. <code>src/ontology/cato-edit.owl</code>) in Protege:</p> <p></p> <p>Still a few things to iron out - there is an UBERON term that we still need to import, and our class is not a subclass of the CATO <code>root node</code>, but we had a good start.</p>"},{"location":"tutorial/dosdp-odk/#re-using-externally-defined-patterns","title":"Re-using externally defined patterns","text":"<p>Re-using terms is at the heart of the OBO philosophy, but when it comes to re-using axiom patterns, such as the ones we can define as part of a ROBOT template, we are (as of 2022) still in the early stages. One thing we can do to facilitate re-use is to share DOSDP templates between different projects. We do that by simply adding the URL at which the pattern is located to <code>src/patterns/dosdp-patterns/external.txt</code>. Note: if you are copying a URL from GitHub, make sure it is the <code>raw</code> url, i.e.:</p> <ul> <li>src/patterns/dosdp-patterns/external.txt</li> </ul> <pre><code>https://raw.githubusercontent.com/obophenotype/bio-attribute-ontology/master/src/patterns/dosdp-patterns/entity_attribute.yaml\n</code></pre> <p>Here, we randomly decided to import a pattern defined by the Ontology of Biological Attributes (an ontology of traits such as <code>tail length</code> or <code>head size</code>), for example to represent cat traits in our Cat Ontology. After adding the above URL to our the <code>external.txt</code> file, we can add it to our pipeline:</p> <pre><code>sh run.sh make update_patterns\n</code></pre> <p>You will now see the <code>entity_attribute.yaml</code> template in <code>src/patterns/dosdp-patterns</code>. We will not do anything with this template as part of this tutorial, so you can remove it again if you wish (by removing the URL from the <code>external.txt</code> file and physically deleting the <code>src/patterns/dosdp-patterns/entity_attribute.yaml</code> file).</p>"},{"location":"tutorial/dosdp-odk/#dosdp-pipelines-in-odk","title":"DOSDP pipelines in ODK","text":"<p>Sometimes, we want to manage more than one DOSDP pipeline at once. For example, in more than one of our projects, we have some patterns that are automatically generated by software tools, and others that are manually curated by ontology developers. In other use cases, we sometimes want to restrict the pattern pipelines to generating only logical axioms. In either case, we can add new pipelines by adding the following to the <code>src/ontology/youront-odk.yaml</code> file:</p> <pre><code>pattern_pipelines_group:\n  products:\n    - id: manual\n      dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\"\n    - id: auto\n      dosdp_tools_options: \"--obo-prefixes=true\"\n</code></pre> <p>This does the following: It tells the ODK that you want</p>"},{"location":"tutorial/dosdp-odk/#reference","title":"Reference","text":""},{"location":"tutorial/dosdp-odk/#a-full-example-odk-configuration","title":"A full example ODK configuration","text":"<pre><code>id: cato\ntitle: \"Cat Anatomy Ontology\"\ngithub_org: obophenotype\ngit_main_branch: main\nuse_dosdps: TRUE\nrepo: cat_anatomy_ontology\nrelease_artefacts:\n  - base\n  - full\n  - simple\nprimary_release: full\nexport_formats:\n  - owl\n  - obo\n  - json\nimport_group:\n  products:\n    - id: ro\n    - id: pato\n    - id: omo\nrobot_java_args: '-Xmx8G'\npattern_pipelines_group:\n  products:\n    - id: manual\n      dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\"\n    - id: auto\n      dosdp_tools_options: \"--obo-prefixes=true\"\n</code></pre>"},{"location":"tutorial/dosdp-odk/#odk-configuration-reference-for-dosdp","title":"ODK configuration reference for DOSDP","text":"Flag Explanation use_dosdps: TRUE Activates DOSDP in your ODK repository setup pattern_pipelines_group:products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" Adding a <code>manual</code> pipeline to your DOSDP setup in which only logical axioms are generated."},{"location":"tutorial/dosdp-overview/","title":"Getting started with DOSDP templates","text":"<p>Dead Simple OWL Design patterns (DOSDP) is a templating system for documenting and generating new OWL classes. The templates themselves are designed to be human readable and easy to author. Separate tables (TSV files) are used to specify individual classes.</p> <p>The complete DOSDP documentation can be found here http://incatools.github.io/dead_simple_owl_design_patterns/.</p> <p>For another DOSDP tutorial see here.</p>"},{"location":"tutorial/dosdp-overview/#anatomy-of-a-dosdp-file","title":"Anatomy of a DOSDP file:","text":"<p>A DOSDP tempaltes are written in YAML) file, an easily editable format for encoding nested data structures. At the top level of nesting is a set of 'keys', which must match those specified in the DOSDP standard. The various types of key and their function are outlined below. Each key is followed by a colon and then a value, which may be a text string, a list or another set of keys. Lists items are indicated using a '-'. Nesting is achieved via indenting using some standard number of spaces (typically 3 or 4). Here's a little illustration:</p> <pre><code>key1: some text\nkey2:\n  - first list item (text; note the indent)\n  - second list item\nkey3:\n  key_under_key3: some text\n  another_key_under_key3:\n    - first list item (text; note the indent)\n    - second list item\n  yet_another_key_under_key3:\n    key_under_yet_another_key_under_key3: some more text\n</code></pre> <p>In the following text, keys and values together are sometimes referred to as 'fields'.</p>"},{"location":"tutorial/dosdp-overview/#pattern-level-keys","title":"Pattern level keys","text":"<p>Reference doc</p> <p>A set of fields that specify general information about a pattern: name, description, IRI, contributors, examples etc</p> <p>e.g.</p> <pre><code>pattern_name: abnormalAnatomicalEntity\npattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml\ndescription: \"Any unspecified abnormality of an anatomical entity.\"\n\ncontributors:\n  - https://orcid.org/0000-0002-9900-7880\n</code></pre>"},{"location":"tutorial/dosdp-overview/#dictionaries","title":"Dictionaries","text":"<p>Reference doc</p> <p>A major aim of the DOSDP system is to produce self-contained, human-readable templates. Templates need IDs in order to be reliably used programatically, but templates that only use IDs are not human readable. DOSDPs therefore include a set of dictionaries that map labels to IDs. Strictly any readable name can be used, but by convention we use class labels. IDs must be OBO curie style e.g. CL:0000001).</p> <p>Separate dictionaries are required for classes, relations (object properties) &amp; annotationProperties e.g.</p> <pre><code>classes:\n  quality: PATO:0000001\n  abnormal: PATO:0000460\n  anatomical entity: UBERON:0001062\n\nrelations:\n  inheres_in_part_of: RO:0002314\n  has_modifier: RO:0002573\n  has_part: BFO:0000051\n</code></pre>"},{"location":"tutorial/dosdp-overview/#variables","title":"Variables","text":"<p>Reference doc</p> <p>These fields specify the names of pattern variables (TSV column names) and map these to a range. e.g. This specifies a variable called 'anatomy' with the range 'anatomical entity':</p> <pre><code>vars:\n  anatomy: \"'anatomical entity'\"\n</code></pre> <p>The var name (anatomy) corresponds to a column name in the table (TSV file) used in combination with this template, to generate new terms based on the template. The range specifies what type of term is allowed in this column - in this case 'anatomical entity' (UBERON:0001062; as specified in the dictionary) or one of its subclasses, e.g.-</p> anatomy UBERON:0001154 <p>There are various types of variables:</p> <p><code>vars</code> are used to specify OWL classes (see example above). data_vars and data_list_vars are used to specify single pieces or data lists respectively. The range of data_vars is specified using XSD types. e.g.</p> <pre><code>data_vars:\n  number: xsd:int\n\ndata_list_vars:\n  xrefs: xsd:string\n</code></pre> <p>A table used to specify classes following this pattern could have the following content. Note that in lists, multiple elements are separated by a '|'.</p> number xrefs 1 pubmed:123456|DOI:10.1016/j.cell.2016.07.054"},{"location":"tutorial/dosdp-overview/#template-fields","title":"Template fields","text":"<p>Template fields are where the content of classes produced by the template is specified. These mostly follow printf format: A <code>text</code> field has variable slots specified using %s (for strings), %d for integers and %f for floats (decimals). Variables slots are filled, in order of appearance in the text, with values coming from a list of variables in an associated <code>vars</code> field e.g.</p> <pre><code>name:\n  text: \"%s of %s\"\n  vars:\n    - neuron\n    - brain_region\n</code></pre> <p>If the value associated with the neuron var is (the class) 'glutamatergic neuron' and the value associated with the = 'brain region' var is 'primary motor cortext', this will generate a classes with the name (label) \"glutamatergic neuron of primary motor cortex\".</p>"},{"location":"tutorial/dosdp-overview/#obo-fields","title":"OBO fields","text":"<p>Reference doc</p> <p>DOSDPs include a set of convenience fields for annotation of classes that follow OBO conventions for field names and their mappings to OWL annotation properties. These include <code>name</code>, <code>def</code>, <code>comment</code>, <code>namespace</code>. When the value of a var is an OWL class, the name (label) of the var is used in the substitution. (see example above).</p> <p>The annotation axioms generated by these template fields can be annotated. One OBO field exists for this purpose: <code>xrefs</code> allows annotation with a list of references using the obo standard xref annotation property (curies)</p> <p>e.g.</p> <pre><code>data_list_vars:\n  xrefs: xsd:string\n\ndef:\n  text: \"Any %s that has a soma located in the %s\"\n  vars:\n    - neuron\n    - brain_region\n  xrefs: xrefs\n</code></pre>"},{"location":"tutorial/dosdp-overview/#logical-axioms-convenience-fields","title":"Logical axioms convenience fields","text":"<p>Reference doc</p> <p>Where a single equivalent Class, subclassOf or GCI axiom is specified, you may use the keys 'EquivalentTo', 'subClassOf' or 'GCI' respectively. If multiple axioms of any type are needed, use the core field <code>logical_axioms</code>.</p>"},{"location":"tutorial/dosdp-overview/#core-fields","title":"Core fields","text":"<pre><code>annotations:\n  - annotationProperty:\n    text:\n    vars:\n    annotations: ...\n  - annotationProperty:\n    text:\n    vars:\n\nlogical_axioms:\n  - axiom_type: subClassOf\n    text:\n    vars:\n      -\n      -\n  - axiom_type: subClassOf\n    text:\n    vars:\n      -\n      -\n    annotations:\n      - ...\n</code></pre>"},{"location":"tutorial/dosdp-overview/#advanced-usage","title":"Advanced usage:","text":""},{"location":"tutorial/dosdp-overview/#optionals-and-multiples-0-many","title":"Optionals and multiples (0-many)","text":"<p>TBA</p>"},{"location":"tutorial/dosdp-overview/#using-dosdp-templates-in-odk-workflows","title":"Using DOSDP templates in ODK Workflows","text":"<p>The Ontology Development Kit (ODK) comes with a few pre-configured workflows involving DOSDP templates. For a detailed tutorial see here.</p>"},{"location":"tutorial/dosdp-template/","title":"Dead Simple Ontology Design Patterns (DOSDP)","text":"<p>Note: This is an updated Version of Jim Balhoff's DOSDP tutorial here.</p> <p>The main use case for <code>dosdp-tools</code> (and the DOS-DP framework) is managing a set of ontology terms, which all follow a common logical pattern, by simply collecting the unique aspect of each term as a line in a spreadsheet. For example, we may be developing an ontology of environmental exposures. We would like to have terms in our ontology which represent exposure to a variety of stressors, such as chemicals, radiation, social stresses, etc.</p>"},{"location":"tutorial/dosdp-template/#creating-an-ontology-of-environmental-exposures","title":"Creating an ontology of environmental exposures","text":"<p>To maximize reuse and facilitate data integration, we can build our exposure concepts by referencing terms from domain-specific ontologies, such as the Chemical Entities of Biological Interest Ontology (ChEBI) for chemicals. By modeling each exposure concept in the same way, we can use a reasoner to leverage the chemical classification provided by ChEBI to provide a classification for our exposure concepts. Since each exposure concept has a logical definition based on our data model for exposure, there is no need to manually manage the classification hierarchy. Let's say our model for exposure concepts holds that an \"exposure\" is an event with a particular input (the thing the subject is exposed to):</p> <p><code>'exposure to X' EquivalentTo 'exposure event' and 'has input' some X</code></p> <p>If we need an ontology class to represent 'exposure to sarin' (bad news!), we can simply use the term sarin from ChEBI, and create a logical definition:</p> <p><code>'exposure to sarin' EquivalentTo 'exposure event' and 'has input' some sarin</code></p> <p>We can go ahead and create some other concepts we need for our exposure data:</p> <p><code>'exposure to asbestos' EquivalentTo 'exposure event' and 'has input' some asbestos</code></p> <p><code>'exposure to chemical substance' EquivalentTo 'exposure event' and 'has input' some 'chemical substance'</code></p> <p>These definitions again can reference terms provided by ChEBI: asbestos and chemical substance</p>"},{"location":"tutorial/dosdp-template/#classifying-our-concepts","title":"Classifying our concepts","text":"<p>Since the three concepts we've created all follow the same logical model, their hierarchical relationship can be logically determined by the relationships of the chemicals they reference. ChEBI asserts this structure for those terms:</p> <pre><code>'chemical substance'\n         |\n         |\n   --------------\n  |              |\n  |              |\nsarin        asbestos\n</code></pre> <p>Based on this, an OWL reasoner can automatically tell us the relationships between our exposure concepts:</p> <pre><code>        'exposure to chemical substance'\n                       |\n                       |\n           --------------------------\n          |                          |\n          |                          |\n'exposure to sarin'        'exposure to asbestos'\n</code></pre> <p>To support this, we simply need to declare the ChEBI OWL file as an <code>owl:import</code> in our exposure ontology, and use an OWL reasoner such as ELK.</p>"},{"location":"tutorial/dosdp-template/#managing-terms-with-dosdp-tools","title":"Managing terms with dosdp-tools","text":"<p>Creating terms by hand like we just did works fine, and relying on the reasoner for the classification will save us a lot of trouble and maintain correctness as our ontology grows. But since all the terms use the same logical pattern, it would be nice to keep this in one place; this will help make sure we always follow the pattern correctly when we create new concepts. We really only need to store the list of inputs (e.g. chemicals) in order to create all our exposure concepts. As we will see later, we may also want to manage separate sets of terms that follow other, different, patterns. To do this with <code>dosdp-tools</code>, we need three main files: a pattern template, a spreadsheet of pattern fillers, and a source ontology. You will also usually need a file of prefix definitions so that the tool knows how to expand your shortened identifiers into IRIs.</p> <p>For our chemical exposures, getting the source ontology is easy: just download chebi.owl. Note\u2014it's about 450 MB.</p> <p>For our pattern fillers spreadsheet, we just need to make a tab-delimited file containing the chemical stressors for which we need exposure concepts. The file needs a column for the term IRI to be used for the generated class (this column is always called <code>defined_class</code>), and also a column for the chemical to reference (choose a label according to your data model). It should look like this:</p> <pre><code>defined_class input\nEXPOSO:1      CHEBI:75701\nEXPOSO:2      CHEBI:46661\nEXPOSO:3      CHEBI:59999\n</code></pre> <p>The columns should be tab-separated\u2014you can download a correctly formatted file to follow along. For now you will just maintain this file by hand, adding chemicals by looking up their ID in ChEBI, and manually choosing the next ID for your generated classes. In the future this may be simplified using the DOS-DP table editor, which is under development.</p> <p>The trickiest part to DOS-DP is creating your pattern template (but it's not so hard). Pattern templates are written in YAML, a simple file format based on keys and values. The keys are text labels; values can be plain values, another key-value structure, or a list. The DOS-DP schema specifies the keys and values which can be used in a pattern file. We'll use most of the common entries in this example. Read the comments (lines starting with #) for explanation of the various fields:</p> <pre><code># We can provide a name for this pattern here.\npattern_name: exposure_with_input\n\n# In 'classes', we define the terms we will use in this pattern.\n# In the OBO community the terms often have numeric IDs, so here\n# we can provide human-readable names we can use further in the pattern.\n# The key is the name to be used; the value is the ID in prefixed form (i.e. a CURIE).\nclasses:\n  exposure event: ExO:0000002\n  Thing: owl:Thing\n\n# Use 'relations' the same way as 'classes',\n# but for the object properties used in the pattern.\nrelations:\n  has input: RO:0002233\n\n# The 'vars' section defines the various slots that can be\n# filled in for this pattern. We have only one, which we call 'input'.\n# The value is the range, meaning the class of things that are valid\n# values for this pattern. By specifying owl:Thing, we're allowing any\n# class to be provided as a variable filler. You need a column in your\n# spreadsheet for each variable defined here, in addition to the `defined class` column.\nvars:\n  input: \"Thing\"\n\n# We can provide a template for an `rdfs:label` value to generate\n# for our new term. dosdp-tools will search the source ontology\n# to find the label for the filler term, and fill it into the\n# name template in place of the %s.\nname:\n  text: \"exposure to %s\"\n  vars:\n    - input\n\n# This works the same as label generation, but instead creates\n# a definition annotation.\ndef:\n  text: \"A exposure event involving the interaction of an exposure receptor to %s. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\"\n  vars:\n    - input\n\n# Here we can generate a logical axiom for our new concept. Create an\n# expression using OWL Manchester syntax. The expression can use any\n# of the terms defined at the beginning of the pattern. A reference\n# to the variable value will be inserted in place of the %s.\nequivalentTo:\n  text: \"'exposure event' and 'has input' some %s\"\n  vars:\n    - input\n</code></pre> <p>Download the pattern template file to follow along.</p> <p>Now we only need one more file before we can run <code>dosdp-tools</code>. A file of prefix definitions (also in YAML format) will specify how to expand the CURIEs we used in our spreadsheet and pattern files:</p> <pre><code>EXPOSO: http://example.org/exposure/\n</code></pre> <p>Here we are specifying how to expand our <code>EXPOSO</code> prefix (used in our spreadsheet <code>defined_class</code> column). To expand the others, we'll pass a convenience option to <code>dosdp-tools</code>, <code>--obo-prefixes</code>, which will activate some predefined prefixes such as <code>owl:</code>, and handle any other prefixes using the standard expansion for OBO IDs: <code>http://purl.obolibrary.org/obo/PREFIX_</code>. Here's a link to the prefixes file.</p> <p>Now we're all set to run <code>dosdp-tools</code>! If you've downloaded or created all the necessary files, run this command to generate your ontology of exposures (assuming you've added the <code>dosdp-tools</code> to your Unix PATH):</p> <pre><code>dosdp-tools generate --obo-prefixes=true --prefixes=prefixes.yaml --infile=exposure_with_input.tsv --template=exposure_with_input.yaml --ontology=chebi.owl --outfile=exposure_with_input.owl\n</code></pre> <p>This will apply the pattern to each line in your spreadsheet, and save the result in an ontology saved at <code>exposure_with_input.owl</code> (it should look something like this). If you take a look at this ontology in a text editor or in Prot\u00e9g\u00e9, you'll see that it contains three classes, each with a generated label, text definition, and equivalent class definition. You're done!</p> <p>Well... you're sort of done. But wouldn't it be nice if your exposure ontology included some information about the chemicals you referenced? Without this our reasoner can't classify our exposure concepts. As we said above, we could add an <code>owl:import</code> declaration and load all of ChEBI, but your exposure ontology has three classes and ChEBI has over 120,000 classes. Instead, we can use the ROBOT tool to extract a module of just the relevant axioms from ChEBI. Later, we will also see how to use ROBOT to merge the outputs from multiple DOS-DP patterns into one ontology. You can download ROBOT from its homepage.</p>"},{"location":"tutorial/dosdp-template/#extracting-a-module-from-the-source-ontology","title":"Extracting a module from the source ontology","text":"<p>ROBOT has a few different methods for extracting a subset from an ontology. We'll use the Syntactic Locality Module Extractor (SLME) to get a set of axioms relevant to the ChEBI terms we've referenced. ROBOT will need a file containing the list of terms. We can use a Unix command to get these out of our spreadsheet file:</p> <pre><code>sed '1d' exposure_with_input.tsv | cut -f 2 &gt;inputs.txt\n</code></pre> <p>We'll end up with a simple list:</p> <pre><code>CHEBI:75701\nCHEBI:46661\nCHEBI:59999\n</code></pre> <p>Now we can use ROBOT to extract an SLME bottom module for those terms out of ChEBI:</p> <pre><code>robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl\n</code></pre> <p>Our ChEBI extract only has 63 classes. Great! If you want, you can merge the ChEBI extract into your exposure ontology before releasing it to the public:</p> <pre><code>robot merge --input exposure_with_input.owl --input chebi_extract.owl --output exposo.owl\n</code></pre> <p>Now you can open <code>exposo.owl</code> in Prot\u00e9g\u00e9, run the reasoner, and see a correct classification for your exposure concepts! You may notice that your ontology is missing labels for <code>ExO:0000002</code> ('exposure event') and <code>RO:0002233</code> ('has input'). If you want, you can use ROBOT to extract that information from ExO and RO.</p>"},{"location":"tutorial/dosdp-template/#working-with-multiple-patterns","title":"Working with multiple patterns","text":"<p>You will often want to generate ontology modules using more than one DOS-DP pattern. For example, you may want to organize environmental exposures by an additional axis of classification, such as exposure to substances with various biological roles, based on information provided by ChEBI. This requires a slightly different logical expression, so we'll make a new pattern:</p> <pre><code>pattern_name: exposure_with_input_with_role\n\nclasses:\n  exposure event: ExO:0000002\n  Thing: owl:Thing\n\nrelations:\n  has input: RO:0002233\n  has role: RO:0000087\n\nvars:\n  input: \"Thing\"\n\nname:\n  text: \"exposure to %s\"\n  vars:\n    - input\n\ndef:\n  text: \"A exposure event involving the interaction of an exposure receptor to a substance with %s role. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\"\n  vars:\n    - input\n\nequivalentTo:\n  text: \"'exposure event' and 'has input' some ('has role' some %s)\"\n  vars:\n    - input\n</code></pre> <p>Let's create an input file for this pattern, with a single filler, neurotoxin:</p> <pre><code>defined_class   input\nEXPOSO:4    CHEBI:50910\n</code></pre> <p>Now we can run <code>dosdp-tools</code> for this pattern:</p> <pre><code>dosdp-tools generate --obo-prefixes --prefixes=prefixes.yaml --infile=exposure_with_input_with_role.tsv --template=exposure_with_input_with_role.yaml --ontology=chebi.owl --outfile=exposure_with_input_with_role.owl\n</code></pre> <p>We can re-run our ChEBI module extractor, first appending the terms used for this pattern to the ones we used for the first pattern:</p> <pre><code>sed '1d' exposure_with_input_with_role.tsv | cut -f 2 &gt;&gt;inputs.txt\n</code></pre> <p>And then run <code>robot extract</code> exactly as before:</p> <pre><code>robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl\n</code></pre> <p>Now we just want to merge both of our generated modules, along with our ChEBI extract:</p> <pre><code>robot merge --input exposure_with_input.owl --input exposure_with_input_with_role.owl --input chebi_extract.owl --output exposo.owl\n</code></pre> <p>If you open the new <code>exposo.owl</code> in Prot\u00e9g\u00e9 and run the reasoner, you'll now see 'exposure to sarin' classified under both 'exposure to chemical substance' and also 'exposure to neurotoxin'.</p>"},{"location":"tutorial/dosdp-template/#conclusion","title":"Conclusion","text":"<p>By using <code>dosdp-tools</code> and <code>robot</code> together, you can effectively develop ontologies which compose parts of ontologies from multiple domains using standard patterns. You will probably want to orchestrate the types of commands used in this tutorial within a Makefile, so that you can automate this process for easy repeatability.</p>"},{"location":"tutorial/exomiser-tutorial/","title":"Introduction to Exomiser","text":""},{"location":"tutorial/exomiser-tutorial/#tutorial-an-introduction-to-exomiser","title":"Tutorial: an introduction to Exomiser","text":"<p>Exomiser is a Java program that ranks potential rare Mendelian disease-causing variants from whole-exome or whole-genome sequencing data. Starting from a patient's VCF file and a set of phenotypes encoded using the Human Phenotype Ontology (HPO), it will annotate, filter and prioritise likely causative variants. The program does this based on user-defined criteria such as a variant's predicted pathogenicity, frequency of occurrence in a population and also how closely the given patient's phenotype matches any known phenotype of genes from human disease and model organism data.</p> <p>In this tutorial, we will learn how to install and run Exomiser with Docker, and how to interpret the results in various output formats detailing the predicted causative genes and variants. If you prefer to work locally, instructions are also provided below for Windows and Linux/Mac users.</p> <p>The complete Exomiser documentation can be found here, including some relevant references here, and the Exomiser GitHub repository here.</p> <p>Please note that this tutorial is up-to-date with the current latest release 13.2.0 and data version up to 2302 (Feb 2023).</p>"},{"location":"tutorial/exomiser-tutorial/#prerequisites","title":"PREREQUISITES","text":"<p>You know:</p> <ul> <li>how to use a command line interface.</li> </ul> <p>You have:</p> <ul> <li> <p>Docker installed and running on your machine. Check out this simple guide to set up Docker for Windows or Docker for Mac.</p> </li> <li> <p>We recommend to have Exomiser installed via Docker prior to the tutorial. Open a terminal and run the command below:</p> </li> </ul> <pre><code>docker pull exomiser/exomiser-cli:13.2.0\n</code></pre> <ul> <li>Exomiser-Tutorial files downloaded from our GoogleDrive.</li> </ul> <p>Alternatively:</p> <p><pre><code># download the data via\nwget https://github.com/iQuxLE/Exomiser-Tutorial/raw/main/Exomiser-Tutorial.zip\n# OR clone the repository\ngit clone https://github.com/iQuxLE/Exomiser-Tutorial.git\n\n# unzip\nunzip Exomiser-Tutorial.zip\n</code></pre> Since the VCF files for parts of the example data are relatively large, you need to download the following separately and put it into the <code>Exomiser-Tutorial</code> folder.</p> <pre><code># download\nwget https://github.com/iQuxLE/Exomiser-Tutorial/raw/main/pfeiffer-family-vcf.zip\n# unzip\nunzip pfeiffer-family-vcf.zip -d Exomiser-Tutorial/exomiser-config/\n</code></pre> <p>The <code>Exomiser-Tutorial</code> folder contains a directory called <code>exomiser-config</code> (with all the VCF and analysis files) and <code>exomiser-overview</code> (with some introductory slides).</p> <ul> <li>Importantly, we highly recommend downloading the Exomiser data prior to the tutorial in order to follow along. The data required to run Exomiser is ~80GB and will take a while to download and unpack (depending on your internet connection, it may take a few hours). For this tutorial, you only need to download the Exomiser phenotype data and the variant data for the hg19 assembly. If you have your own samples to run with Exomiser and the VCF files are built on the hg38 build, then you will need to download the hg38 variant data as well. The current data version that we will use in this tutorial is 2302 (Feb 2023). This will change in the future. Always make sure that you use the latest data version available. To download the Exomiser data from the terminal:</li> </ul> <pre><code># create an empty directory for exomiser-data within the Exomiser-Tutorial folder:\ncd /path/to/Exomiser-Tutorial/\nmkdir exomiser-data\ncd exomiser-data\n# download the data\nwget https://data.monarchinitiative.org/exomiser/latest/2302_phenotype.zip # for the phenotype database\nwget https://data.monarchinitiative.org/exomiser/latest/2302_hg19.zip # for the hg19 variant database\n# unzip the data\nunzip \"2302_*.zip\"\n</code></pre> <p>Otherwise, visit the links and download the data in your own <code>exomiser-data</code> directory:</p> <p>2302 phenotype database</p> <p>2302 hg19 variant database</p> <p>Install 7-Zip for unzipping the database files. The built-in archiving software has issues extracting the zip files. Extract the database files (<code>2302_phenotype.zip</code>, <code>2302_hg19.zip</code>) by right-clicking the archive and selecting 7-Zip &gt; Extract files\u2026 into the <code>exomiser-data</code> directory.</p> <p>Your <code>Exomiser-Tutorial</code> directory should now be structured as follows:</p> <pre><code>Exomiser-Tutorial\n    \u251c\u2500\u2500 exomiser-config\n    \u251c\u2500\u2500 exomiser-data\n    \u2502 \u251c\u2500\u2500 2302_hg19\n    \u2502 \u2514\u2500\u2500 2302_phenotype\n    \u2514\u2500\u2500 exomiser-overview\n      \u2514\u2500\u2500 exomiser-tutorial-slides\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#outline-of-the-tutorial","title":"Outline of the tutorial","text":"<ul> <li>Exomiser overview</li> <li>Exomiser installation<ul> <li>via Docker</li> <li>via Windows</li> <li>via Linux/Mac</li> <li>Configuring the application.properties</li> </ul> </li> <li>Running Exomiser<ul> <li>Using phenopackets</li> <li>Analysis settings</li> <li>Running via Docker</li> <li>Running locally</li> <li>Analysing multi-sample VCF files</li> <li>Running large jobs (batch)</li> </ul> </li> <li>Results<ul> <li>HTML</li> <li>JSON</li> <li>TSV VARIANTS</li> <li>TSV GENES</li> </ul> </li> </ul>"},{"location":"tutorial/exomiser-tutorial/#exomiser-overview","title":"Exomiser overview","text":"<p>For a quick overview of Exomiser take a look at the slides located in the Google Drive or GitHub repo.</p>"},{"location":"tutorial/exomiser-tutorial/#exomiser-installation","title":"Exomiser installation","text":""},{"location":"tutorial/exomiser-tutorial/#via-docker","title":"via Docker","text":"<p>(recommended to be installed prior to the tutorial; if you run the command below again, you should receive the message \"Image is up to date for exomiser/exomiser-cli:13.2.0\")</p> <pre><code>docker pull exomiser/exomiser-cli:13.2.0\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#via-windows","title":"via Windows","text":"<ol> <li>Install 7-Zip for unzipping the archive files. The built-in archiving software has issues extracting the zip files.</li> <li>Download the <code>exomiser-cli-13.2.0-distribution.zip</code> distribution from Monarch.</li> <li>Download the variant <code>2302_hg19.zip</code> and phenotype <code>2302_phenotype.zip</code> data files from Monarch.</li> <li>Extract the distribution files by right-clicking <code>exomiser-cli-13.2.0-distribution.zip</code> and selecting 7-Zip &gt; Extract Here</li> <li>Extract the data files (e.g. <code>2302_phenotype.zip</code>, <code>2302_hg19.zip</code>) by right-clicking the archive and selecting 7-Zip &gt; Extract files\u2026 into the exomiser data directory. By default, Exomiser expects this to be \u2018exomiser-cli-13.2.0/data\u2019, but this can be changed in the application.properties.</li> </ol>"},{"location":"tutorial/exomiser-tutorial/#via-linuxmac","title":"via Linux/Mac","text":"<p>The following shell script should work:</p> <pre><code># download the distribution (won't take long)\nwget https://data.monarchinitiative.org/exomiser/latest/exomiser-cli-13.2.0-distribution.zip\n# download the data (this is ~80GB and will take a while). If you only require a single assembly, only download the relevant files.\nwget https://data.monarchinitiative.org/exomiser/latest/2302_hg19.zip\nwget https://data.monarchinitiative.org/exomiser/latest/2302_phenotype.zip\n# unzip the distribution and data files - this will create a directory called 'exomiser-cli-13.2.0' in the current working directory (with examples and application.properties)\nunzip exomiser-cli-13.2.0-distribution.zip\nunzip '2302_*.zip' -d exomiser-cli-13.2.0/data\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#configuring-the-applicationproperties","title":"Configuring the application.properties","text":"<p>The application.properties file needs to be updated to point to the correct location of the Exomiser data. For the purpose of this tutorial, this is already sorted, pointing to the mounted directory inside the Docker container <code>exomiser.data-directory=/exomiser-data</code>.</p> <p>Also, you want to make sure to edit the file to use the correct data version (currently 2302):</p> <pre><code> exomiser.hg19.data-version=2302\n exomiser.phenotype.data-version=2302\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#tutorials","title":"Tutorials","text":""},{"location":"tutorial/exomiser-tutorial/#monarch-obo-training-tutorial","title":"Monarch OBO Training Tutorial","text":""},{"location":"tutorial/exomiser-tutorial/#running-exomiser","title":"Running Exomiser","text":"<p>For this tutorial, we will focus on running Exomiser on a single-sample (whole-exome) VCF file. Additional instructions for running Exomiser on multi-sample VCF data and large jobs are also provided below.</p>"},{"location":"tutorial/exomiser-tutorial/#using-phenopackets","title":"Using phenopackets","text":"<p>It is recommended to provide Exomiser with the input sample as a Phenopacket. Exomiser will accept this in either JSON or YAML format. We will use the example <code>pfeiffer-phenopacket.yml</code> below:</p> <pre><code>id: manuel\nsubject:\n  id: manuel\n  sex: MALE\nphenotypicFeatures:\n  - type:\n      id: HP:0001159\n      label: Syndactyly\n  - type:\n      id: HP:0000486\n      label: Strabismus\n  - type:\n      id: HP:0000327\n      label: Hypoplasia of the maxilla\n  - type:\n      id: HP:0000520\n      label: Proptosis\n  - type:\n      id: HP:0000316\n      label: Hypertelorism\n  - type:\n      id: HP:0000244\n      label: Brachyturricephaly\nhtsFiles:\n  - uri: exomiser/Pfeiffer.vcf.gz\n    htsFormat: VCF\n    genomeAssembly: hg19\nmetaData:\n  created: '2019-11-12T13:47:51.948Z'\n  createdBy: julesj\n  resources:\n    - id: hp\n      name: human phenotype ontology\n      url: http://purl.obolibrary.org/obo/hp.owl\n      version: hp/releases/2019-11-08\n      namespacePrefix: HP\n      iriPrefix: 'http://purl.obolibrary.org/obo/HP_'\n  phenopacketSchemaVersion: 1.0\n</code></pre> <p>NOTE: This is an example of a v1.0 phenopacket, there is a more recent release of v2.0. Exomiser can run phenopackets built with either v1.0 or v2.0 schema. You can find out more about the v2.0 phenopacket schema and how to build one with Python or Java here. To convert a phenopacket v1.0 to v2.0, you can use phenopacket-tools.</p>"},{"location":"tutorial/exomiser-tutorial/#analysis-settings","title":"Analysis settings","text":"<p>Below are the default analysis settings from <code>pfeiffer-analysis.yml</code> that we will use in our tutorial:</p> <pre><code>---\nanalysis:\n  #FULL or PASS_ONLY\n  analysisMode: PASS_ONLY\n  # In cases where you do not want any cut-offs applied an empty map should be used e.g. inheritanceModes: {}\n  # These are the default settings, with values representing the maximum minor allele frequency in percent (%) permitted for an\n  # allele to be considered as a causative candidate under that mode of inheritance.\n  # If you just want to analyse a sample under a single inheritance mode, delete/comment-out the others. For AUTOSOMAL_RECESSIVE\n  # or X_RECESSIVE ensure *both* relevant HOM_ALT and COMP_HET modes are present.\n  inheritanceModes: {\n    AUTOSOMAL_DOMINANT: 0.1,\n    AUTOSOMAL_RECESSIVE_COMP_HET: 2.0,\n    AUTOSOMAL_RECESSIVE_HOM_ALT: 0.1,\n    X_DOMINANT: 0.1,\n    X_RECESSIVE_COMP_HET: 2.0,\n    X_RECESSIVE_HOM_ALT: 0.1,\n    MITOCHONDRIAL: 0.2\n  }\n  #Possible frequencySources:\n  #Thousand Genomes project http://www.1000genomes.org/\n  #   THOUSAND_GENOMES,\n  #ESP project http://evs.gs.washington.edu/EVS/\n  #   ESP_AFRICAN_AMERICAN, ESP_EUROPEAN_AMERICAN, ESP_ALL,\n  #ExAC project http://exac.broadinstitute.org/about\n  #   EXAC_AFRICAN_INC_AFRICAN_AMERICAN, EXAC_AMERICAN,\n  #   EXAC_SOUTH_ASIAN, EXAC_EAST_ASIAN,\n  #   EXAC_FINNISH, EXAC_NON_FINNISH_EUROPEAN,\n  #   EXAC_OTHER\n  #Possible frequencySources:\n  #Thousand Genomes project - http://www.1000genomes.org/ (THOUSAND_GENOMES)\n  #TOPMed - https://www.nhlbi.nih.gov/science/precision-medicine-activities (TOPMED)\n  #UK10K - http://www.uk10k.org/ (UK10K)\n  #ESP project - http://evs.gs.washington.edu/EVS/ (ESP_)\n  #   ESP_AFRICAN_AMERICAN, ESP_EUROPEAN_AMERICAN, ESP_ALL,\n  #ExAC project http://exac.broadinstitute.org/about (EXAC_)\n  #   EXAC_AFRICAN_INC_AFRICAN_AMERICAN, EXAC_AMERICAN,\n  #   EXAC_SOUTH_ASIAN, EXAC_EAST_ASIAN,\n  #   EXAC_FINNISH, EXAC_NON_FINNISH_EUROPEAN,\n  #   EXAC_OTHER\n  #gnomAD - http://gnomad.broadinstitute.org/ (GNOMAD_E, GNOMAD_G)\n  frequencySources: [\n    THOUSAND_GENOMES,\n    TOPMED,\n    UK10K,\n\n    ESP_AFRICAN_AMERICAN, ESP_EUROPEAN_AMERICAN, ESP_ALL,\n\n    EXAC_AFRICAN_INC_AFRICAN_AMERICAN, EXAC_AMERICAN,\n    EXAC_SOUTH_ASIAN, EXAC_EAST_ASIAN,\n    EXAC_FINNISH, EXAC_NON_FINNISH_EUROPEAN,\n    EXAC_OTHER,\n\n    GNOMAD_E_AFR,\n    GNOMAD_E_AMR,\n    #        GNOMAD_E_ASJ,\n    GNOMAD_E_EAS,\n    GNOMAD_E_FIN,\n    GNOMAD_E_NFE,\n    GNOMAD_E_OTH,\n    GNOMAD_E_SAS,\n\n    GNOMAD_G_AFR,\n    GNOMAD_G_AMR,\n    #        GNOMAD_G_ASJ,\n    GNOMAD_G_EAS,\n    GNOMAD_G_FIN,\n    GNOMAD_G_NFE,\n    GNOMAD_G_OTH,\n    GNOMAD_G_SAS\n  ]\n  # Possible pathogenicitySources: (POLYPHEN, MUTATION_TASTER, SIFT), (REVEL, MVP), CADD, REMM\n  # REMM is trained on non-coding regulatory regions\n  # *WARNING* if you enable CADD or REMM ensure that you have downloaded and installed the CADD/REMM tabix files\n  # and updated their location in the application.properties. Exomiser will not run without this.\n  pathogenicitySources: [ REVEL, MVP ]\n  #this is the standard exomiser order.\n  #all steps are optional\n  steps: [\n    #hiPhivePrioritiser: {},\n    #priorityScoreFilter: {priorityType: HIPHIVE_PRIORITY, minPriorityScore: 0.500},\n    #intervalFilter: {interval: 'chr10:123256200-123256300'},\n    # or for multiple intervals:\n    #intervalFilter: {intervals: ['chr10:123256200-123256300', 'chr10:123256290-123256350']},\n    # or using a BED file - NOTE this should be 0-based, Exomiser otherwise uses 1-based coordinates in line with VCF\n    #intervalFilter: {bed: /full/path/to/bed_file.bed},\n    #genePanelFilter: {geneSymbols: ['FGFR1','FGFR2']},\n      failedVariantFilter: { },\n    #qualityFilter: {minQuality: 50.0},\n      variantEffectFilter: {\n        remove: [\n            FIVE_PRIME_UTR_EXON_VARIANT,\n            FIVE_PRIME_UTR_INTRON_VARIANT,\n            THREE_PRIME_UTR_EXON_VARIANT,\n            THREE_PRIME_UTR_INTRON_VARIANT,\n            NON_CODING_TRANSCRIPT_EXON_VARIANT,\n            UPSTREAM_GENE_VARIANT,\n            INTERGENIC_VARIANT,\n        REGULATORY_REGION_VARIANT,\n        CODING_TRANSCRIPT_INTRON_VARIANT,\n        NON_CODING_TRANSCRIPT_INTRON_VARIANT,\n        DOWNSTREAM_GENE_VARIANT\n      ]\n    },\n    # removes variants represented in the database\n    #knownVariantFilter: {},\n    frequencyFilter: {maxFrequency: 2.0},\n    pathogenicityFilter: {keepNonPathogenic: true},\n    # inheritanceFilter and omimPrioritiser should always run AFTER all other filters have completed\n    inheritanceFilter: {},\n    # omimPrioritiser isn't mandatory.\n    omimPrioritiser: {},\n    #priorityScoreFilter: {minPriorityScore: 0.4},\n    # Other prioritisers: Only combine omimPrioritiser with one of these.\n    # Don't include any if you only want to filter the variants.\n    hiPhivePrioritiser: {},\n    # or run hiPhive in benchmarking mode:\n    #hiPhivePrioritiser: {runParams: 'mouse'},\n    #phivePrioritiser: {}\n    #phenixPrioritiser: {}\n    #exomeWalkerPrioritiser: {seedGeneIds: [11111, 22222, 33333]}\n  ]\noutputOptions:\n    outputContributingVariantsOnly: false\n    #numGenes options: 0 = all or specify a limit e.g. 500 for the first 500 results\n    numGenes: 0\n    #minExomiserGeneScore: 0.7\n    # Path to the desired output directory. Will default to the 'results' subdirectory of the exomiser install directory\n    outputDirectory: results\n    # Filename for the output files. Will default to {input-vcf-filename}-exomiser\n    outputFileName: Pfeiffer-HIPHIVE-exome\n    #out-format options: HTML, JSON, TSV_GENE, TSV_VARIANT, VCF (default: HTML)\n    outputFormats: [HTML, JSON, TSV_GENE, TSV_VARIANT]\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#running-via-docker","title":"Running via Docker","text":"<pre><code>docker run -it -v \"/path/to/Exomiser-Tutorial/exomiser-data:/exomiser-data\" \\\n-v \"/path/to/Exomiser-Tutorial/exomiser-config/:/exomiser\" \\\n-v \"/path/to/Exomiser-Tutorial/exomiser-results:/results\" \\\nexomiser/exomiser-cli:13.2.0 \\\n--sample /exomiser/pfeiffer-phenopacket.yml \\\n--analysis /exomiser/pfeiffer-analysis.yml \\\n--spring.config.location=/exomiser/application.properties\n</code></pre> <p>This command will produce <code>Pfeiffer-HIPHIVE-exome.html</code>, <code>Pfeiffer-HIPHIVE-exome.json</code>, <code>Pfeiffer-HIPHIVE-exome.genes.tsv</code> and <code>Pfeiffer-HIPHIVE-exome.variants.tsv</code> in your <code>exomiser-results</code> directory.</p>"},{"location":"tutorial/exomiser-tutorial/#running-locally","title":"Running locally","text":"<p>Assuming that you are within the <code>exomiser-cli-13.2.0</code> distribution folder:</p> <pre><code>java -jar exomiser-cli-13.2.0.jar --sample examples/pfeiffer-phenopacket.yml \\\n--analysis examples/exome-analysis.yml --output examples/output-options.yml  \n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#analysing-multi-sample-vcf-files","title":"Analysing multi-sample VCF files","text":"<p>When analysing a multi-sample VCF file, you must detail the pedigree information in a phenopacket describing a Family object:</p> <p>e.g. <code>Exomiser-Tutorial/exomiser-config/pfeiffer-family.yml</code></p> <pre><code>id: ISDBM322017-family\nproband:\n  subject:\n    id: ISDBM322017\n    sex: FEMALE\n  phenotypicFeatures:\n    - type:\n        id: HP:0001159\n        label: Syndactyly\n    - type:\n        id: HP:0000486\n        label: Strabismus\n    - type:\n        id: HP:0000327\n        label: Hypoplasia of the maxilla\n    - type:\n        id: HP:0000520\n        label: Proptosis\n    - type:\n        id: HP:0000316\n        label: Hypertelorism\n    - type:\n        id: HP:0000244\n        label: Brachyturricephaly\npedigree:\n  persons:\n    - individualId: ISDBM322017\n      paternalId: ISDBM322016\n      maternalId: ISDBM322018\n      sex: FEMALE\n      affectedStatus: AFFECTED\n    - individualId: ISDBM322015\n      paternalId: ISDBM322016\n      maternalId: ISDBM322018\n      sex: MALE\n      affectedStatus: UNAFFECTED\n    - individualId: ISDBM322016\n      sex: MALE\n      affectedStatus: UNAFFECTED\n    - individualId: ISDBM322018\n      sex: FEMALE\n      affectedStatus: UNAFFECTED\nhtsFiles:\n  - uri: exomiser/Pfeiffer-quartet.vcf.gz\n    htsFormat: VCF\n    genomeAssembly: GRCh37\nmetaData:\n  created: '2019-11-12T13:47:51.948Z'\n  createdBy: julesj\n  resources:\n    - id: hp\n      name: human phenotype ontology\n      url: http://purl.obolibrary.org/obo/hp.owl\n      version: hp/releases/2019-11-08\n      namespacePrefix: HP\n      iriPrefix: 'http://purl.obolibrary.org/obo/HP_'\n  phenopacketSchemaVersion: 1.0\n</code></pre> <p>Running via Docker:</p> <pre><code>docker run -it -v '/path/to/Exomiser-Tutorial/exomiser-data:/exomiser-data' \\\n-v '/path/to/Exomiser-Tutorial/exomiser-config/:/exomiser' \\\n-v '/path/to/Exomiser-Tutorial/exomiser-results:/results' \\\nexomiser/exomiser-cli:13.2.0 \\\n--sample /exomiser/pfeiffer-family.yml \\\n--analysis /exomiser/pfeiffer-analysis.yml \\\n--spring.config.location=/exomiser/application.properties\n</code></pre> <p>Running locally:</p> <p>Assuming that you are within the <code>exomiser-cli-13.2.0</code> distribution folder</p> <pre><code>java -jar exomiser-cli-13.2.0.jar --sample examples/pfeiffer-family.yml --analysis examples/exome-analysis.yml --output examples/output-options.yml\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#running-large-jobs-batch","title":"Running large jobs (batch)","text":"<p>The above commands can be added to a batch file for example in the file <code>Exomiser-Tutorial/exomiser-config/test-analysis-batch-commands.txt</code>. Using it with Docker we recommend creating a new directory for the batch files and mounting that to the Docker container.</p> <p>Running via Docker:</p> <pre><code>docker run -it -v '/path/to/Exomiser-Tutorial/exomiser-data:/exomiser-data' \\\n-v '/path/to/Exomiser-Tutorial/exomiser-config/:/exomiser' \\\n-v '/path/to/Exomiser-Tutorial/exomiser-results:/results' \\\n-v '/path/to/Exomiser-Tutorial/exomiser-batch-files:/batch-files' \\\nexomiser/exomiser-cli:13.2.0 \\\n--batch /batch-files/test-analysis-batch-commands.txt\n--spring.config.location=/exomiser/application.properties\n</code></pre> <p>Running locally:</p> <p>Assuming that you are within the <code>exomiser-cli-13.2.0</code> distribution folder</p> <pre><code>java -jar exomiser-cli-13.2.0.jar --batch examples/test-analysis-batch-commands.txt\n</code></pre> <p>The advantage of this is that a single command will be able to analyse many samples in far less time than starting a new JVM for each as there will be no start-up penalty after the initial start and the Java JIT compiler will be able to take advantage of a longer-running process to optimise the runtime code. For maximum throughput on a cluster consider splitting your batch jobs over multiple nodes.</p>"},{"location":"tutorial/exomiser-tutorial/#results","title":"Results","text":"<p>Depending on the output options provided, Exomiser will write out at least an HTML and JSON output file in the <code>results</code> subdirectory of the Exomiser installation (by default) or a user-defined results directory as indicated in the output options.</p> <p>As a general rule, all output files contain a ranked list of genes and variants with the top-ranked gene/variant displayed first. The exception being the VCF output (if requested in the output options; not requested in this tutorial) which, since version 13.1.0, is sorted according to VCF convention and tabix indexed.</p> <p>In our tutorial, we requested HTML, JSON, TSV_VARIANT and TSV_GENE output formats which are briefly outlined below.</p>"},{"location":"tutorial/exomiser-tutorial/#html","title":"HTML","text":"<p>A few relevant screenshots from <code>Pfeiffer-HIPHIVE-exome.html</code>:  </p>"},{"location":"tutorial/exomiser-tutorial/#json","title":"JSON","text":"<p>The JSON file represents the most accurate representation of the results, as it is referenced internally by Exomiser. As such, we don\u2019t provide a schema for this, but it has been pretty stable and breaking changes will only occur with major version changes to the software. Minor additions are to be expected for minor releases, as per the SemVer specification.</p> <p>We recommend using Python or JQ to extract data from this file. To give you an idea of how you can extract some data with Python, we have provided examples of how you can iterate over the results below. However, there is a lot more information content that you can pull out from the JSON results file, this only provides a snippet of what you can do.</p> <pre><code># import json library\nimport json\n\n# to load in the exomiser json result\nwith open(\"path/to/Exomiser-Tutorial/Pfeiffer-HIPHIVE-exome.json\") as exomiser_json_result:\n    exomiser_result = json.load(exomiser_json_result)\nexomiser_json_result.close()\n\n# to retrieve all predicted genes and corresponding identifier (ENSEMBL)\ngene_results = []\nfor result in exomiser_result:\n    gene_results.append({result[\"geneSymbol\"]: result[\"geneIdentifier\"][\"geneId\"]})\n\n# to retrieve all predicted variants\nvariant_results = []\nfor result in exomiser_result:\n  for moi in result[\"geneScores\"]:  # iterating over all modes of inheritance\n    if \"contributingVariants\" in moi:  #  checking if there is evidence of contributing variants\n        for cv in moi[\"contributingVariants\"]:  # iterating over all contributing variants\n          variant_results.append({\"chromosome\": cv[\"contigName\"],\n                                  \"start_pos\": cv[\"start\"],\n                                  \"end_pos\": cv[\"end\"],\n                                  \"ref_allele\": cv[\"ref\"],\n                                  \"alt_allele\": cv[\"alt\"]})\n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#tsv-variants","title":"TSV VARIANTS","text":"<p>In the <code>Pfeiffer-HIPHIVE-exome.variants.tsv</code> file it is possible for a variant to appear multiple times, depending on the MOI it is compatible with. For example, in the excerpt of the file below, MUC6 has two variants ranked 7th under the AD model and two ranked 8th under an AR (compound heterozygous) model. In the AD case the CONTRIBUTING_VARIANT column indicates whether the variant was (1) or wasn't (0) used for calculating the EXOMISER_GENE_COMBINED_SCORE and EXOMISER_GENE_VARIANT_SCORE.</p> <pre><code>#RANK   ID      GENE_SYMBOL     ENTREZ_GENE_ID  MOI     P-VALUE EXOMISER_GENE_COMBINED_SCORE    EXOMISER_GENE_PHENO_SCORE       EXOMISER_GENE_VARIANT_SCORE     EXOMISER_VARIANT_SCORE  CONTRIBUTING_VARIANT    WHITELIST_VARIANT       VCF_ID  RS_ID   CONTIG  START   END     REF     ALT     CHANGE_LENGTH   QUAL    FILTER  GENOTYPE        FUNCTIONAL_CLASS        HGVS    EXOMISER_ACMG_CLASSIFICATION    EXOMISER_ACMG_EVIDENCE  EXOMISER_ACMG_DISEASE_ID        EXOMISER_ACMG_DISEASE_NAME      CLINVAR_ALLELE_ID       CLINVAR_PRIMARY_INTERPRETATION  CLINVAR_STAR_RATING     GENE_CONSTRAINT_LOEUF   GENE_CONSTRAINT_LOEUF_LOWER     GENE_CONSTRAINT_LOEUF_UPPER     MAX_FREQ_SOURCE MAX_FREQ        ALL_FREQ        MAX_PATH_SOURCE MAX_PATH        ALL_PATH\n1       10-123256215-T-G_AD     FGFR2   2263    AD      0.0000  0.9957  0.9187  1.0000  1.0000  1       1               rs121918506     10      123256215       123256215       T       G       0       900.0000        PASS    0/1     missense_variant        FGFR2:ENST00000346997.2:c.1688A&gt;C:p.(Glu563Ala) PATHOGENIC      PM2,PP3_Strong,PP4,PP5_Strong   ORPHA:87        Apert syndrome  28333   PATHOGENIC_OR_LIKELY_PATHOGENIC 2       0.13692 0.074   0.27                            REVEL   0.965   REVEL=0.965,MVP=0.9517972\n2       5-71755984-C-G_AD       ZNF366  167465  AD      0.0018  0.9237  0.8195  0.7910  0.7910  1       0               rs375204168     5       71755984        71755984        C       G       0       380.8900        PASS    0/1     splice_region_variant   ZNF366:ENST00000318442.5:c.1332+8G&gt;C:p.?        UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.27437 0.155   0.515   EXAC_AMERICAN   0.07975895      THOUSAND_GENOMES=0.01997,TOPMED=0.01096,ESP_EUROPEAN_AMERICAN=0.0116,ESP_ALL=0.0077,EXAC_AMERICAN=0.07975895,EXAC_NON_FINNISH_EUROPEAN=0.010914307,GNOMAD_E_AMR=0.07153929,GNOMAD_E_NFE=0.010890082,GNOMAD_E_OTH=0.018328445\n3       16-2150254-G-A_AD       PKD1    5310    AD      0.0050  0.8272  0.6597  0.8707  0.8707  1       0               rs147967021     16      2150254 2150254 G       A       0       406.0800        PASS    0/1     missense_variant        PKD1:ENST00000262304.4:c.9625C&gt;T:p.(Arg3209Cys) UNCERTAIN_SIGNIFICANCE                          1319391 UNCERTAIN_SIGNIFICANCE  1       0.12051 0.082   0.179   EXAC_AMERICAN   0.06979585      THOUSAND_GENOMES=0.01997,TOPMED=0.007934,EXAC_AMERICAN=0.06979585,EXAC_NON_FINNISH_EUROPEAN=0.0015655332,EXAC_SOUTH_ASIAN=0.012149192,GNOMAD_E_AFR=0.006708708,GNOMAD_E_AMR=0.05070389,GNOMAD_E_NFE=0.002718672,GNOMAD_E_SAS=0.013009822,GNOMAD_G_AFR=0.011462632       MVP     0.8792868       REVEL=0.346,MVP=0.8792868\n4       3-56653839-CTG-C_AD     CCDC66  285331  AD      0.0051  0.8262  0.5463  0.9984  0.9984  1       0               rs751329549     3       56653839        56653841        CTG     C       -2      1872.9400       PASS    0/1     frameshift_truncation   CCDC66:ENST00000326595.7:c.2572_2573del:p.(Val858Glnfs*6)       UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.9703  0.78    1.215   GNOMAD_E_AMR    0.011914691     TOPMED=7.556E-4,EXAC_EAST_ASIAN=0.01155535,EXAC_NON_FINNISH_EUROPEAN=0.0015023135,GNOMAD_E_AMR=0.011914691,GNOMAD_E_EAS=0.0057977736,GNOMAD_E_NFE=8.988441E-4\n5       13-110855918-C-G_AD     COL4A1  1282    AD      0.0075  0.7762  0.5288  0.9838  0.9838  1       0               rs150182714     13      110855918       110855918       C       G       0       1363.8700       PASS    0/1     missense_variant        COL4A1:ENST00000375820.4:c.994G&gt;C:p.(Gly332Arg) UNCERTAIN_SIGNIFICANCE  PP3_Moderate    OMIM:175780     Brain small vessel disease with or without ocular anomalies     333515  CONFLICTING_PATHOGENICITY_INTERPRETATIONS       1       0.065014        0.035   0.128   ESP_EUROPEAN_AMERICAN   0.0233  THOUSAND_GENOMES=0.01997,TOPMED=0.0068,ESP_EUROPEAN_AMERICAN=0.0233,ESP_ALL=0.0154,EXAC_AFRICAN_INC_AFRICAN_AMERICAN=0.009609841,EXAC_NON_FINNISH_EUROPEAN=0.007491759,GNOMAD_E_AFR=0.013068479,GNOMAD_E_NFE=0.0071611437,GNOMAD_G_NFE=0.013324451      MVP     0.9869305       REVEL=0.886,MVP=0.9869305\n6       6-132203615-G-A_AD      ENPP1   5167    AD      0.0079  0.7695  0.5112  0.9996  0.9996  1       0               rs770775549     6       132203615       132203615       G       A       0       922.9800        PASS    0/1     splice_donor_variant    ENPP1:ENST00000360971.2:c.2230+1G&gt;A:p.? UNCERTAIN_SIGNIFICANCE  PVS1_Strong                             NOT_PROVIDED    0       0.41042 0.292   0.586   GNOMAD_E_SAS    0.0032486517    TOPMED=7.556E-4,EXAC_NON_FINNISH_EUROPEAN=0.0014985314,GNOMAD_E_NFE=0.0017907989,GNOMAD_E_SAS=0.0032486517\n7       11-1018088-TG-T_AD      MUC6    4588    AD      0.0089  0.7563  0.5046  0.9990  0.9990  1       0               rs765231061     11      1018088 1018089 TG      T       -1      441.8100        PASS    0/1     frameshift_variant      MUC6:ENST00000421673.2:c.4712del:p.(Pro1571Hisfs*21)    UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.79622 0.656   0.971   GNOMAD_G_NFE    0.0070363074    GNOMAD_E_AMR=0.0030803352,GNOMAD_G_NFE=0.0070363074\n7       11-1018093-G-GT_AD      MUC6    4588    AD      0.0089  0.7563  0.5046  0.9990  0.9989  0       0               rs376177791     11      1018093 1018093 G       GT      1       592.4500        PASS    0/1     frameshift_elongation   MUC6:ENST00000421673.2:c.4707dup:p.(Pro1570Thrfs*136)   NOT_AVAILABLE                                   NOT_PROVIDED    0       0.79622 0.656   0.971   GNOMAD_G_NFE    0.007835763     GNOMAD_G_NFE=0.007835763\n8       11-1018088-TG-T_AR      MUC6    4588    AR      0.0089  0.7562  0.5046  0.9990  0.9990  1       0               rs765231061     11      1018088 1018089 TG      T       -1      441.8100        PASS    0/1     frameshift_variant      MUC6:ENST00000421673.2:c.4712del:p.(Pro1571Hisfs*21)    UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.79622 0.656   0.971   GNOMAD_G_NFE    0.0070363074    GNOMAD_E_AMR=0.0030803352,GNOMAD_G_NFE=0.0070363074\n8       11-1018093-G-GT_AR      MUC6    4588    AR      0.0089  0.7562  0.5046  0.9990  0.9989  1       0               rs376177791     11      1018093 1018093 G       GT      1       592.4500        PASS    0/1     frameshift_elongation   MUC6:ENST00000421673.2:c.4707dup:p.(Pro1570Thrfs*136)   UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.79622 0.656   0.971   GNOMAD_G_NFE    0.007835763     GNOMAD_G_NFE=0.007835763\n9       7-44610376-G-A_AD       DDX56   54606   AD      0.0091  0.7545  0.5036  0.9992  0.9992  1       0               rs774566321     7       44610376        44610376        G       A       0       586.6600        PASS    0/1     stop_gained     DDX56:ENST00000258772.5:c.991C&gt;T:p.(Arg331*)    UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.56071 0.379   0.852   EXAC_SOUTH_ASIAN        0.006114712     EXAC_SOUTH_ASIAN=0.006114712,GNOMAD_E_SAS=0.0032509754\n10      14-96730313-G-A_AD      BDKRB1  623     AD      0.0093  0.7525  0.5018  1.0000  1.0000  1       0                       14      96730313        96730313        G       A       0       378.2200        PASS    0/1     stop_gained     BDKRB1:ENST00000216629.6:c.294G&gt;A:p.(Trp98*)    UNCERTAIN_SIGNIFICANCE                                  NOT_PROVIDED    0       0.52212 0.272   1.097                                        \n</code></pre>"},{"location":"tutorial/exomiser-tutorial/#tsv-genes","title":"TSV GENES","text":"<p>In the <code>Pfeiffer-HIPHIVE-exome.genes.tsv</code> file, all the various phenotypic scores and HPO matches from the HUMAN, MOUSE, FISH and PPI comparisons are reported per each gene. It is possible for a gene to appear multiple times, depending on the MOI it is compatible with, given the filtered variants. For example in the example below MUC6 is ranked 7th under the AD model and 8th under an AR model. <pre><code>#RANK   ID      GENE_SYMBOL     ENTREZ_GENE_ID  MOI     P-VALUE EXOMISER_GENE_COMBINED_SCORE    EXOMISER_GENE_PHENO_SCORE       EXOMISER_GENE_VARIANT_SCORE     HUMAN_PHENO_SCORE       MOUSE_PHENO_SCORE       FISH_PHENO_SCORE        WALKER_SCORE    PHIVE_ALL_SPECIES_SCORE OMIM_SCORE      MATCHES_CANDIDATE_GENE  HUMAN_PHENO_EVIDENCE    MOUSE_PHENO_EVIDENCE    FISH_PHENO_EVIDENCE     HUMAN_PPI_EVIDENCE      MOUSE_PPI_EVIDENCE      FISH_PPI_EVIDENCE\n1       FGFR2_AD        FGFR2   2263    AD      0.0000  0.9957  0.9187  1.0000  0.8671  0.9187  0.0000  0.5057  0.9187  1.0000  0       Apert syndrome (ORPHA:87): Syndactyly (HP:0001159)-Toe syndactyly (HP:0001770), Strabismus (HP:0000486)-Strabismus (HP:0000486), Hypoplasia of the maxilla (HP:0000327)-Hypoplasia of the maxilla (HP:0000327), Proptosis (HP:0000520)-Proptosis (HP:0000520), Hypertelorism (HP:0000316)-Hypertelorism (HP:0000316), Brachyturricephaly (HP:0000244)-Brachyturricephaly (HP:0000244),  Strabismus (HP:0000486)-ocular hypertelorism (MP:0001300), Hypoplasia of the maxilla (HP:0000327)-short maxilla (MP:0000097), Proptosis (HP:0000520)-exophthalmos (MP:0002750), Hypertelorism (HP:0000316)-ocular hypertelorism (MP:0001300), Brachyturricephaly (HP:0000244)-abnormal frontal bone morphology (MP:0000107),                    Proximity to FGF18 Syndactyly (HP:0001159)-abnormal metatarsal bone morphology (MP:0003072), Strabismus (HP:0000486)-abnormal neurocranium morphology (MP:0000074), Hypoplasia of the maxilla (HP:0000327)-maxilla hypoplasia (MP:0000457), Proptosis (HP:0000520)-abnormal neurocranium morphology (MP:0000074), Hypertelorism (HP:0000316)-abnormal neurocranium morphology (MP:0000074), Brachyturricephaly (HP:0000244)-abnormal neurocranium morphology (MP:0000074),\n2       ZNF366_AD       ZNF366  167465  AD      0.0018  0.9237  0.8195  0.7910  0.0000  0.8195  0.0000  0.5015  0.8195  1.0000  0               Syndactyly (HP:0001159)-syndactyly (MP:0000564), Strabismus (HP:0000486)-microphthalmia (MP:0001297), Hypoplasia of the maxilla (HP:0000327)-micrognathia (MP:0002639), Proptosis (HP:0000520)-microphthalmia (MP:0001297), Hypertelorism (HP:0000316)-microphthalmia (MP:0001297), Brachyturricephaly (HP:0000244)-microphthalmia (MP:0001297),                Proximity to CTBP1 associated with Wolf-Hirschhorn syndrome (ORPHA:280): Syndactyly (HP:0001159)-Arachnodactyly (HP:0001166), Strabismus (HP:0000486)-Strabismus (HP:0000486), Hypoplasia of the maxilla (HP:0000327)-Micrognathia (HP:0000347), Proptosis (HP:0000520)-Proptosis (HP:0000520), Hypertelorism (HP:0000316)-Hypertelorism (HP:0000316), Brachyturricephaly (HP:0000244)-Calvarial skull defect (HP:0001362),\n3       PKD1_AD PKD1    5310    AD      0.0050  0.8272  0.6597  0.8707  0.0000  0.6597  0.2697  0.5069  0.6597  1.0000  0               Strabismus (HP:0000486)-micrognathia (MP:0002639), Hypoplasia of the maxilla (HP:0000327)-micrognathia (MP:0002639), Proptosis (HP:0000520)-micrognathia (MP:0002639), Hypertelorism (HP:0000316)-micrognathia (MP:0002639), Brachyturricephaly (HP:0000244)-micrognathia (MP:0002639),         Hypoplasia of the maxilla (HP:0000327)-mandibular arch skeleton malformed, abnormal (ZP:0001708),       Proximity to IFT88 associated with Retinitis pigmentosa (ORPHA:791): Strabismus (HP:0000486)-Ophthalmoplegia (HP:0000602), Hypoplasia of the maxilla (HP:0000327)-Wide nasal bridge (HP:0000431),       Proximity to IFT88 Syndactyly (HP:0001159)-polydactyly (MP:0000562), Strabismus (HP:0000486)-supernumerary molars (MP:0010773), Hypoplasia of the maxilla (HP:0000327)-supernumerary molars (MP:0010773), Proptosis (HP:0000520)-supernumerary molars (MP:0010773), Hypertelorism (HP:0000316)-supernumerary molars (MP:0010773), Brachyturricephaly (HP:0000244)-abnormal coronal suture morphology (MP:0003840),\n4       CCDC66_AD       CCDC66  285331  AD      0.0051  0.8262  0.5463  0.9984  0.0000  0.5463  0.0000  0.0000  0.5463  1.0000  0               Strabismus (HP:0000486)-abnormal cone electrophysiology (MP:0004022), Hypoplasia of the maxilla (HP:0000327)-abnormal rod electrophysiology (MP:0004021), Proptosis (HP:0000520)-abnormal rod electrophysiology (MP:0004021), Hypertelorism (HP:0000316)-abnormal rod electrophysiology (MP:0004021), Brachyturricephaly (HP:0000244)-abnormal retina photoreceptor layer morphology (MP:0003728),\n5       COL4A1_AD       COL4A1  1282    AD      0.0075  0.7762  0.5288  0.9838  0.3882  0.5288  0.0000  0.5047  0.5288  1.0000  0       Brain small vessel disease with or without ocular anomalies (OMIM:175780): Strabismus (HP:0000486)-Exotropia (HP:0000577),      Strabismus (HP:0000486)-buphthalmos (MP:0009274), Hypoplasia of the maxilla (HP:0000327)-abnormal cornea morphology (MP:0001312), Proptosis (HP:0000520)-abnormal cornea morphology (MP:0001312), Hypertelorism (HP:0000316)-abnormal cornea morphology (MP:0001312), Brachyturricephaly (HP:0000244)-abnormal retina morphology (MP:0001325),          Proximity to COL7A1 associated with Localized dystrophic epidermolysis bullosa, pretibial form (ORPHA:79410): Syndactyly (HP:0001159)-Nail dystrophy (HP:0008404), Hypoplasia of the maxilla (HP:0000327)-Carious teeth (HP:0000670),   Proximity to COL7A1 Syndactyly (HP:0001159)-abnormal digit morphology (MP:0002110), Strabismus (HP:0000486)-abnormal tongue morphology (MP:0000762), Hypoplasia of the maxilla (HP:0000327)-abnormal tongue morphology (MP:0000762), Proptosis (HP:0000520)-abnormal tongue morphology (MP:0000762), Hypertelorism (HP:0000316)-abnormal tongue morphology (MP:0000762),\n6       ENPP1_AD        ENPP1   5167    AD      0.0079  0.7695  0.5112  0.9996  0.3738  0.5112  0.0000  0.5044  0.5112  1.0000  0       Autosomal recessive hypophosphatemic rickets (ORPHA:289176): Hypoplasia of the maxilla (HP:0000327)-Tooth abscess (HP:0030757), Brachyturricephaly (HP:0000244)-Craniosynostosis (HP:0001363),  Syndactyly (HP:0001159)-abnormal elbow joint morphology (MP:0013945), Strabismus (HP:0000486)-abnormal retina morphology (MP:0001325), Hypoplasia of the maxilla (HP:0000327)-abnormal snout skin morphology (MP:0030533), Proptosis (HP:0000520)-abnormal retina morphology (MP:0001325), Hypertelorism (HP:0000316)-abnormal retina morphology (MP:0001325), Brachyturricephaly (HP:0000244)-abnormal retina morphology (MP:0001325),                 Proximity to DMP1 associated with Autosomal recessive hypophosphatemic rickets (ORPHA:289176): Hypoplasia of the maxilla (HP:0000327)-Tooth abscess (HP:0030757), Brachyturricephaly (HP:0000244)-Craniosynostosis (HP:0001363),        Proximity to DMP1 Syndactyly (HP:0001159)-abnormal long bone hypertrophic chondrocyte zone (MP:0000165), Strabismus (HP:0000486)-abnormal dental pulp cavity morphology (MP:0002819), Hypoplasia of the maxilla (HP:0000327)-abnormal dental pulp cavity morphology (MP:0002819), Proptosis (HP:0000520)-abnormal dental pulp cavity morphology (MP:0002819), Hypertelorism (HP:0000316)-abnormal dental pulp cavity morphology (MP:0002819), Brachyturricephaly (HP:0000244)-abnormal dental pulp cavity morphology (MP:0002819),\n7       MUC6_AD MUC6    4588    AD      0.0089  0.7563  0.5046  0.9990  0.0000  0.0000  0.0000  0.5046  0.5046  1.0000  0                               Proximity to GALNT2 associated with Congenital disorder of glycosylation, type IIt (OMIM:618885): Syndactyly (HP:0001159)-Sandal gap (HP:0001852), Strabismus (HP:0000486)-Alternating exotropia (HP:0031717), Hypoplasia of the maxilla (HP:0000327)-Tented upper lip vermilion (HP:0010804), Proptosis (HP:0000520)-Hypertelorism (HP:0000316), Hypertelorism (HP:0000316)-Hypertelorism (HP:0000316), Brachyturricephaly (HP:0000244)-Brachycephaly (HP:0000248),\n8       MUC6_AR MUC6    4588    AR      0.0089  0.7562  0.5046  0.9990  0.0000  0.0000  0.0000  0.5046  0.5046  1.0000  0                               Proximity to GALNT2 associated with Congenital disorder of glycosylation, type IIt (OMIM:618885): Syndactyly (HP:0001159)-Sandal gap (HP:0001852), Strabismus (HP:0000486)-Alternating exotropia (HP:0031717), Hypoplasia of the maxilla (HP:0000327)-Tented upper lip vermilion (HP:0010804), Proptosis (HP:0000520)-Hypertelorism (HP:0000316), Hypertelorism (HP:0000316)-Hypertelorism (HP:0000316), Brachyturricephaly (HP:0000244)-Brachycephaly (HP:0000248),\n9       DDX56_AD        DDX56   54606   AD      0.0091  0.7545  0.5036  0.9992  0.0000  0.0000  0.3788  0.5036  0.5036  1.0000  0                       Brachyturricephaly (HP:0000244)-head decreased width, abnormal (ZP:0000407),            Proximity to PAK1IP1 Strabismus (HP:0000486)-abnormal maxilla morphology (MP:0000455), Hypoplasia of the maxilla (HP:0000327)-abnormal maxilla morphology (MP:0000455), Proptosis (HP:0000520)-abnormal maxilla morphology (MP:0000455), Hypertelorism (HP:0000316)-abnormal maxilla morphology (MP:0000455), Brachyturricephaly (HP:0000244)-decreased forebrain size (MP:0012138),\n10      BDKRB1_AD       BDKRB1  623     AD      0.0093  0.7525  0.5018  1.0000  0.0000  0.0000  0.0000  0.5018  0.5018  1.0000  0                                       Proximity to OPN4 Strabismus (HP:0000486)-abnormal visual pursuit (MP:0006156), Hypoplasia of the maxilla (HP:0000327)-abnormal visual pursuit (MP:0006156), Proptosis (HP:0000520)-abnormal visual pursuit (MP:0006156), Hypertelorism (HP:0000316)-abnormal visual pursuit (MP:0006156), Brachyturricephaly (HP:0000244)-abnormal retina ganglion cell morphology (MP:0008056),\n</code></pre></p>"},{"location":"tutorial/exomiser-tutorial/#docker-for-mac","title":"Docker for Mac","text":"<p>Follow this link and download the Docker.dmg for your operating system.</p> <p></p> <p>The Docker.dmg will be found in your <code>/Downloads</code> directory.</p> <p></p> <p>After double-clicking on the Docker.dmg a new window will come up:</p> <p></p> <p>Drag and drop the Docker app into your <code>/Applications</code> folder. Double-click on the Docker symbol. Docker Desktop will start in the background, after you allow it to be opened.</p> <p></p> <p>Additionally, this window will come up to agree the Docker subscription service agreement.</p> <p></p> <p>After running the installation restart your terminal and check the Docker installation again from inside your terminal with:</p> <pre><code>docker --version\n</code></pre> <p>If the output gives you a version and no error you are ready to go. If you have not already restarted your terminal do this now, and the error should be fixed.</p> <p>In case you get an error message like this, please ensure you have downloaded the correct <code>docker.dmg</code>.</p> <p></p> <p>Now, whenever you want to pull images make sure that Docker is running in the background. Otherwise, you may get an error stating it's not able to connect to the Docker deamon.</p>"},{"location":"tutorial/exomiser-tutorial/#docker-for-windows","title":"Docker for Windows","text":"<p>Follow this link and download the Docker installer for Windows.</p> <p>Inside your <code>/Downloads</code> directory, search for the Installer and double-click.</p> <p></p> <p>To run on Windows Docker requires a virtual machine. Docker recommends using WSL2. More information on this can be found here.</p> <p></p> <p>Click \u201cOk\u201d and wait a bit.</p> <p></p> <p>Now you will have to restart your computer.</p> <p></p> <p>After restarting, Docker should start automatically and the Service Agreement will come up, which you will have to agree to use Docker:</p> <p></p> <p>If the Docker desktop app is showing this warning upon start, do not click \u201cRestart\u201d, yet. Instead, follow the link and install the kernel update.</p> <p></p> <p>The link should point you to an address with a separate download link.</p> <p></p> <p>Start and finish the installation for WSL.</p> <p></p> <p>If you still have the Docker Desktop dialog window open in the background, click on Restart. Otherwise, just restart your computer as you normally do.</p> <p></p> <p>If Docker Desktop did not start on its own, simply open it from the shortcut on your Desktop. You can do the initial orientation by clicking \"Start\".</p> <p></p> <p>After this, your Docker Desktop screen should look like this:</p> <p></p> <p>Now, whenever you want to pull images make sure that Docker is running in the background.</p>"},{"location":"tutorial/fhkb/","title":"Family History Knowledge Base (FHKB) tutorial","text":""},{"location":"tutorial/fhkb/#manchester-family-history-advanced-owl","title":"Manchester Family History Advanced OWL","text":"<p>This is a fork of the infamous Manchester Family History Advanced OWL Tutorial version 1.1, located at</p> <p>http://owl.cs.manchester.ac.uk/publications/talks-and-tutorials/fhkbtutorial/</p> <p>The translation to markdown is not without issue, but we are making a start to making the tutorial a bit more accessible. This reproduction is done with kind permission by Robert Stevens.</p>"},{"location":"tutorial/fhkb/#original-credits-version-11-see-pdf","title":"Original credits (Version 1.1, see pdf):","text":"<p>Authors:</p> <ul> <li>Robert Stevens</li> <li>Margaret Stevens</li> <li>Nicolas Matentzoglu</li> <li>Simon Jupp</li> </ul> <pre><code>Bio-Health Informatics Group\nSchool of Computer Science\nUniversity of Manchester\nOxford Road\nManchester\nUnited Kingdom\nM13 9PL\nrobert.stevens@manchester.ac.uk\n</code></pre>"},{"location":"tutorial/fhkb/#contributors","title":"Contributors","text":"<ul> <li>v 1.0 Robert Stevens, Margaret Stevens, Nicolas Matentzoglu and Simon Jupp</li> <li>v 1.1 Robert Stevens, Nicolas Matentzoglu</li> <li>v 2.0 (Web version) Robert Stevens, Nicolas Matentzoglu, Shawn Tan</li> </ul> <pre><code>The University of Manchester\nCopyright\u00a9 The University of Manchester\nNovember 25, 2015\n</code></pre>"},{"location":"tutorial/fhkb/#acknowledgements","title":"Acknowledgements","text":"<p>This tutorial was realised as part of the Semantic Web Authoring Tool (SWAT) project (see http://www.swatproject.org), which is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/G032459/1, to the University of Manchester, the University of Sussex and the Open University.</p>"},{"location":"tutorial/fhkb/#dedication","title":"Dedication","text":"<p>The Stevens family\u2014all my ancestors were necessary for this to happen. Also, for my Mum who gathered all the information.</p>"},{"location":"tutorial/fhkb/#contents","title":"Contents","text":"<p>Preamble</p> <ul> <li>0.1 Licencing</li> <li>0.2 Reporting Errors</li> <li>0.3 Acknowledgements</li> </ul> <p>1. Introduction</p> <ul> <li>1.1 Learning Outcomes</li> <li>1.2 Why Family History?</li> <li>1.3 How to use this Tutorial</li> <li>1.4 FHKB Resources</li> <li>1.5 Conventions used in this Tutorial</li> </ul> <p>2. Adding some Individuals to the FHKB</p> <ul> <li>2.1 A World of Objects</li> <li>2.2 Asserting Parentage Facts</li> <li>2.3 Summary</li> </ul> <p>3. Ancestors and Descendants</p> <ul> <li>3.1 Ancestors and Descendants</li> <li>3.2 Grandparents and Great Grandparents</li> <li>3.3 Summary</li> </ul> <p>4. Modelling the Person Class</p> <ul> <li>4.1 The Class of Person</li> <li>4.2 Describing Sex in the FHKB</li> <li>4.3 Defining Man and Woman</li> <li>4.4 Describing Parentage in the FHKB</li> <li>4.5 Who has a father?</li> <li>4.6 Filling in Domains and Ranges for the FHKB Properties</li> <li>4.7 Inconsistencies</li> <li>4.8 Adding Some Defined Classes for Ancestors and so on</li> <li>4.9 Summary</li> </ul> <p>5. Siblings in the FHKB</p> <ul> <li>5.1 Blood relations</li> <li>5.2 Siblings: Option One</li> <li>5.2.1 Brothers and Sisters</li> <li>5.3 Siblings: Option two</li> <li>5.3.1 Which Modelling Option to Choose for Siblings?</li> <li>5.4 Half-Siblings</li> <li>5.5 Aunts and Uncles</li> <li>5.6 Summary</li> </ul> <p>6. Individuals in Class Expressions</p> <ul> <li>6.1 Richard and Robert\u2019s Parents and Ancestors</li> <li>6.2 Closing Down What we Know About Parents and Siblings</li> <li>6.3 Summary</li> </ul> <p>7. Data Properties in the FHKB</p> <ul> <li>7.1 Adding Some Data Properties for Event Years - 7.1.1 Counting Numbers of Children</li> <li>7.2 The Open World Assumption</li> <li>7.3 Adding Given and Family Names</li> <li>7.4 Summary</li> </ul> <p>8. Cousins in the FHKB</p> <ul> <li>8.1 Introducing Cousins</li> <li>8.2 First Cousins</li> <li>8.3 Other Degrees and Removes of Cousin</li> <li>8.4 Doing First Cousins Properly</li> <li>8.5 Summary</li> </ul> <p>9. Marriage in the FHKB</p> <ul> <li>9.1 Marriage - 9.1.1 Spouses</li> <li>9.2 In-Laws</li> <li>9.3 Brothers and Sisters In-Law</li> <li>9.4 Aunts and Uncles in-Law</li> <li>9.5 Summary</li> </ul> <p>10. Extending the TBox</p> <ul> <li>10.1 Adding Defined Classes</li> <li>10.2 Summary</li> </ul> <p>11. Final remarks</p> <p>A FHKB Family Data</p> <p>All FHKB owl files for download</p>"},{"location":"tutorial/fhkb/#preamble","title":"Preamble","text":""},{"location":"tutorial/fhkb/#01-licencing","title":"0.1 Licencing","text":"<p>The \u2018Manchester Family History Advanced OWL Tutorial\u2019 by Robert Stevens, Margaret Stevens, Nicolas Matentzoglu, Simon Jupp is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.</p>"},{"location":"tutorial/fhkb/#02-reporting-errors","title":"0.2 Reporting Errors","text":"<p>This manual will almost certainly contain errors, defects and infelicities. Do report them to robert.stevens@manchester.ac.uk supplying chapter, section and some actual context in the form of words will help in fixing any of these issues.</p>"},{"location":"tutorial/fhkb/#03-acknowledgements","title":"0.3 Acknowledgements","text":"<p>As well as the author list, many people have contributed to this work. Any contribution, such as reporting bugs etc., is rewarded by an acknowledgement of contribution (in alphabetical order) when the authors get around to adding them:</p> <ul> <li>Graham Goff;</li> <li>Matthew Horridge;</li> <li>Jared Leo;</li> <li>Fennie Liang;</li> <li>Phil Lord;</li> <li>Fiona McNeill;</li> <li>Eleni Mikroyannidi;</li> <li>George Moulton;</li> <li>Bijan Parsia;</li> <li>Alan Rector;</li> <li>Uli Sattler;</li> <li>Dmitry Tsarkov;</li> <li>Danielle Welter.</li> </ul>"},{"location":"tutorial/fhkb/#chapter-1","title":"Chapter 1","text":""},{"location":"tutorial/fhkb/#introduction","title":"Introduction","text":"<p>This tutorial introduces the tutee to many of the more advanced features of the Web Ontology Language (OWL). The topic of family history is used to take the tutee through various modelling issues and, in doing so, using many features of OWL 2 to build a Family History Knowledge Base (FHKB). The exercises are designed to maximise inference about family history through the use of an automated reasoner on an OWL knowledge base (KB) containing many members of the Stevens family.</p> <p>The aim, therefore, is to enable people to learn advanced features of OWL 2 in a setting that involves both classes and individuals, while attempting to maximise the use of inference within the FHKB.</p>"},{"location":"tutorial/fhkb/#11-learning-outcomes","title":"1.1 Learning Outcomes","text":"<p>By doing this tutorial, a tutee should be able to:</p> <ol> <li>Know about the separation of entities into TBox and ABox;</li> <li>Use classes and individuals in modelling;</li> <li>Write fancy class expressions;</li> <li>Assert facts about individuals;</li> <li>Use the effects of property hierarchies, property characteristics, domain/range constraints to drive    inference;</li> <li>Use constraints and role chains on inferences about individuals;</li> <li>Understand and manage the consequences of the open world assumption in the TBox and ABox;</li> <li>Use nominals in class expressions;</li> <li>Appreciate some limits of OWL 2.</li> </ol>"},{"location":"tutorial/fhkb/#12-why-family-history","title":"1.2 Why Family History?","text":"<p>Building an FHKB enables us to meet our learning outcomes through a topic that is accessible to virtually everyone. Family history or genealogy is a good topic for a general tutorial on OWL 2 as it enables us to touch many features of the language and, importantly, it is a field that everyone knows. All people have a family and therefore a family history \u2013 even if they do not know their particular family history. A small caveat was put on the topic being accessible to everyone as some cultures differ, for instance, in the description of cousins and labels given to different siblings. Nevertheless, family history remains a topic that everyone can talk about.</p> <p>Family history is a good topic for an OWL ontology as it obviously involves both individuals \u2013 the people involved \u2013 and classes of individuals \u2013 people, men and women, cousins, etc. Also, it is an area rich in inference; from only knowing parentage and sex of an individual, it is possible to work out all family relationships \u2013 for example, sharing parents implies a sibling relationship; one\u2019s parent\u2019s brothers are one\u2019s uncles; one\u2019s parent\u2019s parents are one\u2019s grandparents. So, we should be able to construct an ontology that allows us to both express family history, but also to infer family relationships between people from knowing relatively little about them.</p> <p>As we will learn through the tutorial, OWL 2 cannot actually do all that is needed to create a FHKB. This is unfortunate, but we use it to our advantage to illustrate some of the limitations of OWL 2. We know that rule based systems can do family history with ease, but that is not the point here; we are not advocating OWL DL as an appropriate mechanism for doing family history, but we do use it as a good educational example.</p> <p>We make the following assumptions about what people know:</p> <ul> <li>We assume that people know OWL to the level that is known at the end of the Pizza tutorial.   Some ground will be covered again, but a lot of basic OWL is assumed.</li> <li>We assume people know how to use Prot\u00e9g\u00e9 or their OWL environment of choice. We do not give   \u2018click by click\u2019 instructions. At some places, some guidance is given, but this is not to be relied   upon as Prot\u00e9g\u00e9 changes and we will not keep up to date.</li> </ul> <p>We make some simplifying assumptions in this tutorial:</p> <ul> <li>We take a conventional western view of family history. This appears to have most effects on naming   of sibling and cousin relationships.</li> <li>We take a straight-forward view on the sex of people; this is explored further in Chapter 4;</li> <li>A \u2018conventional\u2019 view of marriage is taken; this is explored further in Chapter 9.</li> <li>We make no special treatment of time or dates; we are only interested in years and we do not do   anything fancy; this is explored more in Chapter 7.</li> <li>We assume the ancestors of people go back for ever; obviously this is not true, eventually one would   get back to a primordial soup and one\u2019s ancestors are not humans (members of the classPerson),   but we don\u2019t bother with such niceties.</li> </ul> <p>At the end of the tutorial, you should be able to produce a property hierarchy and a TBox or class hierarchy such as shown in Figure 1.1; all supported by use of the automated reasoner and a lot of OWL 2\u2019s features.</p> <p></p> <p>Figure 1.1: A part of the class and property hierarchy of the final FHKB.</p>"},{"location":"tutorial/fhkb/#13-how-to-use-this-tutorial","title":"1.3 How to use this Tutorial","text":"<p>Here are some tips on using this manual to the best advantage:</p> <ul> <li>Start at the beginning and work towards the end.</li> <li>You can just read the tutorial, but building the FHKB will help you learn much more and much   more easily</li> <li>Use the reasoner in each task; a lot of the FHKB tutorial is about using the reasoner and not doing   so will detract from the learning outcomes.</li> </ul>"},{"location":"tutorial/fhkb/#14-fhkb-resources","title":"1.4 FHKB Resources","text":"<p>The following resources are available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial:</p> <ul> <li>A full version of the Stevens FHKB.</li> <li>Some links to papers about the FHKB.</li> <li>Some slides about the FHKB tutorial.</li> <li>A set of OWL resources for each stage of the FHKB.</li> <li>Some blogs about the FHKB are at http://robertdavidstevens.wordpress.com.</li> </ul>"},{"location":"tutorial/fhkb/#15-conventions-used-in-this-tutorial","title":"1.5 Conventions used in this Tutorial","text":"<ul> <li>All OWL is written in Manchester Syntax.</li> <li>When we use FHKB entities within text, we use a sans serif typeface.</li> <li>We use CamelCase for classes and property names.</li> <li>Class names start with upper case.</li> <li>Individual names start with a lower case letter and internal underscores to break words.</li> <li>Property names usually start with \u2018is\u2019 or \u2018has\u2019 and are CamelCase with a lower case initial letter.</li> <li>Many classes and individuals in the FHKB have annotation properties, usually human readable labels. They show up in some of the examples in Manchester syntax, but are not made explicit as part of the tasks in this tutorial.</li> <li>Every object property is necessarily a sub-property of topObjectProperty. It does not have to be asserted as such. Nevertheless, there might be situations where this relationship is made explicit in this tutorial for illustrative reasons.</li> <li>The individuals we are dealing with represent distinct persons. Throughout the tutorial, once the respective axiom is introduced (chapter 7.1.1), the reader should make sure that all his or her individuals are always made distinct, especially when he or she adds a new one.</li> <li>At the end of each chapter, we note the Description Logic Language (expressivity) needed to represent the ontology and the reasoning times for a number of state of the art reasoning systems. This should get the reader a sense how difficult the FHKB becomes for reasoners to deal with over time.</li> <li>When there is some scary OWL or the reasoner may find the FHKB hard work, you will see a \u2018here   be dragons\u2019 image.<sup>1</sup> </li> </ul> <p><sup>1</sup> The image comes fromhttp://ancienthomeofdragon.homestead.com/May 2012.</p>"},{"location":"tutorial/fhkb/#chapter-2","title":"Chapter 2","text":""},{"location":"tutorial/fhkb/#adding-some-individuals-to-the-fhkb","title":"Adding some Individuals to the FHKB","text":"<p>In this chapter we will start by creating a fresh OWL ontology and adding some individuals that will be surrogates for people in the FHKB. In particular you will:</p> <ol> <li>Create a new OWL ontology for the FHKB;</li> <li>Add some individuals that will stand for members of the Stevens family.</li> <li>Describe parentage of people.</li> <li>Add some facts to specific individuals as to their parentage;</li> <li>See the reasoner doing some work.</li> <li>At the moment we will ignore sex; sex will not happen until Chapter 4.</li> </ol>"},{"location":"tutorial/fhkb/#21-a-world-of-objects","title":"2.1 A World of Objects","text":"<p>The \u2018world\u2019<sup>2</sup> or field of interest we model in an ontology is made up of objects or individuals. Such objects include, but are not limited to:</p> <ul> <li>People, their pets, the pizzas they eat;</li> <li>The processes of cooking pizzas, living, running, jumping, undertaking a journey;</li> <li>The spaces within a room, a bowl, an artery;</li> <li>The attributes of things such as colour, dimensions, speed, shape of various objects;</li> <li>Boundaries, love, ideas, plans, hypotheses.</li> </ul> <p><sup>2</sup> we use \u2018world\u2019 as a synonym of \u2018field of interest\u2019 or \u2018domain\u2019. \u2018World\u2019 does not restrict us to modelling the physical world outside our consciousness.</p> <p>We observe these objects, either outside lying around in the world or in our heads. OWL is all about modelling such individuals. Whenever we make a statement in OWL, when we write down an axiom, we are making statements about individuals. When thinking about the axioms in an ontology it is best to think about the individuals involved, even if OWL individuals do not actually appear in the ontology. All through this tutorial we will always be returning to the individuals being described in order to help us understand what we are doing and to help us make decisions about how to do it.</p>"},{"location":"tutorial/fhkb/#22-asserting-parentage-facts","title":"2.2 Asserting Parentage Facts","text":"<p>Biologically, everyone has parents; a mother and a father<sup>3</sup>. The starting point for family history is parentage; we need to relate the family member objects by object properties. An object property relates two objects, in this case a child object with his or her mother or father object. To do this we need to create three object properties:</p> Task 1: Creating object properties for parentage <ol><li>Create a new ontology; </li><li> Create an object property <code>hasMother</code>; </li><li> Create a property <code>isMotherOf</code> and give <code>hasMother</code> the <code>InverseOf: isMotherOf</code>; </li><li> Do the same for the property <code>hasFather</code>; </li><li> Create a property <code>hasParent</code>; give it the obvious inverse; </li><li> Make <code>hasMother</code> and <code>hasFather</code> sub-properties of <code>hasParent</code>. </li><li> Run the reasoner and look at the property hierarchy.</li></ol> <p>Note how the reasoner has automatically completed the sub-hierarchy for <code>isParentOf:</code> <code>isMotherOf</code> and <code>isFatherOf</code> are inferred to be sub-properties of <code>isParentOf</code>.</p> <p>The OWL snippet below shows some parentage fact assertions on an individual. Note that rather than being assertions to an anonymous individual via some class, we are giving an assertion to a named individual.</p> <pre><code>Individual: grant_plinth\nFacts: hasFather mr_plinth, hasMother mrs_plinth\n</code></pre> <p><sup>3</sup> Don\u2019t quibble; it\u2019s true enough here.</p> Task 2: Create the ABox <ol><li>Using the information in Table A.1 (see appendix) about parentage (so the columns about fathers and mothers), enter the fact assertions for the people which appear in rows shaded in grey. We will only use the <code>hasMother</code> and <code>hasFather</code> properties in our fact assertions. You do not need to assert names and birth years yet. This exercise will require you to create an individual for every person we want to talk about, using the <code>Firstname_Secondname_Familyname_Birthyear</code> pattern, as for example in <code>Robert_David_Bright_1965</code>.</li></ol> <p></p> <p>While asserting facts about all individuals in the FHKB will be a bit tedious at times, it might be useful to at least do the task for a subset of the family members. For the impatient reader, there is a convenience snapshot of the ontology including the raw individuals available below.</p> <p> <pre><code>If you are working with Prot\u00e9g\u00e9, you may want to look at the Matrix plugin for\nProt\u00e9g\u00e9 at this point. The plugin allows you to add individuals quickly in the\nform of a regular table, and can significantly reduce the effort of adding any type\nof entity to the ontology. In order to install the matrix plugin, open Prot\u00e9g\u00e9 and\ngo to File \u00bb Check for plugins. Select the \u2018Matrix Views\u2019 plugin. Click install,\nwait until the the installation is confirmed, close and re-open Prot\u00e9g\u00e9; go to the\n\u2018Window\u2019 menu item, select \u2018Tabs\u2019 and add the \u2018Individuals matrix\u2019.\n</code></pre></p> <p>Now do the following:</p> Task 3: DL queries <ol><li> Classify the FHKB. </li><li> Issue the DL query <code>hasFather</code> value <code>David_Bright_1934</code> and look at the answers (remember to check the respective checkbox in Prot\u00e9g\u00e9 to include individuals in your query results). </li><li> Issue the DL query <code>isFatherOf</code> value <code>Robert_David_Bright_1965</code>. Look at the answers. 4. Look at the entailed facts on <code>Robert_David_Bright_1965</code>.</li></ol> <p>You should find the following:</p> <ul> <li>David Bright (1934) is the father of Robert David Bright (1965) and Richard John Bright (1962).</li> <li>Robert David Bright (1965) has David Bright 1934 as a parent.</li> </ul> <p>Since we have said that <code>isFatherOf</code> has an inverse of <code>hasFather</code>, and we have asserted that <code>Robert_David_Bright_1965 hasFather David_Bright_1934</code>, we have a simple entailment that <code>David_Bright_1934 isFatherOf Robert_David_Bright_1965</code>. So, without asserting the <code>isFatherOf</code> facts, we have been able to ask and get answers for that DL query.</p> <p>As we asserted that <code>Robert_David_Bright_1965 hasFather David_Bright_1934</code>, we also infer that he <code>hasParent</code> <code>David_Bright_1934</code>; this is because <code>hasParent</code> is the super-property of <code>hasFather</code> and the sub-property implies the super-property. This works all the way up the property tree until <code>topObjectProperty</code>, so all individuals are related by <code>topObjectProperty</code>\u2014this is always true. This implication \u2018upwards\u2019 is the way to interpret how the property hierarchies work.</p>"},{"location":"tutorial/fhkb/#23-summary","title":"2.3 Summary","text":"<p>We have now covered the basics of dealing with individuals in OWL ontologies. We have set up some properties, but without domains, ranges, appropriate characteristics and then arranged them in a hierarchy. From only a few assertions in our FHKB, we can already infer many facts about an individual: Simple exploitation of inverses of properties and super-properties of the asserted properties.</p> <p>We have also encountered some important principles:</p> <ul> <li>We get inverses for free.</li> <li>The sub-property implies the super-property. So, <code>hasFather</code> implies the <code>hasParent</code> fact between individuals. This entailment of the super-property is very important and will drive much of the inference we do with the FHKB.</li> <li>Upon reasoning we get the inverses of properties between named individuals for free.</li> <li>Lots is still open. For example, we do not know the sex of individuals and what other children, other than those described, people in the FHKB may have.</li> </ul> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of ALHI.\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 0.026 sec (0.00001 % of final), by Pellet\n2.2.0 0.144 sec (0.00116 % of final) and by FaCT++ 1.6.4 is approximately 0.\nsec (0.000 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-3","title":"Chapter 3","text":""},{"location":"tutorial/fhkb/#ancestors-and-descendants","title":"Ancestors and Descendants","text":"<p>In this Chapter you will:</p> <ol> <li>Use sub-properties and the transitive property characteristic to infer ancestors of people;</li> <li>Add properties to the FHKB property hierarchy that will infer ancestors and descendants of a    person without adding any more facts to the FHKB;</li> <li>Explore the use of sub-property chains for grandparents, great grandparents and so on;</li> <li>Place all of these new object properties in the property hierarchy and in that way learn more about    the implications of the property hierarchy.</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage  below.</p>"},{"location":"tutorial/fhkb/#31-ancestors-and-descendants","title":"3.1 Ancestors and Descendants","text":"<p>The FHKB has parents established between individuals and we know that all people have two parents. A parent is an ancestor of its children; a person\u2019s parent\u2019s parents are its ancestors; and so on. So, in our FHKB, Robert\u2019s ancestors are David, Margaret, William, Iris, Charles, Violet, James, another Violet, another William, Sarah and so on. If my parent\u2019s parents are my ancestors, then what we need is a transitive version of the <code>hasParent</code> property. Obviously we do not want <code>hasParent</code> to be transitive, as Robert\u2019s grandparents (and so on) would become his parents (and that would be wrong).</p> <p>We can easily achieve what is necessary. We need a <code>hasAncestor</code> property that has a transitive characteristic. The trick is to make this a super-property of the <code>hasParent</code> property. As explained before, a sub-property implies its super-property. So, if individual x holds a <code>hasParent</code> property with an individual y , then it also holds an instance of its super-property <code>hasAncestor</code> with the individual y. If individual y then holds a <code>hasParent</code> property with another individual z , then there is also, by implication, a <code>hasAncestor</code> property between y and z. As <code>hasAncestor</code> is transitive, x and z also hold a <code>hasAncestor</code> relationship between them.</p> <p>The inverse of <code>hasAncestor</code> can either be <code>isAncestorOf</code> or <code>hasDescendant</code>. We choose the <code>isAncestorOf</code> option.</p> Task 4: Object properties: exploiting the semantics <ol><li>Make a new object property <code>hasRelation</code>, make it symmetric. </li><li> Make a new object property <code>hasAncestor</code>. </li><li> Make it a sub-property of <code>hasRelation</code> and a super-property of <code>hasParent</code>. </li><li> Make <code>hasAncestor</code> transitive. </li><li> Create the inverse <code>isAncestorOf</code>. Do not \u2018stitch\u2019 it into the property hierarchy; the reasoner will sort it all out for you. </li><li> Run the reasoner and issue the DL query <code>hasAncestor value William_George_Bright_1901</code>. </li><li> Issue the query <code>isAncestorOf value Robert_David_Bright_1965</code>.</li></ol> <p>The <code>hasAncestor</code> object property will look like this:</p> <pre><code>ObjectProperty: hasAncestor\nSubPropertyOf: hasRelation\nSuperPropertyOf: hasParent,\nCharacteristics: Transitive\nInverseOf: isAncestorOf\n</code></pre> <p>As usual, it is best to think of the objects or individuals involved in the relationships. Consider the three individuals \u2013 Robert, David and William. Each has a <code>hasFather</code> property, linking Robert to David and then David to William. As <code>hasFather</code> implies its super-property <code>hasParent</code>, Robert also has a <code>hasParent</code> property with David, and David has a <code>hasParent</code> relation to William. Similarly, as <code>hasParent</code> implies <code>hasAncestor</code>, the Robert object has a <code>hasAncestor</code> relation to the David object and the David object has one to the William object. As <code>hasAncestor</code> is transitive, Robert not only holds this property to the David object, but also to the William object (and so on back through Robert\u2019s ancestors).</p>"},{"location":"tutorial/fhkb/#32-grandparents-and-great-grandparents","title":"3.2 Grandparents and Great Grandparents","text":"<p>We also want to use a sort of restricted transitivity in order to infer grandparents, great grandparents and so on. My grandparents are my parent\u2019s parents; my grandfathers are my parent\u2019s fathers. My great grandparents are my parent\u2019s parent\u2019s parents. My great grandmothers are my parent\u2019s parent\u2019s mothers. This is sort of like transitivity, but we want to make the paths only a certain length and, in the case of grandfathers, we want to move along two relationships \u2013 <code>hasParent</code> and then <code>hasFather</code>.</p> <p>We can do this with OWL 2\u2019s sub-property chains. The way to think about sub-property chains is: If we see property x followed by property y linking three objects, then it implies that property z is held between</p> <p></p> <p>Figure 3.1: Three blobs representing objects of the classPerson. The three objects are linked by a <code>hasParent</code> property and this implies a <code>hasGrandparent</code> property.</p> <p>the first and third objects. Figure 3.1 shows this diagrammatically for the <code>hasGrandfather</code> property.</p> <p>For various grandparent object properties we need the following sets of implications:</p> <ul> <li>My parent\u2019s parents are my grandparents;</li> <li>My parent\u2019s fathers are my grandfathers;</li> <li>My parent\u2019s mothers are my grandmothers;</li> <li>My parent\u2019s parent\u2019s parents are my great grandparents or my grandparent\u2019s parents are my great   grandparents.</li> <li>My parent\u2019s parent\u2019s fathers are my great grandfathers or my parent\u2019s grandfathers are my great   grandfathers;</li> <li>My parent\u2019s parent\u2019s mothers are my great grandmothers (and so on).</li> </ul> <p>Notice that we can trace the paths in several ways, some have more steps than others, though the shorter paths themselves employ paths. Tracing these paths is what OWL 2\u2019s sub-property chains achieve. For the new object property <code>hasGrandparent</code> we write:</p> <pre><code>ObjectProperty: hasGrandparent SubPropertyChain: hasParent o hasParent\n</code></pre> <p>We read this as \u2018<code>hasParent</code> followed by <code>hasParent</code> implies <code>hasGrandparent</code>\u2019. We also need to think where the <code>hasGrandparent</code> property fits in our growing hierarchy of object properties. Think about the implications: Does holding a <code>hasParent</code> property between two objects imply that they also hold a <code>hasGrandparent</code> property? Of course the answer is \u2018no\u2019. So, this new property is not a super-property of <code>hasParent</code>. Does the holding of a <code>hasGrandparent</code> property between two objects imply that they also hold an <code>hasAncestor</code> property? The answer is \u2018yes\u2019; so that should be a super-property of <code>hasGrandparent</code>. We need to ask such questions of our existing properties to work out where we put it in the object property hierarchy. At the moment, our <code>hasGrandparent</code> property will look like this:</p> <pre><code>ObjectProperty: hasGrandParent\nSubPropertyOf: hasAncestor\nSubPropertyChain: hasParent o hasParent\nSuperPropertyOf: hasGrandmother, hasGrandfather\nInverseOf: isGrandParentOf\n</code></pre> <p>Do the following task:</p> Task 5: Grandparents object properties <ol><li>Make the <code>hasGrandparent</code>, <code>hasGrandmother</code> and <code>hasGrandfather</code> object properties and the obvious inverses (see OWL code above); </li><li> Go to the individuals tabs and inspects the inferred object property assertions for <code>Robert_David_Bright_1965</code> and his parents.</li></ol> <p>Again, think of the objects involved. We can take the same three objects as before: Robert, David and William. Think about the properties that exist, both by assertion and implication, between these objects. We have asserted only <code>hasFather</code> between these objects. The inverse can be inferred between the actual individuals (remember that this is not the case for class level restrictions \u2013 that all instances of a class hold a property does not mean that the filler objects at the other end hold the inverse; the quantification on the restriction tells us this). Remember that:</p> <ol> <li>Robert holds a <code>hasFather</code> property with David;</li> <li>David holds a <code>hasFather</code> property with William;</li> <li>By implication through the <code>hasParent</code> super-property of <code>hasFather</code>, Robert holds a <code>hasParent</code> property with David, and the latter holds one with William;</li> <li>The sub-property chain on <code>hasGrandfather</code> then implies that Robert holds a <code>hasGrandfather</code> property to William. Use the diagram in figure 3.1 to trace the path; there is a <code>hasParent</code> path from Robert to William via David and this implies the <code>hasGrandfather</code> property between Robert and William.</li> </ol> <p>It is also useful to point out that the inverse of <code>hasGrandfather</code> also has the implication of the sub-property chain of the inverses of <code>hasParent</code>. That is, three objects linked by a path of two <code>isParentOf</code> properties implies that an <code>isGrandfatherOf</code> property is established between the first and third object, in this case William and Robert. As the inverses of <code>hasFather</code> are established by the reasoner, all the inverse implications also hold.</p>"},{"location":"tutorial/fhkb/#33-summary","title":"3.3 Summary","text":"<p>It is important when dealing with property hierarchies to think in terms of properties between objects and of the implications \u2018up the hierarchy\u2019. A sub-property implies its super-property. So, in our FHKB, two person objects holding a <code>hasParent</code> property between them, by implication also hold an <code>hasAncestor</code> property between them. In turn, <code>hasAncestor</code> has a super-property <code>hasRelation</code> and the two objects in question also hold, by implication, this property between them as well.</p> <p>We made <code>hasAncestor</code> transitive. This means that my ancestor\u2019s ancestors are also my ancestors. That a sub-property is transitive does not imply that its super-property is transitive. We have seen that by manipulating the property hierarchy we can generate a lot of inferences without adding any more facts to the individuals in the FHKB. This will be a feature of the whole process \u2013 keep the work to the minimum (well, almost).</p> <p>In OWL 2, we can also trace \u2018paths\u2019 around objects. Again, think of the objects involved in the path of properties that link objects together. We have done simple paths so far \u2013 Robert linked to David via <code>hasParent</code> and David linked to William via <code>hasFather</code> implies the link between Robert and William of <code>hasGrandfather</code>. If this is true for all cases (for which you have to use your domain knowledge), one can capture this implication in the property hierarchy. Again, we are making our work easier by adding no new explicit facts, but making use of the implication that the reasoner works out for us.</p> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity ofALRI+.\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 0.262 sec (0.00014 % of final), by Pellet\n2.2.0 0.030 sec (0.00024 % of final) and by FaCT++ 1.6.4 is approximately 0.004\nsec (0.000 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-4","title":"Chapter 4","text":""},{"location":"tutorial/fhkb/#modelling-the-person-class","title":"Modelling the Person Class","text":"<p>In this Chapter you will:</p> <ol> <li>Create a <code>Person</code> class;</li> <li>Describe <code>Sex</code> classes;</li> <li>Define <code>Man</code> and <code>Woman</code>;</li> <li>Ask which of the people in the FHKB has a father.</li> <li>Add domains and ranges to the properties in the FHKB.</li> <li>Make the FHKB inconsistent.</li> <li>Add some more defined classes about people and see some equivalence inferred between classes.</li> </ol> <p>These simple classes will form the structure for the whole FHKB.</p>"},{"location":"tutorial/fhkb/#41-the-class-of-person","title":"4.1 The Class of Person","text":"<p>For the FHKB, we start by thinking about the objects involved</p> <ol> <li>The people in a family \u2013 Robert, Richard, David, Margaret, William, Iris, Charles, Violet, Eileen,    John and Peter;</li> <li>The sex of each of those people;</li> <li>The marriages in which they participated;</li> <li>The locations of their births;</li> <li>And many more...</li> </ol> <p>There is a class of <code>Person</code> that we will use to represent all these people objects.</p> Task 6: Create the <code>Person</code> class <ol><li>Create a class called <code>DomainEntity</code>; </li><li>Create a subclass of <code>DomainEntity</code> called <code>Person</code>.</li></ol> <p>We use <code>DomainEntity</code> as a house-keeping measure. All of our ontology goes underneath this class. We can put other classes \u2018outside\u2019 the ontology, as siblings of <code>DomainEntity</code>, such as \u2018probe\u2019 classes we wish to use to test our ontology.</p> <p>The main thing to remember about the <code>Person</code> class is that we are using it to represent all \u2018people\u2019 individuals. When we make statements about the <code>Person</code> class, we are making statements about all \u2018people\u2019 individuals.</p> <p>What do we know about people? All members of the <code>Person</code> class have:</p> <ul> <li>Sex \u2013 they are either male or female;</li> <li>Everyone has a birth year;</li> <li>Everyone has a mother and a father.</li> </ul> <p>There\u2019s a lot more we know about people, but we will not mention it here.</p>"},{"location":"tutorial/fhkb/#42-describing-sex-in-the-fhkb","title":"4.2 Describing Sex in the FHKB","text":"<p>Each and every person object has a sex. In the FHKB we will take a simple view on sex \u2013 a person is either male or female, with no intersex or administrative sex and so on. Each person only has one sex.</p> <p>We have two straight-forward options for modelling sex:</p> <ol> <li>Each person object has their own sex object, which is either male or female. Thus Robert\u2019s maleness is different from David\u2019s maleness.</li> <li>There is only one Maleness object and one Femaleness object and each person object has a relationship to either one of these sex objects, but not both.</li> </ol> <p>We will take the approach of having a class of Maleness objects and a class of Femaleness objects. These are qualities or attributes of self-standing objects such as a person. These two classes are disjoint, and each is a subclass of a class called <code>Sex</code>. The disjointness means that any one instance of <code>Sex</code> cannot be both an instance of <code>Maleness</code> and an instance of <code>Femaleness</code> at once. We also want to put in a covering axiom on the class <code>Sex</code>, which means that any instance of <code>Sex</code> must be either <code>Maleness</code> or <code>Femaleness</code>; there is no other kind of <code>Sex</code>.</p> <p></p> <p>Again, notice that we have been thinking at the level of objects. We do the same when thinking about <code>Person</code> and their <code>Sex</code>. Each and every person is related to an instance of <code>Sex</code>. Each <code>Person</code> holds one relationship to a <code>Sex</code> object. To do this we create an object property called <code>hasSex</code>. We make this property functional, which means that any object can hold that property to only one distinct filler object.</p> <p>We make the domain of <code>hasSex</code> to be <code>Person</code> and the range to be <code>Sex</code>. The domain of <code>Person</code> means that any object holding that property will be inferred to be a member of the class <code>Person</code>. Putting the range of <code>Sex</code> on the <code>hasSex</code> property means that any object at the right-hand end of the <code>hasSex</code> property will be inferred to be of the class <code>Sex</code>. Again, think at the level of individuals or objects.</p> <p>We now put a restriction on the <code>Person</code> class to state that each and every instance of the class <code>Person</code> holds a <code>hasSex</code> property with an instance of the <code>Sex</code> class. It has an existential operator \u2018some\u2019 in the axiom, but the functional characteristic means that each <code>Person</code> object will hold only one <code>hasSex</code> property to a distinct instance of a <code>Sex</code> object<sup>4</sup>.</p> <p><sup>4</sup> An individual could hold two <code>hasSex</code> properties, as long as the sex objects at the right-hand end of the property are not different.</p> Task 7: Modelling sex <ol><li>Create a class called <code>Sex</code>; </li><li> Make it a subclass of <code>DomainEntity</code>; </li><li> Make <code>Person</code> and <code>Sex</code> disjoint; </li><li> Create two subclasses of <code>Sex</code>, <code>Maleness</code> and <code>Femaleness</code>; </li><li> Make <code>Maleness</code> and <code>Femaleness</code> disjoint; </li><li> Put a covering axiom on <code>Sex</code> such that it is equivalent to <code>Maleness</code> or <code>Femaleness</code>. </li><li> Create an object property, <code>hasSex</code> , with the domain <code>Person</code>, the range <code>Sex</code> and give it the characteristic of \u2018Functional\u2019; </li><li> Add a restriction <code>hasSex some Sex</code> to the class <code>Person</code>.</li></ol> <p>The <code>hasSex</code> property looks like:</p> <pre><code>ObjectProperty: hasSex\nCharacteristics: Functional\nDomain: Person\nRange: Sex\n</code></pre> <p>The <code>Person</code> class looks like:</p> <pre><code>Class: Person\nSubClassOf: DomainEntity,(hasSex some Sex)\nDisjointWith: Sex\n</code></pre>"},{"location":"tutorial/fhkb/#43-defining-man-and-woman","title":"4.3 Defining Man and Woman","text":"<p>We now have some of the foundations for the FHKB. We have the concept of <code>Person</code>, but we also need to have the concepts of <code>Man</code> and <code>Woman</code>. Now we have <code>Person</code>, together with <code>Maleness</code> and <code>Femaleness</code>, we have the necessary components to define <code>Man</code> and <code>Woman</code>. These two classes can be defined as: Any <code>Person</code> object that has a male sex can be recognised to be a man; any <code>Person</code> object that has a female sex can be recognised as a member of the class woman. Again, think about what conditions are sufficient for an object to be recognised to be a member of a class; this is how we create defined classes through the use of OWL equivalence axioms.</p> <p>To make the <code>Man</code> and <code>Woman</code> classes do the following:</p> Task 8: Describe men and women <ol><li>Create a class <code>Man</code>; </li><li> Make it equivalent to a <code>Person that hasSex some Maleness</code>; </li><li> Do the same, but with <code>Femaleness</code>, to create the <code>Woman</code> class; </li><li> A covering axiom can be put on the <code>Person</code> class to indicate that man and woman are the only kinds of person that can exist. (This is not strictly true due to the way <code>Sex</code> has been described.) </li><li> Run the reasoner and take a look.</li></ol> <p>Having run the reasoner, the <code>Man</code> and <code>Woman</code> classes should appear underneath <code>Person</code><sup>5</sup>.</p> <p><sup>5</sup>Actually in Prot\u00e9g\u00e9, this might happen without the need to run the reasoner.</p> <p>The <code>Man</code> and <code>Woman</code> classes will be important for use as domain and range constraints on many of the properties used in the FHKB. To achieve our aim of maximising inference, we should be able to infer that individuals are members of <code>Man</code>, <code>Woman</code> or <code>Person</code> by the properties held by an object. We should not have to state the type of an individual in the FHKB.</p> <p>The classes for <code>Man</code> and <code>Woman</code> should look like:</p> <pre><code>Class: Man\nEquivalentTo: Person and (hasSex some Maleness)\n</code></pre> <pre><code>Class: Woman\nEquivalentTo: Person and (hasSex some Femaleness)\n</code></pre>"},{"location":"tutorial/fhkb/#44-describing-parentage-in-the-fhkb","title":"4.4 Describing Parentage in the FHKB","text":"<p>To finish off the foundations of the FHKB we need to describe a person object\u2019s parentage. We know that each and every person has one mother and each and every person has one father. Here we are talking about biological mothers and fathers. The complexities of adoption and step parents are outside the scope of this FHKB tutorial.</p> Task 9: Describing Parentage <ol><li> Add the domain <code>Person</code> and the range <code>Woman</code> to the property <code>hasMother</code>.</li><li> Do the same for the property <code>hasFather</code>, but give it the range <code>Man</code>;</li><li> Give the property <code>hasParent</code> domain and range of <code>Person</code>;</li><li> Run the reasoner.</li></ol> <p>The (inferred) property hierarchy in the FHKB should look like that shown in Figure 4.1. Notice that we have asserted the sub-property axioms on one side of the property hierarchy. Having done so, the reasoner uses those axioms, together with the inverses, to work out the property hierarchy for the \u2018other side\u2019.</p> <p>We make <code>hasMother</code> functional, as any one person object can hold only one <code>hasMother</code> property to a distinct <code>Woman</code> object. The range of <code>hasMother</code> is <code>Woman</code>, as a mother has to be a woman. The <code>Person</code> object holding the <code>hasMother</code> property can be either a man or a woman, so we have the domain constraint as <code>Person</code>; this means any object holding a <code>hasMother</code> property will be inferred to be a <code>Person</code>. Similarly, any object at the right-hand end of a <code>hasMother</code> property will be inferred to be a <code>Woman</code>, which is the result we need. The same reasoning goes for <code>hasFather</code> and <code>hasParent</code>, with the sex constraints on the latter being only <code>Person</code>. The inverses of the two functional sub-properties of <code>hasParent</code> are not themselves functional. After all, a <code>Woman</code> can be the mother of many <code>Person</code> objects, but each <code>Person</code> object can have only one mother.</p> <p></p> <p>Figure 4.1: The property hierarchy with the <code>hasSex</code> and the parentage properties</p> <p></p> <p>Figure 4.2: the core TBox for the FHKB with the <code>Person</code> and <code>Sex</code> classes.</p> Task 10: Restrict Person class <ol><li>As each and every person has a mother and each and every person has a father, place restrictions on the <code>Person</code> class as shown below.</li></ol> <pre><code>Class: Person\nSubClassOf: DomainEntity, (hasFather some Man), (hasMother some Woman),\n(hasSex some Sex)\nDisjointWith: Sex\n</code></pre> Task 11: DL queries for people and sex <ol><li>Issue the DL queries for <code>Person</code>, <code>Man</code> and <code>Woman</code>; look at the answers and count the numbers in each class; which individuals have no sex and why? </li><li> You should find that many people have been inferred to be either <code>Man</code> or <code>Woman</code>, but some are, as we will see below, only inferred to be <code>Person</code>.</li></ol> <p>The domain and range constraints on our properties have also driven some entailments. We have not asserted that <code>David_Bright_1934</code> is a member of <code>Man</code>, but the range constraint on <code>hasFather</code> (or the inferred domain constraint on the <code>isFatherOf</code> relation) has enabled this inference to be made. This goes for any individual that is the right-hand-side (either inferred or asserted) of either <code>hasFather</code> or <code>hasMother</code> (where the range is that of <code>Woman</code>). For Robert David Bright, however, he is only the left-hand-side of an <code>hasFather</code> or an <code>hasMother</code> property, so we\u2019ve only entailed that this individual is a member of <code>Person</code>.</p>"},{"location":"tutorial/fhkb/#45-who-has-a-father","title":"4.5 Who has a father?","text":"<p>In our description of the <code>Person</code> class we have said that each and every instance of the class <code>Person</code> has a father (the same goes for mothers). So, when we ask the query \u2018which individuals have a father\u2019, we get all the instances of <code>Person</code> back, even though we have said nothing about the specific parentage of each <code>Person</code>. We do not know who their mothers and fathers are, but we know that they have one of each. We know all the individuals so far entered are members of the <code>Person</code> class; when asserting the type to be either <code>Man</code> or <code>Woman</code> (each of which is a subclass of <code>Person</code>), we infer that each is a person. When asserting the type of each individual via the <code>hasSex</code> property, we know each is a <code>Person</code>, as the domain of <code>hasSex</code> is the <code>Person</code> class. As we have also given the right-hand side of <code>hasSex</code> as either <code>Maleness</code> or <code>Femaleness</code>, we have given sufficient information to recognise each of these <code>Person</code> instances to be members of either <code>Man</code> or <code>Woman</code>.</p>"},{"location":"tutorial/fhkb/#46-filling-in-domains-and-ranges-for-the-fhkb-properties","title":"4.6 Filling in Domains and Ranges for the FHKB Properties","text":"<p>So far we have not systematically added domains and ranges to the properties in the FHKB. As a reminder, when a property has a domain of <code>X</code> any object holding that property will be inferred to be a member of class <code>X</code>. A domain doesn\u2019t add a constraint that only members of class <code>X</code> hold that property; it is a strong implication of class membership. Similarly, a property holding a range implies that an object acting as right-hand-side to a property will be inferred to be of that class. We have already seen above that we can use domains and ranges to imply the sex of people within the FHKB.</p> <p>Do the following:</p> Task 12: Domains and Ranges <ol><li> Make sure the appropriate <code>Person</code>, <code>Man</code> and <code>Woman</code> are domains and ranges for <code>hasFather</code>, <code>hasMother</code> and <code>hasParent</code>. </li><li> Run the reasoner and look at the property hierarchy. </li><li> Also look at the properties <code>hasAncestor</code>, <code>hasGrandparent</code>, <code>hasUncle</code> and so on; look to see what domains and ranges are found. Add any domains and ranges explicitly as necessary.</li></ol> <p> <pre><code>Prot\u00e9g\u00e9 for example in its current version (November 2015) does not visualise\ninherited domains and ranges in the same way as it shows inferred inverse relations.\n</code></pre></p> <p>We typically assert more domains and ranges than strictly necessary. For example, if we say that <code>hasParent</code> has the domain <code>Person</code>, this means that every object <code>x</code> that is connected to another object <code>y</code> via the <code>hasParent</code> relation must be a <code>Person</code>. Let us assume the only thing we said about <code>x</code> and <code>y</code> is that they are connected by a <code>hasMother</code> relation. Since this implies that <code>x</code> and <code>y</code> are also connected by a <code>hasParent</code> relation (<code>hasMother</code> is a sub-property of <code>hasParent</code>) we do not have to assert that <code>hasFather</code> has the domain of <code>Person</code>; it is implied by what we know about the domain and range of <code>hasParent</code>.</p> <p>In order to remove as many assertions as possible, we may therefore choose to assert as much as we know starting from the top of the hierarchy, and only ever adding a domain if we want to constrain the already inferred domain even further (or range respectively). For example, in our case, we could have chosen to assert <code>Person</code> to be the domain of <code>hasRelation</code>. Since <code>hasRelation</code> is symmetric, it will also infer <code>Person</code> to be the range. We do not need to say anything for <code>hasAncestor</code> or <code>hasParent</code>, and only if we want to constrain the domain or range further (like in the case of <code>hasFather</code> by making the range <code>Man</code>) do we need to actually assert something. It is worth noting that because we have built the object property hierarchy from the bottom (<code>hasMother</code> etc.) we have ended up asserting more than necessary.</p>"},{"location":"tutorial/fhkb/#47-inconsistencies","title":"4.7 Inconsistencies","text":"<p>From the Pizza Tutorial and other work with OWL you should have seen some unsatisfiabilities. In Prot\u00e9g\u00e9 this is highlighted by classes going \u2018red\u2019 and being subclasses ofNothing; that is, they can have no instances in that model.</p> Task 13: Inconsistencies <ol><li> Add the fact <code>Robert_David_Bright_1965 hasMother David_Bright_1934</code>. </li><li> Run the classifier and see what happens. </li><li> Remove that fact and run the classifier again. </li><li> Now add the fact that <code>Robert_David_Bright_1965 hasMother Iris_Ellen_Archer_1907</code> </li><li> Run the classifier and see what happens. </li><li> Add and remove the functional characteristic to these properties and see what happens.</li></ol> <p>After asserting the first fact it should be reported by the reasoner that the ontology is inconsistent. This means, in lay terms, that the model you\u2019ve provided in the ontology cannot accommodate the facts you\u2019ve provided in the fact assertions in your ABox\u2014that is, there is an inconsistency between the facts and the ontology... The ontology is inconsistent because <code>David_Bright_1934</code> is being inferred to be a <code>Man</code> and a <code>Woman</code> at the same time which is inconsistent with what we have said in the FHKB.</p> <p>When we, however, say that <code>Robert David Bright</code> has two different mothers, nothing bad happens! Our domain knowledge says that the two women are different, but the reasoner does not know this as yet... ; Iris Ellen Archer and Margaret Grace Rever may be the same person; we have to tell the reasoner that they are different. For the same reason the functional characteristic also has no effect until the reasoner \u2018knows\u2019 that the individuals are different. We will do this in Section 7.1.1 and live with this \u2018fault\u2019 for the moment.</p> <p></p>"},{"location":"tutorial/fhkb/#48-adding-some-defined-classes-for-ancestors-and-so-on","title":"4.8 Adding Some Defined Classes for Ancestors and so on","text":"Task 14: Adding defined classes <ol><li> Add a defined class for <code>Ancestor</code>, <code>MaleAncestor</code>, <code>FemaleAncestor</code>; </li><li> Add a defined class for <code>Descendant</code>, <code>MaleDescendant</code> and <code>FemaleDescendant</code>; </li><li> Run the reasoner and view the resulting hierarchy.</li></ol> <p>The code for the classes looks like:</p> <pre><code>Class: Ancestor EquivalentTo: Person and isAncestorOf some Person\nClass: FemaleAncestor EquivalentTo: Woman and isAncestorOf some Person\nClass: Descendant EquivalentTo: Person and hasAncestor some Person\nClass: MaleDescendant EquivalentTo: Man and hasAncestor some Person\n</code></pre> <p>The TBox after reasoning can be seen in Figure 4.3. Notice that the reasoner has inferred that several of the classes are equivalent or \u2018the same\u2019. These are: <code>Descendant</code> and <code>Person</code>; <code>MaleDescendant</code> and <code>Man</code>, <code>FemaleDescendant</code> and <code>Woman</code>.</p> <p>The reasoner has used the axioms within the ontology to infer that all the instances of <code>Person</code> are also instances of the class <code>Descendant</code> and that all the instances of <code>Woman</code> are also the same instances as the class <code>Female Descendant</code>. This is intuitively true; all people are descendants \u2013 they all have parents that have parents etc. and thus everyone is a descendant. All women are female people that have parents etc. As usual we should think about the objects within the classes and what we know about them. This time it is useful to think about the statements we have made about <code>Person</code> in this Chapter \u2013 that all instances of <code>Person</code> have a father and a mother; add to this the information from the property hierarchy and we know that all instances of <code>Person</code> have parents and ancestors. We have repeated all of this in our new defined classes for <code>Ancestor</code> and <code>Descendant</code> and the reasoner has highlighted this information.</p> <p></p> <p>Figure 4.3: The defined classes from Section 4.8 in the FHKB\u2019s growing class hierarchy</p> Task 15: More Ancestors <ol><li>Query for <code>MaleDescendant</code>. You should get <code>Man</code> back - they are equivalent (and this makes sense). </li><li> As an additional exercise, also add in properties for forefathers and foremothers. You will follow the same pattern as for <code>hasAncestor</code>, but adding in, for instance, <code>hasFather</code> as the sub-property of the transitive super-property of <code>hasForefather</code> and setting the domains and ranges appropriately (or working out if they\u2019ll be inferred appropriately). Here we interpret a forefather as one\u2019s father\u2019s father etc. This isn\u2019t quite right, as a forefather is any male ancestor, but we\u2019ll do it that way anyway. You might want to play around with DL queries. Because of the blowup in inferred relationships, we decided to not include this pattern in the tutorial version of the FHKB.</li></ol>"},{"location":"tutorial/fhkb/#49-summary","title":"4.9 Summary","text":"<p>Most of what we have done in this chapter is straight-forward OWL, all of which would have been met in the pizza tutorial. It is, however, a useful revision and it sets the stage for refining the FHKB. Figure 4.2 shows the basic set-up we have in the FHKB in terms of classes; we have a class to represent person, man and woman, all set-up with a description of sex, maleness and femaleness. It is important to note, however, the approach we have taken: We have always thought in terms of the objects we are modelling.</p> <p>Here are some things that should now be understood upon completing this chapter:</p> <ol> <li>Restrictions on a class in our TBox mean we know stuff about individuals that are members of that class, even though we have asserted no facts on those individuals. We have said, for instance, that all members of the class <code>Person</code> have a mother, so any individual asserted to be a <code>Person</code> must have a mother. We do not necessarily know who they are, but we know they have one.</li> <li>Some precision is missing \u2013 we only know Robert David Bright is a <code>Person</code>, not that he is a <code>Man</code>. This is because, so far, he only has the domain constraint of <code>hasMother</code> and <code>hasFather</code> to help out.</li> <li>We can cause the ontology to be inconsistent, for example by providing facts that cannot be accommodated by the model of our ontology. In the example, David Bright was inferred to be a member of two disjoint classes.</li> </ol> <p>Finally, we looked at some defined classes. We inferred equivalence between some classes where the extents of the classes were inferred to be the same \u2013 in this case the extents of <code>Person</code> and <code>Descendant</code> are the same. That is, all the objects that can appear in <code>Person</code> will also be members of <code>Descendant</code>. We can check this implication intuitively \u2013 all people are descendants of someone. Perhaps not the most profound inference of all time, but we did no real work to place this observation in the FHKB.</p> <p></p> <pre><code>This last point is a good general observation. We can make the reasoner do work\nfor us. The less maintenance we have to do in the FHKB the better. This will be\na principle that works throughout the tutorial.\n</code></pre> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of SRIF.\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 0.884 sec (0.00047 % of final), by Pellet\n2.2.0 0.256 sec (0.00207 % of final) and by FaCT++ 1.6.4 is approximately 0.013\nsec (0.000 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-5","title":"Chapter 5","text":""},{"location":"tutorial/fhkb/#siblings-in-the-fhkb","title":"Siblings in the FHKB","text":"<p>In this chapter you will:</p> <ol> <li>Explore options for determining finding siblings;</li> <li>Meet some of the limitations in OWL;</li> <li>Choose one of the options explored;</li> <li>Add facts for siblings;</li> <li>Use sub-property chains to find aunts and uncles;</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage  below.</p>"},{"location":"tutorial/fhkb/#51-blood-relations","title":"5.1 Blood relations","text":"<p>Do the following first:</p> Task 16: The bloodrelation object property <ol><li> Create an <code>hasBloodrelation</code> object property, making it a sub-property of <code>hasRelation</code>. </li><li> Add appropriate property characteristics. </li><li> Make the already existing <code>hasAncestor</code> property a sub-property of <code>hasBloodrelation</code>.</li></ol> <p>Does a blood relation of Robert have the same relationship to Robert (symmetry)? Is a blood relation of Robert\u2019s blood relation a blood relation of Robert (transitivity)? Think of an aunt by marriage; her children are my cousins and blood relations via my uncle, but my aunt is not my blood relation. My siblings share parents; male siblings are brothers and female siblings are sisters. So far we have asserted parentage facts for the <code>Person</code> in our ABox. Remember that our parentage properties have inverses, so if we have added an <code>hasFather</code> property between a <code>Person</code> and a <code>Man</code>, we infer the <code>isFatherOf</code> property between that <code>Man</code> and that <code>Person</code>.</p>"},{"location":"tutorial/fhkb/#52-siblings-option-one","title":"5.2 Siblings: Option One","text":"<p>We should have enough information within the FHKB to infer siblings. We could use a sub-property chain such as:</p> <pre><code>ObjectProperty: hasSibling\nSubPropertyOf: hasBloodrelation\nCharacteristics: Symmetric, transitive\nSubPropertyChain: hasParent o isParentOf\n</code></pre> <p>We make a property of <code>hasSibling</code> and make it a sub-property of <code>hasBloodrelation</code>. Remember, think of the objects involved and the implications we want to follow; being a sibling implies being a blood relation, it does not imply any of the other relationships we have in the FHKB.</p> <p>Note that we have made <code>hasSibling</code> symmetric; if Robert is sibling of Richard, then Richard is sibling of Robert. We should also think about transitivity; if David is sibling of Peter and Peter is sibling of John, then David is sibling of John. So, we make <code>hasSibling</code> symmetric and transitive (see Figure 5.1). However, we must take care of half-siblings: child 1 and child 2 share a mother, but not a father; child 2 and child 3 share the father, but not the mother \u2013 child 1 and child 3 are not even half-siblings. However, at least for the moment, we will simply ignore this inconvenience, largely so that we can explore what happens with different modelling options.</p> <p></p> <p></p> <p>Figure 5.1: Showing the symmetry and transitivity of the <code>hasSibling</code> (siblingof) property by looking at the brothers David, John and Peter</p> <p>We also have the implication using three objects (see Figure 5.2):</p> <ol> <li>Robert holds a <code>hasParent</code> property with David;</li> <li>David holds an <code>isFatherOf</code> property with Richard;</li> <li>This implies that Robert holds a <code>hasSibling</code> property with Richard;</li> <li>As <code>hasSibling</code> is symmetric, Richard holds an <code>hasSibling</code> property with Robert.</li> </ol> <p></p> <p>Figure 5.2: Tracing out the sub-property chain for <code>hasSibling</code>; note that Robert is a sibling of himself by this path</p> <p>Do the following tasks:</p> Task 17: Siblings <ol><li>Add the <code>hasSibling</code> property as above; </li><li> Run the reasoner; </li><li> Ask the DL query <code>hasSibling</code> value <code>Robert_David_Bright_1965</code>.</li></ol> <p>From this last DL query you should get the answer that both Robert and Richard are siblings of Robert. Think about the objects involved in the sub-property chain: we go from Robert to David via the <code>hasParent</code> and from David to Richard via the <code>isParentOf</code> property; so this is OK. However, we also go from Robert to David and then we can go from David back to Robert again \u2013 so Robert is a sibling of Robert. We do not want this to be true.</p> <p></p> <p>We can add another characteristic to the <code>hasSibling</code> property, the one of being <code>irreflexive</code>. This means that an object cannot hold the property with itself.</p> Task 18: More siblings <ol><li> Add the irreflexive characteristic to the <code>hasSibling</code> property; </li><li> Run the reasoner;</li></ol> <p>Note that the reasoner claims you have an inconsistent ontology (or in some cases, you might get a message box saying \"Reasoner died\"). Looking at the <code>hasSibling</code> property again, the reason might not be immediately obvious. The reason for the inconsistency lies in the fact that we create a logical contradiction: through the property chain, we say that every <code>Person</code> is a sibling of him or herself, and again disallowing just that by adding the irreflexive characteristic. A different explanation lies within the OWL specification itself: In order to maintain decidability irreflexive properties must be simple - for example, they may not be property chains<sup>6</sup>.</p> <p></p> <p><sup>6</sup> http://www.w3.org/TR/owl2-syntax/#The_Restrictions_on_the_Axiom_Closure</p>"},{"location":"tutorial/fhkb/#521-brothers-and-sisters","title":"5.2.1 Brothers and Sisters","text":"<p>We have only done siblings, but we obviously need to account for brothers and sisters. In an analogous way to motherhood, fatherhood and parenthood, we can talk about sex specific sibling relationships implying the sex neutral <code>hasSibling</code>; holding either a <code>hasBrother</code> or an <code>isSisterOf</code> between two objects would imply that a <code>hasSibling</code> property is also held between those two objects. This means that we can place these two sex specific sibling properties below <code>hasSibling</code> with ease. Note, however, that unlike the <code>hasSibling</code> property, the brother and sister properties are not symmetric. Robert <code>hasBrother</code> Richard and vice versa , but if Daisy <code>hasBrother</code> William, we do not want William to hold an <code>hasBrother</code> property with Daisy. Instead, we create an inverse of <code>hasBrother</code>, <code>isBrotherOf</code>, and the do the same for <code>isSisterOf</code>.</p> <p>We use similar, object based, thought processes to choose whether to have transitivity as a characteristic of <code>hasBrother</code>. Think of some sibling objects or individuals and place <code>hasBrother</code> properties between them. Make it transitive and see if you get the right answers. Put in a sister to and see if it stil works. If David <code>hasBrother</code> Peter and Peter <code>hasBrother</code> John, then David <code>hasBrother</code> John; so, transitivity works in this case. Think of another example. Daisy <code>hasBrother</code> Frederick, and Frederick <code>hasBrother</code> William, thus Daisy <code>hasBrother</code> William. The inverses work in the same way; William <code>isBrotherOf</code> Frederick and Frederick <code>isBrotherOf</code> Daisy; thus William <code>isBrotherOf</code> Daisy. All this seems reasonable.</p> Task 19: Brothers and sisters <ol><li> Create the <code>hasBrother</code> object property as shown below; </li><li> Add <code>hasSister</code> in a similar manner; 3. Add appropriate inverses, domains and ranges.</li></ol> <pre><code>ObjectProperty: hasBrother\nSubPropertyOf: hasSibling\nCharacteristics: Transitive\nInverseOf: isBrotherOf\nRange: Man\n</code></pre> <p>We have some <code>hasSibling</code> properties (even if they are wrong). We also know the sex of many of the people in the FHKB through the domains and ranges of properties such as <code>hasFather</code>, <code>hasMother</code> and their inverses..</p> <p>Can we use sub-property chains in the same way as we have used them in the <code>hasSibling</code> property? The issue is that of sex; the property <code>isFatherOf</code> is sex neutral at the child end, as is the inverse <code>hasFather</code> (the same obviously goes for the mother properties). We could use a sub-property chain of the form:</p> <pre><code>ObjectProperty: hasBrother\nSubPropertyChain: hasParent o hasSon\n</code></pre> <p>A son is a male child and thus that object is a brother of his siblings. At the moment we do not have son or daughter properties. We can construct a property hierarchy as shown in Figure 5.3. This is made up from the following properties:</p> <ul> <li><code>hasChild</code> and <code>isChildOf</code></li> <li><code>hasSon</code>(range <code>Man</code> and domain <code>Person</code>) and <code>isSonOf</code>;</li> <li><code>hasDaughter</code>(range <code>Woman</code> domain <code>Person</code>) and <code>isDaughterOf</code></li> </ul> <p>Note that <code>hasChild</code> is the equivalent of the existing property <code>isParentOf</code>; if I have a child, then I am its parent. OWL 2 can accommodate this fact. We can add an equivalent property axiom in the following way:</p> <pre><code>ObjectProperty: isChildOf\nEquivalentTo: hasParent\n</code></pre> <p>We have no way of inferring the <code>isSonOf</code> and <code>isDaughterOf</code> from what already exists. What we want to happen is the implication of \u2018<code>Man</code> and <code>hasParent</code> <code>Person</code> implies <code>isSonOf</code>\u2019. OWL 2 and its reasoners cannot do this implication. It has been called the \u2018man man problem\u2019<sup>7</sup>. Solutions for this have been developed [3], but are not part of OWL 2 and its reasoners.</p> <p></p> <p>Figure 5.3: The property hierarchy for <code>isChildOf</code> and associated son/daughter properties</p> <p><sup>7</sup> http://lists.w3.org/Archives/Public/public-owl-dev/2007JulSep/0177.html</p> Child property Parent Robert David Bright 1965 isSonOf David Bright 1934, Margaret Grace Rever 1934 Richard John Bright 1962 isSonOf David Bright 1934, Margaret Grace Rever 1934 Mark Bright 1956 isSonOf John Bright 1930, Joyce Gosport Ian Bright 1959 isSonOf John Bright 1930, Joyce Gosport Janet Bright 1964 isDaughterOf John Bright 1930, Joyce Gosport William Bright 1970 isSonOf John Bright 1930, Joyce Gosport <p>Table 5.1: Child property assertions for the FHKB</p> <p>Thus we must resort to hand assertions of properties to test out our new path:</p> Task 20: Sons and daughters <ol><li> Add the property hierarchy shown in Figure 5.3, together with the equivalent property axiom and the obvious inverses. </li><li> As a test (after running the reasoner), ask the DL query <code>isChildOf</code> value <code>David_Bright_1934</code> and you should have the answer of Richard and Robert; </li><li> Add the sub-property paths as described in the text; </li><li> Add the assertions shown in Table 5.1; </li><li> Run the reasoner; </li><li> Ask the DL query for the brother of Robert David Bright and the sister of Janet.</li></ol> <p>Of course, it works, but we see the same problem as above. As usual, think of the objects involved. Robert <code>isSonOf</code> David and David <code>isParentOf</code> Robert, so Robert is his own brother. Irreflexivity again causes problems as it does above (Task 18).</p> <p></p>"},{"location":"tutorial/fhkb/#53-siblings-option-two","title":"5.3 Siblings: Option two","text":"<p>Our option one has lots of problems. So, we have an option of asserting the various levels of sibling. We can take the same basic structure of sibling properties as before, but just fiddle around a bit and rely on more assertion while still trying to infer as much as possible. We will take the following approach:</p> <ul> <li>We will take off the sub-property chains of the sibling properties as they do not work;</li> <li>We will assert the leaf properties of the sibling sub-hierarchy sparsely and attempt to infer as much   as possible.</li> </ul> Person Property Person Robert David Bright 1965 isBrotherOf Richard John Bright 1962 David Bright 1934 isBrotherOf John Bright 1930 David Bright 1934 isBrotherOf Peter William Bright 1941 Janet Bright 1964 isSisterOf Mark Bright 1956 Janet Bright 1964 isSisterOf Ian Bright 1959 Janet Bright 1964 isSisterOf William Bright 1970 Mark Bright 1956 isBrotherOf Ian Bright 1959 Mark Bright 1956 isBrotherOf Janet Bright 1964 Mark Bright 1956 isBrotherOf William Bright 1970 <p>Table 5.2: The sibling relationships to add to the FHKB.</p> <p>Do the following:</p> Task 21: Add sibling assertions <ol><li> Remove the sub-property chains of the sibling properties and the <code>isChildOf</code> assertions as explained above. </li><li> Add the Sibling assertions shown in table 5.2; </li><li> Run the reasoner; </li><li> Ask <code>isBrotherOf</code> value <code>Robert_David_Bright_1965</code>; </li><li> Ask <code>isBrotherOf</code> value <code>Richard_John_Bright_1962</code>; </li><li> Ask <code>hasBrother</code> value <code>Robert_David_Bright_1965</code>; </li><li> Ask <code>hasBrother</code> value <code>Richard_John_Bright_1962</code>;</li><li> Ask <code>isSisterOf</code> value <code>William_Bright_1970</code>; </li><li> Ask the query <code>Man and hasSibling value Robert_David_Bright_1965</code>.</li></ol> <p>We can see some problems with this option as well:</p> <ul> <li>With these properties asserted, Richard only has a <code>hasBrother</code> property to Robert. We would really like an <code>isBrotherOf</code> to Robert to hold.</li> <li>The query <code>Man</code> and <code>hasSibling value Robert</code> only retrieves Robert himself. Because we only asserted that Robert is a brother of Richard, and the domain of <code>isBrotherOf</code> is <code>Man</code> we know that Robert is a <code>Man</code>, but we do not know anything about the <code>Sex</code> of Richard.</li> </ul>"},{"location":"tutorial/fhkb/#531-which-modelling-option-to-choose-for-siblings","title":"5.3.1 Which Modelling Option to Choose for Siblings?","text":"<p>Which of the two options gives the worse answers and which is the least effort? Option one is obviously the least effort; we only have to assert the same parentage facts as we already have; then the sub-property chains do the rest. It works OK for <code>hasSibling</code>, but we cannot do brothers and sisters adequately; we need <code>Man</code> and <code>hasSibling</code> \u2290 <code>isBrotherOf</code> and we cannot do that implication. This means we cannot ask the questions we need to ask.</p> <p></p> <p>So, we do option two, even though it is hard work and is still not perfect for query answering, even though we have gone for a sparse assertion mode. Doing full sibling assertion would work, but is a lot of effort.</p> <p>We could start again and use the <code>isSonOfandisDaughterOf</code> option, with the sub-property chains described above. This still has the problem of everyone being their own sibling. It can get the sex specific sibling relationships, but requires a wholesale re-assertion of parentage facts. We will continue with option two, largely because it highlights some nice problems later on.</p>"},{"location":"tutorial/fhkb/#54-half-siblings","title":"5.4 Half-Siblings","text":"<p>In Section 5.2 we briefly talked about half-siblings. So far, we have assumed full-siblings (or, rather, just talked about siblings and made no distinction). Ideally, we would like to accommodate distinctions between full- and half-siblings; here we use half-siblings, where only one parent is in common between two individuals, as the example. The short-answer is, unfortunately, that OWL 2 cannot deal with half-siblings in the way that we want - that is, such that we can infer properties between named individuals indicating full- or half-sibling relationships.</p> <p>It is possible to find sets of half-brothers in the FHKB by writing a defined class or DL-query for a particular individual.} The following fragment of OWL defines a class that looks for the half-brothers of an individual called \u2018Percival\u2019:</p> <p></p> <pre><code>Class: HalfBrotherOfPercival\nEquivalentTo: Man and (((hasFather some (not (isFatherOf value Percival))) and\n(hasMother some (isMotherOf value Percival))) or ((hasFather some (isFatherOf\nvalue Percival)) and (hasMother some (not (isMotherOf value Percival)))))\n</code></pre> <p>Here we are asking for any man that either has Percival\u2019s father but not his mother, or his mother, but not his father. This works fine, but is obviously not a general solution. The OWL description is quite complex and the writing will not scale as the number of options (hypothetically, as the number of parents increases... ) increases; it is fine for man/woman, but go any higher and it will become very tedious to write all the combinations.</p> <p>Another way of doing this half-brother class to find the set of half-brothers of a individual is to use cardinality constraints:</p> <pre><code>Class: HalfBrotherOfPercival\nEquivalentTo: Man and (hasParent exactly 1 (isParentOf value Percival))\n</code></pre> <p>This is more succinct. We are asking for a man that has exactly one parent from the class of individuals that are the class of Percival\u2019s parents. This works, but one more constraint has to be present in the FHKB. We need to make sure that there can be only two parents (or indeed, just a specified number of parents for a person). If we leave it open as to the number of parents a person has, the reasoner cannot work out that there is a man that shares exactly one parent, as there may be other parents. We added this constraint to the FHKB in Section 6.2; try out the classes to check that they work.</p> <p>These two solutions have been about finding sets of half-brothers for an individual. What we really want in the FHKB is to find half-brothers between any given pair of individuals.</p> <p>Unfortunately we cannot, without rules, ask OWL 2 to distinguish full- and half-siblings \u2013 we cannot count the number of routes taken between siblings via different distinct intermediate parent objects.</p>"},{"location":"tutorial/fhkb/#55-aunts-and-uncles","title":"5.5 Aunts and Uncles","text":"<p>An uncle is a brother of either my mother or father. An aunt is a sister of either my mother or father. In common practice, wives and husbands of aunts and uncles are usually uncles and aunts respectively. Formally, these aunts and uncles are aunts-in-law and uncles-in-law. Whatever approach we take, we cannot fully account for aunts and uncles until we have information about marriages, which will not have until Chapter 9. We will, however, do the first part now.</p> <p>Look at the objects and properties between them for the following facts:</p> <ul> <li>Robert has father David and mother Margaret;</li> <li>David has brothers Peter and John;</li> <li>Margaret has a sister Eileen;</li> <li>Robert thus has the uncles John and Peter and an aunt Eileen.</li> </ul> <p>As we are tracing paths or \u2018chains\u2019 of objects and properties we should use sub-property chains as a solution for the aunts and uncles. We can make an <code>hasUncle</code> property as follows (see Figure 5.4):</p> <pre><code>ObjectProperty: hasUncle\nSubPropertyOf: hasBloodrelation\nDomain: Man\nRange: Person\nSubPropertyChain: hasParent o hasBrother\nInverseOf: isUncleOf\n</code></pre> <p></p> <p>Figure 5.4: Tracing out the path between objects to get the <code>hasUncle</code> sub-property chain.</p> <p>Notice we have the domain of <code>Man</code> and range of <code>Person</code>. We also have an inverse. As usual, we can read this as \u2018an object that holds an <code>hasParent</code> property, followed by an object holding a <code>hasBrother</code> property, implies that the first object holds an <code>hasUncle</code> property with the last object\u2019.</p> <p>Note also where the properties (include the ones for aunt) go in the object property hierarchy. Aunts and uncles are not ancestors that are in the direct blood line of a person, but they are blood relations (in the narrower definition that we are using). Thus the aunt and uncle properties go under the <code>hasBloodrelation</code> property (see Figure 5.5). Again, think of the implications between objects holding a property between them; that two objects linked by a property implies that those two objects also hold all the property\u2019s super-properties as well. As long as all the super-properties are true, the place in the object property hierarchy is correct (think about the implications going up, rather than down).</p> <p></p> <p>Figure 5.5: The object property hierarchy with the aunt and uncle properties included. On the right side, we can see the hasUncle property as shown by Prot\u00e9g\u00e9.</p> <p>Do the following tasks:</p> Task 22: Uncles and Aunts <ol><li>Add the <code>hasUncle</code> property as above; </li><li> Add the <code>hasAunt</code> property as well; </li><li> Ask for the uncles of <code>Julie_Bright_1966</code> and for <code>Mark_Bright_1956</code>; </li><li> Add similar properties for <code>hasGreatUncle</code> and <code>hasGreatAunt</code> and place them in the property hierarchy.</li></ol> <p>We can see this works \u2013 unless we have any gaps in the sibling relationships (you may have to fix these). Great aunts and uncles are simply a matter of adding another \u2018parent\u2019 leg into the sub-property chain. We are not really learning anything new with aunts and uncles, except that we keep gaining a lot for</p> <p>free through sub-property chains. We just add a new property with its sub-property chain and we get a whole lot more inferences on individuals. To see what we now know about Robert David Bright, do the following:</p> Task 23: What do we know? <ol><li> Save the ontology and run the reasoner; </li><li> Look at inferences related to the individual Robert David Bright (see warning in the beginning of this chapter). </li><li> If you chose to use DL queries in Prot\u00e9g\u00e9, do not forget to tick the appropriate check-boxes.</li></ol> <p>You can now see lots of facts about Robert David Bright, with only a very few actual assertions directly on Robert David Bright.</p>"},{"location":"tutorial/fhkb/#56-summary","title":"5.6 Summary","text":"<p>Siblings have revealed several things for us:</p> <ul> <li>We can use just the parentage facts to find siblings, but everyone ends up being their own sibling;</li> <li>We cannot make the properties irreflexive, as the knowledge base becomes inconsistent;</li> <li>We would like an implication of <code>Man</code> and <code>hasSibling</code> \u2283 <code>isBrotherOf</code>, but OWL 2 doesn\u2019t do this implication;</li> <li>Whatever way we model siblings, we end up with a bit of a mess;</li> <li>OWL 2 cannot do half-siblings;</li> <li>However, we can get close enough and we can start inferring lots of facts via sub-property chains using the sibling relationships.</li> </ul> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity ofSRIF.\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 1355.614 sec (0.71682 % of final), by\nPellet 2.2.0 0.206 sec (0.00167 % of final) and by FaCT++ 1.6.4 is approximately\n0.039 sec (0.001 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-6","title":"Chapter 6","text":""},{"location":"tutorial/fhkb/#individuals-in-class-expressions","title":"Individuals in Class Expressions","text":"<p>In this chapter you will:</p> <ol> <li>Use individuals within class expressions;</li> <li>Make classes to find Robert and Richard\u2019s parents, ancestors, and so on;</li> <li>Explore equivalence of such classes;</li> <li>Re-visit the closed world.</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage  below.</p>"},{"location":"tutorial/fhkb/#61-richard-and-roberts-parents-and-ancestors","title":"6.1 Richard and Robert\u2019s Parents and Ancestors","text":"<p>So far we have only used object properties between unspecified objects. We can, however, specify a specific individual to act at the right-hand-side of a class restriction or type assertion on an individual. The basic syntax for so-called nominals is:</p> <pre><code>Class: ParentOfRobert\nEquivalentTo: Person and isParentOf valueRobert_David_Bright_1965\n</code></pre> <p>This is an equivalence axiom that recognises any individual that is a <code>Person</code> and a parent of Robert David Bright.</p> Task 24: Robert and Richards parents <ol><li> Create the class <code>ParentOfRobert</code> as described above; </li><li> Classify \u2013 inspect where the class is placed in the FHKB TBox and look at which individuals classify as members of the class; </li><li> Do the same for a class with the value of <code>Richard_John_Bright_1962</code> and classify; </li><li> Finally create a class <code>ParentOfRichardAndRobert</code>, defining it as <code>Person and isParentOf some {Robert_David_Bright_1965 ,Richard_John_Bright_1962 }</code>; again see what happens on classification. Note that the expressions <code>isMotherOf value Robert_David_Bright_1965</code> and <code>isMotherOf some {Robert_David_Bright_1965 }</code> are practically identical. The only difference is that using <code>value</code>, you can only specify one individual, while <code>some</code> relates to a class (a set of individuals).</li></ol> <p>We see that these queries work and that we can create more complex nominal based class expressions. The disjunction above is</p> <pre><code>isParentOf some {Robert_David_Bright_1965, Richard_John_Bright_1965}\n</code></pre> <p>The \u2018{\u2019 and \u2018}\u2019 are a bit of syntax that says \u2018here\u2019s a class of individual\u2019.</p> <p>We also see that the classes for the parents of Robert David Bright and Richard John Bright have the same members according to the FHKB, but that the two classes are not inferred to be equivalent. Our domain knowledge indicates the two classes have the same extents (members) and thus the classes are equivalent, but the automated reasoner does not make this inference. As usual, this is because the FHKB has not given the automated reasoner enough information to make such an inference.</p>"},{"location":"tutorial/fhkb/#62-closing-down-what-we-know-about-parents-and-siblings","title":"6.2 Closing Down What we Know About Parents and Siblings","text":"<p>The classes describing the parents of Richard and Robert are not equivalent, even though, as humans, we know their classes of parent are the same. We need more constraints so that it is known that the four parents are the only ones that exist. We can try this by closing down what we know about the immediate family of Robert David Bright.</p> <p>In Chapter 4 we described that a <code>Person</code> has exactly one <code>Woman</code> and exactly one <code>Man</code> as mother and father (by saying that the <code>hasMother</code> and <code>hasFather</code> properties are functional and thus only one of each may be held by any one individual to distinct individuals). The parent properties are defined in terms of <code>hasParent</code>, <code>hasMother</code> and <code>hasFather</code>. The latter two imply <code>hasParent</code>. The two sub-properties are functional, but there are no constraints on <code>hasParent</code>, so an individual can hold many instances of this property. So, there is no information in the FHKB to say a <code>Person</code> has only two parents (we say there is one mother and one father, but not that there are only two parents). Thus Robert and Richard could have other parents and other grandparents than those in the FHKB; we have to close down our descriptions so that only two parents are possible. There are two ways of doing this:</p> <ol> <li>Using qualified cardinality constraints in a class restriction;</li> <li>Putting a covering axiom on <code>hasParent</code> in the same way as we did for <code>Sex</code> in Chapter 4.</li> </ol> Task 25: Closing the Person class <ol><li> Add the restriction <code>hasParent</code> exactly 2 <code>Person</code> to the <code>classPerson</code>; </li><li> Run the reasoner; </li><li> Inspect the hierarchy to see where <code>ParentOfRobert</code> and <code>ParentOfRichard</code> are placed and whether or not they are found to be equivalent; </li><li> Now add the restriction <code>hasParent max 2 Person</code> to the class <code>Person</code>; </li><li> Run the reasoner (taking note of how long the reasoning takes) and take another look.</li></ol> <p></p> <p>We find that these two classes are equivalent; we have supplied enough information to infer that these two classes are equivalent. So, we know that option one above works, but what about option two? This takes a bit of care to think through, but the basic thing is to think about how many ways there are to have a <code>hasParent</code> relationship between two individuals. We know that we can have either a <code>hasFather</code> or a <code>hasMother</code> property between two individuals; we also know that we can have only one of each of these properties between an individual and a distinct individual. However, the open world assumption tells us that there may be other ways of having a <code>hasParent</code> property between two individuals; we\u2019ve not closed the possibilities. By putting on the <code>hasParent exactly 2 Person</code> restriction on the <code>Person</code> class, we are effectively closing down the options for ways that a person can have parents; we know because of the functional characteristic on <code>hasMother</code> and <code>hasFather</code> that we can have only one of each of these and the two restrictions say that one of each must exist. So, we know we have two ways of having a parent on each <code>Person</code> individual. So, when we say that there are exactly two parents (no more and no less) we have closed down the world of having parents\u2014thus these two classes can be inferred to be equivalent. It is also worth noting that this extra axiom on the <code>Person</code> class will make the reasoner run much more slowly.</p> <p>Finally, for option 2, we have no way of placing a covering axiom on a property. What we\u2019d like to be able to state is something like:</p> <pre><code>ObjectProperty: hasParent\nEquivalentTo: hasFather or hasMother\n</code></pre> <p>but we can\u2019t.</p>"},{"location":"tutorial/fhkb/#63-summary","title":"6.3 Summary","text":"<p>For practice, do the following:</p> Task 26: Additional Practice <ol><li> Add lots more classes using members of the ABox as nominals; </li><li> Make complex expressions using nominals; </li><li> After each addition of a nominal, classify and see what has been inferred within the FHKB. </li><li> See if you can make classes for <code>GrandparentOfRobert</code> and <code>GrandparentOfRichard</code> and make them inferred to be equivalent.</li></ol> <p>In this chapter we have seen the use of individuals within class expressions. It allows us to make useful queries and class definitions. The main things to note is that it can be done and that there is some syntax involved. More importantly, some inferences may not be as expected due to the open world assumption in OWL.</p> <p></p> <pre><code>By now you might have noticed a significant increase in the time the reasoner needs\nto classify. Closing down what we know about family relationships takes its toll on\nthe reasoner performance, especially the usage of 'hasParent exactly 2 Person'. At\nthis point we recommend rewriting this axiom to 'hasParent max 2 Person'. It gives\nus most of what we need, but has a little less negative impact on the reasoning\ntime.\n</code></pre> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ.\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 2067.273 sec (1.09313 % of final), by\nPellet 2.2.0 0.529 sec (0.00428 % of final) and by FaCT++ 1.6.4 is approximately\n0.147 sec (0.004 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-7","title":"Chapter 7","text":""},{"location":"tutorial/fhkb/#data-properties-in-the-fhkb","title":"Data Properties in the FHKB","text":"<p>We now have some individuals with some basic object properties between individuals. OWL 2, however, also has data properties that can relate an object or individual to some item of data. There are data about a <code>Person</code>, such as years of events and names etc. So, in this Chapter you will:</p> <ol> <li>Make some data properties to describe event years to people;</li> <li>Create some simple defined classes that group people by when they were born;</li> <li>Try counting the numbers of children people have...</li> <li>Deal with the open world assumption;</li> <li>Add given and family names to individuals in the FHKB.</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage  below.</p>"},{"location":"tutorial/fhkb/#71-adding-some-data-properties-for-event-years","title":"7.1 Adding Some Data Properties for Event Years","text":"<p>Everyone has a birth year; death year; and some have a marriage year and so on. We can model these simply with data properties and an integer as a filler. OWL 2 has a DateTime datatype, where it is possible to specify a precise time and date down to a second. <sup>7</sup> This proves cumbersome (see http://robertdavidstevens.wordpress.com/2011/05/05/using-the-datetime-data-type-to-describe-birthdays/ for details); all we need is a simple indication of the year in which a person was born. Of course, the integer type has a zero, which the Gregorian calendar for which we use integer as a proxy does not, but integer is sufficient to our needs. Also, there are various ontological treatments of time and information about people (this extends to names etc. as well), but we gloss over that here\u2014that\u2019s another tutorial.</p> <p><sup>7</sup> http://www.w3.org/TR/2008/WD-owl2-quick-reference-20081202/#Built-in_Datatypes_and_Facets</p> <p>We can have dates for birth, death and (eventually) marriage (see Chapter 9) and we can just think of these as event years. We can make a little hierarchy of event years as shown in Figure 7.1).</p> Task 27: Create a data property hierarchy <ol><li> Create the data property <code>hasEventYear</code> with range integer and domain <code>Person</code>; </li><li> Create the data property <code>hasBirthYear</code> and make it a sub-property of <code>hasEventYear</code> (that way, the domain and range of <code>hasEventYear</code> are inherited); </li><li> Create the data property <code>hasDeathYear</code> and make it a sub-property of <code>hasEventYear</code>; </li><li> For each individual add the birth years shown in Table A.1 (see appendix). You do not actually have to go back to the table\u2014it is easier to read the birth years simply off the individual names.</li></ol> <p></p> <p>Again, asserting birth years for all individuals can be a bit tedious.  Find a snapshot of the ontology at this stage below.</p> <p>We now have an ABox with individuals with fact assertions to data indicating a birth year. We can, if we wish, also add a class restriction to the <code>Person</code> class saying that each and every instance of the class <code>Person</code> holds a data property to an integer and that this property is called \u2018hasBirthYear\u2019. As usual when deciding whether to place such a restriction upon a class, ask whether it is true that each and every instance of the class holds that property; this is exactly the same as we did for the object properties in Chapter 4. Everyone does have a birth year, even if it is not known.</p> <p>Once birth years have been added to our individuals, we can start asking some questions.</p> Task 28: DL queries 1. Use a DL query to ask:<ul><li><code>Person</code> born after 1960;</li><li><code>Person</code> born in the 1960s;</li><li><code>Person</code> born in the 1800s;</li><li><code>Person</code> that has fewer than three children;</li><li><code>Person</code> that has more than three children.</li><ul> <p>The DL query for people born in the 1960s is:</p> <pre><code>Person and hasBirthYear some int[&gt;= 1960, &lt; 1970]\n</code></pre> <p>This kind of interval is known as a facet.</p>"},{"location":"tutorial/fhkb/#711-counting-numbers-of-children","title":"7.1.1 Counting Numbers of Children","text":"<p>The last two queries in the list do not work as expected. We have asked, for instance, for <code>Person</code> that have more than three children, but we get no members of <code>Person</code> in the answer, though we know that there are some in the FHKB (e.g., <code>John_Bright_1930</code>). This is because there is not enough information in the FHKB to tell that this person has more than three different people as children. As humans we can look at the four children of John Bright and know that they are different \u2013 for instance, they all have different birth years. The automated reasoner, however, does not know that a <code>Person</code> can only have one birth year.</p> Task 29: Make a functional object property <ol><li> Make the property <code>hasBirthYear</code> functional. </li><li> Ask the query for <code>Person</code> that has more than three children again.</li></ol> <p>This time the query should work. All the other event year properties should be made functional, expect <code>hasEventYear</code>, as one individual can have many event years. As the children have different birth year and an individual can only hold one <code>hasBirthYear</code> property, then these people must be distinct entities.</p> <p>Of course, making birth year functional is not a reliable way of ensuring that the automated reasoner knows that the individual are different. It is possible for two <code>Person</code> to have the same birth year within the same family \u2013 twins and so on. <code>Peter_William_Bright_1941</code> has three children, two of which are twins, so will not be a member of the class of people with at least three children. So, we use the different individuals axiom. Most tools, including Prot\u00e9g\u00e9, have a feature that allows all individuals to be made different.</p> Task 30: Make all individuals different <ol><li> Make all individuals different; </li><li> Ask the above queries again. </li></ol> <p>From now on, every time you add individuals, make sure the different individuals axiom is updated.</p>"},{"location":"tutorial/fhkb/#72-the-open-world-assumption","title":"7.2 The Open World Assumption","text":"<p>We have met again the open world assumption and its importance in the FHKB. In the use of the functional characteristic on the <code>hasBirthYear</code> property, we saw one way of constraining the interpretation of numbers of children. We also introduced the \u2018different individuals\u2019 axiom as a way of making all individuals in a knowledge base distinct. There are more questions, however, for which we need more ways of closing down the openness of OWL 2.</p> <p>Take the questions:</p> <ul> <li>People that have exactly two children;</li> <li>People that have only brothers;</li> <li>People that have only female children.</li> </ul> <p></p> <p>We can only answer these questions if we locally close the world.We have said that David and Margaret have two children, Richard and Robert, but we have not said that there are not any others. As usual, try not to apply your domain knowledge too much; ask yourself what the automated reasoner actually knows. As we have the open world assumption, the reasoner will assume, unless otherwise said, that there could be more children; it simply doesn\u2019t know.</p> <p>Think of a railway journey enquiry system. If I ask a standard closed world system about the possible routes by rail, between Manchester and Buenos Aires, the answer will be \u2019none\u2019, as there are none described in the system. With the open world assumption, if there is no information in the system then the answer to the same question will simply be \u2018I don\u2019t know\u2019. We have to explicitly say that there is no railway route from Manchester to Buenos Aires for the right answer to come back.</p> <p>We have to do the same thing in OWL. We have to say that David and Margaret have only two children. We do this with a type assertion on individuals. So far we have only used fact assertions. A type assertion to close down David Bright\u2019 parentage looks like this:</p> <pre><code>isParentOf only {Robert_David_Bright_1965,Richard_John_Bright_1962 }\n</code></pre> <p></p> <p>This has the same meaning as the closure axioms that you should be familiar with on classes. We are saying that the only fillers that can appear on the right-hand-side of the <code>isParentOf</code> property on this individual are the two individuals for Richard and Robert. We use the braces to represent the set of these two individuals.</p> Task 31: Make a closure axiom <ol><li> Add the closure assertion above to David Bright; </li><li> Issue the DL query <code>isParentOf exactly 2 Person</code>.</li></ol> <p>The last query should return the answer of David Bright. Closing down the whole FHKB ABox is a chore and would really have to be done programmatically. OWL scripting languages such as the Ontology Preprocessing Language<sup>8</sup> (OPPL) [2] can help here. Also going directly to the OWL API [1]<sup>9</sup>, if you know what you are doing, is another route.</p> <p></p> <pre><code>Adding all these closure type assertions can slow down the reasoner; so think about\nthe needs of your system \u2013 just adding it \u2018because it is right\u2019 is not necessarily the\nright route.\n</code></pre> <p><sup>8</sup> http://oppl2.sourceforge.net</p> <p><sup>9</sup> http://owlapi.sourceforge.net/</p>"},{"location":"tutorial/fhkb/#73-adding-given-and-family-names","title":"7.3 Adding Given and Family Names","text":"<p>We also want to add some other useful data facts to people \u2013 their names. We have been putting names as part of labels on individuals, but data fact assertions make sense to separate out family and given names so that we can ask questions such as \u2018give me all people with the family name Bright and the first given name of either James or William\u2019. A person\u2019s name is a fact about that person and is more, in this case, than just a label of the representation of that person. So, we want family names and given names. A person may have more than one given name \u2013 \u2018Robert David\u2019, for instance \u2013 and an arbitrary number of given names can be held. For the FHKB, we have simply created two data properties of <code>hasFirstGivenName</code> and <code>hasSecondGivenName</code>). Ideally, it would be good to have some index on the property to given name position, but OWL has no n-ary relationships. Otherwise, we could reify the <code>hasGivenName</code> property into a class of objects, such as the following:</p> <pre><code>Class: GivenName\nSubClassOf:hasValue some String,\nhasPosition some Integer\n</code></pre> <p>but it is really rather too much trouble for the resulting query potential.</p> <p>As already shown, we will use data properties relating instances of <code>Person</code> to strings. We want to distinguish family and given names, and then different positions of given names through simple conflating of position into the property name. Figure 7.1 shows the intended data property hierarchy.</p> <p></p> <p>Figure 7.1: The event year and name data property hierarchies in the FHKB.</p> <p>Do the following:</p> Task 32: Data properties <ol><li> Create the data properties as described in Figure 7.1; </li><li> Give the <code>hasName</code> property the domain of <code>Person</code> and the range of <code>String</code>; </li><li> Make the leaf properties of given names functional; </li><li> Add the names shown in Table A.1 (appendix); Again, it may be easier to read the names of the individual names. </li><li> Ask the questions:<ul><li>all the people with the first given name \u2018James\u2019;</li><li>all the people with the first given name \u2018William\u2019;</li></ul> </li><li> All the people with the given name \u2018William\u2019; </li><li> All the people with the given name \u2018William\u2019 and the family name \u2018Bright\u2019.</li></ol> <p>The name data property hierarchy and the queries using those properties displays what now should be familiar. Sub-properties that imply the super-property. So, when we ask <code>hasFirstGivenName</code> value <code>\"William\"</code> and then the query <code>hasGivenName value value \"William\"</code> we can expect different answers. There are people with \u2018William\u2019 as either first or second given name and asking the question with the super-property for given names will collect both first and second given names.</p>"},{"location":"tutorial/fhkb/#74-summary","title":"7.4 Summary","text":"<p>We have used data properties that link objects to data such as string, integer, floats and Booleans etc. OWL uses the XML data types. We have seen a simple use of data properties to simulate birth years. The full FHKB also uses them to place names (given and family) on individuals as strings. This means one can ask for the <code>Person</code> with the given name \"James\", of which there are many in the FHKB.</p> <p>Most importantly we have re-visited the open world assumption and its implications for querying an OWL ABox. We have looked at ways in which the ABox can be closed down \u2013 unreliably via the functional characteristic (in this particular case) and more generally via type assertions.</p> <p>All the DL queries used in this chapter can also serve as defined classes in the TBox. It is a useful exercise to progressively add more defined classes to the FHKB TBox. Make more complex queries, make them into defined classes and inspect where they appear in the class hierarchy.</p> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D).\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 1891.157 sec (1.00000 % of final), by\nPellet 2.2.0 1.134 sec (0.00917 % of final) and by FaCT++ 1.6.4 is approximately\n0.201 sec (0.006 % of final). 0 sec indicates failure or timeout.\n</code></pre> <p></p> <pre><code>Note that we now cover the whole range of expressivity of OWL 2. HermiT at\nleast is impossibly slow by now. This may be because HermiT does more work\nthan the others. For now, we recommend to use either Pellet or FaCT++.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-8","title":"Chapter 8","text":""},{"location":"tutorial/fhkb/#cousins-in-the-fhkb","title":"Cousins in the FHKB","text":"<p>In this Chapter you will</p> <ol> <li>Revise or get to know about degrees and removes of cousin;</li> <li>Add the properties and sub-property chains for first and second cousins;</li> <li>Add properties and sub-property chains for some removes of cousins;</li> <li>Find out that the siblings debacle haunts us still;</li> <li>Add a defined class that does first cousins properly.</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage  below.</p> <p></p> <pre><code>Be warned; from here on the reasoner can start running slowly! Please see warning\nat the beginning of the last chapter for more information.\n</code></pre>"},{"location":"tutorial/fhkb/#81-introducing-cousins","title":"8.1 Introducing Cousins","text":"<p>Cousins can be confusing, but here is a brief summary:</p> <ul> <li>First cousins share a grandparent, but are not siblings;</li> <li>Second cousins share a great grandparent, but are not first cousins or siblings;</li> <li>Degrees such as first and second cousin give the distance to the nearest common ancestor;</li> <li>Removes give differences in generation. So, my Dad\u2019s first cousins (his generation) are my (Robert David Bright\u2019s) first cousins once removed.</li> </ul> <p>Simply, my first cousins are my parent\u2019s sibling\u2019s children. As usual, we can think about the objects and put in place some sub-property chains.</p>"},{"location":"tutorial/fhkb/#82-first-cousins","title":"8.2 First Cousins","text":"<p>Figure 8.1: Tracing out the sub-property chain for cousins going from a child to a parent, to its sibling, and down to its child, a cousin</p> <p>Figure 8.1 shows the sub-property chain for first cousins. As usual, think at the object level; to get to the first cousins of Robert David Bright, we go to the parents of Robert David Bright, to their siblings and then to their children. We go up, along and down. The OWL for this could be:</p> <pre><code>ObjectProperty: hasFirstCousin\nSubPropertyOf: hasCousin\nSubPropertyChain: hasParent o hasSibling o hasChild\nCharacteristics: Symmetric\n</code></pre> <p>Note that we follow the definitions in Section 8.1 of first cousins sharing a grandparent, but not a parent. The sub-property chain goes up to children of a grandparent (a given person\u2019s parents), along to siblings and down to their children. We do not want this property to be transitive. One\u2019s cousins are not necessarily my cousins. The blood uncles of Robert David Bright have children that are his cousins. These first cousins, however, also have a mother that is not a blood relation of Robert David Bright and the mother\u2019s sibling\u2019s children are not cousins of Robert David Bright.</p> <p>We do, however, want the property to be symmetric. One\u2019s cousins have one\u2019s-self as a cousin.</p> <p>We need to place the cousin properties in the growing object property hierarchy. Cousins are obviously blood relations, but not ancestors, so they go off to one side, underneath <code>hasBloodrelation</code>. We should group the different removes and degree of cousin underneath one <code>hasCousin</code> property and this we will do.</p> <p>Do the following:</p> Task 33: First cousins <ol><li>Add the property of <code>hasCousin</code> to the hierarchy underneath <code>hasBloodrelation</code>;</li><li>Add <code>hasFirstCousin</code> underneath this property;</li><li>Add the sub-property chain as described above;</li><li>Run the reasoner and look at the first cousins of Robert David Bright.</li><ol> <p>You should see the following people as first cousins of Robert David Bright: Mark Anthony Heath, Nicholas Charles Heath, Mark Bright, Ian Bright, Janet Bright, William Bright, James Bright, Julie Bright, Clare Bright, Richard John Bright and Robert David Bright. The last two, as should be expected, are first cousins of Robert David Bright and this is not correct. As David Bright will be his own brother, his children are his own nieces and nephews and thus the cousins of his own children. Our inability to infer siblings correctly in the FHKB haunts us still and will continue to do so.</p> <p></p> <pre><code>Although the last query for the cousins of Robert David Bright should return the\nsame results for every reasoner, we have had experiences where the results differ.\n</code></pre>"},{"location":"tutorial/fhkb/#83-other-degrees-and-removes-of-cousin","title":"8.3 Other Degrees and Removes of Cousin","text":"<p>Other degrees of cousins follow the same pattern as for first cousins; we go up, along and down. For second cousins we go up from a given individual to children of a great grandparent, along to their siblings and down to their grandchildren. The following object property declaration is for second cousins (note it uses the <code>isGrandparentOf</code> and its inverse properties, though the parent properties could be used) :</p> <pre><code>ObjectProperty: hasSecondCousin\nSubPropertyOf: hasCousin\nSubPropertyChain: hasGrandParent o hasSibling o isGrandParentOf\nCharacteristics: Symmetric\n</code></pre> <p>\u2018 Removes \u2019 simply add in another \u2018leg\u2019 of either \u2018up\u2019 or \u2018down\u2019 either side of the \u2018along\u2019\u2014that is, think of the actual individuals involved and draw a little picture of blobs and lines\u2014then trace your finger up, along and down to work out the sub-property chain. The following object property declaration does it for first cousins once removed (note that this has been done by putting this extra \u2018leg\u2019 on to the <code>hasFirstCousin</code> property; the symmetry of the property makes it work either way around so that a given person is the first cousin once removed of his/her first cousins once removed):</p> <pre><code>ObjectProperty: hasFirstCousinOnceRemoved\nSubPropertyOf: hasCousin\nSubPropertyChain: hasFirstCousin o hasChild\nCharacteristics: Symmetric\n</code></pre> <p>To exercise the cousin properties do the following:</p> Task 34: Cousin properties <ol><li>Add properties for second degree cousins; </li><li> Add removes for first and second degree cousins; </li><li> Run the reasoner and check what we know about Robert David Bright\u2019 other types of cousin.</li></ol> <p>You should see that we see some peculiar inferences about Robert David Bright\u2019 cousins \u2013 not only are his brother and himself his own cousins, but so are his father, mother, uncles and so on. This makes sense if we look at the general sibling problem, but also it helps to just trace the paths around. If we go up from one of Robert David Bright\u2019 true first cousins to a grandparent and down one parent relationship, we follow the first cousin once removed path and get to one of Robert David Bright\u2019 parents or uncles. This is not to be expected and we need a tighter definition that goes beyond sub-property chains so that we can exclude some implications from the FHKB.</p>"},{"location":"tutorial/fhkb/#84-doing-first-cousins-properly","title":"8.4 Doing First Cousins Properly","text":"<p>As far as inferring first cousin facts for Robert David Bright, we have failed. More precisely, we have recalled all Robert David Bright\u2019s cousins, but the precision is not what we would desire. What we can do is ask for Robert David Bright\u2019 cousins, but then remove the children of Robert David Bright\u2019 parents. The following DL query achieves this:</p> <pre><code>Person that hasFirstCousin valueRobert_David_Bright_1965\nand (not (hasFather valueDavid_Bright_1934) or not (hasMother valueMar-\ngaret_Grace_Rever_1934)\n</code></pre> <p>This works, but only for a named individual. We could make a defined class for this query; we could also make a defined class <code>FirstCousin</code>, but it is not of much utility. We would have to make sure that people whose parents are not known to have siblings with children are excluded. That is, people are not \u2018first cousins\u2019 whose only first cousins are themselves and their siblings. The following class does this:</p> <pre><code>Class: FirstCousin\nEquivalentTo: Person that hasFirstCousin some Person\n</code></pre> Task 35: Roberts first cousins <ol><li>Make a defined class <code>FirstCousin</code> as shown above;</li><li>Make a defined class <code>FirstCousinOfRobert</code>;</li><li>Create a DL query that looks at <code>Robert_David_Bright_1965</code> first cousins and takes away the children of <code>Robert_David_Bright_1965</code>\u2019 parents as shown above.</li></ol> <p></p> <p>This gives some practice with negation. One is making a class and then \u2018taking\u2019 some of it away \u2013 \u2018these, but not those\u2019.</p>"},{"location":"tutorial/fhkb/#85-summary","title":"8.5 Summary","text":"<p>We have now expanded the FHKB to include most blood relationships. We have also found that cousins are hard to capture just using object properties and sub-property chains. Our broken sibling inferences mean that we have too many cousins inferred at the instance level. We can get cousins right at the class level by using our inference based cousins, then excluding some using negation. Perhaps not neat, but it works.</p> <p>We have reinforced that we can just add more and more relationships to individuals by just adding more properties to our FHKB object property hierarchy and adding more sub-property chains that use the object properties we have built up upon parentage and sibling properties; this is as it should be.</p> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D).\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet\n2.2.0 111.395 sec (0.90085 % of final) and by FaCT++ 1.6.4 is approximately 0.868\nsec (0.024 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-9","title":"Chapter 9","text":""},{"location":"tutorial/fhkb/#marriage-in-the-fhkb","title":"Marriage in the FHKB","text":"<p>In this chapter you will:</p> <ol> <li>Model marriages and relationships;</li> <li>Establish object properties for husbands, wives and various in-laws;</li> <li>Re-visit aunts and uncles to do them properly;</li> <li>Use more than one sub-property chain on a given property.</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage  below.</p> <p></p> <pre><code>Much of what is in this chapter is really revision; it is more of the same - making\nlots of properties and using lots of sub-property chains. However, it is worth it as\nit will test your growing skills and it also makes the reasoners and yourself work\nhard. There are also some good questions to ask of the FHKB as a result of adding\nmarriages.\n</code></pre>"},{"location":"tutorial/fhkb/#91-marriage","title":"9.1 Marriage","text":"<p>Marriage is a culturally complex situation to model. The FHKB started with a conservative model of a marriage involving only one man and one woman.<sup>10</sup> Later versions are more permissive; a marriage simply has a minimum of two partners. This leaves it open to numbers and sex of the people involved. In fact, \u2018marriage\u2019 is probably not the right name for it. Using <code>BreedingRelationship</code> as a label (the one favoured by the main author\u2019s mother) may be a little too stark and might be a little exclusive.... In any case, some more generic name is probably better and various subclasses of the FHKB\u2019s <code>Marriage</code> class are probably necessary.</p> <p><sup>10</sup> There being no funny stuff in the Stevens family.</p> <p>To model marriage do the following:</p> Task 36: Marriage <ol><li> Create a class <code>Marriage</code>, subclass of <code>DomainEntity</code>;</li><li> Create the properties:<ul><li><code>hasPartner</code>(domain <code>Marriage</code> and range <code>Person</code>) and <code>isPartnerIn</code></li><li><code>hasFemalePartner</code>(domain <code>Marriage</code> and range <code>Woman</code>, sub-property of <code>hasPartner</code>) and its inverse <code>isFemalePartnerIn</code>;</li><li> a sub-property of <code>hasPartner</code> has <code>MalePartner</code> (domain <code>Marriage</code> and range <code>Man</code>)and its inverse <code>isMalePartnerIn</code>;</li></ul> </li><li> Create the data property <code>hasMarriageYear</code>, making us a sub-property of <code>hasEventYear</code>,make it functional;</li><li>Create an individual <code>m001</code> with the label <code>Marriage of David and Margaret</code> and add the facts: <ul><li><code>hasMalePartner David_Bright_1934</code>;</li><li><code>hasFemalePartner Margaret_Grace_Rever_1934</code></li><li><code>hasMarriageYear 1958</code>;</li></ul> </li><li>Create an individual <code>m002</code> with the label <code>Marriage of John and Joyce</code> and add the facts:<ul><li><code>hasMalePartner</code> <code>John_Bright_1930</code>;</li><li><code>hasFemalePartner</code> <code>Joyce_Gosport</code>(you may have to add Joyce if you did not already did that);</li><li><code>hasMarriageYear</code> <code>1955</code>;</li></ul><li>Create an individual <code>m003</code> with the label <code>Marriage of Peter and Diana</code> and add the facts: <ul><li><code>hasMalePartner</code> <code>Peter_William_Bright_1941</code>;</li><li><code>hasFemalePartner</code> <code>Diana_Pool</code>(you may have to add Diana if you did not already did that);</li><li><code>hasMarriageYear</code> <code>1964</code>;</li></ul> </li> <p>We have the basic infrastructure for marriages. We can ask the usual kinds of questions; try the following:</p> Task 37: DL queries <ol><li> Ask the following DL queries: <ul><li>The Women partners in marriages; </li><li>Marriages that happened before 1960 (see example below); </li><li>Marriages that happened after 1960; </li><li>Marriages that involved a man with the family name \u2018Bright\u2019.</li> <pre><code>DL query: Marriage and hasMarriageYear some int[&lt;= 1960]\n</code></pre>"},{"location":"tutorial/fhkb/#911-spouses","title":"9.1.1 Spouses","text":"<p>This marriage infrastructure can be used to infer some slightly more interesting things for actual people. While we want marriage objects so that we can talk about marriage years and even locations, should we want to, we also want to be able to have the straight-forward spouse relationships one would expect. We can use sub-property chains in the usual manner; do the following:</p> Task 38: Wifes and Husbands <ol><li> Create a property <code>hasSpouse</code> with two sub-properties <code>hasHusband</code> and <code>hasWife</code>. </li><li>Create the inverses <code>isSpouseOf</code>, <code>isWifeOf</code> and <code>isHusbandOf</code>. </li><li>To the <code>hasWife</code> property, add the sub-property chain <code>isMalePartnerIn o hasFemalePartner</code>. </li><li>Follow the same pattern for the <code>hasHusband</code> property. <p>Figure 9.1 shows what is happening with the sub-property chains. Note that the domains and ranges of the spouse properties come from the elements of the sub-property chains. Note also that the <code>hasSpouse</code> relationship will be implied from its sub-property chains.</p> <p>The following questions can now be asked:</p> <ul> <li>Is wife of David Bright;</li> <li>Has a husband born before 1940;</li> <li>The wife of an uncle of William Bright 1970.</li> </ul> <p></p> <p>Figure 9.1: The sub-property chain path used to infer the spouse relationships via the marriage partnerships.</p> <p>and many more. This is really a chance to explore your querying abilities and make some complex nested queries that involve going up and down the hierarchy and tracing routes through the graph of relationships between the individuals you\u2019ve inferred.</p>"},{"location":"tutorial/fhkb/#92-in-laws","title":"9.2 In-Laws","text":"<p>Now we have spouses, we can also have in-laws. The path is simple: <code>isSpouseOf o hasMother</code> implies <code>hasMotherInLaw</code>. The path involved in mother-in-laws can be seen in Figure 9.2. The following OWL code establishes the sub-property chains for <code>hasMotherInLaw</code>:</p> <pre><code>ObjectProperty: hasMotherInLaw\nSubPropertyOf: hasParentInLaw\nSubPropertyChain: isSpouseOf o hasMother\nDomain: Person\nRange: Woman\nInverseOf: isMotherInLawOf\n</code></pre> <p></p> <p>Figure 9.2: Tracing out the path between objects to make the sub-property chain for mother-in-laws</p> <p>Do the following to make the parent in-law properties:</p> Task 39: Parents in-law <ol><li> Create <code>hasParentInLaw</code> with two sub-properties of <code>hasMotherInLaw</code> and <code>hasFatherInLaw</code>; </li><li> Create the inverses, but remember to let the reasoner infer the hierarchy on that side of the hierarchy; </li><li> Add the sub-property chains as described in the pattern for <code>hasMotherInLaw</code> above; </li><li> Run the reasoner and check that the mother-in-law of Margaret Grace Rever is Iris Ellen Archer.</li></ol>"},{"location":"tutorial/fhkb/#93-brothers-and-sisters-in-law","title":"9.3 Brothers and Sisters In-Law","text":"<p>Brothers and sisters in law have the interesting addition of having more than one path between objects to establish a sister or brother in law relationship. The OWL code below establishes the relationships for \u2018is sister in law of\u2019:</p> <pre><code>ObjectProperty: hasSisterInLaw\nSubPropertyOf: hasSiblingInLaw\nSubPropertyChain: hasSpouse o hasSister\nSubPropertyChain: hasSibling o isWifeOf\n</code></pre> <p>A wife\u2019s husband\u2019s sister is a sister in law of the wife. Figure 9.3 shows the two routes to being a sister-in-law. In addition, the wife is a sister in law of the husband\u2019s siblings. One can add as many sub-property chains to a property as one needs. You should add the properties for <code>hasSiblingInLawOf</code> and its obvious sub-properties following the inverse of the pattern above.</p> Task 40: Siblings in-law <ol><li> Create the relationships for siblings-in-law as indicated in the owl code above.</li></ol> <p></p> <pre><code>By now, chances are high that the realisation takes a long time. We recommend to\nremove the very computationally expensive restriction `hasParent` exactly 2 Person\non the `Person` class, if you have not done it so far.\n</code></pre> <p></p> <p>Figure 9.3: The two routes to being a sister-in-law.</p>"},{"location":"tutorial/fhkb/#94-aunts-and-uncles-in-law","title":"9.4 Aunts and Uncles in-Law","text":"<p>The uncle of Robert David Bright has a wife, but she is not the aunt of Robert David Bright, she is the aunt-in-law. This is another kith relationship, not a kin relationship. The pattern has a familiar feel:</p> <pre><code>ObjectProperty: isAuntInLawOf\nSubPropertyOf: isInLawOf\nSubPropertyChain: isWifeOf o isBrotherOf o isParentOf\n</code></pre> Task 41: Uncles and aunts in-law <ol><li> Create <code>hasAuntInLaw</code> and <code>hasUncleInLaw</code> in the usual way; </li><li> Test in the usual way; </li><li> Tidy up the top of the property hierarchy so that it looks like Figure 9.4. We have a top property of <code>hasRelation</code> and two sub-properties of <code>isBloodRelationOf</code> and <code>isInLawOf</code> to establish the kith and kin relationships respectively; </li><li> All the properties created in this chapter (except for spouses) should be underneath <code>isInLawOf</code>.</li></ol> <p></p> <p>Figure 9.4: The object property hierarchy after adding the various in-law properties.</p>"},{"location":"tutorial/fhkb/#95-summary","title":"9.5 Summary","text":"<p>This has really been a revision chapter; nothing new has really been introduced. We have added a lot of new object properties and one new data property. The latest object property hierarchy with the \u2018in-law\u2019 branch can be seen in Figure 9.4. Highlights have been:</p> <ul> <li>Having an explicit marriage object so that we can say things about the marriage itself, not just the people in the marriage;</li> <li>We have seen that more than one property chain can be added to a property;</li> <li>We have added a lot of kith relationships to join the kin or blood relationships;</li> <li>As usual, the reasoner can establish the hierarchy for the inverses and put a lot of the domain and ranges in for free.</li> </ul> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D).\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet\n2.2.0 123.655 sec (1.00000 % of final) and by FaCT++ 1.6.4 is approximately 1.618\nsec (0.046 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-10","title":"Chapter 10","text":""},{"location":"tutorial/fhkb/#extending-the-tbox","title":"Extending the TBox","text":"<p>In this chapter you will:</p> <ol> <li>Just add lots of defined classes for all the aspects we have covered in this FHKB tutorial;</li> <li>You will learn that the properties used in these defined classes must be chosen with care.</li> </ol> <p></p> <p>Find a snapshot of the ontology at this stage below.</p>"},{"location":"tutorial/fhkb/#101-adding-defined-classes","title":"10.1 Adding Defined Classes","text":"<p>Add the following defined classes:</p> Task 42: Adding defined classes <ol><li> Relation and blood relation; </li><li> Forefather and Foremother; </li><li> Grandparent, Grandfather and Grandmother; </li><li> GreatGrandparent, GreatGrandfather and GreatGrandmother; </li><li> GreatGrandparentOfRobert, GreatGrandfatherOfRobert and GreatGrandMotherOfRobert </li><li> Daughter, Son, Brother, Sister, Child; </li><li> Aunt, Uncle, AuntInLaw, UncleInLaw, GreatAunt and GreatUncle; </li><li> FirstCousin and SecondCousin; </li><li> First cousin once removed; </li><li> InLaw, MotherInLaw, FatherInLaw, ParentInLaw, SiblingInLaw, SisterInLaw, BrotherInLaw; </li><li> Any defined class for any property in the hierarchy and any nominal variant of these classes.</li></ol> <p>The three classes of <code>Child</code>, <code>Son</code> and <code>Daughter</code> are of note. They are coded in the following way:</p> <pre><code>Class: Child EquivalentTo: Person that hasParent Some Person\nClass: Son EquivalentTo: Man that hasParent Some Person\nClass: Daughter EquivalentTo: Woman that hasParent Some Person\n</code></pre> <p>After running the reasoner, you will find that <code>Person</code> is found to be equivalent to <code>Child</code>; <code>Daughter</code> is equivalent to <code>Woman</code> and that <code>Son</code> is equivalent to <code>Man</code>. This does, of course, make sense \u2013 each and every person is someone\u2019s child, each and every woman is someone\u2019s daughter. We will forget evolutionary time-scales where this might be thought to break down at some point \u2013 all <code>Person</code> individuals are also <code>Descendant</code> individuals, but do we expect some molecule in some prebiotic soup to be a member of this class?</p> <p>Nevertheless, within the scope of the FHKB, such inferred equivalences are not unreasonable. They are also instructive; it is possible to have different intentional descriptions of a class and for them to have the same logical extents. You can see another example of this happening in the amino acids ontology, but for different reasons.</p> <p>Taking <code>Grandparent</code> as an example class, there are two ways of writing the defined class:</p> <pre><code>Class: Grandparent EquivalentTo: Person and isGrandparentOf some Person\nClass: Grandparent EquivalentTo: Person and (isParentOf some (Person and (is-\nParentOf some Person))\n</code></pre> <p>Each comes out at a different place in the class hierarchy. They both capture the right individuals as members (that is, those individuals in the ABox that are holding a <code>isGrandparentOf</code> property), but the class hierarchy is not correct. By definition, all grandparents are also parents, but the way the object property hierarchy works means that the first way of writing the defined class (with the <code>isGrandparentOf</code> property) is not subsumed by the class <code>Parent</code>. We want this to happen in any sensible class hierarchy, so we have to use the second pattern for all the classes, spelling out the sub-property path that implies the property such as <code>isGrandparentOf</code> within the equivalence axiom.</p> <p>The reason for this need for the \u2018long-form\u2019 is that the <code>isGrandparentOf</code> does not imply the <code>isParentOf</code> property. As described in Chapter 3 if this implication were the case, being a grandparent of Robert David Bright, for instance, would also imply that the same <code>Person</code> were a parent of Robert David Bright; an implication we do not want. As these two properties (<code>isParentOf</code> and <code>isGrandparentOf</code>) do not subsume each other means that the defined classes written according to pattern one above will not subsume each other in the class hierarchy. Thus we use the second pattern. If we look at the class for grandparents of Robert:</p> <pre><code>Class: GrandparentOfRobert\nEquivalentTo: Person that isParentOf some (Person that isParentOf value Robert\nDavid Bright)\n</code></pre> <p>If we make the equivalent class for Richard John Bright, apply the reasoner and look at the hierarchy, we see that the two classes are not logically equivalent, even though they have the same extents of William George Bright, Iris Ellen Archer, Charles Herbert Rever and Violet Sylvia Steward. We looked at this example in Section 6.2, where there is an explanation and solutions.</p>"},{"location":"tutorial/fhkb/#102-summary","title":"10.2 Summary","text":"<p>We can add defined classes based on each property we have put into the object property hierarchy. We see the expected hierarchy; as can be seen from Figure 10.1 it has an obvious symmetry based on sex. We also see a lot of equivalences inferred \u2013 all women are daughters, as well as women descendants. Perhaps not the greatest insight ever gained, but it at least makes sense; all women must be daughters. It is instructive to use the explanation feature in Prot\u00e9g\u00e9 to look at why the reasoner has made these inferences. For example, take a look at the class <code>hasGrandmother some Woman</code> \u2013 it is instructive to see how many there are.</p> <p>Like the Chapter on marriage and in-law (Chapter 9), this chapter has largely been revision. One thing of note is, however, that we must not use the object properties that are inferred through sub-property chains as definitions in the TBox; we must spell out the sub-property chain in the definition, otherwise the implications do not work properly.</p> <p>One thing is almost certain; the resulting TBox is rather complex and would be almost impossible to maintain by hand.</p> <p></p> <p>Figure 10.1: The full TBox hierarchy of the FHKB</p> <p></p> <pre><code>The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D).\n</code></pre> <p></p> <pre><code>The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop\nmachine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet\n2.2.0 0.000 sec (0.00000 % of final) and by FaCT++ 1.6.4 is approximately 35.438\nsec (1.000 % of final). 0 sec indicates failure or timeout.\n</code></pre>"},{"location":"tutorial/fhkb/#chapter-11","title":"Chapter 11","text":""},{"location":"tutorial/fhkb/#final-remarks","title":"Final remarks","text":"<p>If you have done all the tasks within this tutorial, then you will have touched most parts of OWL 2. Unusually for most uses of OWL we have concentrated on individuals, rather than just on the TBox. One note of warning \u2013 the full FHKB has some 450 members of the Bright family and takes a reasonably long time to classify, even on a sensible machine. The FHKB is not scalable in its current form.</p> <p>One reason for this is that we have deliberately maximised inference. We have attempted not to explicitly type the individuals, but drive that through domain and range constraints. We are making the property hierarchy do lots of work. For the individual Robert David Bright, we only have a couple of assertions, but we infer some 1 500 facts between Robert David Bright and other named individuals in the FHKB\u2013displaying this in Prot\u00e9g\u00e9 causes problems. We have various complex classes in the TBox and so on.</p> <p></p> <p>We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do. As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing.However, it serves its role to teach about OWL 2.</p> <p>OWL 2 on its own and using it in this style, really does not work for family history. We have seen that siblings and cousins cause problems. rules in various forms can do this kind of thing easily\u2014it is one of the primary examples for learning about Prolog. Nevertheless, the FHKB does show how much inference between named individuals can be driven from a few fact assertions and a property hierarchy. Assuming a powerful enough reasoner and the ability to deal with many individuals, it would be possible to make a family history application using the FHKB; as long as one hid the long and sometimes complex queries and manipulations that would be necessary to \u2018prune\u2019 some of the \u2018extra\u2019 facts found about individuals. However, the FHKB does usefully show the power of OWL 2, touch a great deal of the language and demonstrate some of its limitations.</p>"},{"location":"tutorial/fhkb/#appendix-a","title":"Appendix A","text":""},{"location":"tutorial/fhkb/#fhkb-family-data","title":"FHKB Family Data","text":"<p>Table A.1: The list of individuals in the FHKB</p> Person First given name Second given name Family name Birth year Mother Father Alec John Archer 1927 Alec John Archer 1927 Violet Heath 1887 James Alexander Archer 1882 Charles Herbert Rever 1895 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 William Rever 1870 Charlotte Caroline Jane Bright 1894 Charlotte Caroline Jane Bright 1894 Charlotte Hewett 1863 Henry Edmund Bright 1862 Charlotte Hewett 1863 Charlotte none Hewett 1863 not specified not specified Clare Bright 1966 Clare none Bright 1966 Diana Pool Peter William Bright 1941 Diana Pool Diana none Pool none not specified not specified David Bright 1934 David none Bright 1934 Iris Ellen Archer 1906 William George Bright 1901 Dereck Heath Dereck none Heath 1927 not specified not specified Eileen Mary Rever 1929 Eileen Mary Rever 1929 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 Elizabeth Frances Jessop 1869 not specified not specified Ethel Archer 1912 Ethel none Archer 1912 Violet Heath 1887 James Alexander Archer 1882 Frederick Herbert Bright 1889 Frederick Herbert Bright 1889 Charlotte Hewett 1863 Henry Edmund Bright 1862 Henry Edmund Bright 1862 Henry Edmund Bright 1862 not specified not specified Henry Edmund Bright 1887 Henry Edmund Bright 1887 Charlotte Hewett 1863 Henry Edmund Bright 1862 Ian Bright 1959 Ian none Bright 1959 Joyce Gosport John Bright 1930 Iris Ellen Archer 1906 Iris Ellen Archer 1906 Violet Heath 1887 James Alexander Archer 1882 James Alexander Archer 1882 James Alexander Archer 1882 not specified not specified James Bright 1964 James none Bright 1964 Diana Pool Peter William Bright 1941 James Frank Hayden Bright 1891 James Frank Bright 1891 Charlotte Hewett 1863 Henry Edmund Bright 1862 Janet Bright 1964 Janet none Bright 1964 Joyce Gosport John Bright 1930 John Bright 1930 John none Bright 1930 Iris Ellen Archer 1906 William George Bright 1901 John Tacey Steward 1873 John Tacey Steward 1873 not specified not specified Joyce Archer 1921 Joyce none Archer 1921 Violet Heath 1887 James Alexander Archer 1882 Joyce Gosport Joyce none Gosport not specified not specified not specified Julie Bright 1966 Julie none Bright 1966 Diana Pool Peter William Bright 1941 Kathleen Minnie Bright 1904 Kathleen Minnie Bright 1904 Charlotte Hewett 1863 Henry Edmund Bright 1862 Leonard John Bright 1890 Leonard John Bright 1890 Charlotte Hewett 1863 Henry Edmund Bright 1862 Lois Green 1871 Lois none Green 1871 not specified not specified Margaret Grace Rever 1934 Margaret Grace Rever 1934 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Mark Anthony Heath 1960 Mark Anthony Heath 1960 Eileen Mary Rever 1929 Dereck Heath Mark Bright 1956 Mark none Bright 1956 Joyce Gosport John Bright 1930 Nicholas Charles Heath 1964 Nicholas Charles Heath 1964 Eileen Mary Rever 1929 Dereck Heath Nora Ada Bright 1899 Nora Ada Bright 1899 Charlotte Hewett 1863 Henry Edmund Bright 1862 Norman James Archer 1909 Norman James Archer 1909 Violet Heath 1887 James Alexander Archer 1882 Peter William Bright 1941 Peter William Bright 1941 Iris Ellen Archer 1906 William George Bright 1901 Richard John Bright 1962 Richard John Bright 1962 Margaret Grace Rever 1934 David Bright 1934 Robert David Bright 1965 Robert David Bright 1965 Margaret Grace Rever 1934 David Bright 1934 Violet Heath 1887 Violet none Heath 1887 not specified not specified Violet Sylvia Steward 1894 Violet Sylvia Steward 1894 Lois Green 1871 John Tacey Steward 1873 William Bright 1970 William none Bright 1970 Joyce Gosport John Bright 1930 William George Bright 1901 William George Bright 1901 Charlotte Hewett 1863 Henry Edmund Bright 1862 William Rever 1870 William none Rever 1870 not specified not specified <p></p>"},{"location":"tutorial/fhkb/#fhkb-owl-files-for-download","title":"FHKB OWL Files for Download","text":"Chapter FHKB snapshot 2 download 3 download 4 download 5 download 6 download 7 download 8 download 9 download 10 download"},{"location":"tutorial/fhkb/#bibliography","title":"Bibliography","text":"<p>[1] M. Horridge and S. Bechhofer. The owl api: a java api for working with owl 2 ontologies. Proc. of OWL Experiences and Directions , 2009, 2009.</p> <p>[2] Luigi Iannone, Alan Rector, and Robert Stevens. Embedding knowledge patterns into owl. In European Semantic Web Conference (ESWC09) , pages 218\u2013232, 2009.</p> <p>[3] Dmitry Tsarkov, Uli Sattler, Margaret Stevens, and Robert Stevens. A Solution for the Man-Man Problem in the Family History Knowledge Base. In Sixth International Workshop on OWL: Experiences and Directions 2009 , 2009.</p>"},{"location":"tutorial/github-fundamentals/","title":"GitHub Fundamentals for OBO Engineers","text":""},{"location":"tutorial/github-fundamentals/#introduction-to-github","title":"Introduction to GitHub","text":""},{"location":"tutorial/github-fundamentals/#back-to-getting-started","title":"Back to Getting Started","text":""},{"location":"tutorial/github-fundamentals/#back-to-main-repo","title":"Back to Main Repo","text":""},{"location":"tutorial/github-fundamentals/#overview","title":"Overview:","text":"<ul> <li>Getting started</li> <li>Organization</li> <li>Markdown</li> <li>Content types</li> </ul>"},{"location":"tutorial/github-fundamentals/#getting-started","title":"Getting started","text":"<p>GitHub is increasingly used by software developers, programmers and project managers for uploading and sharing content, as well as basic project management. You build a profile, upload projects to share and connect with other users by \"following\" their accounts. Many users store programs and code projects, but you can also upload text documents or other file types in your project folders to share publicly (or privately). It is capable of storing any file type from text, to structured data, to software. And more features are being added by the day. The real power of Git, however, is less about individuals publishing content (many places can do that, including google docs etc). It is more about that content being easily shared, built upon, and credited in a way that is robust to the realities of distributed collaboration. You don't have to know how to code or use the command line. It is a powerful way to organize projects with multiple participants.</p>"},{"location":"tutorial/github-fundamentals/#organization","title":"Organization","text":"<p>Git supports the following types of primary entities:</p> <ul> <li>Individual: A person who contributes to GitHub (that's you!)</li> <li>Example individual http://github.com/nicolevasilevsky</li> <li>Organization: An entity that may correspond to an actual organization (such as a university) or to a meaningful grouping of repositories. Organizations are like individuals except that they can establish teams.</li> <li>Example organization: https://github.com/monarch-initiative</li> <li>Repository: A collection of versioned files (of any type)</li> <li>Example repository https://github.com/monarch-initiative/mondo/</li> <li>Teams: A group of individuals assembled by the administrators of an organization. An individual may participate in many teams and organizations, however a team is always bound to a single organization. Nesting teams saves time; instructions here.</li> </ul> <p>The relationships between any combination of these entities is many-to-many, with the nuanced exception of repositories. For our purposes today we will oversimplify by saying that a repositoy belongs either to a single organization or to a single individual.</p> <p></p>"},{"location":"tutorial/github-fundamentals/#markdown","title":"Markdown","text":"<p>Content in GitHub is written using Markdown, a text-to-HTML conversion tool for web writers (ref).</p> <p>For more help with Markdown, see this GitHub guide.</p> Raw markup syntax As rendered <code>Header - use # for H1, ## for H2, etc.</code> # Header, ## Header (note, the header is not displaying properly in this table) <code>Emphasis, aka italics, with *asterisks* or _underscores_.</code> Emphasis, aka italics, with asterisks or underscores. <code>Strong emphasis, aka bold, with **asterisks** or __underscores__.</code> Strong emphasis, aka bold, with asterisks or underscores. <code>Combined emphasis with **asterisks and _underscores_**.</code> Combined emphasis with asterisks and underscores. <code>Strikethrough uses two tildes. ~~Scratch this.~~</code> Strikethrough uses two tildes. Scratch this. <p>Lists: To introduce line breaks in markdown, add two spaces For a bulleted list, use * or - (followed by a space)</p> <p>Here is an example of a list: One Two Three</p> <p>Here is an example of a bulleted list:</p> <ul> <li>One</li> <li>Two</li> <li>Three</li> </ul>"},{"location":"tutorial/github-fundamentals/#content-types","title":"Content types","text":"<p>GitHub can store any kind of content, provided it isn't too big. (And now even this is possible). However, it is more capable for some filetypes than it is for others. Certain filetypes can be viewed 'natively' within the GitHub interface. These are:</p> <ul> <li>Images: png, jpg, svg</li> <li>GEOJSON</li> <li>CSV, TSV (note that files named type '.tab' will not render properly in the UI.)</li> <li>Markdown</li> <li>Software code (eg. including json, HTML, xml etc)</li> </ul>"},{"location":"tutorial/github-fundamentals/#task-create-a-new-github-repository","title":"Task - create a new GitHub repository","text":"<ul> <li>Create your GitHub account if you do not already have one</li> <li>Customize your avatar if you haven't already</li> <li>Go to settings and upload any picture (it doesn't have to be your face)</li> <li>Create a repository</li> </ul>"},{"location":"tutorial/github-fundamentals/#task-update-the-content-in-your-readme","title":"Task - update the content in your README","text":"<ul> <li>Go back to the repository you just created</li> <li>Click the pencil icon in the right corner of your README.md file</li> <li>Add some content to your file that includes a header, italics, bold, strikethrough, and lists</li> <li>You can preview your changes before committing by clicking 'Preview changes'.</li> <li>Commit your changes by clicking the commit button at the bottom of the page.</li> </ul>"},{"location":"tutorial/github-fundamentals/#task-add-content-to-your-repository","title":"Task - add content to your repository","text":"<ul> <li>Click on the code button</li> <li>Click upload file</li> <li>Upload a file by dragging and dropping or browse for file</li> <li>Trying uploading an Excel file vs a TSV or CSV file. How are these displayed differently?</li> </ul>"},{"location":"tutorial/github-fundamentals/#additional-resources","title":"Additional Resources","text":"<ul> <li>Frequently Asked Questions</li> <li>Git and GitHub for Documentation</li> <li>Markdown Cheatsheet</li> <li>Git 101: Git and GitHub for Beginners</li> <li>Mastering Issues (10 min read)</li> <li>Nomi's intro to GitHub slides</li> <li>Udemy course: Command Line Essentials: Git Bash for Windows</li> <li>Udemy course: Git: Become an Expert in Git &amp; GitHub in 4 Hours</li> <li>Google: Introduction to Git and GitHub</li> <li>Udemy: Git Started with GitHub</li> </ul>"},{"location":"tutorial/github-fundamentals/#acknowledgements","title":"Acknowledgements","text":"<p>Adopted from CD2H MTIP tutorial</p>"},{"location":"tutorial/github-issues/","title":"GitHub Issue for OBO Engineers","text":""},{"location":"tutorial/github-issues/#intro-to-managing-and-tracking-issues-in-github","title":"Intro to managing and tracking issues in GitHub","text":""},{"location":"tutorial/github-issues/#overview","title":"Overview","text":"<ul> <li>How to create issues</li> <li>How to assign issues</li> <li>How to communicate about issues</li> <li>How to organize issues</li> <li>How to query issues</li> <li>How to close issues</li> <li>How to assign teams</li> <li>Where to go when you need help</li> <li>Miscellany that is good to know</li> </ul>"},{"location":"tutorial/github-issues/#create-issues","title":"Create issues","text":"<p>Back to top</p> <p>Why: \"Issues are a great way to keep track of tasks, enhancements, and bugs for your projects or for anyone else's. As long as you are a registered GitHub user you can log an issue, or comment on an issue for any open repo on GitHub. Issues are a bit like email\u2014except they can be shared, intelligently organized, and discussed with the rest of your team. GitHub\u2019s tracker is called Issues, and has its own section in every repository.\" (From: https://guides.github.com/features/issues/)</p> <p>How:</p> <p>How to create an issue in GitHub:</p> <ul> <li>We will practice creating tickets in this repository https://github.com/nicolevasilevsky/c-path-practice</li> <li>Click \"issues\"</li> <li>Click \"New Issue\" (note the word 'issue' and 'ticket' are frequently used interchangeably)</li> <li>Write an informative title</li> <li>Write a detailed explanation of your issue</li> <li>In the case of reporting software bugs, provide some context in which the issue was encountered (e.g. bug detected when using Google Chrome on a Mac OS)</li> <li>If you know the sub-tasks that are involved, list these using <code>- [ ]</code> markdown syntax before each bullet. Note, you can also add sub-tasks by clicking the 'add a task list' button in the tool bar. The status of the tasks in an issue (eg. https://github.com/nicolevasilevsky/c-path-practice/issues/1 will then be reflected in any summary view. Eg. https://github.com/nicolevasilevsky/c-path-practice/issues.</li> <li>Click Submit new issue</li> <li>Edit the issue (if needed) (Note that post-hoc edits will not propagate to email notifications).</li> </ul> <p>Your turn:</p> <p>Follow the instructions above to create a ticket about a hypothetical issue (such as an improvement to this tutorial) that includes a sub-task list.</p>"},{"location":"tutorial/github-issues/#assign-issues","title":"Assign issues","text":"<p>Back to top</p> <p>Assign issues to people</p> <ul> <li>On the top right hand side, click \"Assignees\"</li> <li>You can assign issues to yourself or other people who are part of the repository</li> <li>In the box, start typing type their name or GitHub handle</li> <li>It is possible to assign up to 10 handles at once (assignment to a team is currently not supported)</li> </ul> <p>Add labels</p> <ul> <li>On the top right hand side, click \"Labels\"</li> <li>Assign a relevant label to your ticket</li> <li>Note, by default, every GitHub repo comes with some standard labels</li> <li>You can also create new labels that are specific to your project. For example see the labels on the Mondo GitHub tracker</li> </ul> <p>New Labels</p> <ul> <li>In GitHub, navigate to the Issues page or the pull requests tab</li> <li>Click Labels button next to the search field</li> <li>Click New Label to create a new label, or click Edit to edit an existing one.</li> <li>In the text box, type your new label name.</li> <li>Select a color for the label from the color bar. You can customize this color by editing the hexadecimal number above the color bar. For a list of hexadecimal numbers see HTML color codes</li> <li>Click Create Label to save the new label.</li> </ul> <p>Your turn:</p> <p>On the ticket you previously created:</p> <ul> <li>Assign the ticket to someone</li> <li>Add a label for an enhancement</li> <li>Create a new label and add it to the ticket</li> </ul>"},{"location":"tutorial/github-issues/#communicate-about-issues","title":"Communicate about issues","text":"<p>Back to top</p> <p>Comment on issues</p> <ul> <li>Click on an issue in the issue tracker in the https://github.com/nicolevasilevsky/c-path-practice/issues repo</li> <li>Scroll to the bottom of the issue, and add content in the \"Leave a comment\" field</li> <li>Use the top tool bar to format your text, add bold, italic, lists etc.</li> <li>Preview your text to see how your formatting looks</li> <li>Click Comment.</li> </ul> <p>Close issues</p> <ul> <li>If an issue has been addressed, click Close Issue. Best practice is to point to the work (whether code, documentation, etc) that has been done to close it.</li> <li>Only close the ticket if the issue has been resolved, usually someone will write a comment describing the action they did to close the issue and click Close Issue.</li> <li>The issue will no longer be dispalyed in the list of open issue, but will be archived.</li> <li>When making a change to code or documentation in GitHub, it is possible to automatically couple a change to an issue and close it. Just use 'fixes' or 'closes' followed by the issue number.</li> </ul> <p>Use direct @ mentions</p> <ul> <li>You can mention someone in a issue without assigning it to them</li> <li>In the comments section, type @github handle. For example, to mention Nicole, you would type @nicolevasilevsky. You can either start typing their name or GitHub handle and GitHub will autosuggest their handle.</li> </ul> <p>Link documents</p> <p>You can link documents and files by:</p> <ul> <li>copy and pasting URL</li> <li>you can attach files by dragging and dropping</li> <li>You can link one issue to another in the same repo by typing '#' followed by the title of the ticket</li> <li>This approach also works across repos but you need to use the full URL (no autocomplete available). Doing this will also cause the referent issue to display that it has been referenced.</li> </ul> <p>Cross reference to another ticket</p> <ul> <li>If your ticket is a duplicate or related to another ticket, you can cross reference another ticket</li> <li>Type # and you will see a list of other tickets in that repo</li> <li>Type #TicketNumber and that will link to the other ticket.</li> </ul> <p>Before saving your changes, you can preview the comment to ensure the correct formatting.</p> <p>Your turn:</p> <ul> <li>Follow the instructions above to comment on a ticket that someone created.</li> <li>Mention Nicole</li> <li>Attach a picture (such as a picture you copy from the internet, or attach a picture you have saved on your computer)</li> <li>Include a comment that says, 'related to #1' and link to ticket #1</li> </ul>"},{"location":"tutorial/github-issues/#organize-issues","title":"Organize issues","text":"<p>Back to top</p> <p>Milestones</p> <ul> <li>To create a milestone, navigate to the issues page in your repository</li> <li>Next to the search field, click Milestones</li> <li>Click New Milestone to create a new milestone, click Edit to edit an existing one</li> <li>Create a milestone that is broad enough to be meaningful, but specific enough to be actionable.</li> <li>Set a due date for the milestone (note that specific tasks can not be formally assigned due dates, though you can mention a desired due date in the narrative text of a ticket.</li> <li>Each ticket can only be associated to ONE milestone, however it can have as many labels as appropriate.</li> <li>A given issue can be part of multiple \"project\" boards (see below)</li> </ul> <p>Your turn</p> <p>Create a new milestone, and add the milestone to an existing ticket.</p> <p>Projects</p> <ul> <li>Projects is a lot like Trello, it uses cards on a list that you can name and organize as you see fit.</li> <li>You can create as many projects within a repository as you like</li> </ul> <p>To create project:</p> <ul> <li>Click on Projects</li> <li>Click New Project</li> <li>Name the project</li> <li>Write a description of the project</li> <li>Create columns and give them names</li> <li>Add 'cards' to the columns</li> </ul> <p>Your turn</p> <p>Create a new project and add columns and add cards to the columns.</p>"},{"location":"tutorial/github-issues/#query-issues","title":"Query issues","text":"<p>Back to top</p> <p>Once you start using GitHub for lots of things it is easy to get overwhelmed by the number of issues. The query dashboard https://github.com/issues allows you to filter on tickets.</p> <ul> <li>All issues assigned to me: https://github.com/issues/assigned</li> <li>All issues on which I am @ mentioned: https://github.com/issues/mentioned</li> </ul> <p>More complex queries are also possible.</p> <ul> <li>All issues either assigned to me OR on which I have commented OR am mentioned: https://github.com/issues?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Aissue+involves%3Anicolevasilevsky+</li> </ul> <p>Note, you must be signed in to GitHub to view the above links.</p> <p>Further reading on Issue querys</p>"},{"location":"tutorial/github-issues/#nofifications","title":"Nofifications","text":"<ul> <li>When you join a repository, by default, you are 'watching' all activity. You can change the setting to 'Participating and @mentions' to only get notifications that mention you</li> <li>You can set rules in your email to filter for emails that mention you, ie @username.</li> </ul>"},{"location":"tutorial/github-issues/#help","title":"Help","text":"<p>Back to top</p> <ul> <li>You may find the answers you seek in StackOverflow, although it is primarily geared towards programmers.</li> <li>GitHub kind of monitors https://github.com/isaacs/github/issues/ but not with any rigor.</li> <li>To be safe, contact GitHub directly at https://github.com/contact, but recognize that they support literally millions of users and responsiveness is not guaranteed. Forums like isaacs sometimes offer some help because other users can help identify workarounds, for instance, as shown here.</li> </ul>"},{"location":"tutorial/github-issues/#acknowledgements","title":"Acknowledgements","text":"<p>Adopted from CD2H MTIP tutorial</p>"},{"location":"tutorial/intro-cli-1/","title":"Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 1","text":"<p>As a modern ontology curator, you are an engineer - you are curating computable knowledge, testing the integrity of your curation using quality control testing, and are responsible for critical components of modern knowledge systems that directly affect user experience - the ontologies.</p> <p>Scientific computing is a big, scary world comprising many different tools, methodologies, training resources and philosophies, but nearly all modern workflows share one key aspect: the ability to execute commands that help you find and manipulate data with the command line. Some examples of that include:</p> <ol> <li>Running and Ontology Development Kit (ODK) commands like <code>sh run.sh make prepare_release</code></li> <li>Creating a new branch in <code>git</code> and committing changes</li> <li>Downloading a file with <code>curl</code> or <code>wget</code></li> <li>Finding which file in my ontology repository mentions HP:0000118 to figure out where that \"weird axiom is coming from\"</li> <li>Searching for strange symbols in the ontology</li> <li>Filtering and sorting text files</li> </ol> <p>Here we are doing a basic hands on tutorial which will walk you through the must-know commands. For a more comprehensives introduction into thinking about automation please see our lesson on Automating Ontology Development Workflows: Make, Shell and Automation Thinking</p> <p>The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an <code>sh</code> file in the following there exists a corresponding <code>bat</code> file that can be run in the windows powershell, or CMD.</p>"},{"location":"tutorial/intro-cli-1/#prerequisites","title":"Prerequisites","text":"<p>You have:</p> <ul> <li>a Mac or Linux Operating system</li> <li>alternatively, you have the Ontology Development Kit installed and learned how to \"get inside a running container\".</li> </ul>"},{"location":"tutorial/intro-cli-1/#monarch-obo-training-tutorial","title":"Monarch OBO Training Tutorial","text":"<p>Intro to Command Lind Interface Part 1</p>"},{"location":"tutorial/intro-cli-1/#tutorial","title":"Tutorial","text":"<ol> <li>Baby steps: whoami, ls and cd</li> <li>Working with files and directories</li> <li>Downloading and searching files</li> <li>The Dark Art of Piping and Redirects</li> <li>Tutorial videos</li> <li>Further reading</li> </ol>"},{"location":"tutorial/intro-cli-1/#baby-steps-whoami-pwd-and-ls","title":"Baby steps: whoami, pwd and ls","text":"<p>We are not going to discuss here in any detail what the command line is. We will focus on what you can do with it: for more information skip to the further reading section.</p> <p>The basic idea behind the command line is that you run a command to achieve a goal. Among the most common goals relevant to you as a semantic engineer will be:</p> <ol> <li>Navigating the file system (changing directories, logging into remote servers and more)</li> <li>Reading and writing files</li> <li>Searching stuff</li> </ol> <p>Most commands result in some kind of printed statement. Lets try one. Open your terminal (a terminal is the program you use to enter commands. For a nice overview of how shell, terminal, command line and console relate, see here). On Mac, you can type CMD+Space to search for programs and then type \"terminal\". For this tutorial we use the default Terminal.app, but there are many others, including iterm2. For this introduction, it does not matter which terminal you use. When first opening the terminal you will see something like this:</p> <p></p> <p>or</p> <p></p> <p>Note that your terminal window may look slightly different, depending on your configuration. More on that later.</p> <p>Let's type our first command and hit enter:</p> <pre><code>whoami\n</code></pre> <p>On my machine I get</p> <pre><code>(base) matentzn@mbp.local:~ $ whoami\nmatentzn\n</code></pre> <p>This does not seem like a useful command, but sometimes, we forget who we are, and it is good to be reminded. So, what happened here? We ran a command, named <code>whoami</code> and our command line executed that command which is implemented somewhere on our machine as a program. That program simply determined who I am in some way, and then printed the result again.</p> <p>Ok so, lets lets look a bit closer at the command prompt itself:</p> <pre><code>matentzn@mbp.local:~ $\n</code></pre> <p>Two interesting things to not here for today:</p> <ol> <li>The <code>~</code>. This universally (on all Unix systems) refers to your user directory on your computer. In this case here, it tells you that in your terminal, you are \"in your user directory\".</li> <li>The <code>$</code> sign. It simply denotes where your command line starts (everything before the $ is information provided to you, everything will be about your commands). Make sure that you do not accidentally copy based the <code>$</code> sign from the examples on the web into your command prompt:</li> </ol> <pre><code>(base) matentzn@mbp.local:~ $ $ whoami\n-bash: $: command not found\n(base) matentzn@mbp.local:~ $\n</code></pre> <p><code>whoami</code> did not do anything.</p> <p>Ok, based on the <code>~</code> we know that we are \"in the user home directory\". Let as become a bit more confident about that and ask the command prompt where we are:</p> <pre><code>matentzn@mbp.local:~ $ pwd\n/Users/matentzn\n</code></pre> <p>The <code>pwd</code> command prints out the full path of our current location in the terminal. As you can see, the default location when opening the command prompt is, indeed, the home director, located in <code>/Users/matentzn</code>. We will use it later again.</p> <p>A word about paths. <code>/Users/matentzn</code> is what we call a path. On UNIX systems, <code>/</code> separates one directory from another. So <code>matentzn</code> is a directory inside of the <code>Users</code> directory.</p> <p>Let us now take a look what our current directory contains (type <code>ls</code> and hit enter):</p> <pre><code>matentzn@mbp.local:~ $ ls\nApplications    Library ...\n</code></pre> <p>This command will simply list all of the files in your directory as a big list. We can do this a bit nicer:</p> <pre><code>matentzn@mbp.local:~ $ ls -l\ntotal 80000\ndrwx------@   4 matentzn  staff       128 31 Jul  2020 Applications\ndrwx------@  26 matentzn  staff       832 12 Sep  2021 Desktop\n</code></pre> <p><code>-l</code> is a short command line option which allows you specify that you would like print the results in a different format (a long list). We will not go into any detail here what this means but a few things to not in the output: You can see some pieces of information that are interesting, like when the file or directory was last modified (i.e. 31. July 2020), who modified it (me) and, of course, the name e.g. <code>Applications</code>.</p> <p>Before we move on to the next section, let us <code>clear</code> the current terminal from all the command outputs we ran:</p> <pre><code>clear\n</code></pre> <p>Your command prompt should now be empty again.</p> <p></p>"},{"location":"tutorial/intro-cli-1/#working-with-files-and-directories","title":"Working with files and directories","text":"<p>In the previous section we learned how to figure out who we are (<code>whoami</code>), where we are (<code>pwd</code>) and how to see what is inside the current directory (<code>ls -l</code>) and how to clear all the output (<code>clear</code>).</p> <p>Let us know look at how we can programmatically create a new directory and change the location in our terminal.</p> <p>First let us create a new directory:</p> <pre><code>mkdir tutorial-my\n</code></pre> <p>Now if we list the contents of our current directory again (<code>ls -l</code>), we will see our newly created directory listed! Unfortunately, we just realised that we chose the wrong name for our directory! It should have been <code>my-tutorial</code> instead of <code>tutorial-my</code>! So, let us rename it. In the command prompt, rather than \"renaming\" files and directories, we \"move\" them (<code>mv</code>).</p> <pre><code>mv tutorial-my my-tutorial\n</code></pre> <p>Now, lets enter our newly created directory using the _c_hange _d_irectory command (<code>cd</code>), and create another sub-directory in <code>my-tutorial</code>, called \"data\" (<code>mkdir data</code>):</p> <pre><code>cd my-tutorial\nmkdir data\n</code></pre> <p>You can check again with <code>ls -l</code>. If you see the data directory listed, we are all set! Feel free to run <code>clear</code> again to get rid of all the current output on the command prompt.</p> <p>Let us also enter this directory now: <code>cd data</code>.</p> <p>If we want to leave the directory again, feel free to do that like this:</p> <pre><code>cd ..\n</code></pre> <p>The two dots (<code>..</code>) mean: \"parent directory.\" This is very important to remember during your command line adventures: <code>..</code> stands for \"parent directory\", and <code>.</code> stands for \"current/this directory\" (see more on that below).</p> <p>Now, let's get into something more advanced: downloading files.</p> <p></p>"},{"location":"tutorial/intro-cli-1/#downloading-and-searching-files","title":"Downloading and searching files","text":"<p>Our first task is to download the famous Human Phenotype Ontology Gene to Phenotype Annotations (aka HPOA). As you should already now, whenever we download ontologies, or ontology related files, we should always use a persistent URL, if available! This is the one for HPOA: <code>http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt</code>.</p> <p>There are two very popular commands for downloading content: <code>curl</code> and <code>wget</code>. I think most of my colleagues prefer <code>curl</code>, but I like <code>wget</code> because it simpler for beginners. So I will use it here. Lets us try downloading the file!</p> <pre><code>wget http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt -O genes_to_phenotype.txt\n</code></pre> <p>The <code>-O</code> parameter is optional and specifies a filename. If you do not add the parameter, <code>wget</code> will try to guess the filename from the URL. This does not always go so well with complex URLs, so I personally recommend basically always specifying the <code>-O</code> parameter.</p> <p>You can also use the curl equivalent of the wget command;</p> <pre><code>curl -L http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt --output genes_to_phenotype.txt\n</code></pre> <p>Try before reading on: Exercises!</p> <ol> <li>Move the downloaded file <code>genes_to_phenotype.txt</code> to the data directory you previously created.</li> <li>Change into the <code>data</code> directory.</li> <li>Download the OBO format version of the Human Phenotype Ontology from its PURL.</li> </ol> <p>Do not move on to the next step unless your data directory looks similar to this:</p> <pre><code>matentzn@mbp.local:~/my-tutorial/data $ pwd\n/Users/matentzn/my-tutorial/data\nmatentzn@mbp.local:~/my-tutorial/data $ ls -l\ntotal 53968\n-rw-r--r--  1 matentzn  staff  19788987 11 Jun 19:09 genes_to_phenotype.txt\n-rw-r--r--  1 matentzn  staff   7836327 27 Jun 22:50 hp.obo\n</code></pre> <p>Ok, let us look at the first 10 lines of genes_to_phenotype.txt using the <code>head</code> command:</p> <pre><code>head genes_to_phenotype.txt\n</code></pre> <p><code>head</code> is a great command to familiarise yourself with a file. You can use a parameter to print more or less lines:</p> <pre><code>head -3 genes_to_phenotype.txt\n</code></pre> <p>This will print the first 3 lines of the genes_to_phenotype.txt file. There is another analogous command that allows us to look at the last lines off a file:</p> <pre><code>tail genes_to_phenotype.txt\n</code></pre> <p><code>head</code>, <code>tail</code>. Easy to remember.</p> <p>Next, we will learn the most important of all standard commands on the command line: <code>grep</code>. <code>grep</code> stands for \"Global regular expression print\" and allows us to search files, and print the search results to the command line. Let us try some simple commands first.</p> <pre><code>grep diabetes genes_to_phenotype.txt\n</code></pre> <p>You will see a list of hundreds of lines out output. Each line corresponds to a line in the <code>genes_to_phenotype.txt</code> file which contains the word \"diabetes\".</p> <pre><code>grep is case sensitive. It wont find matches like Diabetes, with capital D!\n\nUse the `-i` parameter in the grep command to instruct grep to\nperform case insensitive matches.\n</code></pre> <p>There is a lot more to grep than we can cover here today, but one super cool thing is searching across an entire directory.</p> <pre><code>grep -r \"Elevated circulating follicle\" .\n</code></pre> <p>Assuming you are in the <code>data</code> directory, you should see something like this:</p> <pre><code>./genes_to_phenotype.txt:190    NR0B1   HP:0008232  Elevated circulating follicle stimulating hormone level -   HP:0040281      orphadata   ORPHA:251510\n./genes_to_phenotype.txt:57647  DHX37   HP:0008232  Elevated circulating follicle stimulating hormone level -       -   mim2gene    OMIM:273250\n...... # Removed other results\n./hp.obo:name: Elevated circulating follicle stimulating hormone level\n</code></pre> <p>There are two new aspects to the command here:</p> <ol> <li>The <code>-r</code> option (\"recursive\") allows is to search a directory and all directories within in.</li> <li>The <code>.</code> in the beginning. Remember, in the previous use of the <code>grep</code> command we had the name of a file in the place where now the <code>.</code> is. The <code>.</code> means \"this directory\" - i.e. the directory you are in right now (if lost, remember <code>pwd</code>).</li> </ol> <p>As you can see, <code>grep</code> does not only list the line of the file in which the match was found, it also tells us which filename it was found in! We can make this somewhat more easy to read as well by only showing filenames using the <code>-l</code> parameter:</p> <pre><code>matentzn@mbp.local:~/my-tutorial/data $ grep -r -l \"Elevated circulating follicle\" .\n./genes_to_phenotype.txt\n./hp.obo\n</code></pre> <p></p>"},{"location":"tutorial/intro-cli-1/#the-dark-art-of-piping-and-redirects","title":"The Dark Art of Piping and Redirects","text":"<p>The final lesson for today is about one of the most powerful features of the command line: the ability to chain commands together. Let us start with a simple example (make sure you are inside the data directory):</p> <pre><code>grep -r \"Elevated circulating follicle\" . | head -3\n</code></pre> <p>This results in:</p> <pre><code>./genes_to_phenotype.txt:190    NR0B1   HP:0008232  Elevated circulating follicle stimulating hormone level -   HP:0040281      orphadata   ORPHA:251510\n./genes_to_phenotype.txt:57647  DHX37   HP:0008232  Elevated circulating follicle stimulating hormone level -       -   mim2gene    OMIM:273250\n./genes_to_phenotype.txt:57647  DHX37   HP:0008232  Elevated circulating follicle stimulating hormone level -   HP:0040281      orphadata   ORPHA:251510\n</code></pre> <p>So, what is happening here? First, we use the <code>grep</code> command to find \"Elevated circulating follicle\" in our data directory. As you may remember, there are more than 10 results for this command. So the grep command now wants to print these 10 results for you, but the <code>|</code> pipe symbol intercepts the result from <code>grep</code> and passes it on to the next command, which is <code>head</code>. Remember <code>head</code> and <code>tail</code> from above? Its exactly the same thing, only that, rather than printing the first lines of a file, we print the first lines of the output of the previous command. You can do incredible things with pipes. Here a taster which is beyond this first tutorial, but should give you a sense:</p> <pre><code>grep \"Elevated circulating follicle\" genes_to_phenotype.txt | cut -f2 | sort | uniq | head -3\n</code></pre> <p>Output:</p> <pre><code>AR\nBNC1\nC14ORF39\n</code></pre> <p>What is happening here?</p> <ol> <li><code>grep</code> is looking for \"Elevated circulating follicle\" in all files in the directory, then \"|\" is passing the output on to</li> <li><code>cut</code>, which extracts the second column of the table (how cool?), then \"|\" is passing the output on to</li> <li><code>sort</code>, which sorts the output, then \"|\" is passing the output on to</li> <li><code>uniq</code>, which removes all duplicate values from the output, then \"|\" is passing the output on to</li> <li><code>head</code>, which is printing only the first 3 rows of the result.</li> </ol> <p>Another super cool use of piping is searching your command history. Try running:</p> <pre><code>history\n</code></pre> <p>This will show you all the commands you have recently run. Now if you want to simply look for some very specific commands that you have run in the past you can combine <code>history</code> with <code>grep</code>:</p> <pre><code>history | grep follicle\n</code></pre> <p>This will print every command you ran in the last hour that contains the word \"follicle\". Super useful if you, like me, keep forgetting your commands!</p> <p>The last critical feature of the command line we cover today is the \"file redirect\". Instead of printing the output to file, we may chose to redirect the results to a file instead:</p> <pre><code>matentzn@mbp.local:~/my-tutorial/data $ grep \"Elevated circulating follicle\" genes_to_phenotype.txt | cut -f2 | sort | uniq | head -3 &gt; gene.txt\nmatentzn@mbp.local:~/my-tutorial/data $ head gene.txt\nAR\nBNC1\nC14ORF39\n</code></pre> <p><code>&gt; gene.txt</code> basically tells the command line: instead of printing the results to the command line, \"print\" them into a file which is called <code>gene.txt</code>.</p> <p></p>"},{"location":"tutorial/intro-cli-1/#videos","title":"Videos","text":""},{"location":"tutorial/intro-cli-1/#sam-bail-intro-to-terminal","title":"Sam Bail: Intro to Terminal","text":"<p>Sam also did here PhD in and around ontologies but has moved entirely to data engineering since. I really liked her 1 hour introduction into the terminal, this should fill some of the yawning gaps in this introduction here.</p> <p></p>"},{"location":"tutorial/intro-cli-1/#further-reading","title":"Further reading","text":"<ul> <li>Automating Ontology Development Workflows: Make, Shell and Automation Thinking</li> <li>Data Science at the Command Line: Free online book that covers everything you need to know to be a command line magician</li> <li>A whirlwind introduction to the command line by James Overton</li> </ul>"},{"location":"tutorial/intro-cli-2/","title":"Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 2","text":"<p>Today we will pick up where we left off after the first CLI tutorial, and discuss some more usages of the command line. In particular, we will:</p> <ul> <li>Introduce you into the art of managing your shell profile</li> <li>Learn how to manage your path</li> <li>Talk about how to make your shell hacking more efficient with aliases and functions.</li> </ul>"},{"location":"tutorial/intro-cli-2/#prerequisites","title":"Prerequisites","text":"<p>You have:</p> <ul> <li>Completed the first CLI tutorial</li> <li>(Optional) installed the amazing ohmyzsh! - advanced CLI for managing your ZSH profile. Important: Before installing ohmyzsh, back up you <code>~/.zshrc</code> file in case you have had any previous customisations you wish to preserve.</li> </ul>"},{"location":"tutorial/intro-cli-2/#monarch-obo-training-tutorial","title":"Monarch OBO Training Tutorial","text":"<p>Introduction to Command Line Interface Part 2 </p>"},{"location":"tutorial/intro-cli-2/#preparation","title":"Preparation","text":"<ul> <li>Install https://ohmyz.sh/ (optional)</li> <li>For advanced windows users with docker installed, you can:</li> <li>Follow the instructions here to set yourself up for ODK development.</li> <li>Place <code>odk.bat</code> as instructed above in some directory on your machine (the path to the odk.bat file should have no spaces!)</li> <li>Create a new file <code>.bash_profile</code> in the same directory as your odk.bat file.</li> <li>Add something like <code>-v %cd%\\.bash_profile:/root/.bash_profile</code> to the odk.bat file (this is mounting the <code>.bash_profile</code> file inside your ODK container). There is already a similar -v statement in this file, just copy it right after</li> <li>Enter the ODK using <code>odk.bat bash</code> on your CMD (first, <code>cd</code> to the directory containing the odk.bat file).</li> <li>Now you can follow most of this tutorial here as well.</li> </ul>"},{"location":"tutorial/intro-cli-2/#tutorial","title":"Tutorial","text":"<ul> <li>ohmyzsh! - advanced CLI (OPTIONAL)</li> <li>Managing the \"Path\": A first peak at your shell profile</li> <li>Managing aliases and functions in your bash profile</li> </ul>"},{"location":"tutorial/intro-cli-2/#ohmyzsh-advanced-cli-optional","title":"ohmyzsh! - advanced CLI (OPTIONAL)","text":"<p>If you have not done so, install https://ohmyz.sh/. It is not strictly speaking necessary to use ohmyzsh to follow the rest of this tutorial, but it is a nice way to managing your Zsh (z-shell) configuration. Note that the ODK is using the much older <code>bash</code>, but it should be fine for you to work with anyways.</p> <p></p>"},{"location":"tutorial/intro-cli-2/#managing-the-path-a-first-peak-at-your-shell-profile","title":"Managing the \"Path\": A first peak at your shell profile","text":"<p>As Semantic Engineers or Ontology Curators we frequently have to install custom tools like ROBOT, owltools, and more on our computer. These are frequently downloaded from the internet as \"binaries\", for example as Java \"jar\" files. In order for our shell to \"know\" about these downloaded programs, we have to \"add them to the path\".</p> <p>Let us first look at what we currently have loaded in our path:</p> <pre><code>echo $PATH\n</code></pre> <p>What you see here is a list of paths. To read this list a bit more easily, let us remember our lesson on piping commands:</p> <pre><code>echo $PATH | tr ':' '\\n'  | sort\n</code></pre> <p>What we do here:</p> <ol> <li>Using the <code>echo</code> command to print the contents of the $PATH variable. In Unix systems, the <code>$</code>signifies the beginning of a variable name (if you are curious about what other \"environment variables\" are currently active on your system, use the<code>printenv</code>command). The output of the<code>echo</code> command is piped to the next command (<code>tr</code>).</li> <li>The <code>tr \u2013 translate characters</code> command copies the input of the previous command to the next with substitution or deletion of selected characters. Here, we substitute the <code>:</code> character, which is used to separate the different directory paths in the <code>$PATH</code> variable, with \"\\n\", which is the all important character that denotes a \"new line\".</li> <li>Just because, we also sort the output alphabetically to make it more readable.</li> </ol> <p>So, how do we change the \"$PATH\"? Let's try and install ROBOT and see! Before we download ROBOT, let us think how we will organise our custom tools moving forward. Everyone has their own preferences, but I like to create a <code>tools</code> directory right in my Users directory, and use this for all my tools moving forward. In this spirit, lets us first go to our user directory in the terminal, and then create a \"tools\" directory:</p> <pre><code>cd ~\nmkdir -p tools\n</code></pre> <p>The <code>-p</code> parameter simply means: create the tools directory only if it does not exist. Now, let us go inside the tools directory (<code>cd ~/tools</code>) and continue following the instructions provided here.</p> <p>First, let us download the latest ROBOT release using the <code>curl</code> command:</p> <pre><code>curl -L https://github.com/ontodev/robot/releases/latest/download/robot.jar &gt; robot.jar\n</code></pre> <p>ROBOT is written in the Java programming language, and packaged up as an executable JAR file. It is still quite cumbersome to directly run a command with that JAR file, but for the hell of it, let us just do it (for fun):</p> <pre><code>java -jar robot.jar --version\n</code></pre> <p>If you have worked with ROBOT before, this looks quite a bit more ugly then simply writing:</p> <pre><code>robot --version\n</code></pre> <p>If you get this (or a similar) error:</p> <pre><code>zsh: permission denied: robot\n</code></pre> <p>You will have to run the following command as well, which makes the <code>robot</code> wrapper script executable:</p> <pre><code>chmod +x ~/tools/robot\n</code></pre> <p>So, how can we achieve this? The answer is, we download a \"wrapper script\" and place it in the same folder as the Jar. Many tools provide such wrapper scripts, and they can sometimes do many more things than just \"running the jar file\". Let us know download the latest wrapper script:</p> <pre><code>curl https://raw.githubusercontent.com/ontodev/robot/master/bin/robot &gt; robot\n</code></pre> <p>If everything went well, you should be able to print the contents of that file to the terminal using <code>cat</code>:</p> <pre><code>cat robot\n</code></pre> <p>You should see something like:</p> <pre><code>#!/bin/sh\n\n## Check for Cygwin, use grep for a case-insensitive search\nIS_CYGWIN=\"FALSE\"\nif uname | grep -iq cygwin; then\n    IS_CYGWIN=\"TRUE\"\nfi\n\n# Variable to hold path to this script\n# Start by assuming it was the path invoked.\nROBOT_SCRIPT=\"$0\"\n\n# Handle resolving symlinks to this script.\n# Using ls instead of readlink, because bsd and gnu flavors\n# have different behavior.\nwhile [ -h \"$ROBOT_SCRIPT\" ] ; do\n  ls=`ls -ld \"$ROBOT_SCRIPT\"`\n  # Drop everything prior to -&gt;\n  link=`expr \"$ls\" : '.*-&gt; \\(.*\\)$'`\n  if expr \"$link\" : '/.*' &gt; /dev/null; then\n    ROBOT_SCRIPT=\"$link\"\n  else\n    ROBOT_SCRIPT=`dirname \"$ROBOT_SCRIPT\"`/\"$link\"\n  fi\ndone\n\n# Directory that contains the this script\nDIR=$(dirname \"$ROBOT_SCRIPT\")\n\nif [ $IS_CYGWIN = \"TRUE\" ]\nthen\n    exec java $ROBOT_JAVA_ARGS -jar \"$(cygpath -w $DIR/robot.jar)\" \"$@\"\nelse\n    exec java $ROBOT_JAVA_ARGS -jar \"$DIR/robot.jar\" \"$@\"\nfi\n</code></pre> <p>We are not getting into the details of what this wrapper script does, but note that, you can fine the actually call the the ROBOT jar file towards the end: <code>java $ROBOT_JAVA_ARGS -jar \"$DIR/robot.jar\" \"$@\"</code>. The cool thing is, we do not need to ever worry about this script, but it is good for use to know, as Semantic Engineers, that it exists.</p> <p>Now, we have downloaded the ROBOT jar file and the wrapper script into the <code>~/tools</code> directory. The last step remaining is to add the <code>~/tools</code> directory to your path. It makes sense to try to at least understand the basic idea behind environment variables: variables that are \"loaded\" or \"active\" in your environment (your shell). The first thing you could try to do is change the variable right here in your terminal. To do that, we can use the <code>export</code> command:</p> <pre><code>export PATH=$PATH:~/tools\n</code></pre> <p>What you are doing here is using the <code>export</code> command to set the <code>PATH</code> variable to <code>$PATH:~/tools</code>, which is the old path (<code>$PATH</code>), a colon (<code>:</code>) and the new directory we want to add (<code>~/tools</code>). And, indeed, if we now look at our path again:</p> <pre><code>echo $PATH | tr ':' '\\n'  | sort\n</code></pre> <p>We will see the path added. We can now move around to any directory on our machine and invoke the <code>robot</code> command. Try it before moving on!</p> <p>Unfortunately, the change we have now applied to the <code>$PATH</code> variable is not persistent: if you open a new tab in your Terminal, your <code>$PATH</code> variable is back to what it was. What we have to do in order to make this persistent is to add the <code>export</code> command to a special script which is run every time the you open a new terminal: your shell profile.</p> <p>There is a lot to say about your shell profiles, and we are taking a very simplistic view here that covers 95% of what we need: If you are using <code>zsh</code> your profile is managed using the <code>~/.zshrc</code> file, and if you are using <code>bash</code>, your profile is managed using the <code>~/.bash_profile</code> file. In this tutorial I will assume you are using <code>zsh</code>, and, in particular, after installing \"oh-my-zsh\". Let us look at the first 5 lines of the <code>~/.zshrc</code> file:</p> <pre><code>head ~/.zshrc\n</code></pre> <p>If you have installed oh-my-zsh, the output will look something like:</p> <pre><code># If you come from bash you might have to change your $PATH.\n# export PATH=$HOME/bin:/usr/local/bin:$PATH\n\n# Path to your oh-my-zsh installation.\nexport ZSH=\"$HOME/.oh-my-zsh\"\n\n# Set name of the theme to load --- if set to \"random\", it will\n# load a random theme each time oh-my-zsh is loaded, in which case,\n# to know which specific one was loaded, run: echo $RANDOM_THEME\n# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes\n</code></pre> <p>This <code>~/.zshrc</code> profile script is loaded every time you open up a new shell. What we want to do is add our <code>export</code> command above to this script, so that it is running every time. That is the basic concept of a shell profile: providing a series of commands that is run every time a new shell (terminal window, tab) is opened.</p> <p>For this tutorial, we use <code>nano</code> to edit the file, but feel free to use your text editor of choice. For example, you can open the profile file using <code>TextEdit</code> on Mac like this:</p> <pre><code>open -a TextEdit ~/.zshrc\n</code></pre> <p>We will proceed using <code>nano</code>, but feel free to use any editor.</p> <pre><code>nano ~/.zshrc\n</code></pre> <p>Using terminal-based editors like nano or, even worse, vim, involves a bit of a learning curve. <code>nano</code> is by far the least powerful and simple to use. If you typed the above command, you should see its contents on the terminal. The next step is to copy the following (remember, we already used it earlier)</p> <pre><code>export PATH=$PATH:~/tools\n</code></pre> <p>and paste it somewhere into the file. Usually, there is a specific section of the file that is concerned with setting up your path. Eventually, as you become more of an expert, you will start organising your profile according to your own preferences! Today we will just copy the command anywhere, for example:</p> <pre><code># If you come from bash you might have to change your $PATH.\n# export PATH=$HOME/bin:/usr/local/bin:$PATH\nexport PATH=~/tutorial:$PATH\n# ..... other lines in the file\n</code></pre> <p>Note that the <code>#</code> symbol denotes the beginning of a \"comment\" which is ignored by the shell/CLI. After you have pasted the above, you use the following keyboard key-combinations to safe and close the file:</p> <pre><code>control + O\n</code></pre> <p>This saves the file. Confirm with Enter.</p> <pre><code>control + x\n</code></pre> <p>This closes the file. Now, we need to tell the shell we are currently in that it should reload our profile we have just edited. We do that using the <code>source</code> command.</p> <pre><code>source ~/.zshrc\n</code></pre> <p>Great! You should be able open a new tab in your terminal (with command+t on a Mac, for example) and run the following command:</p> <pre><code>robot --version\n</code></pre> <p></p>"},{"location":"tutorial/intro-cli-2/#managing-aliases-and-custom-commands-in-your-shell-profile","title":"Managing aliases and custom commands in your shell profile","text":"<p>This section will only give a sense of the kinds of things you can do with your shell profile - in the end you will have to jump into the cold water and build your skills up yourself. Let us start with a very powerful concept: aliases. Aliases are short names for your commands you can use if you use them repeatedly but are annoyed typing them out every time. For example, tired of typing out long paths all the time to jump between your Cell Ontology and Human Phenotype Ontology directories? Instead of:</p> <pre><code>cd /Users/matentzn/ws/human-phenotype-ontology/src/ontology\n</code></pre> <p>wouldn't it be nice to be able to use, instead,</p> <pre><code>cdhp\n</code></pre> <p>or, if you are continuously checking <code>git status</code>, why not implement a alias <code>gits</code>? Or activating your python environment (<code>source ~/.pyenv/versions/oak/bin/activate</code>) with a nice <code>env-oak</code>? To achieve this we do the following:</p> <p>(1) Open your profile in a text editor of your choice, e.g.</p> <pre><code>nano ~/.zshrc\n</code></pre> <p>add the following lines:</p> <pre><code>alias cdt='cd ~/tools'\nalias hg='history | grep'\n</code></pre> <p>Save (control+o) and close (control+x) the profile. Reload the profile:</p> <pre><code>source ~/.zshrc\n</code></pre> <p>(Alternatively, just open a new tab in your Terminal.) Now, lets try our new aliases:</p> <pre><code>cdt\n</code></pre> <p>Will bring you straight to your <code>tools</code> directory you created in the previous lesson above.</p> <pre><code>hg robot\n</code></pre> <p>Will search your terminal command history for every command you have executed involving <code>robot</code>.</p>"},{"location":"tutorial/intro-cli-2/#list-of-ideas-for-aliases","title":"List of ideas for aliases","text":"<p>In the following, we provide a list of aliases we find super useful:</p> <ol> <li><code>alias cdt='cd ~/tools'</code> - add shortcuts to all directories you frequently visit!</li> <li><code>alias orcid='echo '\\''https://orcid.org/0000-0002-7356-1779'\\'' | tr -d '\\''\\n'\\'' | pbcopy'</code> - if you keep having to look up your ORCID, your favourite ontologies PURL or the your own zoom room, why not add a shortcut that copies it straight into your clipboard?</li> <li><code>alias opent='open ~/tools'</code> - why not open your favourite directory in finder without faving to search the User Interface? You can use the same idea to open your favourite ontology from wherever you are, i.e. <code>alias ohp='open ~/ws/human-phenotype-ontology/src/ontology/hp-edit.owl'</code>.</li> <li><code>alias env-linkml='source ~/.pyenv/versions/linkml/bin/activate'</code> - use simple shortcuts to active your python environments. This will become more important if you learn to master special python tools like OAK.</li> <li><code>alias update_repo='sh run.sh make update_repo'</code> - for users of ODK - alias all your long ODK commands!</li> </ol>"},{"location":"tutorial/intro-cli-2/#functions","title":"Functions","text":"<p>The most advanced thought we want to cover today is \"functions\". You can not only manage simple aliases, but you can actually add proper functions into your shell profile. Here is an example of one that I use:</p> <pre><code>ols() {\n    open https://www.ebi.ac.uk/ols/search?q=\"$1\"\n}\n</code></pre> <p>This is a simple function in my bash profile that I can use to search on OLS:</p> <pre><code>ols \"lung disorder\"\n</code></pre> <p>It will open this search straight in my browser.</p> <pre><code>rreport() {\n    robot report -i \"$1\" --fail-on none -o /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv\n}\n</code></pre> <p>This allows me to quickly run a robot report on an ontology.</p> <pre><code>rreport cl.owl\n</code></pre> <p>Why not expand the function and have it open in my atom text editor right afterwards?</p> <pre><code>rreport() {\n    robot report -i \"$1\" --fail-on none -o /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv &amp;&amp; atom /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv\n}\n</code></pre> <p>The possibilities are endless. Some power-users have hundreds of such functions in their shell profiles, and they can do amazing things with them. Let us know about your own ideas for functions on the OBOOK issue tracker. Or, why not add a function to create a new, titled issue on OBOOK?</p> <pre><code>obook-issue() {\n  open https://github.com/OBOAcademy/obook/issues/new?title=\"$1\"\n}\n</code></pre> <p>and from now on run:</p> <pre><code>obook-issue \"Add my awesome function\"\n</code></pre> <p></p>"},{"location":"tutorial/intro-cli-2/#further-reading","title":"Further reading","text":"<ul> <li>Automating Ontology Development Workflows: Make, Shell and Automation Thinking</li> <li>Data Science at the Command Line: Free online book that covers everything you need to know to be a command line magician</li> <li>A whirlwind introduction to the command line by James Overton</li> </ul>"},{"location":"tutorial/lexmatch-tutorial/","title":"Practical introduction to OAK Lexmatch","text":"<p>In this tutorial, we will learn to use a very basic lexical matching tool (OAK Lexmatch). The goal is not only to enable the learner to design their own matching pipelines, but also to to think about how they fit into their mapping efforts. Note that this tutorial is not about how to do proper matching: the goal here is simply to introduce you to the general workflow. Proper ontology matching is a major discipline with many tools, preprocessing and tuning approaches and often intricate interplay between matching tools and human curators. Today, you will just get a sense of the general method.</p>"},{"location":"tutorial/lexmatch-tutorial/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Introduction to mapping curation with SSSOM</li> <li>ROBOT tutorial (extract)</li> <li>ROBOT tutorial (merge)</li> </ul>"},{"location":"tutorial/lexmatch-tutorial/#tutorial","title":"Tutorial","text":"<p>In this tutorial, you will learn how to match fruit juices in Wikidata with FOODON using a simple lexical matching tool (OAK). The idea is simple: We obtain the ontologies we like to match, ask OAK to generate the matches and then curate the results.</p> <ol> <li>Setting up OAK, preparing the ontology: You will learn how to install OAK into a new Python environment, and create a simple <code>Makefile</code> to prepare your input ontology with ROBOT.</li> <li>Download Ontologies and extract subsets: Prepare the data/ontologies you want to match.</li> <li>Generate the matches with OAK: Generate the matches using a simple tool.</li> <li>Curating matching results: Manually review the results of the mars  </li> </ol> <p></p>"},{"location":"tutorial/lexmatch-tutorial/#setting-up-oak-preparing-the-ontology","title":"Setting up OAK, preparing the ontology","text":"<p>Setting up <code>oak</code> is described in its documentation. Note that, aside from <code>oak</code> itself, you also need <code>relation-graph</code>, <code>rdftab</code> and <code>riot</code> installed, see https://incatools.github.io/ontology-access-kit/intro/tutorial07.html#without-docker.  This tutorial requires OAK version 0.1.59 or higher.</p> <p>Note that if you are using the ODK docker image, <code>oaklib</code> is already installed. In the following, we will use the ODK wrapper to ensure that everyone has a consistent experience. If you want to use the local (non-docker) setup, you have to follow the instructions above before continuing and ignore the <code>sh odk.sh</code> part of the commands.</p>"},{"location":"tutorial/lexmatch-tutorial/#important-for-users-of-odk-132","title":"IMPORTANT for users of ODK &lt; 1.3.2:","text":"<p>ODK 1.3.1, the version still active on the 8th December 2022, does not have the latest dependencies of OAK installed.  To follow the tutorial you have to use the ODK development snapshot.</p> <p>Install the ODK Development snapshot:</p> <pre><code>docker pull obolibrary/odkfull:dev\n</code></pre> <p>After downloading https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/resources/odk.sh into your local working directory, open it with a text editor and change: </p> <pre><code>docker ... obolibrary/odkfull ...\n</code></pre> <p>to</p> <pre><code>docker ... obolibrary/odkfull:dev ...\n</code></pre> <p></p>"},{"location":"tutorial/lexmatch-tutorial/#download-ontologies-and-extract-subsets","title":"Download Ontologies and extract subsets","text":"<p>First, we download <code>FOODON</code> ontology. You can do this in whatever way you want, for example with <code>wget</code>:</p> <pre><code>sh odk.sh wget http://purl.obolibrary.org/obo/foodon.owl -O foodon.owl\n</code></pre> <p>Next, we extract the subset of FOODON that is relevant to our task at hand: relevant terms about fruit juices. The right method of subset extraction will differ from task to task. For this tutorial, we are using ROBOT extract to obtain a <code>MIREOT</code> module containing all the fruit juices. We do this by selecting everything between <code>fruit juice food product</code> as the <code>upper-term</code> and fruit juices (<code>apple juice</code>, <code>orange juice</code> and <code>grapefruit juice</code>) as the <code>lower-term</code> of the <code>FOODON</code> subset.</p> <pre><code>sh odk.sh robot extract --method MIREOT --input foodon.owl --upper-term \"FOODON:00001140\" --lower-term \"FOODON:00001277\" --lower-term \"FOODON:00001059\" --lower-term \"FOODON:03306174 \" --output fruit_juice_food_foodon.owl\n</code></pre> <p>If you open <code>fruit_juice_food_foodon.owl</code> in Protege, you will see something similar to:</p> <p></p> <p>Next, we use OAK to extract juices and their labels from wikidata by selecting the descendants of <code>juice</code> from <code>wikidata</code>, store the result as a <code>ttl</code> file and then convert it to <code>OWL</code> using <code>ROBOT</code>.</p> <pre><code>sh odk.sh runoak -i wikidata: descendants wikidata:Q8492 -p i,p -o juice_wd.ttl -O rdf\nsh odk.sh robot convert -i juice_wd.ttl  -o juice_wd.owl\n</code></pre> <p>Note that you wont be able to see anything when opening <code>juice_wd.owl</code> in wikidata, because it does not have any OWL types (class, individual assertions) attached to it. However, you can convince yourself all is well by opening <code>juice_wd.owl</code> in a text editor, and see expressions such as:</p> <pre><code>&lt;rdf:Description rdf:about=\"http://www.wikidata.org/entity/Q10374646\"&gt;\n    &lt;rdfs:label&gt;cashew apple juice&lt;/rdfs:label&gt;\n&lt;/rdf:Description&gt;\n</code></pre> <p>The last preparation step is merging the two subsets (from FOODON and wikidata) into a single file using <code>ROBOT</code>:</p> <pre><code>sh odk.sh robot merge -i fruit_juice_food_foodon.owl -i juice_wd.owl -o foodon_wd.owl\n</code></pre> <p></p>"},{"location":"tutorial/lexmatch-tutorial/#generate-the-matches-with-oak","title":"Generate the matches with OAK","text":"<p>Now we are ready to create our first set of matches. First, let's run <code>oak</code>'s <code>lexmatch</code> command to generate lexical matches between the contents of the merged file:</p> <pre><code>sh odk.sh runoak -i sqlite:foodon_wd.owl lexmatch -o foodon_wd_lexmatch.tsv\n</code></pre> <p>This will generate an SSSOM tsv file with the mapped contents as shown below:</p> <pre><code># curie_map:\n#   FOODON: http://purl.obolibrary.org/obo/FOODON_\n#   owl: http://www.w3.org/2002/07/owl#\n#   rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n#   rdfs: http://www.w3.org/2000/01/rdf-schema#\n#   semapv: https://w3id.org/semapv/\n#   skos: http://www.w3.org/2004/02/skos/core#\n#   sssom: https://w3id.org/sssom/\n#   wikidata: http://www.wikidata.org/entity/\n# license: https://w3id.org/sssom/license/unspecified\n# mapping_set_id: https://w3id.org/sssom/mappings/091390a2-6f64-436d-b2d1-309045ff150c\n</code></pre> subject_id subject_label predicate_id object_id object_label mapping_justification mapping_tool confidence subject_match_field object_match_field match_string FOODON:00001059 apple juice skos:closeMatch wikidata:Q618355 apple juice semapv:LexicalMatching oaklib 0.5 rdfs:label rdfs:label apple juice FOODON:00001059 apple juice skos:closeMatch wikidata:Q618355 apple juice semapv:LexicalMatching oaklib 0.5 oio:hasExactSynonym rdfs:label apple juice FOODON:03301103 orange juice skos:closeMatch wikidata:Q219059 orange juice semapv:LexicalMatching oaklib 0.5 rdfs:label rdfs:label orange juice FOODON:03306174 grapefruit juice skos:closeMatch wikidata:Q1138468 grapefruit juice semapv:LexicalMatching oaklib 0.5 rdfs:label rdfs:label grapefruit juice wikidata:Q15823640 cherry juice skos:closeMatch wikidata:Q62030277 cherry juice semapv:LexicalMatching oaklib 0.5 rdfs:label rdfs:label cherry juice wikidata:Q18201657 must skos:closeMatch wikidata:Q278818 must semapv:LexicalMatching oaklib 0.5 rdfs:label rdfs:label must <p>This is great - we get a few mappings without much work. If you need some help interpreting this table, please refer to the SSSOM tutorials for details.</p> <p>Just eyeballing the labels in our ontology with OAK:</p> <pre><code>sh odk.sh runoak -i sqlite:foodon_wd.owl terms | grep juice\n</code></pre> <p>We notice rows like:</p> <pre><code>...\nFOODON:00001001 ! orange juice (liquid)\n...\n</code></pre> <p>It may be beneficial for us to pre-process the labels a bit before performing the matches, for example, by excluding comments in the labels provided in brackets (essentially removing <code>(liquid)</code>).</p> <p>To do this, we will define a few simple mapping rules in a file called <code>matcher_rules.yaml</code>. OAK provides a standard for representing the matching rules. You can see an example here.</p> <p>Here is an example file:</p> <pre><code>rules:\n  - description: default\n    postconditions:\n      predicate_id: skos:closeMatch\n      weight: 0.0\n\n  - description: exact to exact\n    preconditions:\n      subject_match_field_one_of:\n        - oio:hasExactSynonym\n        - rdfs:label\n        - skos:prefLabel\n      object_match_field_one_of:\n        - oio:hasExactSynonym\n        - rdfs:label\n        - skos:prefLabel\n    postconditions:\n      predicate_id: skos:exactMatch\n      weight: 2.0\n\n  - preconditions:\n      subject_match_field_one_of:\n        - oio:hasExactSynonym\n        - rdfs:label\n      object_match_field_one_of:\n        - oio:hasBroadSynonym\n    postconditions:\n      predicate_id: skos:broadMatch\n      weight: 2.0\n\n  - synonymizer:\n      the_rule: Remove parentheses bound info from the label.\n      match: r'\\([^)]*\\)'\n      match_scope: \"*\"\n      replacement: \"\"\n\n  - synonymizer:\n      the_rule: Replace \"'s\" by \"s\" in the label.\n      match: r'\\'s'\n      match_scope: \"*\"\n      replacement: \"s\"\n</code></pre> <p>As you can see, there are basically two kinds of rules: normal ones, and <code>synonimizer</code> ones. The normal rules provide preconditions and postconditions. For example, the second rule says:  if an exact synonym, preferred label or label of the subject matches an exact synonym, preferred label or label of the object, then assert a <code>skos:exactMatch</code>. The <code>synonimizer</code> rules are preprocessing rules which are applied to the labels and synonyms prior to matching. Let's now run the matcher again:</p> <pre><code>sh odk.sh runoak -i sqlite:foodon_wd.owl lexmatch -R matcher_rules.yaml -o foodon_wd_lexmatch_with_rules.tsv \n</code></pre> <p>This will generate an SSSOM tsv file with a few more matches than the previous output (the exact matches may differ from version to version):</p> <pre><code># curie_map:\n#   FOODON: http://purl.obolibrary.org/obo/FOODON_\n#   IAO: http://purl.obolibrary.org/obo/IAO_\n#   owl: http://www.w3.org/2002/07/owl#\n#   rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n#   rdfs: http://www.w3.org/2000/01/rdf-schema#\n#   semapv: https://w3id.org/semapv/\n#   skos: http://www.w3.org/2004/02/skos/core#\n#   sssom: https://w3id.org/sssom/\n#   wikidata: http://www.wikidata.org/entity/\n# license: https://w3id.org/sssom/license/unspecified\n# mapping_set_id: https://w3id.org/sssom/mappings/6b9c727f-9fdc-4a78-bbda-a107b403e3a9\n</code></pre> subject_id subject_label predicate_id object_id object_label mapping_justification mapping_tool confidence subject_match_field object_match_field match_string subject_preprocessing object_preprocessing FOODON:00001001 orange juice (liquid) skos:exactMatch FOODON:00001277 orange juice (unpasteurized) semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label orange juice semapv:RegularExpressionReplacement semapv:RegularExpressionReplacement FOODON:00001001 orange juice (liquid) skos:exactMatch FOODON:03301103 orange juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label orange juice semapv:RegularExpressionReplacement FOODON:00001001 orange juice (liquid) skos:exactMatch wikidata:Q219059 orange juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label orange juice semapv:RegularExpressionReplacement FOODON:00001059 apple juice skos:exactMatch wikidata:Q618355 apple juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label apple juice FOODON:00001059 apple juice skos:exactMatch wikidata:Q618355 apple juice semapv:LexicalMatching oaklib 0.8 oio:hasExactSynonym rdfs:label apple juice FOODON:00001277 orange juice (unpasteurized) skos:exactMatch FOODON:03301103 orange juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label orange juice semapv:RegularExpressionReplacement FOODON:00001277 orange juice (unpasteurized) skos:exactMatch wikidata:Q219059 orange juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label orange juice semapv:RegularExpressionReplacement FOODON:00002403 food material skos:exactMatch FOODON:03430109 food (liquid, low viscosity) semapv:LexicalMatching oaklib 0.8 oio:hasExactSynonym rdfs:label food semapv:RegularExpressionReplacement FOODON:00002403 food material skos:exactMatch FOODON:03430130 food (liquid) semapv:LexicalMatching oaklib 0.8 oio:hasExactSynonym rdfs:label food semapv:RegularExpressionReplacement FOODON:03301103 orange juice skos:exactMatch wikidata:Q219059 orange juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label orange juice FOODON:03306174 grapefruit juice skos:exactMatch wikidata:Q1138468 grapefruit juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label grapefruit juice FOODON:03430109 food (liquid, low viscosity) skos:exactMatch FOODON:03430130 food (liquid) semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label food semapv:RegularExpressionReplacement semapv:RegularExpressionReplacement wikidata:Q15823640 cherry juice skos:exactMatch wikidata:Q62030277 cherry juice semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label cherry juice wikidata:Q18201657 must skos:exactMatch wikidata:Q278818 must semapv:LexicalMatching oaklib 0.8497788951776651 rdfs:label rdfs:label must <p></p>"},{"location":"tutorial/lexmatch-tutorial/#curate","title":"Curate","text":"<p>As we have described in detail in our introduction to Semantic Matching, it is important to remember that matching in its raw form should not be understood to result in semantic mappings: they are better understood as mapping candidates. Therefore, it is always to plan for a review of false positives and false negatives:</p> <ol> <li>False positive review (wrong mappings). The goal here is to identify mappings that do not work for our use case, or do not use the correct semantic mapping relation. If you look carefully through the matched files, you'll notice that manual intervention is definitely required for the matches to be accurate. For example, <code>orange juice [wikidata:Q219059]</code> and <code>orange juice (unpasteurized) [FOODON:00001277]</code> may not be considered as the same thing in the sense of <code>skos:exactMatch</code>. </li> <li>False negative review (missing mappings). The goal here is to understand if we can do anything to improve the matching process by tuning it. Here, we did a little bit of that earlier when we were looking at the labels of the wikidata and FOODON juice hierarchies.</li> </ol> <p>For a more detailed introduction into manual mapping curation with SSSOM we recommend following this tutorial: https://mapping-commons.github.io/sssom/tutorial/.</p>"},{"location":"tutorial/linking-data/","title":"Tutorial: From Tables to Linked Data","text":"<p>These are the kinds of things that I do when I need to work with a new dataset. My goal is to have data that makes good sense and that I can integrate with other data using standard technologies: Linked Data.</p>"},{"location":"tutorial/linking-data/#0-before","title":"0. Before","text":"<p>The boss just sent me this new table to figure out:</p> datetime investigator subject species strain sex group protocol organ disease qualifier comment 1/1/14 10:21 AM JAO 12 RAT F 344/N FEMALE 1 HISTOPATHOLOGY LUNG ADENOCARCINOMA SEVERE 1/1/14 10:30 AM JO 31 MOUSE B6C3F1 MALE 2 HISTOPATHOLOGY NOSE INFLAMMATION MILD 1/1/14 10:45 AM JAO 45 RAT F 344/N MALE 1 HISTOPATHOLOGY ADRENAL CORTEX NECROSIS MODERATE <p>It doesn't seem too bad, but there's lots of stuff that I don't quite understand. Where to start?</p>"},{"location":"tutorial/linking-data/#1-getting-organized","title":"1. Getting Organized","text":"<p>Before I do anything else, I'm going to set up a new project for working with this data. Maybe I'll change my mind later and want to merge the new project with an existing project, but it never hurts to start from a nice clean state.</p> <p>I'll make a new directory in a sensible place with a sensible name. In my case I have a <code>~/Repositories/</code> directory, with subdirectories for GitHub and various GitLab servers, a <code>local</code> directory for projects I don't plan to share, and a <code>temp</code> directory for projects that I don't need to keep. I'm not sure if I'm going to share this work, so it can go in a new subdirectory of <code>local</code>. I'll call it \"linking-data-tutorial\" for now.</p> <p>Then I'll run <code>git init</code> to turn that directory into a git repository. For now I'm just going to work locally, but later I can make a repository on GitHub and push my local repository there.</p> <p>Next I'll create a <code>README.md</code> file where I'll keep notes for myself to read later. My preferred editor is Kakoune.</p> <p>So I'll open a terminal and run these commands:</p> <pre><code>$ cd ~/Repositories/local/\n$ mkdir linking-data-tutorial\n$ cd linking-data-tutorial\n$ git init\n$ kak README.md\n</code></pre> <p>In the README I'll start writing something like this:</p> <pre><code># Linking Data Tutorial\n\nAn example of how to convert a dataset to Linked Data.\n\nThe source data is available from\n&lt;https://github.com/jamesaoverton/obook/tree/master/03-RDF/data.csv&gt;\n</code></pre> <p>Maybe this information should go somewhere else eventually, but the README is a good place to start.</p> <p>\"Commit early, commit often\" they say, so:</p> <pre><code>$ git add README.md\n$ git commit -m \"Initial commit\"\n</code></pre>"},{"location":"tutorial/linking-data/#2-getting-copies","title":"2. Getting Copies","text":"<p>Data has an annoying tendency to get changed. You don't want it changing out from under you while you're in the middle of something. So the next thing to do is get a copy of the data and store it locally. If it's big, you can store a compressed copy. If it's too big to fit on your local machine, well keep the best notes you can of how to get to the data, and what operations you're doing on it.</p> <p>I'm going to make a <code>cache</code> directory and store all my \"upstream\" data there. I'm going to fetch the data and that's it -- I'm not going to edit these files. When I want to change the data I'll make copies in another directory. I don't want git to track the cached data, so I'll add <code>/cache/</code> to <code>.gitignore</code> and tell git to track that. Then I'll use <code>curl</code> to download the file.</p> <pre><code>$ mkdir cache\n$ echo \"/cache/\" &gt;&gt; .gitignore\n$ git add .gitignore\n$ git commit -m \"Ignore /cache/ directory\"\n$ cd cache\n$ curl -LO \"https://github.com/jamesaoverton/obook/raw/master/03-RDF/data.csv\"\n$ ls\ndata.csv\n$ cd ..\n$ ls -a\n.gitignore data README.md\n</code></pre>"},{"location":"tutorial/linking-data/#3-getting-my-bearings","title":"3. Getting My Bearings","text":"<p>The first thing to do is look at the data. In this case I have just one table in CSV format, so I can use any number of tools to open the file and look around. I bet the majority of people would reach for Excel. My (idiosyncratic) preference is VisiData.</p> <p>What am I looking for? A bunch of different things:</p> <ul> <li>what do the rows represent?</li> <li>what columns do I have?</li> <li>for each column, what sorts of values do I have?</li> </ul> <p>In my README file I'll make a list of the columns like this:</p> <pre><code>- datetime\n- investigator\n- subject\n- species\n- strain\n- sex\n- group\n- protocol\n- organ\n- disease\n- qualifier\n- comment\n</code></pre> <p>Then I'll make some notes for myself:</p> <pre><code>- datetime: American-style dates, D/M/Y or M/D/Y?\n- investigator: initials, ORCID?\n- subject: integer ID\n- species: common name for species, NCBITaxon?\n- strain: some sort of code with letters, numbers, spaces, some punctuation\n- sex: string female/male\n- group: integer ID\n- protocol: string, OBI?\n- organ: string, UBERON?\n- disease: string, DO/MONDO?\n- qualifier: string, PATO?\n- comment: ???\n</code></pre> <p>You can see that I'm trying to figure out what's in each column. I'm also thinking ahead to OBO ontologies that I know of that may have terms that I can use for each column.</p>"},{"location":"tutorial/linking-data/#4-getting-structured","title":"4. Getting Structured","text":"<p>In the end, I want to have nice, clean Linked Data. But I don't have to get there in one giant leap. Instead I'll take a bunch of small, incremental steps.</p> <p>There's lots of tools I can use, but this time I'll use SQLite.</p> <p>First I'll set up some more directories. I'll create a <code>build</code> directory where I'll store temporary files. I don't want git to track this directory, so I'll add it to <code>.gitignore</code>.</p> <pre><code>$ mkdir build/\n$ echo \"/build/\" &gt;&gt; .gitignore\n$ git add .gitignore\n$ git commit -m \"Ignore /build/ directory\"\n</code></pre> <p>I'll also add a <code>src</code> directory to store code. I do want to track <code>src</code> with git.</p> <pre><code>$ mkdir src\n$ kak src/data.sql\n</code></pre> <p>In <code>src/data.sql</code> I'll add just enough to import <code>build/data.csv</code>:</p> <pre><code>-- import build/data.csv\n.mode csv\n.import build/data.csv data_csv\n</code></pre> <p>This will create a <code>build/data.db</code> file and import <code>build/data.csv</code> into a <code>data_csv</code> table. Does it work?</p> <pre><code>$ sqlite3 build/data.db &lt; src/data.sql\n$ sqlite3 build/data.db &lt;&lt;&lt; \"SELECT * FROM data_csv LIMIT 1;\"\n2014-01-01 10:21:00-0500|JAO|12|RAT|F 344/N|FEMALE|1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE|\n</code></pre> <p>Nice!</p> <p>Note that I didn't even specify a schema for <code>data_csv</code>. It uses the first row as the column names, and the type of every column is <code>TEXT</code>. Here's the schema I end up with:</p> <pre><code>$ sqlite3 build/data.db &lt;&lt;&lt; \".schema data_csv\"\nCREATE TABLE data_csv(\n  \"datetime\" TEXT,\n  \"investigator\" TEXT,\n  \"subject\" TEXT,\n  \"species\" TEXT,\n  \"strain\" TEXT,\n  \"sex\" TEXT,\n  \"group\" TEXT,\n  \"protocol\" TEXT,\n  \"organ\" TEXT,\n  \"disease\" TEXT,\n  \"qualifier\" TEXT,\n  \"comment\" TEXT\n);\n</code></pre> <p>I'm going to want to update <code>src/data.sql</code> then rebuild the database over and over. It's small, so this will only take a second. If it was big, then I would copy a subset into <code>build/data.csv</code> for now so that I the script still runs in a second or two and I can iterate quickly. I'll write a <code>src/build.sh</code> script to make life a little easier:</p> <pre><code>#!/bin/sh\n\nrm -f build/*\ncp cache/data.csv build/data.csv\nsqlite3 build/data.db &lt; src/data.sql\n</code></pre> <p>Does it work?</p> <pre><code>$ sh src/build.sh\n</code></pre> <p>Nice! Time to update the README:</p> <pre><code>## Requirements\n\n- [SQLite3](https://sqlite.org/index.html)\n\n## Usage\n\nRun `sh src/build.sh`\n</code></pre> <p>I'll commit my work in progress:</p> <pre><code>$ git add src/data.sql src/build.sh\n$ git add --update\n$ git commit -m \"Load data.csv into SQLite\"\n</code></pre> <p>Now I have a script that executes a SQL file that loads the source data into a new database. I'll modify the <code>src/data.sql</code> file in a series of small steps until it has the structure that I want.</p>"},{"location":"tutorial/linking-data/#5-getting-clean","title":"5. Getting Clean","text":"<p>In the real world, data is always a mess. It takes real work to clean it up. And really, it's almost never perfectly clean.</p> <p>It's important to recognize that cleaning data has diminishing returns. There's low hanging fruit: easy to clean, often with code, and bringing big benefits. Then there's tough stuff that requires an expert to work through the details, row by row.</p> <p>The first thing to do is figure out the schema you want. I'll create a new <code>data</code> table and start with the default schema from <code>data_csv</code>. Notice that in the default schema all the column names are quoted. That's kind of annoying. But when I remove the quotation marks I realize that one of the column names is \"datetime\", but <code>datetime</code> is a keyword in SQLite! You can't use it as a column name without quoting. I'll rename it to \"assay_datetime\". I have the same problem with \"group\". I'll rename \"group\" to \"group_id\" and \"subject\" to \"subject_id\". The rest of the column names seem fine.</p> <p>I want \"assay_datetime\" to be in standard ISO datetime format, but SQLite stores these as TEXT. The \"subject\" and \"group\" columns are currently integers, but I plan to make them into URIs to CURIEs. So everything will still be TEXT.</p> <pre><code>CREATE TABLE data(\n  assay_datetime TEXT,\n  investigator TEXT,\n  subject_id TEXT,\n  species TEXT,\n  strain TEXT,\n  sex TEXT,\n  group_id TEXT,\n  protocol TEXT,\n  organ TEXT,\n  disease TEXT,\n  qualifier TEXT,\n  comment TEXT\n);\n</code></pre> <p>The dates currently look like \"1/1/14 10:21 AM\". Say I know that they were done on Eastern Standard Time. How do I convert to ISO dates like \"2014-01-01 10:21:00-0500\"? Well SQLite isn't the right tool for this. The Unix <code>date</code> command does a nice job, though:</p> <pre><code>$ date -d \"1/1/14 10:21 AM EST\" +\"%Y-%m-%d %H:%M:%S%z\"\n2014-01-01 10:21:00-0500\n</code></pre> <p>I can run that over each line of the file using <code>awk</code>. So I update the <code>src/build.sh</code> to rework the <code>build/data.csv</code> before I import:</p> <pre><code>#!/bin/sh\n\nrm -f build/*\n\nhead -n1 cache/data.csv &gt; build/data.csv\ntail -n+2 cache/data.csv \\\n| awk 'BEGIN{FS=\",\"; OFS=\",\"} {\n  \"date -d \\\"\"$1\" EST\\\" +\\\"%Y-%m-%d %H:%M:%S%z\\\"\" | getline $1;\n  print $0\n}' \\\n&gt;&gt; build/data.csv\n\nsqlite3 build/data.db &lt; src/data.sql\n</code></pre> <p>One more problem I could clean up is that \"JO\" should really be \"JAO\" -- that's just a typo, and they should both refer to James A. Overton. I could make that change in <code>src/build.sh</code>, but I'll do it in <code>src/data.sql</code> instead. I'll write a query to copy all the rows of <code>data_csv</code> into <code>data</code> and then I'll update <code>data</code> with some fixes.</p> <pre><code>-- copy from data_csv to data\nINSERT INTO data SELECT * FROM data_csv;\n\n-- clean data\nUPDATE data SET investigator=\"JAO\" WHERE investigator=\"JO\";\n</code></pre> <p>Honestly, it took me quite a while to write that <code>awk</code> command. It's a very powerful tool, but I don't use it enough to remember how it works. You might prefer to write yourself a Python script, or some R code. You could use that instead of this SQL UPDATE as well. I just wanted to show you two of the thousands of ways to do this. If there's a lot of replacements like \"JO\", then you might also consider listing them in another table that you can read into your script.</p> <p>The important part is to automate your cleaning!</p> <p>Why didn't I just edit <code>cache/data.csv</code> in Excel? In step 2 I saved a copy of the data because I didn't want it to change while I was working on it, but I do expect it to change! By automating the cleaning process, I should be able to just update <code>cache/data.csv</code> run everything again, and the fixes will be applied again. I don't want to do all this work manually every time the upstream data is updated.</p> <p>I'll commit my work in progress:</p> <pre><code>$ git add --update\n$ git commit -m \"Start cleaning data\"\n</code></pre> <p>Cleaning can take a lot of work. This is example table is pretty clean already. The next hard part is sorting out your terminology.</p>"},{"location":"tutorial/linking-data/#6-getting-connected","title":"6. Getting Connected","text":"<p>It's pretty easy to convert a table structure to triples. The hard part is converting the table contents. There are some identifiers in the table that would be better as URLs, and there's a bunch of terminology that would be better if it was linked to an ontology or other system.</p> <p>I'll start with the identifiers that are local to this data: subject_id and group_id. I can convert them to URLs by defining a prefix and then just using that prefix. I'll use string concatenation to update the table:</p> <pre><code>-- update subject and groupd IDs\nUPDATE data SET subject_id='ex:subject-' || subject_id;\nUPDATE data SET group_id='ex:group-' || group_id;\n</code></pre> <p>Now I'll check my work:</p> <pre><code>$ sqlite3 build/data.db &lt;&lt;&lt; \"SELECT * FROM data_csv LIMIT 1;\"\n2014-01-01 10:21:00-0500|JAO|ex:subject-12|RAT|F 344/N|FEMALE|ex:group-1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE|\n</code></pre> <p>I should take a moment to tell you, that while I was writing the Turtle conversion code later in this essay, I had to come back here and change these identifiers. The thing is that Turtle is often more strict than I expect about identifier syntax. Turtle identifiers look like CURIEs, but they're actually QNames. CURIEs are pretty much just just URLs shortened with a prefix, so almost anything goes. QNames come from XML, and Turtle identifiers have to be valid XML element names.</p> <p>I always remember that I need to stick to alphanumeric characters, and that I have to replace whitespace and punctuation with a <code>-</code> or <code>_</code>. I didn't remember that the local part (aka \"suffix\", aka \"NCName\") can't start with a digit. So I tried to use \"subject:12\" and \"group:1\" as my identifiers. That worked fine until I generated Turtle. The Turtle looked fine, so it took me quite a while to figure out why it looked very wrong when I converted it into RDXML format.</p> <p>This kind of thing happens to me all the time. I'm almost always using a mixture of technologies based on different sets of assumptions, and there are always things that don't line up. That's why I like to work in small iterations, checking my work as I go (preferrably with automated tests), and keeping everything in version control. When I need to make a change like this one, I just circle back and iterate again.</p> <p>The next thing is to tackle the terminology. First I'll just make a list of the terms I'm using from the relevant columns in <code>build/term.tsv</code>:</p> <pre><code>$ sqlite3 build/data.db &lt;&lt; EOF &gt; build/term.tsv\nSELECT investigator FROM data\nUNION SELECT species FROM data\nUNION SELECT strain FROM data\nUNION SELECT strain FROM data\nUNION SELECT sex FROM data\nUNION SELECT protocol FROM data\nUNION SELECT organ FROM data\nUNION SELECT disease FROM data\nUNION SELECT qualifier FROM data;\nEOF\n</code></pre> <p>It's a lot of work to go through all those terms and find good ontology terms. I'm going to do that hard work for you (just this once!) so we can keep moving. I'll add this table to <code>src/term.tsv</code></p> id code label obo:NCBITaxon_10116 RAT Rattus norvegicus obo:NCBITaxon_10090 MOUSE Mus musculus ex:F344N F 344/N F 344/N ex:B6C3F1 B6C3F1 B6C3F1 obo:PATO_0000383 FEMALE female obo:PATO_0000384 MALE male obo:OBI_0600020 HISTOPATHOLOGY histology obo:UBERON_0002048 LUNG lung obo:UBERON_0007827 NOSE external nose obo:UBERON_0001235 ADRENAL CORTEX adrenal cortex obo:MPATH_268 ADENOCARCINOMA adenocarcinoma obo:MPATH_212 INFLAMMATION inflammation obo:MPATH_4 NECROSIS necrosis obo:PATO_0000396 SEVERE severe intensity obo:PATO_0000394 MILD mild intensity obo:PATO_0000395 MODERATE moderate intensity orcid:0000-0001-5139-5557 JAO James A. Overton <p>And I'll add these prefixes to <code>src/prefix.tsv</code>:</p> prefix base rdf http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs http://www.w3.org/2000/01/rdf-schema# xsd http://www.w3.org/2001/XMLSchema# owl http://www.w3.org/2002/07/owl# obo http://purl.obolibrary.org/obo/ orcid http://orcid.org/ ex https://example.com/ subject https://example.com/subject/ group https://example.com/group/ <p>Now I can import these tables into SQL and use the term table as a FOREIGN KEY constraint on data:</p> <pre><code>.mode tabs\n\nCREATE TABLE prefix (\n  prefix TEXT PRIMARY KEY,\n  base TEXT UNIQUE\n);\n.import --skip 1 src/prefix.tsv prefix\n\nCREATE TABLE term (\n  id TEXT PRIMARY KEY,\n  code TEXT UNIQUE,\n  label TEXT UNIQUE\n);\n.import --skip 1 src/term.tsv term\n\nCREATE TABLE data(\n  assay_datetime TEXT,\n  investigator TEXT,\n  subject_id TEXT,\n  species TEXT,\n  strain TEXT,\n  sex TEXT,\n  group_id TEXT,\n  protocol TEXT,\n  organ TEXT,\n  disease TEXT,\n  qualifier TEXT,\n  comment TEXT,\n  FOREIGN KEY(investigator) REFERENCES term(id),\n  FOREIGN KEY(species) REFERENCES term(id),\n  FOREIGN KEY(strain) REFERENCES term(id),\n  FOREIGN KEY(sex) REFERENCES term(id),\n  FOREIGN KEY(protocol) REFERENCES term(id),\n  FOREIGN KEY(organ) REFERENCES term(id),\n  FOREIGN KEY(disease) REFERENCES term(id),\n  FOREIGN KEY(qualifier) REFERENCES term(id)\n);\n\n-- copy from data_csv to data\nINSERT INTO data SELECT * FROM data_csv;\n\n-- clean data\nUPDATE data SET investigator='JAO' WHERE investigator='JO';\n\n-- update subject and groupd IDs\nUPDATE data SET subject_id='ex:subject-' || subject_id;\nUPDATE data SET group_id='ex:group-' || group_id;\n</code></pre> <p>I'll update the README:</p> <pre><code>See `src/` for:\n\n- `prefix.tsv`: shared prefixes\n- `term.tsv`: terminology\n</code></pre> <p>I'll commit my work in progress:</p> <pre><code>$ git add src/prefix.tsv src/term.tsv\n$ git add --update\n$ git commit -m \"Add and apply prefix and term tables\"\n</code></pre> <p>Now all the terms are linked to controlled vocabularies of one sort or another. If I want to see the IDs for those links instead of the \"codes\" I can define a VIEW:</p> <pre><code>CREATE VIEW linked_data_id AS\nSELECT assay_datetime,\n  investigator_term.id AS investigator,\n  subject_id,\n  species_term.id AS species,\n  strain_term.id AS strain,\n  sex_term.id AS sex,\n  group_id,\n  protocol_term.id AS protocol,\n  organ_term.id AS organ,\n  disease_term.id AS disease,\n  qualifier_term.id AS qualifier\nFROM data\nJOIN term as investigator_term ON data.investigator = investigator_term.code\nJOIN term as species_term ON data.species = species_term.code\nJOIN term as strain_term ON data.strain = strain_term.code\nJOIN term as sex_term ON data.sex = sex_term.code\nJOIN term as protocol_term ON data.protocol = protocol_term.code\nJOIN term as organ_term ON data.organ = organ_term.code\nJOIN term as disease_term ON data.disease = disease_term.code\nJOIN term as qualifier_term ON data.qualifier = qualifier_term.code;\n</code></pre> <p>I'll check:</p> <pre><code>$ sqlite3 build/data.db &lt;&lt;&lt; \"SELECT * FROM linked_ids LIMIT 1;\"\n2014-01-01 10:21:00-0500|orcid:0000-0001-5139-5557|ex:subject-12|obo:NCBITaxon_10116|ex:F344N|obo:PATO_0000383|ex:group-1|obo:OBI_0600020|obo:UBERON_0002048|obo:MPATH_268|obo:PATO_0000396\n</code></pre> <p>I can also define a similar view for their \"official\" labels:</p> <pre><code>CREATE VIEW linked_data_label AS\nSELECT assay_datetime,\n  investigator_term.label AS investigator,\n  subject_id,\n  species_term.label AS species,\n  strain_term.label AS strain,\n  sex_term.label AS sex,\n  group_id,\n  protocol_term.label AS protocol,\n  organ_term.label AS organ,\n  disease_term.label AS disease,\n  qualifier_term.label AS qualifier\nFROM data\nJOIN term as investigator_term ON data.investigator = investigator_term.code\nJOIN term as species_term ON data.species = species_term.code\nJOIN term as strain_term ON data.strain = strain_term.code\nJOIN term as sex_term ON data.sex = sex_term.code\nJOIN term as protocol_term ON data.protocol = protocol_term.code\nJOIN term as organ_term ON data.organ = organ_term.code\nJOIN term as disease_term ON data.disease = disease_term.code\nJOIN term as qualifier_term ON data.qualifier = qualifier_term.code;\n</code></pre> <p>I'll check:</p> <pre><code>$ sqlite3 build/data.db &lt;&lt;&lt; \"SELECT * FROM linked_data_label LIMIT 1;\"\n2014-01-01 10:21:00-0500|James A. Overton|ex:subject-12|Rattus norvegicus|F 344/N|female|ex:group-1|histology|lung|adenocarcinoma|severe intensity\n</code></pre> <p>I'll commit my work in progress:</p> <pre><code>$ git add --update\n$ git commit -m \"Add linked_data tables\"\n</code></pre> <p>Now the tables use URLs and is connected to ontologies and stuff. But are we Linked yet?</p>"},{"location":"tutorial/linking-data/#7-getting-triples","title":"7. Getting Triples","text":"<p>SQL tables aren't an official Linked Data format. Of all the RDF formats, I prefer Turtle. It's tedious but not difficult to get Turtle out of SQL. These query do what I need them to do, but note that if the literal data contained quotation marks (for instance) then I'd have to do more work to escape those. First I create a triple table:</p> <pre><code>CREATE TABLE triple (\n  subject TEXT,\n  predicate TEXT,\n  object TEXT,\n  literal INTEGER -- 0 for object IRI, 1 for object literal\n);\n\n-- create triples from term table\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT id, 'rdfs:label', label, 1\nFROM term;\n\n-- create triples from data table\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-assay_datetime', assay_datetime, 1\nFROM data;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-investigator', term.id, 0\nFROM data\nJOIN term AS term ON data.investigator = term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-subject_id', subject_id, 0\nFROM data;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-species', term.id, 0\nFROM data\nJOIN term AS term ON data.species = term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-strain', term.id, 0\nFROM data\nJOIN term AS term ON data.strain = term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-sex', term.id, 0\nFROM data\nJOIN term AS term ON data.sex = term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-group_id', group_id, 0\nFROM data;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-protocol', term.id, 0\nFROM data\nJOIN term AS term ON data.protocol = term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-organ',term.id, 0\nFROM data\nJOIN term AS term ON data.organ= term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-disease', term.id, 0\nFROM data\nJOIN term AS term ON data.disease = term.code;\n\nINSERT INTO triple(subject, predicate, object, literal)\nSELECT 'ex:assay-' || data.rowid, 'ex:column-qualifier', term.id, 0\nFROM data\nJOIN term AS term ON data.qualifier = term.code;\n</code></pre> <p>Then I can turn triples into Turtle using string concatenation:</p> <pre><code>SELECT '@prefix ' || prefix || ': &lt;' || base || '&gt; .'\nFROM prefix\nUNION ALL\nSELECT ''\nUNION ALL\nSELECT subject || ' ' ||\n  predicate || ' ' ||\n  CASE literal\n    WHEN 1 THEN '\"' || object || '\"'\n    ELSE object\n  END\n  || ' . '\nFROM triple;\n</code></pre> <p>I can add this to the <code>src/build.sh</code>:</p> <pre><code>sqlite3 build/data.db &lt; src/turtle.sql &gt; build/data.ttl\n</code></pre> <p>Here's just a bit of that <code>build/data.ttl</code> file:</p> <pre><code>@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n\norcid:0000-0001-5139-5557 rdfs:label \"James A. Overton\" .\nassay:1 column:assay_datetime \"2014-01-01 10:21:00-0500\"^^xsd:datetime .\nassay:1 column:investigator orcid:0000-0001-5139-5557 .\n</code></pre> <p>SQL is not a particularly expressive language. Building the triple table is straightforward but verbose. I could have done the same thing with much less Python code. (Or I could have been clever and generated some SQL to execute!)</p> <p>I'll commit my work in progress:</p> <pre><code>$ git add src/turtle.sql\n$ git add --update\n$ git commit -m \"Convert to Turtle\"\n</code></pre> <p>So technically I have a Turtle file. Linked Data! Right? Well, it's kind of \"flat\". It still looks more like a table than a graph.</p>"},{"location":"tutorial/linking-data/#8-getting-linked","title":"8. Getting Linked","text":"<p>The table I started with is very much focused on the data: there was some sort of assay done, and this is the information that someone recorded about it. The Turtle I just ended up with is basically the same.</p> <p>Other people may have assay data. They may have tables that they converted into Turtle. So can I just merge them? Technically yes: I can put all these triples in one graph together. But I'll still just have \"flat\" chunks of data representing rows sitting next to other rows, without really linking together.</p> <p>The next thing I would do with this data is reorganized it based on the thing it's talking about. I know that:</p> <ul> <li>there was an assay</li> <li>the assay was performed at a certain time,   using a certain protocol</li> <li>there was a person who performed the assay</li> <li>there was a subject animal of some species, strain, and sex</li> <li>the subject animal belonged to a study group</li> <li>the subject animal had some organs</li> <li>the assay resulted in some measurements</li> </ul> <p>Most of these are things that I could point to in the world, or could have pointed to if I was in the right place at the right time.</p> <p>By thinking about these things, I'm stepping beyond what it was convenient for someone to record, and thinking about what happened in the world. If somebody else has some assay data, then they might have recorded it differently for whatever reason, and so it wouldn't line up with my rows. I'm trying my best to use the same terms for the same things. I also want to use the same \"shapes\" for the same things. When trying to come to an agreement about what is connected to what, life is easier if I can point to the things I want to talk about: \"See, here is the person, and the mouse came from here, and he did this and this.\"</p> <p>I could model the data in SQL by breaking the big table into smaller tables. I could have tables for:</p> <ul> <li>person</li> <li>group</li> <li>subject: species, strain, sex, group</li> <li>assay: date, investigator, subject, protocol</li> <li>measurement: assay, organ, disease, qualifier</li> </ul> <p>Then I would convert each table to triples more carefully. That's a good idea. Actually it's a better idea than what I'm about to do...</p> <p>Since we're getting near the end, I'm going to show you how you can do that modelling in SPARQL. SPARQL has a CONSTRUCT operation that you use to build triples. There's lots of tools that I could use to run SPARQL but I'll use ROBOT. I'll start with the \"flat\" triples in <code>build/data.ttl</code>, select them with my WHERE clause, then CONSTRUCT better triples, and save them in <code>build/model.ttl</code>.</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\nPREFIX ex: &lt;https://example.com/&gt;\n\nCONSTRUCT {\n  ?group\n    rdfs:label      ?group_label .\n  ?subject\n    rdf:type        ?species ;\n    rdfs:label      ?subject_label ;\n    ex:strain       ?strain ;\n    obo:RO_0000086  ?sex ;         # has quality\n    ex:group        ?group .\n  ?sex\n    rdf:type        ?sex_type ;\n    rdfs:label      ?sex_label .\n  ?organ\n    rdf:type        ?organ_type ;\n    rdfs:label      ?organ_label ;\n    obo:BFO_0000050 ?subject .     # part of\n  ?assay\n    rdf:type        ?assay_type ;\n    rdfs:label      ?assay_label ;\n    obo:OBI_0000293 ?subject ;     # has specified input\n    obo:IAO_0000136 ?organ .       # is about\n}\nWHERE {\n  ?subject_row\n    ex:column-assay_datetime ?datetime ;\n    ex:column-investigator   ?investigator ;\n    ex:column-subject_id     ?subject ;\n    ex:column-species        ?species ;\n    ex:column-sex            ?sex_type ;\n    ex:column-group_id       ?group ;\n    ex:column-protocol       ?assay_type ;\n    ex:column-organ          ?organ_type ;\n    ex:column-disease        ?disease ;\n    ex:column-qualifier      ?qualifier .\n\n  ?assay_type\n    rdfs:label ?assay_type_label .\n  ?sex_type\n    rdfs:label ?sex_type_label .\n  ?organ_type\n    rdfs:label ?organ_type_label .\n\n  BIND (URI(CONCAT(STR(?subject), \"-assay\")) AS ?assay)\n  BIND (URI(CONCAT(STR(?subject), \"-sex\"))   AS ?sex)\n  BIND (URI(CONCAT(STR(?subject), \"-organ\")) AS ?organ)\n  BIND (CONCAT(\"subject \", REPLACE(STR(?subject), \"^.*-\", \"\")) AS ?subject_label)\n  BIND (CONCAT(\"group \", REPLACE(STR(?group), \"^.*-\", \"\"))     AS ?group_label)\n  BIND (CONCAT(?subject_label, \" \", ?assay_type_label)    AS ?assay_label)\n  BIND (CONCAT(?subject_label, \" sex: \", ?sex_type_label) AS ?sex_label)\n  BIND (CONCAT(?subject_label, \" \", ?organ_type_label)    AS ?organ_label)\n}\n</code></pre> <p>I can add this to the <code>src/build.sh</code>:</p> <pre><code>java -jar robot.jar query \\\n  --input build/data.ttl \\\n  --query src/model.rq build/model.ttl\n</code></pre> <p>Then I get <code>build/model.ttl</code> that looks (in part) like this:</p> <pre><code>ex:subject-31  a        obo:NCBITaxon_10090 ;\n        rdfs:label      \"subject 31\" ;\n        obo:RO_0000086  ex:subject-31-sex ;\n        ex:group        ex:group-2 .\n\nex:group-2  rdfs:label  \"group 2\" .\n</code></pre> <p>Now that's what I call Linked Data!</p> <p>I'll update the README:</p> <pre><code>## Modelling\n\nThe data refers to:\n\n- investigator\n- subject\n- group\n- assay\n- measurement data\n  - subject organ\n  - disease\n\nTODO: A pretty diagram.\n</code></pre> <p>I'll commit my work in progress:</p> <pre><code>$ git add src/model.rq\n$ git add --update\n$ git commit -m \"Build model.ttl\"\n</code></pre>"},{"location":"tutorial/linking-data/#9-getting-it-done","title":"9. Getting It Done","text":"<p>That was a lot of work for a small table. And I did all the hard work of mapping the terminology to ontology terms for you!</p> <p>There's lots more I can do. The SPARQL is just one big chunk, but it would be better in smaller pieces. The modelling isn't all that great yet. Before changing that I want to run it past the boss and see what she thinks.</p> <p>It's getting close to the end of the day. Before I quit I should update the README, clean up anything that's no longer relevant or correct, and make any necessary notes to my future self:</p> <pre><code>$ git add --update\n$ git commit -m \"Update README\"\n$ quit\n</code></pre>"},{"location":"tutorial/managing-dynamic-imports-odk/","title":"Managing Dynamic Imports with the Ontology Development Kit","text":"<p>In this tutorial, we discuss the general workflow of managing dynamic imports, i.e. importing terms from other ontologies which can be kept up to date.</p>"},{"location":"tutorial/managing-dynamic-imports-odk/#tutorial","title":"Tutorial","text":"<p>Follow instructions for the PATO dynamic import process here.</p>"},{"location":"tutorial/managing-ontology-project/","title":"Tutorial on Managing OBO Ontology Projects","text":"<p>This tutorial is not about editing ontologies and managing the evolution of its content (aka ontology curation), but the general process of managing an ontology project overall. In this lesson, we will cover the following:</p> <ol> <li>How to effectively manage an ontology project using GitHub projects and teams</li> <li>How to coordinate the evolution of ontologies across projects and grants</li> </ol> <p>It is important to understand that the following is just one good way of doing project management for OBO ontologies, and most projects will do it slightly differently. We do however believe that thinking about your project management process and the roles involved will benefit your work in the long term, and hope that the following will help you as a starting point.</p>"},{"location":"tutorial/managing-ontology-project/#roles-in-obo-ontology-project-management-activities","title":"Roles in OBO Ontology project management activities","text":"<ol> <li>Ontology Editor (OE): manage the content of ontologies and interact with users</li> <li>Principal Ontology Editor (POE): coordinate the curation activities and have always fixed hours assigned to the project.</li> <li>Ontology Pipeline Developer (OPD): Manage the technical workflows around ontologies, such as release workflows, continuous integration and QC, and setting up data pipelines. Also helps with bulk editing activities.</li> <li>Principal Investigators (PI): Manage the projects that fund ontology curation activities.</li> </ol> <p>For an effective management of an ontology, the following criteria are recommended:</p> <ol> <li>There should be at least one Principal Ontology Editor for every ontology project. The importance is not whether this editor (or sometimes called 'ontology curator') has a specific number of hours per week allocated to the project (although based on our experience, 1 day per week is minimum), but whether the editor has a sense of ownership, i.e. they understand that they are the primary responsible person for maintaining the ontology moving forward. Because of potential grant overlapping issues, we recommend to have at least 1 Principal Ontology Editor for every grant/funded project that has a stake in the ontology.</li> <li>Every effective ontology needs at least a few hours per week from an Ontology Pipeline Developer (OPD). More on that role later. The OPD does not always have as strong a sense of ownership of the ontology project, but typically has a strong sense of responsibility to members of the curation team.</li> <li>There should be separate meetings for curation and technical activities - both problems are hard, and need different team members being present. We recommend at least monthly technical and biweekly curation calls, but for many of the most effective ontology projects we manage, weekly technical and weekly curation calls are normal.</li> </ol> <p>Without the above minimum criteria, the following recommendations will be very hard to implement.</p>"},{"location":"tutorial/managing-ontology-project/#the-project-management-toolbox","title":"The Project Management Toolbox","text":"<p>We make use of three tools in the following recommendation:</p> <p>Project boards: Project boards, sometimes referred to as Kanban boards, GitHub boards or agile boards, are a great way to organise outstanding tickets and help maintain a clear overview of what work needs to be done. They are usually realised with either GitHub projects or ZenHub. If you have not worked with project boards before, we highly recommend watching a quick tutorial on Youtube, such as:</p> <p>GitHub teams. GitHub teams, alongside with organisations, are a powerfull too to organise collaborative workflows on GitHub. They allow you to communicate and organise permissions for editing your ontology in a transparent way. You can get a sense of GitHub teams by watching one of the the numerous tutorials on GitHub, such as:</p> <p>Markdown-based documentation system. Writing great documentation is imperative for a sustainable project. Across many of our recent projects, were are using mkdocs, which we have also integrated with the Ontology Development Kit, but there are others to consider. We deeply recommend to complete a very short introduction to Markdown, this tutorial on YouTube.</p>"},{"location":"tutorial/managing-ontology-project/#what-do-you-need-for-your-project","title":"What do you need for your project?","text":"<p>Every ontology or group of related ontologies (sometimes it is easier to manage multiple ontologies at once, because their scope or technical workflows are quite uniform or they are heavily interrelated) should have:</p> <ol> <li>at least two teams, an Editorial Team and a Technical Team, with clearly defined members. We recommend to create two teams on GitHub and keep their members always up to date (i.e. remove members that are not actively participating), but many of our projects merely maintain a \"core team\", which is a more liberal team containing everyone from stakeholders, principal investigators, editors and users (for managing write permissions see later in the \"best practice\" section) and listing the members of the Editorial and Technical Teams on a page in the documentation (example). Note that it is a good idea to be careful of who on your team has \"admin\" rights on your repo, so sometimes, a distinct \"admin\" team can be very helpful. Admins are allowed to do \"dangerous\" things like deleting the repository.</li> <li>two distinct project boards. We recommend two distinct project boards, one for the Curation/Editorial Team, and one for the Technical Team. The details on how to design the boards is up to the respective teams, but we found a simple 4 stage board with sections for <code>To Do</code> (issues that are important but not urgent), <code>Priority</code> (issues that are important and urgent), <code>In Progress</code> (issues that are being worked on) and <code>Under review</code> (issues that need review). From years of experience with project boards, we recommend against the common practice of keeping a <code>Backlog</code> column (issues that are neither important nor urgent nor likely to be addressed in the next 6 months), nor a <code>Done</code> column (to keep track of closed issues) - they just clutter the view.</li> <li>A documentation system (often realised using <code>mkdocs</code> in OBO projects) with a page listing the members of the team (example). This page should provide links to all related team pages from Github and their project boards, as well as a table listing all current team members with the following information:</li> <li>Name</li> <li>ORCiD</li> <li>Funding Information</li> <li>Allocated FTEs (0 if on volunteering basis)</li> <li>Associated teams</li> <li>Role</li> <li>Responsibilities (What kind of issues can they be assigned to review? How are they involved in the Project?)</li> </ol>"},{"location":"tutorial/managing-ontology-project/#responsibilities","title":"Responsibilities","text":"<ol> <li>Effective Ontology Pipeline Developers (OPDs) are extremely rare and are typically active across many different projects. Therefore their attention is scattered. Understanding and accepting this is key for the following points.</li> <li>Principal Investigators explicitly assign target weekly hours for Ontology Editors and Ontology Pipeline Developers to the project. These should be captured on the documentation systems team page (see above).</li> <li>The Ontology Editors are responsible for the entire Curation Team Board and the <code>To Do</code> and <code>Priority</code> columns of the Technical Team. The later is important: it is the job of the curation team to prioritise the technical issues. The Technical Team can add tickets to the <code>To Do</code> and <code>Priority</code> columns, but this usually happens only in response to a request from the Curation Team.</li> <li>When the technical team meets, the Principal Ontology Editor(s) (POE) are present, i.e. the POEs are members of the technical team as well. They will help clarifying the <code>Priority</code> tickets. The Technical Team is responsible to</li> <li>assign issues and reviewers among themselves (ideally, the reviewer should be decided at the same time the issue is assigned)</li> <li>move issues from the <code>Priority</code> to the <code>In Progress</code> and later to the <code>Done</code> section.</li> <li>communicate through the POE to the PIs when resources are insufficient to address <code>Priority</code> issues.</li> <li>The Principal Ontology Editor is responsible for ensuring that new issues on the issue tracker are dealt with. Usually this happens in the following ways:</li> <li>They ensure that each external issue (i.e. an issue from anyone outside the core team) is (a) responded to in a polite manner and (b) assigned to someone appropriate or politely rejected due to lack of resources.</li> <li>They ensure that each internal issue is assigned to the person that made them. No issue should appear unassigned.</li> <li>The ensure that pull requests are (a) assigned to someone to handle and (b) merged in a timely manner. Too many open PRs cause problems with conflicts.</li> </ol>"},{"location":"tutorial/managing-ontology-project/#best-practices","title":"Best Practices","text":"<ul> <li>The <code>To Do</code> issues should first be moved to the <code>Priority</code> section before being addressed. This prevents focusing on easy to solve tickets in favour of important ones.</li> <li>Even if Google Docs are used to manage team meetings, at the end of each meetings all open issues must be captured as GitHub tickets and placed in the appropriate box on the board. We recommend that <code>Backlog</code> items are not added at all to the board - if they ever become important, they tend to resurface all by themselves.</li> <li>The single most important point of failure is the absence of an Principal Ontology Editor with a strong sense of ownership. This should be the projects priority to determine first.</li> <li>All new members of the project should undergo an onboarding. It is a good idea to prepare walkthroughs of the project (as video or pages in the documentation system) covering everything from Curation to Technical and Project Management.</li> <li>The Principal Ontology Editor responsible for dealing with external issues should be named explicitly on the team page.</li> <li>We recommend the following practices for write permissions:</li> <li>The <code>main</code> (formerly <code>master</code>) branch should be write protected with suitable rules. For example, requiring QC to pass and 1 approving review as a minimum.</li> <li>The curation and technical teams are mainly for social organisation, they do not have to physically exist. However, having a small team with \"admin rights\" and a team (e.g. the core team mentioned above) with \"write\" rights greatly helps with organising the permissions in a transparent manner.</li> </ul>"},{"location":"tutorial/managing-ontology-releases-odk/","title":"Managing Ontology Releases with the Ontology Development Kit","text":"<p>In this tutorial, we discuss the general workflow of ontology releases.</p>"},{"location":"tutorial/managing-ontology-releases-odk/#tutorial","title":"Tutorial","text":"<p>Follow instructions for the PATO release process here.</p>"},{"location":"tutorial/migrating-ontology-to-odk/","title":"Migrating your old Ontology Release System to the Ontology Development Kit","text":"<p>Content TBP, recording exists on request. * see also: How to Migrate an Existing Ontology Repository to the Ontology Development Kit</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/","title":"Neo4j tutorial","text":""},{"location":"tutorial/monarch-kg-neo4j-basics/#running-locally-your-very-own-monarch-graph","title":"Running locally (your very own Monarch Graph)","text":"<p>The new Monarch Knowledge Graph has a more streamlined focus on the core Monarch data model, centering on Diseases, Phenotypes and Genes and the associations between them. This has the benefit of being a graph that can be build in 2 hours instead of 2 days, and that you can run locally on your laptop.</p> <p>Note: As of the writing of this tutorial, (Feb 2023), the graph is just starting to move from its initial construction phrase into real use, and so there are still bugs to find. Some of which show up in this tutorial. </p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#check-out-the-repository","title":"Check out the repository","text":"<p>https://github.com/monarch-initiative/monarch-neo4j</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#download-data","title":"Download Data","text":"<ul> <li>Download monarch.neo4j.dump from data.monarchinitiative.org and put in the <code>dumps</code> directory</li> </ul>"},{"location":"tutorial/monarch-kg-neo4j-basics/#set-up-the-environment-file","title":"Set up the environment file","text":"<p>copy dot_env_template to .env and edit the values to look like:</p> <pre><code># This Environment Variable file is referenced by the docker-compose.yaml build\n\n# Set this variable to '1' to trigger an initial loading of a Neo4j dump\nDO_LOAD=1\n\n# Name of Neo4j dump file to load, assumed to be accessed from within\n# the 'dumps' internal Volume path within the Docker container\nNEO4J_DUMP_FILENAME=monarch-kg.neo4j.dump\n</code></pre> <p>That should mean uncommenting DO_LOAD and NEO4j_DUMP_FILENAME</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#optional-plugin-setup","title":"Optional Plugin Setup","text":"You may wish to install additional plugins <p>  #### Download plugins   * Download the [APOC plugin jar file](https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/4.4.0.13/apoc-4.4.0.13-all.jar) and put in the `plugins` directory  * Download, the [GDS plugin](https://graphdatascience.ninja/neo4j-graph-data-science-2.3.0.zip), unzip the download and copy jar file to the `plugins` directory  #### Environment setup  In addition to the changes above to .env, you will need to uncomment the following lines in the .env file:  <pre><code>NEO4J_apoc_export_file_enabled=true\nNEO4J_apoc_import_file_enabled=true\nNEO4J_apoc_import_file_use__neo4j__config=true\nNEO4JLABS_PLUGINS=\\[\\\"apoc\\\", \\\"graph-data-science\\\"\\]\n</code></pre> </p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#tutorials","title":"Tutorials","text":""},{"location":"tutorial/monarch-kg-neo4j-basics/#monarch-obo-training-tutorials","title":"Monarch OBO training Tutorials","text":""},{"location":"tutorial/monarch-kg-neo4j-basics/#querying-the-monarch-kg-using-neo4j","title":"Querying the Monarch KG using Neo4J","text":""},{"location":"tutorial/monarch-kg-neo4j-basics/#start-neo4j","title":"Start Neo4j","text":"<p>On the command line, from the root of the monarch-neo4j repository you can launch the neo4j with:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#querying","title":"Querying","text":""},{"location":"tutorial/monarch-kg-neo4j-basics/#return-details-for-a-single-disease","title":"Return details for a single disease","text":"<p>Nodes in a cypher query are expressed with <code>()</code> and the basic form of a query is <code>MATCH (n) RETURN n</code>. To limit the results to just our disease of interest, we can restrict by a property, in this case the <code>id</code> property.</p> <pre><code>MATCH (d {id: 'MONDO:0007038'}) RETURN d\n</code></pre> <p>This returns a single bubble, but by exploring the controls just to the left of the returned query, you can see a json or table representation of the returned node.</p> <pre><code>{\n  \"identity\": 480388,\n  \"labels\": [\n    \"biolink:Disease\",\n    \"biolink:NamedThing\"\n  ],\n  \"properties\": {\n    \"name\": \"Achoo syndrome\",\n    \"provided_by\": [\n      \"phenio_nodes\"\n    ],\n    \"id\": \"MONDO:0007038\",\n    \"category\": [\n      \"biolink:Disease\"\n    ]\n  },\n  \"elementId\": \"480388\"\n}\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#connections-out-from-our-disease","title":"Connections out from our disease","text":"<p>Clicking back to the graph view, you can expand to see direct connections out from the node by clicking on the node and then clicking on the graph icon. This will return all nodes connected to the disease by a single edge. </p> <p>Tip: the node images may not be labeled the way you expect. Clicking on the node reveals a panel on the right, clicking on that node label at the top of the panel will reveal a pop-up that lets you pick which property is used as the caption in the graph view.</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#querying-for-connections-out-from-our-disease","title":"Querying for connections out from our disease","text":"<p>In cypher, nodes are represented by <code>()</code> and edges are represented by <code>[]</code> in the form of <code>()-[]-()</code>, and your query is a little chance to express yourself with ascii art. To get the same results as the expanded graph view, you can query for any edge connecting to any node. Note that the query also asks for the connected node to be returned.</p> <pre><code>MATCH (d {id: 'MONDO:0007038'})-[]-(n) RETURN d, n\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#expanding-out-further-and-restricting-the-relationship-direction","title":"Expanding out further and restricting the relationship direction","text":"<p>It's possible to add another edge to the query to expand out further. In this case, we're adding a second edge to the query, and restricting the direction of the second edge to be outgoing. This will return all nodes connected to the disease by a single edge, and then all nodes connected to those nodes by a single outgoing edge. It's important to note that without limiting the direction of the association, this query will traverse up, and then back down the subclass tree.  </p> <pre><code>MATCH (d {id: 'MONDO:0007038'})-[]-&gt;(n)-[]-&gt;(m) RETURN d,n,m\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#exploring-the-graph-schema","title":"Exploring the graph schema","text":"<p>Sometimes, we don't know what kind of questions to ask without seeing the shape of the data. Neo4j provides a graph representation of the schema by calling a procedure</p> <pre><code>CALL db.schema.visualization\n</code></pre> <p>If you tug on nodes and zoom, you may find useful information, but it's not a practical way to explore the schema.</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#whats-connected-to-a-gene","title":"What's connected to a gene?","text":"<p>We can explore the kinds of connections available for a given category of node. Using property restriction again, but this time instead of restricting by the ID, we'll restrict by the category. Also, instead of returning nodes themselves, we'll return the categories of those nodes. </p> <pre><code>MATCH (g:`biolink:Gene`)-[]-&gt;(n) RETURN DISTINCT labels(n)\n</code></pre> <p>Tip: the <code>DISTINCT</code> keyword is used to remove duplicate results. In this case, we're only interested in the unique categories of nodes connected to genes.</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#also-how-is-it-connected","title":"Also, how is it connected?","text":"<p>Expanding on the query above, we can also return the type of relationship connecting the gene to the node.</p> <pre><code>MATCH (g:`biolink:Gene`)-[rel]-&gt;(n) RETURN DISTINCT type(rel), labels(n)\n</code></pre> <p>Which returns tabular data like:</p> <pre><code>\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502\"type(rel)\"                                         \u2502\"labels(n)\"                                                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502\"biolink:located_in\"                                \u2502[\"biolink:NamedThing\",\"biolink:CellularComponent\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:part_of\"                                   \u2502[\"biolink:NamedThing\",\"biolink:MacromolecularComplexMixin\"]\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_or_within\"                \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:enables\"                                   \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:actively_involved_in\"                      \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:colocalizes_with\"                          \u2502[\"biolink:NamedThing\",\"biolink:CellularComponent\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:active_in\"                                 \u2502[\"biolink:NamedThing\",\"biolink:CellularComponent\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_or_within\"                \u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:actively_involved_in\"                      \u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:contributes_to\"                            \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:orthologous_to\"                            \u2502[\"biolink:NamedThing\",\"biolink:Gene\"]                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:participates_in\"                           \u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:interacts_with\"                            \u2502[\"biolink:NamedThing\",\"biolink:Gene\"]                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:has_phenotype\"                             \u2502[\"biolink:NamedThing\",\"biolink:GeneticInheritance\"]        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:has_phenotype\"                             \u2502[\"biolink:NamedThing\",\"biolink:PhenotypicQuality\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:risk_affected_by\"                          \u2502[\"biolink:NamedThing\",\"biolink:Disease\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:gene_associated_with_condition\"            \u2502[\"biolink:NamedThing\",\"biolink:Disease\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:has_phenotype\"                             \u2502[\"biolink:NamedThing\",\"biolink:ClinicalModifier\"]          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_positive_effect\"          \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of\"                          \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:risk_affected_by\"                          \u2502[\"biolink:NamedThing\",\"biolink:Gene\"]                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:gene_associated_with_condition\"            \u2502[\"biolink:NamedThing\",\"biolink:Gene\"]                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_or_within_positive_effect\"\u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:has_mode_of_inheritance\"                   \u2502[\"biolink:NamedThing\",\"biolink:GeneticInheritance\"]        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_negative_effect\"          \u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of\"                          \u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_positive_effect\"          \u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_or_within_negative_effect\"\u2502[\"biolink:NamedThing\",\"biolink:Occurrent\"]                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:has_phenotype\"                             \u2502[\"biolink:NamedThing\",\"biolink:PhenotypicFeature\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_or_within_negative_effect\"\u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_or_within_positive_effect\"\u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:expressed_in\"                              \u2502[\"biolink:NamedThing\",\"biolink:GrossAnatomicalStructure\"]  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:expressed_in\"                              \u2502[\"biolink:NamedThing\",\"biolink:AnatomicalEntity\"]          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:acts_upstream_of_negative_effect\"          \u2502[\"biolink:NamedThing\",\"biolink:Pathway\"]                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:expressed_in\"                              \u2502[\"biolink:NamedThing\",\"biolink:Cell\"]                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:located_in\"                                \u2502[\"biolink:NamedThing\",\"biolink:MacromolecularComplexMixin\"]\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:expressed_in\"                              \u2502[\"biolink:NamedThing\",\"biolink:CellularComponent\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:expressed_in\"                              \u2502[\"biolink:NamedThing\",\"biolink:MacromolecularComplexMixin\"]\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:part_of\"                                   \u2502[\"biolink:NamedThing\",\"biolink:CellularComponent\"]         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:expressed_in\"                              \u2502[\"biolink:NamedThing\"]                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note: the DISTINCT keyword will only remove duplicate results if the entire result is the same. In this case, we're interested in the unique combinations of relationship type and node category.</p>"},{"location":"tutorial/monarch-kg-neo4j-basics/#kinds-of-associations-between-two-entity-types","title":"Kinds of associations between two entity types","text":"<p>Further constraining on the type of the connecting node, we can ask what kinds of associations exist between two entity types. For example, what kinds of associations exist between genes and diseases?</p> <pre><code>MATCH (g:`biolink:Gene`)-[rel]-&gt;(n:`biolink:Disease`) RETURN DISTINCT type(rel)\n</code></pre> <pre><code>\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502\"type(rel)\"                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502\"biolink:gene_associated_with_condition\"\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:risk_affected_by\"              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#diseases-associated-with-a-gene","title":"Diseases associated with a gene","text":"<pre><code>MATCH (g:`biolink:Gene`{id:\"HGNC:1100\"})-[]-(d:`biolink:Disease`) RETURN g,d\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#phenotypes-associated-with-diseases-associated-with-a-gene","title":"Phenotypes associated with diseases associated with a gene","text":"<pre><code>MATCH (g:`biolink:Gene`{id:\"HGNC:1100\"})-[]-&gt;(d:`biolink:Disease`)-[]-&gt;(p:`biolink:PhenotypicFeature`) RETURN g,d,p\n</code></pre> <p>Why doesn't this return results?  This is a great opportunity to track down an unexpected problem. </p> <p>First, try a less constrained query, so that the 3rd node can be anything:</p> <pre><code>MATCH (g:`biolink:Gene`{id:\"HGNC:1100\"})-[]-&gt;(d:`biolink:Disease`)-[]-&gt;(p) RETURN g,d,p\n</code></pre> <p>With a little tugging and stretching, a good picture emerges, and by clicking our phenotype bubbles, they look like they're showing as PhenotypicQuality rather than PhenotypicFeature. This is likely a bug, but a sensible alternative for this same intent might be:</p> <pre><code>MATCH (g:`biolink:Gene`{id:\"HGNC:1100\"})-[]-&gt;(d:`biolink:Disease`)-[:`biolink:has_phenotype`]-&gt;(p) RETURN g,d,p\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#recursive-traversal","title":"Recursive traversal","text":"<p>Sometimes, we don't know the specific number of hops. What if we want to answer the question \"What genes affect the risk for an inherited auditory system disease?\" </p> <p>First, lets find out how are diseases connected to one another. Name the relationship to query for just the predicates. <pre><code>MATCH (d:`biolink:Disease`)-[rel]-(d2:`biolink:Disease`) RETURN DISTINCT type(rel)\n</code></pre></p> <pre><code>\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502\"type(rel)\"                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502\"biolink:subclass_of\"                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:related_to\"                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:associated_with\"               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:has_phenotype\"                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:gene_associated_with_condition\"\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:risk_affected_by\"              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>(* Please ignore <code>biolink:gene_associated_with_condition</code> and <code>biolink:risk_affected_by</code> showing up here, those are due to a bug in our OMIM ingest)</p> <p>We'll construct a query that fixes the super class disease, then connects at any distance to any subclass of that disease, and then brings genes that affect risk for those diseases. To avoid a big hairball graph being returned, we can return the results as a table showing the diseases and genes. </p> <pre><code>MATCH (d:`biolink:Disease`{id:\"MONDO:0002409\"})&lt;-[:`biolink:subclass_of`*]-(d2:`biolink:Disease`)&lt;-[`biolink:risk_affected_by`]-(g:`biolink:Gene`) RETURN d.id, d.name, d2.id, d2.name,g.symbol,g.id\n</code></pre> <p>once you trust the query, you can also use the DISTINCT keyword again focus in on just the gene list</p> <pre><code>MATCH (d:`biolink:Disease`{id:\"MONDO:0002409\"})&lt;-[:`biolink:subclass_of`*]-(d2:`biolink:Disease`)&lt;-[`biolink:risk_affected_by`]-(g:`biolink:Gene`) RETURN DISTINCT g.id\n</code></pre>"},{"location":"tutorial/monarch-kg-neo4j-basics/#gene-to-gene-associations","title":"Gene to Gene Associations","text":"<p>First, we can ask what kind of associations we have between genes.</p> <pre><code>MATCH (g:`biolink:Gene`)-[rel]-&gt;(g2:`biolink:Gene`) RETURN DISTINCT type(rel)\n</code></pre> <pre><code>\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502\"type(rel)\"                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502\"biolink:orthologous_to\"                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:interacts_with\"                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:risk_affected_by\"              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"biolink:gene_associated_with_condition\"\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Again, please ignore <code>biolink:gene_associated_with_condition</code> and <code>biolink:risk_affected_by</code>. </p> <p>Let's say that from the list above, we're super interested in the DIABLO gene, because, obviously, it has a cool name. We can find it's orthologues by querying through the <code>biolink:orthologous_to</code> relationship.</p> <pre><code>MATCH (g {id:\"HGNC:21528\"})-[:`biolink:orthologous_to`]-(o:`biolink:Gene`) RETURN g,o \n</code></pre> <p>We can then make the question more interesting, by finding phenotypes associated with these orthologues.</p> <pre><code>MATCH (g {id:\"HGNC:21528\"})-[:`biolink:orthologous_to`]-(og:`biolink:Gene`)-[:`biolink:has_phenotype`]-&gt;(p) RETURN g,og,p\n</code></pre> <p>That was a dead end. What about gene expression? </p> <pre><code>MATCH (g {id:\"HGNC:21528\"})-[:`biolink:orthologous_to`]-(og:`biolink:Gene`)-[:`biolink:expressed_in`]-&gt;(a) RETURN g,og,a\n</code></pre> <p>We can add this one step further by connecting our gene expression list in UBERON terms </p> <pre><code>MATCH (g {id:\"HGNC:21528\"})-[:`biolink:orthologous_to`]-(og:`biolink:Gene`)-[:`biolink:expressed_in`]-&gt;(a)-[`biolink:subclass_of`]-(u) \nWHERE u.id STARTS WITH 'UBERON:'\nRETURN distinct u.id, u.name\n</code></pre> <p>In particular, it's a nice confirmation to see that we started at the high level MONDO term \"inherited auditory system disease\", passed through subclass relationships to more specific diseases, connected to genes that affect risk for those diseases, focused on a single gene, and were able to find that it is expressed in the cochlea.</p> <pre><code>\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502\"u.id\"          \u2502\"u.name\"                        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502\"UBERON:0000044\"\u2502\"dorsal root ganglion\"          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0000151\"\u2502\"pectoral fin\"                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0000948\"\u2502\"heart\"                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0000961\"\u2502\"thoracic ganglion\"             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001017\"\u2502\"central nervous system\"        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001555\"\u2502\"digestive tract\"               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001675\"\u2502\"trigeminal ganglion\"           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001700\"\u2502\"geniculate ganglion\"           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001701\"\u2502\"glossopharyngeal ganglion\"     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001844\"\u2502\"cochlea\"                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001991\"\u2502\"cervical ganglion\"             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0002107\"\u2502\"liver\"                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0002441\"\u2502\"cervicothoracic ganglion\"      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0003060\"\u2502\"pronephric duct\"               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0003922\"\u2502\"pancreatic epithelial bud\"     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0004141\"\u2502\"heart tube\"                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0004291\"\u2502\"heart rudiment\"                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0005426\"\u2502\"lens vesicle\"                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0007269\"\u2502\"pectoral appendage musculature\"\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0019249\"\u2502\"2-cell stage embryo\"           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0000965\"\u2502\"lens of camera-type eye\"       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0001645\"\u2502\"trigeminal nerve\"              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\"UBERON:0003082\"\u2502\"myotome\"                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial/named-entity-normalization/","title":"Named Entity Normalization with Gilda","text":"<p>Named Entity Normalization (NEN), also called grounding or entity resolution, is the process of identifying the appropriate ontology term for a given text string. For example, apoptotic process grounds to GO:0006915 and k-ras grounds to HGNC:6407. In this tutorial, we show how to use Gilda to apply named entity normalization both in an interactive and a programmatic setting.</p>"},{"location":"tutorial/named-entity-normalization/#interactive-grounding","title":"Interactive Grounding","text":"<p>In order to introduce grounding, we refer to the web-based deployment of Gilda at http://grounding.indra.bio. First, type the text string you want to ground into the \"Text\" field. In the following example, we use k-ras.</p> <p></p> <p>The results are returned showing the ontology identifier, the name, and a score. Multiple groundings may be retrieved for a given text string for one of several reasons:</p> <ol> <li>The concept is duplicated across ontologies (e.g., common between DOID, MONDO, and HP)</li> <li>There are multiple different concepts that share the same name or synonyms</li> </ol> <p>Warning Gilda's pre-build index is primarily targeted towards supporting biomedical relation extraction. This means it does not index all ontologies, so don't be alarmed if you get no results when trying to ground a potentially common entity label. Later, we describe how to build a custom Gilda index.</p> <p></p> <p>In some situations where multiple concepts share the same name or synonym, Gilda is able to use a machine learned disambiguation model. For example, the acronym ER can refer to the endoplasmic reticulum, the emergency room, or potentially be used for a chemical. In these situations, context can be given to leverage Gilda's machine learned disambiguation models.</p> <p></p> <p>Because the context most closely resembled the text associated with endoplasmic reticulum, this result was scored the highest.</p> <p></p>"},{"location":"tutorial/named-entity-normalization/#programmatic-grounding","title":"Programmatic Grounding","text":"<p>Gilda can be installed with <code>pip install gilda</code> and exposes a high-level interface similar to the web interface. k-ras can be grounded in the same way as before:</p> <pre><code>import gilda\n\nscored_matches = gilda.ground(\"k-ras\")\n\nrows = [\n    (\n        scored_match.term.db + \":\" + scored_match.term.id,\n        scored_match.term.entry_name,\n        scored_match.score,\n    )\n    for scored_match in scored_matches\n]\n</code></pre> CURIE Name Score HGNC:6407 KRAS 0.9936 <p>Disambiguation can be run by adding the <code>context</code> argument.</p> <pre><code>import gilda\n\nscored_matches = gilda.ground(\"ER\", context=\"Calcium is released from the ER.\")\n</code></pre>"},{"location":"tutorial/named-entity-normalization/#remote-grounding","title":"Remote Grounding","text":"<p>The following two examples show how to run grounding by invoking the web service. Below is an example request using curl:</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"text\": \"kras\"}' http://grounding.indra.bio/ground\n</code></pre> <p>The same request using Python's request package would be as follows:</p> <pre><code>import requests\n\nres = requests.post(\"http://grounding.indra.bio/ground\", json={\"text\": \"kras\"})\n</code></pre> <p>In both examples, <code>\"context\"</code> can be used as an additional key to invoke disambiguation.</p>"},{"location":"tutorial/named-entity-normalization/#data-science-application","title":"Data Science Application","text":"<p>In the tutorial From Tables to Linked Data, a table is presented that has several columns with free text, and in the course of the tutorial, the groundings are given. This table (truncated) begins as the following:</p> species organ disease RAT LUNG ADENOCARCINOMA MOUSE NOSE INFLAMMATION RAT ADRENAL CORTEX NECROSIS <p>Here, we show how to use Gilda to accomplish this in practice.</p> <pre><code>import pandas as pd\nimport gilda\n\nurl = \"https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/linking_data/data.csv\"\ndf = pd.read_csv(url)\nfor column in [\"species\", \"organ\", \"disease\"]:\n    gilda.ground_df(df, source_column=column, target_column=f\"{column}_curie\")\nprint(df.to_markdown(index=False))\n</code></pre> <p>This results in the following (truncated):</p> species organ disease species_curie organ_curie disease_curie RAT LUNG ADENOCARCINOMA mesh:D051381 mesh:D008168 mesh:D000230 MOUSE NOSE INFLAMMATION mesh:D051379 mesh:D009666 GO:0006954 RAT ADRENAL CORTEX NECROSIS mesh:D051381 mesh:D000302 GO:0070265 <p>Note that MeSH terms may appear instead of OBO ontology terms because the highest scored is given. This can be changed by a <code>namespaces</code> argument to <code>gilda.ground_df</code>. Typically, the results from named entity normalization are used in conjunction with named entity standardization.</p>"},{"location":"tutorial/named-entity-normalization/#custom-index","title":"Custom Index","text":"<p>A custom index \"grounder\" object, which exposes all the previously demonstrated functionality, can be created using PyOBO with:</p> <pre><code>from pyobo.gilda_utils import get_grounder\n\ngrounder = get_grounder([\"mesh\", \"cvx\"])\ngrounder.ground(\"inflammation\")\n</code></pre> <p>A custom index can be created by instantiating <code>gilda.Term</code> objects and instantiating a <code>gilda.Grounder</code> object.</p>"},{"location":"tutorial/named-entity-recognition/","title":"Named Entity Recognition with Gilda","text":"<p>Named entity recognition (NER) is the process of identifying spans of text that correspond to concepts. Often, named entity recognition is paired with named entity normalization (NEN) to identify ontology terms for the spans of text. In this tutorial, we show how to use Gilda to do named entity recognition.</p>"},{"location":"tutorial/named-entity-recognition/#interactive-ner","title":"Interactive NER","text":"<p>In order to introduce grounding, we refer to the web-based deployment of Gilda at http://grounding.indra.bio/ner. First, type the sentence or paragraph you want to ground into the \"Text\" field. In the following example, we use the sentence Calcium is released from the ER.</p> <p></p> <p>The results show two entities were identified - calcium and ER. Gilda implements a simple dictionary-based named entity algorithm that is incredibly fast, has the benefit that it does named entity normalization, and also can do disambiguation based on the whole given text.</p> <p></p>"},{"location":"tutorial/named-entity-recognition/#programmatic-ner","title":"Programmatic NER","text":"<p>Gilda can be installed with <code>pip install gilda</code> and exposes a high-level interface similar to the web interface. Calcium is released from the ER can be annotated in the same way as before:</p> <pre><code>from gilda.ner import annotate\ntext = \"Calcium is released from the ER\"\nresults = annotate(text)\n\nrows = [\n    (\n        start,\n        end,\n        text,\n        scored_match.term.db + \":\" + scored_match.term.id,\n        scored_match.term.entry_name\n    )\n    for text, scored_match, start, end in results\n]\n</code></pre> start end text curie name 0 7 Calcium CHEBI:29108 calcium(2+) 29 31 ER GO:0005783 endoplasmic reticulum"},{"location":"tutorial/named-entity-recognition/#custom-index","title":"Custom Index","text":"<p>A custom index \"grounder\" object, which exposes all the previously demonstrated functionality, can be created using PyOBO with:</p> <pre><code>from gilda.ner import annotate\nfrom pyobo.gilda_utils import get_grounder\n\ngrounder = get_grounder([\"mesh\", \"cvx\"])\ntext = \"Calcium is released from the ER\"\nresults = annotate(text, grounder=grounder)\n</code></pre> <p>A custom index can be created by instantiating <code>gilda.Term</code> objects and instantiating a <code>gilda.Grounder</code> object.</p>"},{"location":"tutorial/named-entity-standardization/","title":"Named Entity Standardization","text":"<p>Named entity standardization (NES) is the process of identifying the preferred ontology (or database) term for a concept from many equivalent concepts. This process typically relies on the existence of high quality semantic mappings such as equivalence relationships and some usage of inference tools.</p> <p>The algorithm for NES is:</p> <ol> <li>Generate equivalence classes, e.g. using semantic mappings from ontologies, databases,    and Biomappings</li> <li>Apply inference and reasoning, e.g., such as chaining rules</li> <li>Apply priority rules (often, a ranked list in terms of prefixes) to identify the preferred term for each equivalent    class</li> <li>Generate a surjective mapping (i.e., means many keys can point to the same value) that can be used to map any given    entity to its preferred entity</li> <li>Apply this mapping to data, such as the results from named entity recognition (NER)    and named entity normalization (NEN).</li> </ol>"},{"location":"tutorial/named-entity-standardization/#nes-tools","title":"NES Tools","text":"<ul> <li>Semantic Mapping Reasoning Assembler (SeMRA)</li> <li>Node Normalizer</li> </ul>"},{"location":"tutorial/odk-toolbox/","title":"Using the ODK Toolbox","text":"<p>This tutorial will show you how to use the tools that are made available by the ODK Docker images, independently of an ODK-generated repository and of ODK-managed workflows.</p>"},{"location":"tutorial/odk-toolbox/#prerequisites","title":"Prerequisites","text":"<p>You have:</p> <ul> <li>Docker installed and running on your machine.</li> </ul> <p>You know:</p> <ul> <li>how to use a command line interface (e.g., run commands and navigate through   a filesystem from a terminal prompt).</li> </ul>"},{"location":"tutorial/odk-toolbox/#managing-docker-images","title":"Managing Docker images","text":"<p>Let\u2019s check which Docker images, if any, are available in your Docker installation:</p> <pre><code>$ docker images\nREPOSITORY            TAG       IMAGE ID       CREATED        SIZE\n</code></pre> <p>Here, the listing comes up empty, meaning there are no images at all. This is what you would expect if you have just installed Docker and have yet to do anything with it.</p> <p>Let\u2019s download the main ODK image:</p> <pre><code>$ docker pull obolibrary/odkfull\nUsing default tag: latest\nlatest: Pulling from obolibrary/odkfull\n[\u2026 Output truncated for brevity \u2026]\nDigest: sha256:272d3f788c18bc98647627f9e6ac7311ade22f35f0d4cd48280587c15843beee\nStatus: Downloaded newer image for obolibrary/odkfull:latest\ndocker.io/obolibrary/odkfull:latest\n</code></pre> <p>Let\u2019s see the images list again:</p> <pre><code>$ docker images\nREPOSITORY           TAG       IMAGE ID       CREATED        SIZE\nobolibrary/odkfull   latest    0947360954dc   6 months ago   2.81GB\n</code></pre> <p>Docker images can exist in several versions, which are called tags in Docker parlance. In our <code>pull</code> command, since we have not specified any tag, Docker had automatically defaulted to the <code>latest</code> tag, which by convention is the latest ODK release.</p> <p>To download a specific version, append the tag after the image name (you can check which tags are available on DockerHub). For example, let\u2019s download the 1.3.1 release from June 2022:</p> <pre><code>$ docker pull obolibrary/odkfull:v1.3.1\nv1.3.1: Pulling from obolibrary/odkfull\nDigest: sha256:272d3f788c18bc98647627f9e6ac7311ade22f35f0d4cd48280587c15843beee\nStatus: Downloaded newer image for obolibrary/odkfull:v1.3.1\ndocker.io/obolibrary/odkfull:v1.3.1\n</code></pre> <p>Again, let\u2019s see the output of <code>docker images</code>:</p> <pre><code>$ docker images\nREPOSITORY           TAG       IMAGE ID       CREATED        SIZE\nobolibrary/odkfull   latest    0947360954dc   6 months ago   2.81GB\nobolibrary/odkfull   v1.3.1    0947360954dc   6 months ago   2.81GB\n</code></pre> <p>Note how both the <code>latest</code> and the <code>v1.3.1</code> images have the same ID. This is because, at the time of this writing, the 1.3.1 release is the latest ODK release, so the <code>latest</code> tag actually points to the same image as the <code>v1.3.1</code> tag. This will change when the ODK v1.3.2 is released: then, using <code>latest</code> (explicitly or by not specifying any tag at all) will point to the new release, while <code>v1.3.1</code> will forever continue to point to the June 2022 release.</p> <p>In the rest of this tutorial, we will always use the <code>latest</code> image, and so we will dispense with the explicit tag. But remember that anywhere you see <code>obolibrary/odkfull</code> in one of the commands below, you can always use <code>obolibrary/odkfull:TAG</code> to force Docker to use a specific ODK version.</p>"},{"location":"tutorial/odk-toolbox/#starting-a-container","title":"Starting a container","text":"<p>Now that we have the ODK image available, let\u2019s try to start it. The command for that is <code>docker run</code>, which has the following syntax:</p> <pre><code>docker run [OPTIONS] &lt;IMAGE&gt; [COMMAND [ARGUMENTS...]]\n</code></pre> <p>where <code>IMAGE</code> is the name of the image to use (in our case, always <code>obolibrary/odkfull</code>).</p> <p>With the ODK, you will always need the <code>--rm</code> option. It instructs the Docker engine to automatically remove the container it creates to run a command, once that command terminates. (Not using the <code>--rm</code> option would lead to those \u201cspent\u201d containers to accumulate on your system, ultimately forcing you to manually remove them with the <code>docker container rm</code> command.)</p> <p>If we don\u2019t specify an explicit command, the simplest command line we can have is thus:</p> <pre><code>$ docker run --rm obolibrary/odkfull\nUsage: odk.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  create-dynfile   For testing purposes\n  create-makefile  For testing purposes\n  dump-schema      Dumps the python schema as json schema.\n  export-project   For testing purposes\n  seed             Seeds an ontology project\n$\n</code></pre> <p>In the absence of an explicit command, the default command <code>odk.py</code> is automatically invoked by Docker. Since it has been invoked without any argument, <code>odk.py</code> does nothing but printing its \u201cusage\u201d message before terminating. When it terminates, the Docker container terminates as well, and we are back at the terminal prompt.</p> <p>To invoke one of the tools available in the toolbox (we\u2019ll see what those tools are later in this document), just complete the command line as needed. For example, to test that ROBOT is there (and to see which version we have):</p> <pre><code>$ docker run --rm obolibrary/odkfull robot --version\nROBOT version 1.9.0\n</code></pre>"},{"location":"tutorial/odk-toolbox/#accessing-your-files-from-within-the-container","title":"Accessing your files from within the container","text":"<p>Since we have ROBOT, let\u2019s use it. Move to a directory containing some ontology files (here, I\u2019ll use a file from the Drosophila Anatomy Ontology, because if you have to pick an ontology, why not picking an ontology that describes the One True Model Organism?).</p> <pre><code>$ ls\nfbbt.obo\n</code></pre> <p>We want to convert that OBO file to a file in, say, the OWL Functional Syntax. So we call ROBOT with the appropriate command and options:</p> <pre><code>$ docker run ---rm obolibrary/odkfull robot convert -i fbbt.obo -f ofn -o fbbt.ofn\norg.semanticweb.owlapi.io.OWLOntologyInputSourceException: java.io.FileNotFoundException: fbbt.obo (No such file or directory)\nUse the -vvv option to show the stack trace.\nUse the --help option to see usage information.\n</code></pre> <p>Huh? Why the \u201cNo such file or directory\u201d error? We just checked that <code>fbbt.obo</code> is present in the current directory, why can\u2019t ROBOT find it?</p> <p>Because Docker containers run isolated from the rest of the system \u2013 that\u2019s kind of the entire point of such containers in general! From within a container, programs can, by default, only access files from the image from which the container has been started.</p> <p>For the ODK Toolbox to be at all useful, we need to explicitly allow the container to access some parts of our machine. This is done with the <code>-v</code> option, as in the following example:</p> <pre><code>$ docker run --rm -v /home/alice/fbbt:/work [\u2026rest of the command omitted for now\u2026]\n</code></pre> <p>This <code>-v /home/alice/fbbt:/work</code> has the effect of binding the directory <code>/home/alice/fbbt</code> from our machine to the directory <code>/work</code> inside the container. This means that if a program that runs within the container tries to have a look at the <code>/work</code> directory, what this program will actually see is the contents of the <code>/home/alice/fbbt</code> directory. Figuratively, the <code>-v</code> option opens a window in the container\u2019s wall, allowing to see parts of what\u2019s outside from within the container.</p> <p>With that window, and assuming our <code>fbbt.obo</code> file is within the <code>/home/alice/fbbt</code> directory, we can try again invoking the conversion command:</p> <pre><code>$ docker run --rm -v /home/alice/fbbt:/work obolibrary/odkfull robot convert -i /work/fbbt.obo -f ofn -o /work/fbbt.ofn\n$ ls\nfbbt.obo\nfbbt.ofn\n</code></pre> <p>This time, ROBOT was able to find out <code>fbbt.obo</code> file, and to convert it as we asked.</p> <p>We can slightly simplify the last command line in two ways.</p> <p>First, instead of explicitly specifying the full pathname to the current directory (<code>/home/alice/fbbt</code>), we can use the shell variable <code>$PWD</code>, which is automatically expanded to that pathname: <code>-v $PWD:/work</code>.</p> <p>Second, to avoid having to explicitly refer to the <code>/work</code> directory in the command, we can ask the Docker engine to run our command as if the current directory, within the container, was already <code>/work</code>. This is done with the <code>-w /work</code> option.</p> <p>The command above now becomes:</p> <pre><code>$ docker run --rm -v $PWD:/work -w /work obolibrary/odkfull robot convert -i fbbt.obo -f ofn -o fbbt.ofn\n</code></pre> <p>This is the typical method of invoking a tool from the ODK Toolbox to work on files from the current directory.</p> <p>In fact, this is exactly how the <code>src/ontology/run.sh</code> wrapper script, that is automatically created in an ODK-generated repository, works. If you work with an ODK-managed ontology, you can invoke an arbitrary ODK tool by using the <code>run.sh</code> instead of calling <code>docker run</code> yourself. Assuming for example that you already are in the <code>src/ontology</code> directory of an ODK-managed ontology, you could use:</p> <pre><code>./run.sh robot convert -i fbbt.obo -f ofn -o fbbt.ofn\n</code></pre> <p>If you want to use the ODK toolbox with ontologies that are not managed by the ODK (so, where a <code>run.sh</code> script is not readily available), you can set up an independent wrapper script, as explained in the Setting up the ODK tutorial.</p>"},{"location":"tutorial/odk-toolbox/#running-a-shell-session-within-the-container","title":"Running a shell session within the container","text":"<p>If you have several commands to invoke in a row involving files from the same directory, you do not have to repeatedly invoke <code>docker run</code> once for each command. Instead, you can invoke a shell, from which you will be able to run successively as many commands as you need:</p> <pre><code>$ docker run --rm -ti -v $PWD:/work -w /work obolibrary/odkfull bash\nroot@c1c2c80c491b:/work# \n</code></pre> <p>The <code>-ti</code> options allow to use your current terminal to control the shell that is started within the container. This is confirmed by the modified prompt that you can see above, which indicates that you are now \u201cin\u201d the container. You can now directly use all the tools that you need:</p> <pre><code>root@c1c2c80c491b:/work# robot convert -i fbbt.obo -f owx -o fbbt.owl\nroot@c1c2c80c491b:/work# Konclude consistency -i fbbt.owl\n{info} 18:21:14.543 &gt;&gt; Starting Konclude \u2026\n[\u2026]\n{info} 18:21:16.949 &gt;&gt; Ontology \u2018out.owl\u2019 is consistent.\nroot@c1c2c80c491b:/work#\n</code></pre> <p>When you are done, exit the shell by hitting <code>Ctrl-D</code> or with the <code>exit</code> command. The shell will terminate, and with it, the container will terminate as well, sending you back to your original terminal.</p>"},{"location":"tutorial/odk-toolbox/#whats-in-the-toolbox-actually","title":"What\u2019s in the toolbox, actually?","text":"<p>Now that you know how to invoke any tool from the ODK Toolbox, here\u2019s a quick overview of which tools are available.</p> <p>For a definitive list, the authoritative source is the ODK repository, especially the <code>Dockerfile</code> and <code>requirements.txt.full</code> files. And if you miss a tool that you think should be present in the toolbox, don\u2019t hesitate to open a ticket to suggest that the tool be added in a future ODK release!</p>"},{"location":"tutorial/odk-toolbox/#programming-language-and-associated-tools","title":"Programming language and associated tools","text":"<ul> <li>Java Development Kit, Maven</li> <li>Ammonite (Scala scripting engine)</li> <li>Python3</li> <li>NodeJS</li> </ul>"},{"location":"tutorial/odk-toolbox/#ontology-manipulation-tools","title":"Ontology manipulation tools","text":"<ul> <li>ROBOT</li> <li>OWLTools</li> <li>Ontology Access Kit</li> <li>Fastobo</li> <li>DOSDPTools</li> </ul>"},{"location":"tutorial/odk-toolbox/#other-semantic-tools","title":"Other semantic tools","text":"<ul> <li>Apache Jena</li> <li>Souffl\u00e9</li> <li>Konclude</li> <li>SSSOM-py</li> <li>SPARQLProg</li> <li>curies</li> </ul>"},{"location":"tutorial/odk-tutorial-2/","title":"ODK in 20 minutes: a complete walk through the core workflows","text":"<p>The goal of this tutorial is to quickly showcase key ODK workflows. It is not geared at explaining individual steps in detail. For a much more detailed tutorial for creating a fresh ODK repo, see here for a tutorial for setting up your first workflow. We recommend to complete this tutorial before attempting this one.</p>"},{"location":"tutorial/odk-tutorial-2/#tutorial","title":"Tutorial","text":"<ol> <li>Background</li> <li>Seeding a new ontology repo</li> <li>Import workflow</li> <li>Integration Testing</li> <li>Release workflow</li> <li>Customisable documentation</li> </ol>"},{"location":"tutorial/odk-tutorial-2/#background","title":"Background","text":"<p>This is some useful background from the ICBO 2022 OBO Tutorial:</p> <p></p>"},{"location":"tutorial/odk-tutorial-2/#seeding","title":"Seeding","text":"<ol> <li>Create a new folder.</li> <li>Download an example ODK config or create one yourself. Save it in the directory created above.</li> <li>Important: in the <code>cato-odk.yaml</code> change <code>github_org</code> to your GitHub username. If you dont do this, some ODK features wont work perfectly, like documentation.     <pre><code>github_org: matentzn\nrepo: cat-ontology\n</code></pre></li> <li>Run the ODK seeding script.     <pre><code>curl https://raw.githubusercontent.com/INCATools/ontology-development-kit/v1.3.1/seed-via-docker.sh | bash -s --  --clean -C cato-odk.yaml\n</code></pre></li> <li>Push the newly created repo to GitHub (for example with GitHub Desktop).</li> </ol>"},{"location":"tutorial/odk-tutorial-2/#the-import-workflow","title":"The import workflow","text":"<p>Let us now import planned process:</p> <ol> <li>Open the term file <code>src/ontology/imports/cob_terms.txt</code> in your favourite text editor</li> <li>Add <code>COB:0000082</code> to the term file (this is the <code>planned process</code> class in COB).</li> <li>From within the <code>src/ontology</code> directory, run <code>sh run.sh make refresh-cob</code>.</li> <li>Inspect the diff. Rather than importing just one term, it seems that we have important a whole bunch. This is because by default, ODK is using the SLME module extraction technique, which ensures that not only the terms we explicitly request are imported - but all the logically dependent ones as well.</li> <li>In <code>src/ontology/cato-odk.yaml</code>, locate the entry for importing <code>cob</code> and switch it to a different module type: <code>filter</code>.     <pre><code>import_group:\n  products: \n    - id: ro\n    - id: cob\n      module_type: filter\n</code></pre></li> <li>Run <code>sh run.sh make update_repo</code> to apply the changes. Check out the git diff to the <code>Makefile</code> to convince yourself that the new extraction method has been applied.</li> <li>Let us refresh the COB import again: From within the <code>src/ontology</code> directory, run <code>sh run.sh make refresh-cob</code>. Convince yourself that now only the <code>planned process</code> term is imported.</li> </ol> <p></p>"},{"location":"tutorial/odk-tutorial-2/#integration-testing","title":"Integration Testing","text":"<ol> <li>Switch to a new git branch and commit your changes to <code>Makefile</code>, <code>cato-odk.yaml</code>, <code>imports/cob_terms.txt</code> and <code>imports/cob_import.owl</code>.</li> <li>Push the branch and create a Pull Request.</li> <li>After a few seconds, the automated testing should start:        Feel free to click on details to see what is happening!</li> <li>Once the test passes (turns green) the PR is ready to be reviewed.      Since this tutorial is for illustration purposes only, we just merge.</li> </ol>"},{"location":"tutorial/odk-tutorial-2/#the-release-workflow","title":"The release workflow","text":"<p>Great, we have done our change, now we are ready to make a release!</p> <ol> <li>Switch to the <code>main</code> branch in <code>git</code>.</li> <li>Make sure you you pull all changes (<code>git pull</code>).</li> <li>In <code>src/ontology</code> execute the release workflow: <code>sh run.sh make prepare_release_fast</code> (we are using <code>fast</code> release here which skips refreshing imports again - we just did that).</li> <li>Inspect the changes. You should see that the <code>planned process</code> class has been added to all ontology release artefacts.</li> <li>Create a branch and commit the changes. Push. Create pull request. Request review (skipped in this tutorial). Wait for QC to pass. Merge.</li> <li>On GitHub (repository front page), click on \"Create a new release\".</li> <li>In the next Window in the \"Choose a tag\" field select <code>v2022-09-01</code>. Note the leading <code>v</code>. Select the correct date (the date you made the release, <code>YYYY-MM-dd</code>). Fill in all the other form elements as you see fit. Click <code>Publish release</code>.</li> </ol> <p></p>"},{"location":"tutorial/odk-tutorial-2/#customisable-documentation","title":"Customisable documentation","text":"<p>With our ODK setup, we also have a completely customisable documentation system installed. We just need to do a tiny change to the GitHub pages settings:</p> <ol> <li>On your GitHub repo page, click on \"Settings\".</li> <li>In the menu on the left, click on \"Pages\".</li> <li>On the right, under <code>Build and deployment</code> select <code>Deploy from branch</code>.</li> <li>Underneath, select <code>gg-pages</code> as the branch (this is where ODK deploys to), and <code>/(root)</code> as the directory.     </li> <li>Hit <code>Save</code>.</li> <li>Wait for about 4 minutes for the site to be deployed (you can click on <code>Actions</code> in the main menu to follow the build process).</li> <li>Go back to the <code>Pages</code> section in <code>Settings</code>. You should see a button <code>Visit site</code>. Click on it. If everything went correctly, you should see your new page:     </li> <li>Let's make a quick change: On the main page, click on the pen in the top right corner (this only works if you have correctly configured your <code>github_org</code>, see seeding). If you have not configured your repo, go to the GitHub front page of your repo, go into the <code>docs</code> directory, click on  <code>index.md</code> and edit it from here. Make a small random edit.</li> <li>Commit the change straight to <code>main</code> or do it properly, create a branch, PR, ask for reviews, merge.</li> <li>After the ODK updates your site, you should be able to see your changes reflected on the life site!</li> </ol>"},{"location":"tutorial/odk-tutorial-2/#summary","title":"Summary","text":"<p>That's it! In about 20 minutes, we</p> <ol> <li>Seeded a new ontology repo.</li> <li>Imported a term.</li> <li>Made a pull request and watched the ODK Testing framework at work.</li> <li>Ran a release.</li> <li>Deployed customisable documentation pages to help our users and curators documenting processes and use instructions.</li> </ol>"},{"location":"tutorial/ontogpt-workshop/","title":"Using OntoGPT in practice","text":"<p>OntoGPT is useful for extracting structured data from loosely-structured text and grounding it to consistent identifiers.</p> <p>Because it is an interface for LLMs, OntoGPT can also help to automate otherwise tedious formatting and data cleaning tasks.</p> <p>The full OntoGPT documentation may be found here.</p> <p>The examples below demonstrate how to do the following:</p> <ol> <li>Set up OntoGPT</li> <li>Assemble a custom schema for extracting concepts (and then run it)</li> <li>Assemble a custom schema for extracting relationships (and then run that, too)</li> <li>Examine the results</li> </ol>"},{"location":"tutorial/ontogpt-workshop/#setup","title":"Setup","text":"<p>OntoGPT requires Python 3.9 or greater to be installed. It also requires access to an LLM, either through an API or a local model.</p> <p>OntoGPT may be installed with <code>pip</code>:</p> <pre><code>pip install ontogpt\n</code></pre> <p>This tutorial uses OntoGPT version 1.0.6.</p> <p>To set an API key (in this case, for OpenAI), run the following:</p> <pre><code>runoak set-apikey -e openai &lt;your openai api key&gt;\n</code></pre> <p>Alternatively, you may run this:</p> <pre><code>export OPENAI_API_KEY=&lt;your openai api key&gt;\n</code></pre>"},{"location":"tutorial/ontogpt-workshop/#using-a-local-model","title":"Using a local model","text":"<p>OntoGPT works with local models through the <code>ollama</code> package.</p> <p>Full setup instructions are in the OntoGPT docs, but in brief:</p> <ol> <li>Install ollama as described on their GitHub repository.</li> <li>Download a local model, e.g., for <code>llama3</code> run <code>ollama pull llama3</code>.</li> <li>Ensure the ollama service is running so OntoGPT can access it. This may require a command like <code>ollama serve</code>.</li> <li>In OntoGPT commands below, specify the local model with the <code>--model</code> option, e.g., <code>--model ollama/llama3</code>.</li> </ol>"},{"location":"tutorial/ontogpt-workshop/#extract-concepts","title":"Extract concepts","text":"<p>Let's use OntoGPT to extract the names of vertebrate animals and ground them to the Vertebrate Breed Ontology (VBO).</p> <p>We'll start with an example text - copy and paste the following into a document named <code>pigeons.txt</code>.</p> <pre><code>The Fantail is a popular breed of pigeon known for its distinctive fan-shaped tail feathers and comes in a variety of colors, including white, black, and gray.\nThe Jacobin is another well-known breed, characterized by its distinctive crest and hood of feathers around its head and neck.\nThe Homing Pigeon, also known as the Racing Homer, is a breed prized for its speed and navigational abilities, with some individuals able to fly at speeds of up to 80 km/h.\n</code></pre> <p>Next, create a document named <code>vbo_names.yaml</code>. This will be our first extraction schema.</p> <p>It should contain the following:</p> <pre><code>id: http://w3id.org/ontogpt/vbo_names\nname: vbo_names\ntitle: Extraction Template for Animal Names\ndescription: &gt;-\n  An extraction template for animal names present in VBO\nlicense: https://creativecommons.org/publicdomain/zero/1.0/\nprefixes:\n  rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n  linkml: https://w3id.org/linkml/\n  vbo_names: http://w3id.org/ontogpt/vbo_names\n  vbo: http://purl.obolibrary.org/obo/vbo\n\ndefault_prefix: vbo_names\ndefault_range: string\n\nimports:\n  - linkml:types\n  - core\n\nclasses:\n  NameSet:\n    tree_root: true\n    is_a: NamedEntity\n    attributes:\n      animal_names:\n        range: AnimalName\n        multivalued: true\n        description: &gt;-\n          A semicolon-separated list of names of animals\n          used in the input text. These are general names,\n          e.g., if any breed of cat is mentioned, this\n          list should include \"Cat breed\", or for any pig,\n          include \"Pig breed\".\n      names:\n        range: BreedName\n        multivalued: true\n        description: &gt;-\n          A semicolon-separated list of names of animal breeds\n          used in the input text. These should be as specific\n          as possible about the breed of the animal.\n          Examples include:\n          Gimbsheimer Enten, Debao pony, Baixi\n\n  AnimalName:\n    is_a: NamedEntity\n    annotations:\n      annotators: sqlite:obo:vbo\n      prompt: &gt;-\n        The name of a grouping category of vertebrate\n        animal breeds.\n\n  BreedName:\n    is_a: NamedEntity\n    annotations:\n      annotators: sqlite:obo:vbo\n      prompt: &gt;-\n        The name of a vertebrate animal breed.\n</code></pre> <p>Note that this schema includes a root class (<code>NameSet</code>) with two slots, each intended to contain a list of extracted names.</p> <p>We use the two different classes (<code>AnimalName</code> and <code>BreedName</code>) to extract two different types of entities: general names such as \"Pigeon\" and more specific breed names.</p> <p>This reflects the structure and format of VBO, allowing us to provide instructions to the LLM to get better grounding, and it allows us to capture instances where animal names aren't described in as much detail as we may like.</p> <p>Each of the classes will be annotated with <code>sqlite:obo:vbo</code> - this is shorthand for \"the SQLite version of VBO as provided by the OBO Foundry\". This will be downloaded when OntoGPT needs it.</p> <p>If you had a local file for VBO, such as <code>vbo.obo</code>, you could use that as the annotator here instead of <code>sqlite:obo:vbo</code>.</p> <p>You may also use multiple annotators - for example, this value could be <code>sqlite:obo:vbo, sqlite:obo:vto</code>.</p> <p>VBO already contains NCBI Taxonomy identifiers, so these may be included in our grounded identifiers.</p> <p>To run the extraction - assuming this new extraction template and the <code>pigeons.txt</code> input file are in the current directory, run this command:</p> <pre><code>ontogpt extract -i pigeons.txt -t vbo_names.yaml\n</code></pre> <p>After a few seconds, you should get a result like the following:</p> <pre><code>---\ninput_text: |+\n  The Fantail is a popular breed of pigeon known for its distinctive fan-shaped tail feathers and comes in a variety of colors, including white, black, and gray.\n  The Jacobin is another well-known breed, characterized by its distinctive crest and hood of feathers around its head and neck.\n  The Homing Pigeon, also known as the Racing Homer, is a breed prized for its speed and navigational abilities, with some individuals able to fly at speeds of up to 80 km/h.\n\nraw_completion_output: |-\n  animal_names: Pigeon breed\n  names: Fantail; Jacobin; Homing Pigeon; Racing Homer\n  label:\nprompt: |+\n  From the text below, extract the following entities in the following format:\n\n  animal_names: &lt;A semicolon-separated list of names of animals used in the input text. These are general names, e.g., if any breed of cat is mentioned, this list should include \"Cat breed\", or for any pig, include \"Pig breed\".&gt;\n  names: &lt;A semicolon-separated list of names of animal breeds used in the input text. These should be as specific as possible about the breed of the animal. Examples include: Gimbsheimer Enten, Debao pony, Baixi&gt;\n  label: &lt;The label (name) of the named thing&gt;\n\n\n  Text:\n  The Fantail is a popular breed of pigeon known for its distinctive fan-shaped tail feathers and comes in a variety of colors, including white, black, and gray.\n  The Jacobin is another well-known breed, characterized by its distinctive crest and hood of feathers around its head and neck.\n  The Homing Pigeon, also known as the Racing Homer, is a breed prized for its speed and navigational abilities, with some individuals able to fly at speeds of up to 80 km/h.\n\n\n\n  ===\n\nextracted_object:\n  id: ca691ef7-4073-4339-9101-99e6ce8a8340\n  animal_names:\n    - VBO:0400015\n  names:\n    - VBO:0013279\n    - AUTO:Jacobin\n    - AUTO:Homing%20Pigeon\n    - VBO:0016851\nnamed_entities:\n  - id: VBO:0400015\n    label: Pigeon breed\n  - id: VBO:0013279\n    label: Fantail\n    original_spans:\n      - 4:10\n  - id: AUTO:Jacobin\n    label: Jacobin\n    original_spans:\n      - 164:170\n  - id: AUTO:Homing%20Pigeon\n    label: Homing Pigeon\n    original_spans:\n      - 291:303\n  - id: VBO:0016851\n    label: Racing Homer\n    original_spans:\n      - 324:335\n</code></pre> <p>The <code>raw_completion_output</code> field contains the LLM's generated response, before any of these terms are grounded to unique identifiers.</p> <p>The list of animal names includes <code>VBO:0400015</code> (Pigeon breed), as expected.</p> <p>Of the three pigeon breeds mentioned in the text, two are grounded to VBO CURIEs.</p> <p>VBO contains a term for the Jacobin (<code>VBO:0001216</code>) but refers to it as the \"Danish Jacobin Pigeon\" - not close enough to match in this case.</p>"},{"location":"tutorial/ontogpt-workshop/#extract-relationships","title":"Extract relationships","text":"<p>Let's take this extraction a step further and identify relationships between animal breeds and their characteristics.</p> <p>We can edit the existing extraction schema from above (<code>vbo_names.yaml</code>):</p> <pre><code>id: http://w3id.org/ontogpt/vbo_char\nname: vbo_char\ntitle: Extraction Template for Animal Breeds and their Characteristics\ndescription: &gt;-\n  An extraction template for animal names present in VBO,\n  along with the characteristics of each breed\nlicense: https://creativecommons.org/publicdomain/zero/1.0/\nprefixes:\n  rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#\n  linkml: https://w3id.org/linkml/\n  vbo_char: http://w3id.org/ontogpt/vbo_char\n  vbo: http://purl.obolibrary.org/obo/vbo\n\ndefault_prefix: vbo_char\ndefault_range: string\n\nimports:\n  - linkml:types\n  - core\n\nclasses:\n  NameSet:\n    tree_root: true\n    is_a: NamedEntity\n    attributes:\n      animal_names:\n        range: AnimalName\n        multivalued: true\n        description: &gt;-\n          A semicolon-separated list of names of animals\n          used in the input text. These are general names,\n          e.g., if any breed of cat is mentioned, this\n          list should include \"Cat breed\", or for any pig,\n          include \"Pig breed\".\n      names:\n        range: BreedName\n        multivalued: true\n        description: &gt;-\n          A semicolon-separated list of names of animal breeds\n          used in the input text. These should be as specific\n          as possible about the breed of the animal.\n          Examples include:\n          Gimbsheimer Enten, Debao pony, Baixi\n      characteristics:\n        range: BreedToCharacteristic\n        multivalued: true\n        inlined: true\n        inlined_as_list: true\n        description: &gt;-\n          A semicolon-separated list of names of animal breeds\n          used in the input text, along with a single characteristic\n          mentioned for that breed. These should be as specific\n          as possible about the breed of the animal. The characteristic\n          may be color, dimensions, physical properties, abilities,\n          or other features. Each statement should contain just one\n          pair of breed name and characteristic.\n          It should be formatted as\n          \"Breed IS Characteristic\", or \"Breed HAS Characteristic\",\n          e.g., Gimbsheimer Enten IS blue,\n          Debao pony IS short,\n          Baixi IS circular\n          Each additional characteristic for a breed should get\n          its own statement, e.g., \"breed name is tall and wide\"\n          should become \"Breed IS tall\" and \"Breed is wide\".\n\n  AnimalName:\n    is_a: NamedEntity\n    annotations:\n      annotators: sqlite:obo:vbo\n      prompt: &gt;-\n        The name of a grouping category of vertebrate\n        animal breeds.\n\n  BreedName:\n    is_a: NamedEntity\n    annotations:\n      annotators: sqlite:obo:vbo\n      prompt: &gt;-\n        The name of a vertebrate animal breed.\n\n  Characteristic:\n    is_a: NamedEntity\n    annotations:\n      annotators: sqlite:obo:uberon, sqlite:obo:pato\n      prompt: &gt;-\n        The name of a characteristic of an animal.\n\n  Descriptor:\n    is_a: NamedEntity\n    annotations:\n      annotators: sqlite:obo:pato\n      prompt: &gt;-\n        A descriptor for a characteristic.\n\n  BreedToCharacteristic:\n    is_a: Triple\n    description: &gt;- \n      A triple in which the subject is an animal breed,\n      the object is a characteristic, and the predicate\n      is usually \"IS\" or \"HAS\".\n    slot_usage:\n      subject:\n        range: BreedName\n      object:\n        description: &gt;-\n          The specific characteristic.\n          This is generally a noun of the characteristic,\n          e.g., with \"blue fin\" the object is \"fin\".\n        range: Characteristic\n      predicate:\n        range: NamedEntity\n        description: &gt;-\n          The relationship type, generally IS or HAS to indicate a breed\n          is defined by having a specific characteristic.\n      subject_qualifier:\n        range: NamedEntity\n        description: &gt;-\n          An optional qualifier or modifier for the breed.\n      object_qualifier:\n        range: Descriptor\n        description: &gt;-\n          An optional qualifier or modifier for the characteristic.\n          This is generally a descriptor of the characteristic,\n          e.g., with \"blue fin\" the qualifier is \"blue\".\n</code></pre> <p>Save this as <code>vbo_char.yaml</code>.</p> <p>We may now run this as before:</p> <pre><code>ontogpt extract -i pigeons.txt -t vbo_char.yaml\n</code></pre> <p>The output will be quite a bit longer, and it will resemble that from before (no surprises here - it's extracting the same material and more).</p> <p>The extracted characteristic relationships may look like so:</p> <pre><code>  characteristics:\n    - subject: VBO:0013279\n      predicate: AUTO:HAS\n      object: UBERON:0018537\n    - subject: AUTO:Jacobin\n      predicate: AUTO:HAS\n      object: UBERON:4200133\n    - subject: AUTO:Jacobin\n      predicate: AUTO:HAS\n      object: UBERON:0000022\n    - subject: AUTO:Homing%20Pigeon\n      predicate: AUTO:IS\n      object: AUTO:fast\n    - subject: AUTO:Homing%20Pigeon\n      predicate: AUTO:HAS\n      object: AUTO:abilities\n</code></pre>"},{"location":"tutorial/ontogpt-workshop/#extracting-from-other-sources","title":"Extracting from other sources","text":"<p>OntoGPT can use PubMed citations and PubMed Central full texts as its inputs with the <code>pubmed-annotate</code> command, from Wikipedia pages with the <code>wikipedia-extract</code> command, or from web pages with the <code>web-extract</code> command.</p> <p>Here, we'll re-use the extraction schema from above, but with the <code>web-extract</code> command and a page about Fengjing pigs (https://breeds.okstate.edu/swine/fengjing-swine.html).</p> <p>Run the following:</p> <pre><code>ontogpt web-extract -t vbo_char.yaml https://breeds.okstate.edu/swine/fengjing-swine.html\n</code></pre> <p>This will produce quite a verbose output, with an extracted set of values like this:</p> <pre><code>extracted_object:\n  id: 9afe16d9-9f4c-4596-943d-bbf5a124a236\n  label: Fengjing Swine\n  animal_names:\n    - VBO:0001199\n    - NCBITaxon:9823\n  names:\n    - VBO:0012847\n    - VBO:0001175\n    - VBO:0012819\n  characteristics:\n    - subject: VBO:0012847\n      predicate: IS\n      object: UBERON:0001456\n    - subject: VBO:0012847\n      predicate: IS\n      object: UBERON:0001003\n    - subject: VBO:0012847\n      predicate: IS\n      object: growing\n    - subject: VBO:0012847\n      predicate: IS\n      object: UBERON:0001013\n    - subject: VBO:0012847\n      predicate: IS\n      object: GO:0050909\n    - subject: VBO:0012847\n      predicate: IS\n      object: diseases\n    - subject: VBO:0012847\n      predicate: IS\n      object: ability\n    - subject: Fengjing Sows\n      predicate: IS\n      object: high\n    - subject: Fengjing Sows\n      predicate: ARE\n      object: UBERON:0001443\n    - subject: Fengjing Sows\n      predicate: ARE\n      object: live-weight\n    - subject: Fengjing Sows\n      predicate: HAS\n      object: backfat thickness\n    - subject: VBO:0012847\n      predicate: HAS\n      object: percentage\n    - subject: VBO:0012847\n      predicate: IS\n      object: prolific\n    - subject: Fengjing Sows\n      predicate: HAVE\n      object: PATO:0000161\n    - subject: Fengjing Sows\n      predicate: HAS\n      object: PATO:0000276\n    - subject: Fengjing Sows\n      predicate: HAVE\n      object: litters\n    - subject: Fengjing Sows\n      predicate: HAVE\n      object: PATO:0000128\n    - subject: Fengjing Sows\n      predicate: HAS\n      object: ADG\n</code></pre> <p>That's a lot of details about these swine.</p> <p>Some of the relationships may not be particularly informative or useful (e.g., VBO:0012847 IS diseases; the source text states this breed is \"resistant to some diseases\") and may suggest areas for further refinement of the extraction schema.</p>"},{"location":"tutorial/ontogpt/","title":"Using OntoGPT to boost ontology curation","text":"<p>OntoGPT is a Python package for extracting structured information from text with large language models (LLMs), instruction prompts, and ontology-based grounding.</p> <p>Two different strategies for knowledge extraction are currently implemented in OntoGPT:</p> <ul> <li>SPIRES: A Zero-shot learning (ZSL) approach to extracting nested semantic structures from text</li> <li>TALISMAN (previously known as SPINDOCTOR): Summarizes gene set descriptions (pseudo gene-set enrichment)</li> </ul> <p>More info about OntoGPT:</p> <ul> <li>Documentation: https://monarch-initiative.github.io/ontogpt/</li> <li>Source: https://github.com/monarch-initiative/ontogpt</li> <li>Preprints:<ul> <li>Caufield JH, Hegde H, Emonet V, Harris NL, Joachimiak MP, Matentzoglu N, et al. Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. http://arxiv.org/abs/2304.02711</li> <li>Joachimiak MP, Caufield JH, Harris NL, Kim H, Mungall CJ. Gene Set Summarization using Large Language Models. http://arxiv.org/abs/2305.13338</li> </ul> </li> </ul> <p>We are also working on a curation-specific tool, CurateGPT, a prototype web application and framework for performing general-purpose AI-guided curation and curation-related operations over collections of objects.</p> <p>Please see this walkthrough to learn how to leverage OntoGPT for ontology curation.</p>"},{"location":"tutorial/project-ontology-development/","title":"Project Ontology Development","text":""},{"location":"tutorial/project-ontology-development/#summary","title":"Summary","text":"<p>A project ontology, sometimes and controversially referred to as an application ontology, is an ontology which is composed of other ontologies for a particular use case, such as Natural Language Processing applications, Semantic Search and Knowledge Graph integration. A defining feature of a project ontology is that it is not intended to be used as a domain ontology. Concretely, this means that content from project ontologies (such as terms or axioms) is not to be re-used by domain ontologies (under no circumstances). Project ontology developers have the freedom to slice &amp; dice, delete and add relationships, change labels etc as their use case demands it. Usually, such processing is minimal, and in a well developed environment such as OBO, new project ontology-specific terms are usually kept at a minimum.</p> <p>In this tutorial, we discuss the fundamental building blocks of application ontologies and show you how to build one using the Ontology Development Kit as one of several options.</p>"},{"location":"tutorial/project-ontology-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>A basic understanding of Ontology Pipelines using ROBOT is helpful to follow this tutorial.</li> </ul>"},{"location":"tutorial/project-ontology-development/#learning-objectives","title":"Learning objectives","text":"<ul> <li>Understand how to plan a project ontology project independent of any particular methodology</li> <li>Develop an application ontology using the Ontology Development Kit (ODK)</li> <li>Be aware off pitfalls when dealing with very large application ontologies</li> </ul>"},{"location":"tutorial/project-ontology-development/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Why do we need project ontologies?</li> <li>Overview</li> <li>The three \"ingredients\" of project ontologies</li> <li>The five \"phases\" of project ontology development</li> </ul>"},{"location":"tutorial/project-ontology-development/#why-do-we-need-project-ontologies","title":"Why do we need project ontologies?","text":"<p>There are a few reasons for developing project ontologies. Here are two that are popular in our domain:</p> <ul> <li>Semantic integration. You have curated a lot of data using standard ontologies and now you wish to access this data using the \"semantic fabric\" provided by the ontology. Concrete examples:</li> <li>Adding a \"semantic layer to your knowledge graph\". For example, your data is annotated using specific anatomy terms, and you wish to query your knowledge graph through anatomical groupings, such as \"anatomical entities that are part of the cardio-vascular system\".</li> <li>Offering \"semantic search\". For example, you may want to restrict a certain search widget to \"diseases\" only, or try to figure out whether a user is searching for phenotypes associated with diseases. A concrete example is populating a Solr or elastic-search index using not only the labels and synonyms of an ontology, but also their relationships. Try it: https://platform.opentargets.org/, https://monarchinitiative.org/.</li> <li>Example ontologies:<ul> <li>https://github.com/monarch-initiative/phenio</li> <li>https://github.com/EBISPOT/efo</li> <li>https://github.com/EBISPOT/scatlas_ontology</li> </ul> </li> <li>Natural language processing (NLP). You are developing an NLP application such as an annotator for text. Here, you may like to use ontologies to tag specific phrases in your documents, like those related to COVID. Ontologies in these cases serve essentially as more or less sophisticated dictionaries. But there are some more sophisticated uses of ontologies for NER.</li> <li>Example ontologies:<ul> <li>https://github.com/berkeleybop/bero</li> <li>https://github.com/EBISPOT/covoc</li> </ul> </li> <li>Mapping work. When developing mappings across ontologies and terminologies, it is often useful to have access to all of them at once. This helps to explore the consequences of mapping decisions, as well providing a single interface for ontology matching tools which usually operate on single ontologies. Advanced Machine Learning based approaches are used to generate graph embeddings on such merged ontologies. Example ontologies:</li> <li>https://github.com/monarch-initiative/mondo-ingest</li> </ul> <p></p>"},{"location":"tutorial/project-ontology-development/#basic-architecture","title":"Basic architecture","text":""},{"location":"tutorial/project-ontology-development/#three-ingredients-of-project-ontologies","title":"Three \"ingredients\" of project ontologies","text":"<p>Any application ontology will be concerned with at least 3 ingredients:</p> <ul> <li>The seed. This is a the set of terms you wish to import into your application ontology. The seed can take many forms:</li> <li>a simple list of terms, e.g. <code>MONDO:123, MONDO:231</code></li> <li>a list of terms including additional relational selectors, e.g. <code>MONDO:123, incl. all children</code></li> <li>a list of terms including a logical selector, <code>MONDO:123, incl. all terms that are in some way logically related to MONDO:123</code></li> <li>a general selector, like \"all classes\" or simply \"everything\".</li> <li>There are probably more, but these are the main ones we work with in the context of biomedical application ontologies.</li> <li>The source ontologies, often referred to as \"mirrors\" (at least by those working with ODK). These are the full ontologies which we want to use in our application ontology. For example, we may want to include anatomical entities from the Uberon ontology into our application ontology. These are usually downloaded from the internet into the application ontology workspace, and then processed by the application ontology extraction workflow (see later).</li> <li>Additional ontology metadata and customisations, such as axioms used to connect entities (classes) across your source ontologies to fulfil a use case, but also your regular ontology metadata (title, comments, etc).</li> </ul> <p></p>"},{"location":"tutorial/project-ontology-development/#the-five-phases-of-project-ontology-development","title":"The five \"phases\" of project ontology development","text":"<p>There are five phases on project ontology development which we will discuss in detail in this section:</p> <ul> <li>Managing the seed</li> <li>Extracting modules</li> <li>Managing metadata and customisations</li> <li>Merging and post-processing</li> <li>Validation</li> </ul> <p>There are other concerns, like continuous integration (basically making sure that changes to the seed or project ontology pipelines do not break anything) and release workflows which are not different from any other ontology.</p> <p></p>"},{"location":"tutorial/project-ontology-development/#managing-the-seed","title":"Managing the seed","text":"<p>As described above, the seed is the set of terms that should be extracted from the source ontologies into the project ontology. The seed comprises any of the following:</p> <ul> <li>terms, such as <code>MONDO:0000001</code></li> <li>selectors, such as <code>all, children, descendants, ancestors, annotations</code></li> <li>combinations of the two (a term with all its children)</li> </ul> <p>Users of ODK will be mostly familiar with term files located in the imports directory, such as <code>src/ontology/imports/go_terms.txt</code>. Selectors are usually hidden from the user by the ODK build system, but they are much more important now when building project ontologies.</p> <p>Regardless of which system you use to build your project ontology, it makes sense to carefully plan your seed management. In the following, we will discuss some examples:</p> <ol> <li>Using annotated data. For the Single Cell Atlas Ontology (SCAO) we already have a spreadsheet with the raw data, annotated with the ontology terms we wish to import. We only want to import these exact terms. Our strategy therefore is to extract the ontology terms from table and use these as our seed.</li> <li>User requests. For the Experimental Factor Ontology (EFO), we rely on user requests. Curators provide us with individual term requests, or lists of terms they need for curating their data. We usually include these terms along with their parents to maintain rich hierarchies.</li> <li>Use case specific considerations. For the Phenomics Integrated Ontology (PHENIO) we wish to combine all disease, phenotype and anatomy terms together and furthermore pull in related chemicals and biological processes (and more).</li> </ol> <p>It makes sense to document your seed management plan. You should usually account for the possibility of changes (terms being added or removed) during the design phase.</p> <p></p>"},{"location":"tutorial/project-ontology-development/#extracting-modules","title":"Extracting modules","text":"<p>Module extraction is the process for selecting an appropriate subset from an ontology. There are many ways to extracting subsets from an ontology:</p> <ol> <li>Using logical modules (SLME, pronounced 'slime'): this will allow you to extract not only all the terms in your seed, but furthermore all logical axioms that could theoretically impact reasoning. SLME modules are typically much larger than what you would expect from a 'relevant' subset.</li> <li>Using ROBOT filter, a system to first select entities in your seed, then selectively including or excluding descendants, annotations.</li> <li>Using MIREOT, a technique that will extract terms and their subClass relationships, without any attempt to include other kinds of axioms.</li> <li>Using ROBOT subset, which extract a sub-ontology that contains only the seed terms and the relationships between them.</li> </ol> <p>You can consult the ROBOT documentation for some details on module extraction.</p> <p>Let's be honest - none of these module extraction techniques are really ideal for project ontologies. SLME modules are typically used for domain ontology development to ensure logical consistency with imported ontologies, but otherwise contain too much information (for most project ontology use cases). ROBOT filter has a hard time with dealing with closures of existential restrictions: for example you cant be sure that, if you import \"endocardial endothelium\" and \"heart\" using filter, that the one former is still part of the latter (it is only indirectly a part). For most purposes, the newer ROBOT subset command best matches most requirements, as it essentially walks up the graph from the provided seed terms (using customizable object properties). While this is not guaranteed to be \"entailment preserving\", it should suffice for most reasoning purposes.</p> <p>One of the design confusions in this part of the process is that most use cases of application ontologies really do not care at all about OWL. Remember, OWL really only matters for the design of domain ontologies, to ensure a consistent representation of the domain and enable reasoning-based classification. </p> <p></p>"},{"location":"tutorial/project-ontology-development/#managing-metadata-and-customisations","title":"Managing metadata and customisations","text":"<p>Just like any other ontology, a project ontology should be well annotated according to the standards of FAIR Semantics, for example using the OBO Foundry conventions. In particular, project ontologies should be</p> <ul> <li>annotated with a title, a license, a description</li> <li>versioned and annotated with versionIRIs</li> <li>get associated with some PURL system (OBO Foundry often wont accept such ontologies, but other free options like https://w3id.org/ exist)</li> </ul> <p>Furthermore, it is often necessary to add additional terms to the ontology which are not covered by other upstream ontologies. Here we need to distinguish two cases:</p> <ul> <li>The need to quickly add terms that belong somewhere else</li> <li>Adding terms that have no obvious home in any of your declared source ontologies.</li> </ul> <p>With our OBO hat on, if you start adding terms \"quickly\", you should develop a procedure to get these terms into suitable upstream ontologies at a later stage. This is not so much a necessity as a matter of \"open data ethics\": if you use other people's work to make your life easier, its good to give back!</p> <p>Lastly, our use cases sometimes require us to add additional links between the terms in our ontologies. For example, we may have to add subClassOf links between classes of different ontologies that cover the same domain. Or we want to add additional information. As with \"quickly adding terms\", if the information is generally useful, you should consider to add them to the respective upstream source ontology (synonyms of disease terms from Mondo, for example). We often manage such axioms as ROBOT templates and curate them as simple to read tables.</p> <p></p>"},{"location":"tutorial/project-ontology-development/#merging-and-post-processing","title":"Merging and post-processing","text":"<p>Just like with most ontologies, the last part of the process is merging the various pieces (modules from external sources, customisations, metadata) together into a single whole. During this phase a few things can happen, but these are the most common ones:</p> <ul> <li>Merging: All separate parts are merged into one file.</li> <li>Restructure: Sometimes, we run additional processes to update the structure of the final ontology. One common post-processing step is to remove obsolete classes that may have come in during the extraction phase, add additional links between classes using approaches such as relation graph or prune away unsatisfiability-causing axioms such as disjointness axioms and negation. The latter is sometimes necessary when terms from multiple logically incompatible ontologies are imported.</li> <li>Annotate version information: owl:versionInfo and versionIRI annotations are added to the merged ontology.</li> </ul>"},{"location":"tutorial/project-ontology-development/#validation","title":"Validation","text":"<p>One thing to remember is that you are not building a domain ontology. You are usually not concerned with typical issues in ontology engineering, such as logical consistency (or coherence, i.e. the absence of unsatisfiable classes). The key for validating an application ontology comes from its intended use case: Can the ontology deliver the use case it promised? There are many approaches to ensure that, chief among them competency questions. What we usually do is try to express competency questions as SPARQL queries, and ensure that there is at least one result. For example, for one of the project ontologies the author is involved with (CPONT), we have developed a synthetic data generator, which we combine with the ontology to ask questions such as: \"Give me all patients which has a recorded diagnosis of scoliosis\" (SPARQL). So the ontology does a \"good job\" if it is able to return, say, at least 100 patients in our synthetic data for which we know that they are diagnoses with scoliosis or one of its subtypes.</p>"},{"location":"tutorial/project-ontology-development/#frameworks-for-building-project-ontologies","title":"Frameworks for building project ontologies","text":"<p>The perfect framework for building project ontologies does not exist yet. The Ontology Development Kit (ODK) has all the tools you need set up a basic application ontology, but the absence of a \"perfect\" module extraction algorithm for this use case is still unsatisfactory. However, for many use cases, <code>filter</code> modules like the ones described above are actually good enough. Here we will go through a simple example.</p> <p>An alternative framework for application ontology development based on a Web User Interface and tables for managing the seed is developed by James Overton at (ontodev).</p> <p>Another potential alternative is to go all the way to graph-land and build the application ontology with KGX and LinkML. See here for an example. Creating a project ontology this way feels more like a Knowledge Graph ETL task than building an ontology!</p>"},{"location":"tutorial/project-ontology-development/#example-application-ontology-with-odk","title":"Example application ontology with ODK","text":"<p>Set up a basic ODK ontology. We are not covering this again in this tutorial, please refer to the tutorial on setting up your ODK repo.</p>"},{"location":"tutorial/project-ontology-development/#dealing-with-large-imports","title":"Dealing with large imports","text":"<p>Many of the larger imports in application ontologies do not fit into the normal GitHub file size limit. In this cases it is better to attach them to a GitHub release rather than to check them into version control.</p> <p>TBD</p>"},{"location":"tutorial/project-ontology-development/#additional-materials-and-resources","title":"Additional materials and resources","text":"<ul> <li>TBD</li> </ul>"},{"location":"tutorial/project-ontology-development/#contributors","title":"Contributors","text":"<ul> <li>Nicolas Matentzoglu</li> </ul>"},{"location":"tutorial/pull-requests/","title":"Pull Requests","text":""},{"location":"tutorial/pull-requests/#prerequisites","title":"Prerequisites","text":"<p>Participants will need to have access to the following resources and tools prior to the training:</p> <ul> <li>GitHub account - register for a free GitHub account here</li> <li>Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop</li> <li>Protege - Install Protege 5.5 or higher; download it here</li> </ul>"},{"location":"tutorial/pull-requests/#preparation-optional","title":"Preparation (optional)","text":"<ul> <li>Review tutorial on Contributing to OBO Ontologies</li> <li>See 'How to' guide on Pull Requests</li> </ul>"},{"location":"tutorial/pull-requests/#what-is-delivered-as-part-of-the-course","title":"What is delivered as part of the course","text":"<p>Description: How to create and manage pull requests to ontology files in GitHub.</p>"},{"location":"tutorial/pull-requests/#learning-objectives","title":"Learning objectives","text":"<ul> <li>How to create a really good pull request</li> <li>GitHub Pull Request Workflow</li> <li>How to find a reviewer for your pull request in an open source environment</li> <li>How to review a pull request</li> <li>How to change a pull request in response to review</li> <li>How to update your branch from master</li> <li>Resolve conflicts on branch</li> </ul>"},{"location":"tutorial/pull-requests/#contributors","title":"Contributors","text":"<ul> <li>Nicole Vasilevsky</li> <li>Nico Matentzoglu</li> </ul>"},{"location":"tutorial/pull-requests/#tutorials","title":"Tutorials","text":""},{"location":"tutorial/pull-requests/#monarch-obo-training-tutorials","title":"Monarch OBO training Tutorials","text":""},{"location":"tutorial/pull-requests/#pull-requests-part-1","title":"Pull Requests Part 1","text":""},{"location":"tutorial/pull-requests/#pull-requests-part-2","title":"Pull Requests Part 2","text":""},{"location":"tutorial/pull-requests/#how-to-create-a-really-good-pull-request","title":"How to create a really good pull request","text":""},{"location":"tutorial/pull-requests/#what-is-a-pull-request","title":"What is a Pull Request?","text":"<p>A pull request (PR) is an event in Git where a contributor (you!) asks a maintainer of a Git repository to review changes (e.g. edits to an ontology file) they want to merge into a project (e.g. the owl file) (see reference). A contributor creates a pull request to propose and collaborate on changes to a repository. These changes are proposed in a branch, which ensures that the default branch only contains finished and approved work. See more details here.</p> <p></p>"},{"location":"tutorial/pull-requests/#how-to-write-a-great-descriptive-title","title":"How to write a great descriptive title","text":"<p>When committing a pull request, you must include a title and a description (more details in the workflow below.) Tips below (adapted from Hugo Dias):</p> <ul> <li> <p>The title of the PR should be self-explanatory</p> </li> <li> <p>Do: Describe what was changed in the pull request</p> </li> <li> <p>Example: Add new term: MONDO:0100503 DPH5-related diphthamide-deficiency syndrome`</p> </li> <li> <p>Don't: write a vague title that has very little meaning.</p> </li> <li> <p>Example: Add new term</p> </li> <li> <p>Don't: use the branch name in the pull request (sometimes GitHub will offer this as a default name)</p> </li> <li>Example: issue-5024</li> </ul>"},{"location":"tutorial/pull-requests/#what-kind-of-information-to-include-in-the-description","title":"What kind of information to include in the description","text":"<ul> <li>Describe what was changed in the pull request</li> <li>Explain why this PR exists</li> <li>Make it clear how it does what it sets out to do. E.g., Does it edit the ontology-edit.owl file? Does it edit another file(s)?</li> <li>What was your motivation for the chosen solution?</li> <li>Use screenshots to demonstrate what has changed. See How to guide on creating screenshots</li> </ul> <p>Example:</p> <p></p>"},{"location":"tutorial/pull-requests/#general-tips","title":"General tips","text":"<ul> <li>Do:</li> <li>Follow the Single Responsibility Principle: The pull request should do only one thing.<ul> <li>Note: sometimes a small edit can change a lot of code, for example, if you want to change all of the created_by annotations to dc:creator. That's okay.</li> </ul> </li> <li>The pull request should be atomic: it should be small and self contained with simple changes that affect as little code as possible</li> <li>Whenever possible, break pull-requests into smaller ones</li> <li>Commit early, commit often</li> <li>Include specific information like the ID and label for terms changed. Note, you can easily obtain term metadata like OBO ID, IRI, or the term label in Protege by clicking the three lines above the Annotations box (next to the term name); see screenshot below. You can also copy the IRI in markdown, which is really convenient for pasting into GitHub.</li> </ul> <ul> <li>Don't:</li> <li>Make additional changes on a single PR that goes beyond the scope of the ticket or PR. For example, if you are adding a new term, don't also fix definitions or formatting for other terms.</li> </ul>"},{"location":"tutorial/pull-requests/#github-pull-request-workflow","title":"GitHub Pull Request Workflow","text":""},{"location":"tutorial/pull-requests/#update-the-local-copy-of-the-ontology","title":"Update the local copy of the ontology","text":"<ol> <li>In GitHub Desktop, navigate to your local ontology directory of your ontology</li> <li>Make sure you are on the master/main branch and click Pull origin (or Fetch origin)</li> </ol>"},{"location":"tutorial/pull-requests/#create-a-new-working-branch","title":"Create a New Working Branch","text":"<ol> <li>When starting to work on a ticket or making edits to an ontology, you should create a new branch of the repository to edit the ontology file.</li> <li>Make sure you are on the master branch before creating a new branch. Please do not create a new branch off of an existing branch (unless the situation explicitly calls for it).</li> <li>To create a new branch, click on Current Branch and select New Branch</li> </ol> <ol> <li>Name your branch. Some recommended best practices for branch name are to name the branch after the issue number, for example issue-206. If you are not addressing a ticket per se, you could name the branch: 'initals-edits-date', e.g. nv-edits_2022-07-12, or give it a name specific to what you are doing, e.g. fix-typos-2022-07-12.</li> </ol>"},{"location":"tutorial/pull-requests/#continuing-work-on-an-existing-working-branch","title":"Continuing work on an existing Working Branch","text":"<ol> <li>If you are continuing to do work on an existing branch, in addition to updating master, go to your branch by selecting Current Branch in GitHub Desktop and either search for or browse for the branch name.</li> </ol>"},{"location":"tutorial/pull-requests/#video-explanation","title":"Video Explanation","text":"<p>A video is below.</p> <p>OPTIONAL: To update the working branch with respect to the current version of the ontology, select Branch from the top menu, Update from master. This step is optional because it is not necessary to work on the current version of the ontology; all changes will be synchronized when git merge is performed.</p>"},{"location":"tutorial/pull-requests/#editing-an-ontology-on-a-branch","title":"Editing an ontology on a branch","text":"<ol> <li>Create a new branch, open Protege. Protege will display your branch name in the lower left corner (or it will show Git: master)</li> </ol> <ol> <li>Make necessary edits in Protege.</li> </ol>"},{"location":"tutorial/pull-requests/#committing-pushing-and-making-pull-requests","title":"Committing, pushing and making pull requests","text":"<ol> <li>Review: GitHub Desktop will display the diff or changes made to the ontology.</li> <li>Before committing, view the diff and ensure the changes were intended. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask for help instead or consider discarding your changes and starting the edits again. To discard changes, right click on the changed file name and select Discard changes.</li> </ol> <p>Example diffs:</p> <p>Example 1 (Cell Ontology):</p> <p></p> <p>Example 2 (Mondo):</p> <p></p>"},{"location":"tutorial/pull-requests/#write-a-good-commit-messages","title":"Write a good commit messages","text":"<p>For more details about writing good commit messages see here (mandatory reading).</p> <ol> <li> <p>Commit message: Before Committing, you must add a commit message. In GitHub Desktop in the Commit field in the lower left, there is a subject line and a description.</p> </li> <li> <p>Give a very descriptive title: Add a descriptive title in the subject line. For example: add new class ONTOLOGY:ID [term name] (e.g. add new class MONDO:0000006 heart disease)</p> </li> <li> <p>Write a detailed summary of what the change is in the Description box, referring to the issue. The sentence should clearly state how the issue is addressed.</p> </li> <li> <p>NOTE: You can use the word \u2018fixes\u2019 or \u2018closes\u2019 in the commit message - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages.</p> </li> <li> <p>\u2018Fixes\u2019 and \u201cCloses\u2019 is case-insensitive and can be plural or singular (fixes, closes, fix, close).</p> </li> </ol> <p></p> <ol> <li> <p>If you don\u2019t want to close the ticket, just refer to the ticket # without the word \u2018fixes\u2019 or use \u2018adresses\u2019 or 'addresses'. The commit will be associated with the correct ticket but the ticket will remain open.</p> </li> <li> <p>Push: To incorporate the changes into the remote repository, click Commit to [branch name], then click Push.</p> </li> </ol>"},{"location":"tutorial/pull-requests/#make-a-pull-request-pr","title":"Make a pull request (PR)","text":"<ol> <li>You can either make a pull request (PR) directly from GitHub Desktop, or via the GitHub web browser.</li> <li>To make a PR from GitHub Desktop, click the button 'Create Pull Request'. You will be directed to your web browser and GitHub repo.</li> <li>Click Create Pull Request.</li> </ol> <ol> <li>If your PR is a work-in-progress and not ready for review, you can save it as a draft PR and convert it to a PR when it is ready for review.</li> </ol> <ol> <li>If you do not create a PR directly from GitHub Dekstop, you can go to your GitHub repo and you will see a yellow banner on top that notifies you of a pending PR.</li> <li>Navigate to the tab labeled \u2018Code\u2019. You should see your commit listed at the top of the page in a light yellow box. If you don\u2019t see it, click on the \u2018Branches\u2019 link to reveal it in the list, and click on it.</li> </ol> <ol> <li>Click the green button \u2018Compare &amp; pull request\u2019 on the right.</li> <li>You may now add comments, if applicable and request a reviewer. See section below on reviewers.</li> <li>You can see the diff for your file by clicking 'Files Changed'. Examine it as a sanity check.</li> <li>Go back to the Conversation tab.</li> <li>Click on the green box \u2018Pull request\u2019 to generate a pull request.</li> </ol>"},{"location":"tutorial/pull-requests/#how-to-find-a-reviewer-for-your-pull-request-in-an-open-source-environment","title":"How to find a reviewer for your pull request in an open source environment","text":"<ol> <li>Publicly managed ontologies do not have a structure in place to automatically deal with PRs.</li> <li>If you make a PR to an open source project, you should open a separate social channel to the developers to notify them of your PR (e.g. find the project mailing list, Slack, etc.). You should introduce yourself and give some context.</li> <li>Depending on the level of your permissions for the repository, you may or may not be able to assign a reviewer yourself.</li> <li>If you have write access to the repository, you can assign a reviewer.</li> <li>Otherwise, you can tag people in the description of your pull request.</li> <li> <p>Tips for finding reviewers:</p> </li> <li> <p>An ontology repository should have an owner assigned. This may be described in the ReadMe file or on the OBO Foundry website. For example, the contact person for Mondo is Nicole Vasilevsky.</p> </li> <li>The primary owner can likely review your PR or triage your request to the appropriate person.</li> <li>If you are addressing a specific ticket, you may want to assign the person who created the ticket to review.</li> </ol>"},{"location":"tutorial/pull-requests/#how-to-review-a-pull-request-pr","title":"How to review a pull request (PR)","text":"<p>If you are assigned to review a pull request, you should receive an email notification. You can also check for PRs assigned to you by going to https://github.com/pulls/assigned.</p>"},{"location":"tutorial/pull-requests/#what-kind-of-person-do-we-need-for-what-kind-of-pull-request","title":"What kind of person do we need for what kind of pull request?","text":"<p>It depends on what the pull request is addressing. Remember the QC checks will check for things like unsatisfiable classes and many other checks (that vary between ontologies). Your job as a reviewer is to check for things that the QC checks won't pick up and need human judgement.</p> <ul> <li>If it is content changes, like adding new terms, or reclassifying a term, an ontology curator could review your PR.</li> <li>If the PR is addressing quality control or technical aspects, one of the ontology semantic engineers would probably be a good fit.</li> <li> <p>If you don't know who to assign, we recommend assigning the ontology contact person and they can triage the request.</p> </li> <li> <p>To review a PR, you should view the 'Files changed' and view the diff(s). You can review changes in a pull request one file at a time.</p> </li> <li>While reviewing the files in a pull request, you can leave individual comments on specific changes.</li> </ul> <p>Example: </p>"},{"location":"tutorial/pull-requests/#things-to-look-out-for-when-reviewing-a-pr","title":"Things to look out for when reviewing a PR:","text":"<ol> <li> <p>Make sure the changes made address the ticket. In the example above, Sabrina addressed a ticket that requested adding a new term to Mondo, which is what she did on the PR (see https://github.com/monarch-initiative/mondo/pull/5078).</p> </li> <li> <p>Examples of things to look for in content changes (like adding new terms or revising existing terms):</p> </li> <li>Poorly written definitions</li> <li>Missing or misformatted database cross-references</li> <li>Incorrectly scoped synonyms</li> <li> <p>appropriate annotations</p> </li> <li> <p>Make sure there are not any unintended or unwanted changes on the PR. See example below. Protege reordered the location of a term in the file.</p> </li> </ol> <p></p> <ol> <li>Check that the logic is correct. This can be a difficult thing to do. Some tips:</li> <li>Open the branch in Protege and examine the hierarchy in Protege</li> <li>Compare the logic that was used to the logic used in an existing term</li> <li>If the ontology uses Design Patterns, ensure the logic is consistent with the Design Patterns</li> <li>Ask an expert in ontology logic to help review the PR</li> <li>Remember there is no magic bullet to ensuring an ontology is logically sound, but do the best you can</li> </ol>"},{"location":"tutorial/pull-requests/#adding-your-review","title":"Adding your review","text":"<ol> <li>After you finish reviewing each file, you can mark the file as viewed. This collapses the file, helping you identify the files you still need to review.</li> <li>A progress bar in the pull request header shows the number of files you've viewed.</li> <li>You can leave comments and requests for changes on the PR inline when viewing the 'Files changed'.</li> <li>You can add a single comment, or start a review if you have multiple comments.</li> <li> <p>After reviewing the file(s), you can approve the pull request or request additional changes by submitting your review with a summary comment.</p> </li> <li> <p>Comment (Submit general feedback without explicit approval)</p> </li> <li>Approve (Submit feedback and approve merging these changes)</li> <li> <p>Request changes (Submit feedback that must be addressed before merging)</p> </li> <li> <p>In addition or instead of adding inline comments, you can leave comments on the Conversation page. The conversation page is a good place to discuss the PR, and for the original creator to respond to the reviewer comments.</p> </li> </ol>"},{"location":"tutorial/pull-requests/#inline-commits","title":"Inline commits","text":"<p>GitHub added a 'suggested Changes' feature that allows a PR reviewer to suggest an exact change in a comment in a PR. You can add inline comments and commit your comment using 'inline commits'. Read more about it here.</p> <ol> <li>Go to the 'Files changed' tab of a pull request</li> <li>Hover over the line you want to fix, and a blue box with a plus sign appears near the gutter on the left</li> <li>Click that to display the normal line comment form</li> <li>Click the button with a plus and minus sign, it adds a suggestion block to the comment text area with the existing text</li> </ol> <p></p> <ol> <li>You can make changes to the text inside the suggestion box. Note that you can add context for your suggested changes outside of the suggestion block</li> <li>When you create the comment, it will show up to the maintainer as a diff</li> <li>The maintainer can see what changes you are suggesting and accept them with a click</li> </ol>"},{"location":"tutorial/pull-requests/#when-are-you-done-with-your-review","title":"When are you done with your review?","text":"<p>If you review the PR and the changes properly address what was described in the description, then it should be sufficient. Not every PR needs comments, it can be approved without any comments or requests for changes. Feel free to ask for help with your review, and/or assign additional reviewers.</p> <p>Some of the content above was adapted from GitHub Docs.</p>"},{"location":"tutorial/pull-requests/#how-to-change-a-pull-request-in-response-to-review","title":"How to change a pull request in response to review","text":"<ol> <li>Check out your branch in GitHub Desktop and open the file in Protege.</li> <li>Make the suggested changes.</li> <li>Check the diff.</li> <li>Commit your changes on your branch.</li> <li>Note, you do not need to create another PR, your commits will show up on the same PR.</li> <li>Resolve the comments on the PR.</li> <li>Notify the reviewer that your PR is ready for re-review.</li> </ol>"},{"location":"tutorial/pull-requests/#how-to-update-from-master","title":"How to update from master","text":"<ol> <li>In GitHub Desktop, navigate to your branch.</li> <li>In the top file menu, select Branch -&gt; Update from master.</li> </ol>"},{"location":"tutorial/pull-requests/#resolve-conflicts-on-branch","title":"Resolve conflicts on branch","text":"<p>Conflicts arise when edits are made on two separate branches to the same line in a file. (reference). When editing an ontology file (owl file or obo file), conflicts often arise when adding new terms to an ontology file on separate branches, or when there are a lot of open pull requests.</p> <p>Conflicts in ontology files can be fixed either on the command line or using GitHub Desktop. In this lesson, we describe how to fix conflicts using GitHub Desktop.</p>"},{"location":"tutorial/pull-requests/#fix-conflicts-in-github-desktop","title":"Fix conflicts in GitHub desktop","text":"<ol> <li>In GitHub Desktop, go to your master/main branch and fetch pull.</li> <li>Go to branch with conflict.</li> <li>Pull branch.</li> <li>Branch -&gt; update from master.</li> <li>Open in Sublime or Atom.</li> <li>Make changes in file (open the ontology file in a text editor (like Sublime) and search for the conflicts. These are usually preceded by &lt;&lt;&lt;&lt;&lt;. Fix the conflicts, then save).</li> <li>In GitHub Desktop, continue merge.</li> <li>Push.</li> <li>In terminal: <code>open [ontology file name]</code> (e.g.<code>open mondo-edit.obo</code>) or open in Protege manually.</li> <li>Save as (nothing should have changed in the diff).</li> <li>Check the diff in GitHub online.</li> </ol>"},{"location":"tutorial/pull-requests/#video-explanation_1","title":"Video Explanation","text":"<p>Watch a video below with an example fixing a conflict in the Mondo ontology file.</p> <p>Some examples of conflicts that Nicole fixed in Mondo are below:</p> <p> </p>"},{"location":"tutorial/pull-requests/#further-reading","title":"Further reading","text":""},{"location":"tutorial/pull-requests/#gene-ontology-daily-workflow","title":"Gene Ontology Daily Workflow","text":"<p>Gene Ontology Editing Guide</p>"},{"location":"tutorial/pull-requests/#github-merge-conflicts","title":"GitHub Merge Conflicts","text":"<ul> <li>Resolving a merge conflict on GitHub</li> <li>Git merge conflicts   </li> </ul>"},{"location":"tutorial/pull-requests/#the-anatomy-of-a-perfect-pull-request","title":"The anatomy of a perfect pull request","text":"<p>Blog post by Hugo Dias</p>"},{"location":"tutorial/pull-requests/#suggesting-changes-on-github-includes-description-of-how-to-make-inline-commits","title":"Suggesting Changes on GitHub - includes description of how to make inline commits","text":""},{"location":"tutorial/robot-tutorial-1/","title":"ROBOT Mini-Tutorial 1: Convert, Extract and Template","text":"<p>This tutorial covers three ROBOT commands:</p> <ul> <li>Convert</li> <li>Extract</li> <li>Template</li> </ul> <p>Before starting this tutorial, either:</p> <ul> <li>make sure Docker is running and you are in the container</li> <li>download and install ROBOT for your operating system</li> </ul> <p>We will be using the files from the Ontologies 101 Tutorial. In your terminal, navigate to the repository that you cloned and then into the <code>BDK14_exercises</code> folder.</p>"},{"location":"tutorial/robot-tutorial-1/#convert","title":"Convert","text":"<p>So far, we have been saving our ontologies in Protege using the default RDF/XML syntax, but there are many flavors of OWL. We will discuss each of these serializations in more detail during the class session, but ROBOT supports the following:</p> <ul> <li>owl - RDF/XML</li> <li>owx - OWL/XML</li> <li>ttl - Turtle</li> <li>obo - OBO Format</li> <li>ofn - OWL Functional</li> <li>omn - OWL Manchester</li> <li>json - obographs JSON</li> </ul>"},{"location":"tutorial/robot-tutorial-1/#lets-try-it","title":"Let's Try It!","text":"<p>Navigate to the <code>basic-subclass/</code> folder. Open <code>chromosome-parts.owl</code> in your text editor and you will see it's in RDF/XML format. We're going to take this file and convert it to Turtle (<code>ttl</code>) serialization. Return to your terminal and enter the following command:</p> <pre><code>robot convert --input chromosome-parts.owl --format ttl --output chromosome-parts.ttl\n</code></pre> <p>ROBOT convert is smart about detecting formats, so since the output file ends with <code>.ttl</code>, the <code>--format ttl</code> parameter isn't really required. If you wanted to use a different file ending, say <code>.owl</code>, you will need to include the format flag to force ROBOT to write Turtle.</p> <p>Now open <code>chromosome-parts.ttl</code> in your text editor and see what's changed! RDF/XML and Turtle are very different serializations, but the actual data that is contained in these two files is exactly the same.</p>"},{"location":"tutorial/robot-tutorial-1/#on-your-own","title":"On Your Own","text":"<ol> <li>Convert <code>chromosome-parts.owl</code> into the following formats: <code>obo</code> (OBO Format), <code>ofn</code> (OWL Functional), and <code>omn</code> (OWL Manchester).</li> <li>Open each file and take a minute to scroll through (we don't expect you to be able to read these, they're mostly meant for computers!)</li> <li>Why do you think we need these different serializations? What do you think the purpose of OWL Manchester vs. RDF/XML is?</li> </ol>"},{"location":"tutorial/robot-tutorial-1/#extract","title":"Extract","text":"<p>Sometimes we only want to browse or share a subset of an ontology, especially with some of the larger OBO Foundry ontologies. There are two main methods for creating subsets:</p> <ul> <li>MIREOT</li> <li>SLME</li> </ul> <p>Right now, we will use use MIREOT and talk more about SLME in our class session. MIREOT makes sure that you have the minimal amount of information you need to reuse an existing ontology term. It allows us to extract a small portion of the class hierarchy by specifying upper and lower boundaries, which you will see in the example below. We need to know the identifiers (as CURIEs) of the terms that we want to set as our boundaries.</p>"},{"location":"tutorial/robot-tutorial-1/#lets-try-it_1","title":"Let's Try It!","text":"<p>Open <code>chromosome-parts.owl</code> in Protege and open the Class hierarchy. We are going to create a subset relevant to the term \"chromosome\". First, we will find the CURIE of our desired term. Search for \"chromosome\" and find the \"id\" annotation property. This will be our lower term. Right now, we won't set an upper boundary. That means this subset will go all the way up to the top-level ancestor of \"chromosome\".</p> <p>Return to your terminal and enter the following command (where the <code>--lower-term</code> is the CURIE that we just found):</p> <pre><code>robot extract --method MIREOT --input chromosome-parts.owl --lower-term GO:0005694 --output chromosome-full.owl\n</code></pre> <p>Now open <code>chromosome-full.owl</code> in Protege and open the Class hierarchy. When you open the \"cellular_component\" node, you'll notice that most of the terms are gone! Both \"organelle\" and \"intracellular part\" remain because they are in the path between \"chromosome\" and the top-level \"cellular_component\". Keep clicking down and you'll find \"chromosome\" at the very bottom. Since \"chromosome\" has two named parents, both of those parents are included, which is why we ended up with \"organelle\" and \"intracellular part\".</p> <p>Now let's try it with an upper term. This time, we want \"organelle\" to be the upper boundary. Find the CURIE for \"organelle\".</p> <p>Return to your terminal and enter the following command (where the <code>--upper-term</code> is the new CURIE we just found):</p> <pre><code>robot extract --method MIREOT \\\n  --input chromosome-parts.owl \\\n  --lower-term GO:0005694 \\\n  --upper-term GO:0043226 \\\n  --output chromosome.owl\n</code></pre> <p>Open <code>chromosome.owl</code> and again return to the Class hierarchy. This time, we see \"organelle\" directly below <code>owl:Thing</code>. \"intracellular part\" is also now missing because it does not fall under \"organelle\".</p>"},{"location":"tutorial/robot-tutorial-1/#on-your-own_1","title":"On Your Own","text":"<ol> <li>Play with different upper- and lower-level terms to create different subsets</li> <li>Compare the terms that are in the subsets to the terms in the original <code>chromosome-parts.owl</code> file.</li> <li>What is missing from the terms in the subsets? What has been included as our \"minimal\" information?</li> </ol>"},{"location":"tutorial/robot-tutorial-1/#template","title":"Template","text":"<p>Most of the knowledge encapsulated in ontologies comes from domain experts. Often, these domain experts are not computer scientists and are not familiar with the command line. Luckily, most domain experts are familiar with spreadsheets!</p> <p>ROBOT provides a way to convert spreadsheets into OWL ontologies using template strings. We'll get more into these during the class session, but if you want to get a head start, they are all documented here. Essentially, the first row of a ROBOT template is a human-readable header. The second row is the ROBOT template string. Each row below that represents an entity to be created in the output ontology. We can create new entities by giving them new IDs, but we can also reference existing entities just by label. For now, we're going to create a new, small ontology with new terms using a template.</p>"},{"location":"tutorial/robot-tutorial-1/#lets-try-it_2","title":"Let's Try It!","text":"<p>Download (or copy/paste) the animals.tsv file and move it to the <code>basic-subclass/</code> folder (or whatever folder you would like to work in; we will not be using any of the Ontology 101 files anymore). This contains the following data:</p> CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000001 animal Any animal in the world. obo:0000002 canine animal A member of the genus Canis. obo:0000003 feline animal A member of the genus Felis. <p>In the first column, we use the special <code>ID</code> keyword to say that this is our term's unique identifier. The second column contains the <code>LABEL</code> keyword which is a shortcut for the <code>rdfs:label</code> annotation property. The third column uses the <code>SC</code> keyword to state that this column will be a subclass statement. The <code>%</code> sign is replaced by the value in the cell. We'll talk more about this keyword and the <code>%</code> symbol during the class session. Finally, the last column begins with <code>A</code> to denote that this will be an annotation, and then is followed by the annotation property we're using.</p> <p>Just looking at the template, you can begin to predict what a class hierarchy using these terms would look like in an ontology. We can turn this into reality!</p> <p>In your terminal, enter the following command:</p> <pre><code>robot template --template animals.tsv --output animals.owl\n</code></pre> <p>Note that in this command, we don't use the <code>--input</code> parameter. That parameter is reserved for input ontologies, and we are not using one right now. More on this later.</p> <p>Open <code>animals.owl</code> in Protege, and you'll be able to see the class hierarchy we defined in the template as an actual structure.</p> <p>Now let's make another small ontology that reuses some terms from our <code>animals.owl</code> file. Download (or copy/paste) animals2.tsv into the same folder. This contains the following:</p> CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000004 dog canine A member of the subspecies Canis lupus familiaris. obo:0000005 cat feline A member of the species Felis catus. <p>You'll notice that we are referencing two terms from our other spreadsheet in this one.</p> <p>In your terminal, enter the following command:</p> <pre><code>robot template --input animals.owl --template animals2.tsv --output animals2.owl\n</code></pre> <p>This time, we did use the <code>--input</code> parameter and provided the animals ontology we just created. This allows us to use any term in the <code>animals.owl</code> file in our <code>animals2.tsv</code> template and ROBOT will know what we're talking about.</p> <p>Go ahead and open <code>animals2.owl</code> in Protege. What's missing? The parent classes for \"dog\" and \"cat\" don't have labels, and the \"animal\" term is missing entirely. This is because, even though ROBOT knew about these classes, we didn't ask for the original ontology to be included in the output, so no axioms from that ontology can be found in this newly-created one. Next week, we'll learn about combining ontologies with the Merge command.</p> <p>For now, let's add the original <code>animals.owl</code> file as an import:</p> <ol> <li>Go to the \"Active ontology\" tab and find the \"Imported ontologies\" section at the bottom</li> <li>Click the + next to \"Direct imports\"</li> <li>Select \"Import an ontology contained in a local file\" and click Continue</li> <li>Browse for the path to <code>animals.owl</code>, click Continue, and then click Finish</li> </ol> <p>Prot\u00e9g\u00e9 will now load <code>animals.owl</code> as an import. When you return to the Entities tab, you'll see all those upper-level terms. Note the difference in how the terms are displayed in the class hierarchy.</p>"},{"location":"tutorial/robot-tutorial-1/#on-your-own_2","title":"On Your Own","text":"<ol> <li>Try adding another class or two to the <code>animals.tsv</code> template and regenerating <code>animals.owl</code>.</li> <li>Can you create your own template?</li> </ol>"},{"location":"tutorial/robot-tutorial-2/","title":"ROBOT Tutorial 2: Annotate, Merge, Reason and Diff","text":"<p>In week 6, we got some hands-on experience with ROBOT using <code>convert</code>, <code>extract</code>, and <code>template</code>. This week, we will learn four new ROBOT commands:</p> <ul> <li>Annotate</li> <li>Merge</li> <li>Reason</li> <li>Diff</li> </ul> <p>The goal of these and previous commands is to build up to creating an ontology release workflow.</p> <p>Before starting this tutorial, either:</p> <ul> <li>make sure Docker is running and you are in the container</li> <li>download and install ROBOT for your operating system</li> </ul> <p>To start, we will be working in the same folder as the first ROBOT Mini-Tutorial. Navigate to this folder in your terminal and list the contents of the current directory by running <code>ls</code>. You should see <code>catalog-v001.xml</code> listed as one of these files. We want to delete this so that we can fix the ontology IRI problem we ran into last week! Before going any further with this tutorial, do this by running either <code>del catalog-v001.xml</code> for Windows or <code>rm catalog-v001.xml</code> if you're using Docker, MacOS, or other Linux system.</p>"},{"location":"tutorial/robot-tutorial-2/#annotate","title":"Annotate","text":"<p>The <code>annotate</code> command allows you to attach metadata to your ontology in the form of IRIs and ontology annotations. Like the annotations on a term, ontology annotations help users to understand how they can use the ontology.</p>"},{"location":"tutorial/robot-tutorial-2/#ontology-iris","title":"Ontology IRIs","text":"<p>As we discussed during previous parts of the course, ontology IRIs are very important! We saw how importing an ontology without an IRI into another ontology without an IRI can cause some problems in the <code>catalog-v001.xml</code> file. We're going to fix that problem by giving IRIs to both our <code>animals.owl</code> and <code>animals2.owl</code> files.</p> <p>Let's start with <code>animals.owl</code>:</p> <pre><code>robot annotate --input animals.owl \\\n  --ontology-iri http://example.com/animals.owl \\\n  --output animals.owl\n</code></pre> <p>You'll notice we gave the same file name as the input file; we're just updating our previous file so we don't need to do this in a separate OWL file.</p> <p>On your own, give <code>animals2.owl</code> the ontology IRI <code>http://example.com/animals2.owl</code>. Remember that, in reality, we always want our ontology IRIs to be resolvable, so these would be pretty bad IRIs for an actual ontology.</p> <p>Let's fix our import statement now. Open <code>animals2.owl</code> in Prot\u00e9g\u00e9 and go to the Entities tab. You'll see that even though we still have the import statement in the Active ontology tab, the top-level terms are no longer labeled. Since we changed the ontology IRI, Prot\u00e9g\u00e9 can no longer resolve our local file (because the <code>catalog-v001.xml</code> file was not updated). Go back to the Active ontology tab and click the X to the right of our original import. Then, re-add <code>animals.owl</code> as an import using the same steps as last time. When you return to the Entities tab, you'll once again see the labels of the top-level terms.</p>"},{"location":"tutorial/robot-tutorial-2/#version-iris","title":"Version IRIs","text":"<p>When we release our ontologies, we want to make sure to include a version IRI. Like the ontology IRI, this should always resolve to the version of the ontology at the time of the release. For clarity, we usually use dates in our version IRIs in the OBO Foundry. That way, you know when you navigate to a specific version IRI, that's what the ontology looked like on that date. (Note: edit files don't usually have version IRIs as they are always changing, and we don't expect to be able to point to a stable version)</p> <p>While you can add a version IRI in Prot\u00e9g\u00e9, if you're trying to create an automated release workflow, this is a manual step you don't want to have to include. Keeping it in your release workflow also makes sure that the verion IRIs are consistent (we'll see how to do this with <code>make</code> later). For now, let's add a version IRI to <code>animals.owl</code> (feel free to replace the <code>2021-05-20</code> with today's date):</p> <pre><code>robot annotate --input animals.owl \\\n  --version-iri http://example.com/animals/2021-05-20/animals.owl \\\n  --output animals.owl\n</code></pre> <p>Let's break down this version IRI. We have the host (<code>http://example.com/</code>) followed by our ontology's namespace (<code>animals</code>). Next, we provided the date in the format of <code>YYYY-MM-DD</code>. Finally, we have the name of the file. This is standard for OBO Foundry, except with a different host. For example, you can find a release of OBI from April 6, 2021 at <code>http://purl.obolibrary.org/obo/obi/2021-04-06/obi.owl</code>. In this case, the host is <code>http://purl.obolibrary.org/obo/</code>. Of course, you may see different patterns in non-OBO-Foundry ontologies, but they should always resolve (hopefully!).</p> <p>Go ahead and open or reload <code>animals.owl</code> in Protege. You'll see in the Active Ontology tab that now both the ontology IRI and version IRI fields are filled out.</p>"},{"location":"tutorial/robot-tutorial-2/#ontology-annotations","title":"Ontology Annotations","text":"<p>In addition to ontology and version IRIs, you may also want to add some other metadata to your ontology. For example, when we were introduced to <code>report</code>, we added a description to the ontology to fix one of the report problems. The three ontology annotations that are required by the OBO Foundry are:</p> <ul> <li>Title (<code>dc11:title</code>)</li> <li>License (<code>dc:license</code>)</li> <li>Description (<code>dc11:description</code>)</li> </ul> <p>These three annotation properties all come from the Dublin Core, but they have slightly different namespaces. This is because DC is split into two parts: the <code>/terms/</code> and <code>/elements/1.1/</code> namespaces. Just remember to double check that you're using the correct namespace. If you click on the DC link, you can find the complete list of DC terms in their respective namespaces.</p> <p>ROBOT contains some built-in prefixes, which can be found here. The prefix <code>dc:</code> corresponds to the <code>/terms/</code> namespace and <code>dc11:</code> to <code>/elements/1.1/</code>. You may see different prefixes used (for example, <code>/terms/</code> is sometimes <code>dcterms:</code> or just <code>terms:</code>), but the full namespace is what really matters as long as the prefix is defined somewhere.</p> <p>Let's go ahead and add a title and description to our <code>animals.owl</code> file. We'll do this using the <code>--annotation</code> option, which expects two arguments: (1) the CURIE of the annotation property, (2) the value of the annotation. The value of the annotation must be enclosed in double quotes if there are spaces. You can use any annotation property you want here, and include as many as you want! For now, we'll start with two:</p> <pre><code>robot annotate --input animals.owl \\\n  --annotation dc11:title \"Animal Ontology\" \\\n  --annotation dc11:description \"An ontology about animals\" \\\n  --output animals.owl\n</code></pre> <p><code>--annotation</code> adds these as strings, but remember that an annotation can also point to an link or IRI. We want our license to be a link, so we'll use <code>--link-annotation</code> instead to add that:</p> <pre><code>robot annotate --input animals.owl \\\n  --link-annotation dc:license https://creativecommons.org/licenses/by/4.0/ \\\n  --output animals.owl\n</code></pre> <p>OBO Foundry recommends using Creative Commons for all licenses. We just gave our ontology the most permissive of these, CC-BY.</p> <p>When you open <code>animals.owl</code> in Prot\u00e9g\u00e9 again, you'll see these annotations added to the Active ontology tab. You can also click on the CC-BY link!</p>"},{"location":"tutorial/robot-tutorial-2/#merge","title":"Merge","text":"<p>We've already learned how to include external ontologies as imports. Usually, for the released version of an ontology, the imports are merged in so that all contents are in one file.</p> <p>Another reason you may want to merge two ontologies is if you're adding new terms to an ontology using <code>template</code>, like how we created new animal terms in <code>animals2.tsv</code> last time. We're going to demonstrate two methods of merging now. The first involves merging two (or more!) separate files and the second involves merging all imports into the current input ontology.</p>"},{"location":"tutorial/robot-tutorial-2/#merging-multiple-files","title":"Merging Multiple Files","text":"<p>First, copy <code>animals2.owl</code> to <code>animals-new.owl</code>. In Windows, this command is <code>copy animals2.owl animals-new.owl</code>. For Docker and other Linux operating systems, this is <code>cp animals2.owl animals-new.owl</code>. Open <code>animals-new.owl</code> in Prot\u00e9g\u00e9 and remove the import we added last time. This is done in the Imported ontologies section of the Active ontology tab. Just click the X on the right side of the imported animals ontology. Don't forget to save!</p> <p>Continuing with the <code>animals.owl</code> file we created last week, now run the following command:</p> <pre><code>robot merge --input animals.owl --input animals-new.owl --output animals-full.owl\n</code></pre> <p>When you just import an external ontology into your ontology, you'll notice in the Prot\u00e9g\u00e9 class hierarchy that all terms from the external ontology are a less-bold text than internal terms. This can be seen when you open <code>animals2.owl</code>, where we imported <code>animals.owl</code>. This is simply Prot\u00e9g\u00e9's way of telling us that these terms are not part of your current ontology. Now that we've merged these two ontologies together, when you open <code>animals-full.owl</code> in Prot\u00e9g\u00e9, you'll see that all the terms are bold.</p> <p>By default, the output ontology will get the ontology IRI of the first input ontology. We picked <code>animals.owl</code> as our first ontology here because this is the ontology that we're adding terms to, so we want our new output ontology to replace the original while keeping the same IRI. <code>merge</code> will also copy over all the ontology annotations from <code>animals.owl</code> (the first input) into the new file. The annotations from <code>animals2.owl</code> are ignored, but we'll talk more about this in our class session.</p> <p>If we were editing an ontology in the wild, we'd probably now replace the original with this new file using <code>cp</code> or <code>copy</code>. For now, don't replace <code>animals.owl</code> because we'll need it for this next part.</p> <p>IMPORTANT: Be very careful to check that the format is the same if you're replacing a file! Remember, you can always output OWL Functional syntax or another syntax by ending your output with <code>.ofn</code>, for example: <code>--output animals-full.ofn</code>.</p>"},{"location":"tutorial/robot-tutorial-2/#merging-imports","title":"Merging Imports","text":"<p>When we want to merge all our imports into our working ontology, we call this collapsing the import closure. Luckily (since we're lazy), you don't need to type out each of your imports as an input to do this.</p> <p>We already have <code>animals.owl</code> imported into <code>animals2.owl</code>. Let's collapse the import closure:</p> <pre><code>robot merge --input animals2.owl --collapse-import-closure true --output animals-full-2.owl\n</code></pre> <p>Even though we gave this a different file name, if you open <code>animals-full-2.owl</code> in Prot\u00e9g\u00e9, you'll notice that it's exactly the same as <code>animals-full.owl</code>! This is because we merged the same files together, just in a slightly different way. This time, though, the ontology IRI is the one for <code>animals2.owl</code>, not <code>animals.owl</code>. That is because that was our first input file.</p>"},{"location":"tutorial/robot-tutorial-2/#reason","title":"Reason","text":"<p>As we saw in the prepwork for Week 5, running a reasoner in Prot\u00e9g\u00e9 creates an inferred class hierarchy. In the OBO Foundry, releases versions of ontologies usually have this inferred hierarchy asserted, so you see the full inferred hierarchy when you open the ontology without running the reasoner. ROBOT <code>reason</code> allows us to output a version of the ontology with these inferences asserted.</p> <p>As we discussed, ELK and HermiT are the two main reasoners you'll be using. Instead of using our example ontologies (the asserted and inferred hierarchies for these will look exactly the same), we're going to use another ontology from the Ontologies 101 tutorial from week 5. Navigate back to that directory and then navigate to <code>BDK14_exercises/basic-classification</code>.</p> <p>Like running the reasoner in Prot\u00e9g\u00e9, running <code>reason</code> does three things:</p> <ol> <li>Check for inconsistency</li> <li>Check for unsatisfiable classes</li> <li>Assert the inferred class hierarchy</li> </ol> <p>Remember, when we run the reasoner in Prot\u00e9g\u00e9, if the ontology is inconsistent, <code>reason</code> will fail. If there are unsatisfiable classes, these will be asserted as <code>owl:Nothing</code>. ROBOT will always fail in both cases, but has some tools to help us figure out why. Let's introduce an unsatifiable class into our test and see what happens.</p> <p>First, let's make a copy of <code>ubiq-ligase-complex.owl</code> and call this new file <code>unreasoned.owl</code> (<code>copy</code> or <code>cp</code>).</p> <p>Open <code>unreasoned.owl</code> in Prot\u00e9g\u00e9 and follow the steps below. These are things we've covered in past exercises, but if you get stuck, please don't hesitate to reach out.</p> <ol> <li>Find 'organelle' in the class hierarchy below 'cellular_component' (or just search for it by label)</li> <li>Make 'organelle' disjoint with 'organelle part' (either use the class hierarchy or type it in the expression editor)</li> <li>Find 'intracellular organelle part' below 'intracellular part' or 'organelle part' (or search for it by label)</li> <li>Add 'organelle' as a parent class to 'intracellular organelle part' (remember that you only need to include the single quotes if the label has spaces)</li> </ol> <p>Like we did in the Disjointness part of the Ontologies 101 tutorial, we've made 'intracellular organelle part' a subclass of two classes that should have no overlap based on the disjointness axiom. Save the ontology and return to your terminal. Now, we'll run <code>reason</code>. The default reasoner is ELK, but you can specify the reasoner you want to use with the <code>--reasoner</code> option. For now, we'll just use ELK.</p> <pre><code>robot reason --input unreasoned.owl --output unsatisfiable.owl\n</code></pre> <p>You'll notice that ROBOT printed an error message telling us that the term with the IRI <code>http://purl.obolibrary.org/obo/GO_0044446</code> is unsatisfiable and ROBOT didn't create <code>unsatisfiable.owl</code>. This is ideal for automated pipelines where we don't want to be releasing unsatisfiable classes.</p> <p>We can still use ROBOT to investigate the issue, though. It already gave us the IRI, but we can get more details using the <code>--dump-unsatisfiable</code> option. We won't provide an output this time because we know it won't succeed.</p> <pre><code>robot reason --input unreasoned.owl --dump-unsatisfiable unsatisfiable.owl\n</code></pre> <p>You can open <code>unsatisfiable.owl</code> in Prot\u00e9g\u00e9 and see that 'intracellular organelle part' is not the only term included, even though it was the only unsatisfiable class. Like with the SLME method of extraction, all the terms used in unsatisfiable class or classes logic are included in this unsatisfiable module. We can then use Prot\u00e9g\u00e9 to dig a little deeper in this small module. This is especially useful when working with large ontologies and/or the HermiT reasoner, which both can take quite some time. By extracting a smaller module, we can run the reasoner again in Prot\u00e9g\u00e9 to get detailed explanations. In this case, we already know the problem, so we don't need to investigate any more.</p> <p>Now let's reason over the original <code>ubiq-ligase-complex.owl</code> and see what happens:</p> <pre><code>robot reason --input ubiq-ligase-complex.owl --output reasoned.owl\n</code></pre> <p>If you just open <code>reasoned.owl</code> in Prot\u00e9g\u00e9, you won't really notice a different between this and the input file unless you do some digging. This takes us to our next command...</p>"},{"location":"tutorial/robot-tutorial-2/#diff","title":"Diff","text":"<p>The <code>diff</code> command can be used to compare the axioms in two ontologies to see what has been added and what has been removed. While the diffs on GitHub are useful for seeing what changed, it can be really tough for a human to read the raw OWL formats. Using ROBOT, we can output these diffs in a few different formats (using the <code>--format</code> option):</p> <ul> <li><code>plain</code>: plain text with just the added and removed axioms listed in OWL functional syntax (still tough for a human to read, but could be good for passing to other scripts)</li> <li><code>pretty</code>: similar to <code>plain</code>, but the IRIs are replaced with CURIEs and labels where available (still hard to read)</li> <li><code>html</code>: a nice, sharable HTML file with the diffs sorted by term</li> <li><code>markdown</code>: like the HTML diff, but in markdown for easy sharing on platforms like GitHub (perfect for pull requests!)</li> </ul> <p>We're going to generate an HTML diff of <code>ubiq-ligase-complex.owl</code> compared to the new <code>reasoned.owl</code> file to see what inferences have been asserted. <code>diff</code> takes a left (\"original\") and a right (\"new\") input to compare.</p> <pre><code>robot diff --left ubiq-ligase-complex.owl \\\n  --right reasoned.owl \\\n  --format html \\\n  --output diff.html\n</code></pre> <p>Open <code>diff.html</code> in your browser side-by-side with <code>reasoned.owl</code> and you can see how the changes look in both.</p> <p>Homework question: Running <code>reason</code> should assert inferences, yet there are some removed axioms in our diff. Why do you think these axioms were removed?</p>"},{"location":"tutorial/robot-tutorial-qc/","title":"ROBOT Tutorial: Quality Control with ROBOT","text":"<p>In this tutorial you will learn how to set up your QC pipeline with ROBOT <code>report</code>, <code>verify</code>, <code>validate-profile</code> and <code>reason</code>.</p>"},{"location":"tutorial/robot-tutorial-qc/#preparation","title":"Preparation","text":"<ul> <li>You should know how to run ROBOT commands on your machine</li> <li>You should have a basic understanding of OWL and reasoning</li> </ul>"},{"location":"tutorial/robot-tutorial-qc/#overview","title":"Overview","text":"<p>Quality control is a very large concern in ontologies. For example, we want to make sure that our editors use the right annotation properties to attach metadata to terms (such as a date, or a label), or to make sure that our last edit did not accidentally introduce a logical error. In ROBOT, we have four commands that help us in particular to ensure the quality of our ontologies:</p> <ul> <li>ROBOT validate-profile: Ensures that your ontology is a syntactically valid OWL ontology.</li> <li>ROBOT verify: Define \"bad examples\", i.e. situations you want to avoid as SPARQL queries and use <code>verify</code> to ensure they do not appear in your ontology.</li> <li>ROBOT report: Use dozens of time tested best practice checks curated by the OBO Technical Working Group to check your ontology for typical errors, like missing labels or wrong license declarations.</li> <li>ROBOT reason: Use <code>reason</code> to ensure that your ontology is consistent and coherent and test the \"unique name assumption\".</li> </ul> <p>In the following, we will learn about all of these and how they fit in the wider concerns of ontology quality control.</p>"},{"location":"tutorial/robot-tutorial-qc/#download-test-ontology","title":"Download test ontology","text":"<p>Download <code>example.owl</code>, or get it via the command line:</p> <pre><code>curl https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/robot_tutorial_qc/example.owl &gt; example.owl\n</code></pre> <p>Let us ensure we are using the same ROBOT version:</p> <pre><code>robot --version\n</code></pre> <p>We see:</p> <pre><code>ROBOT version 1.8.3\n</code></pre>"},{"location":"tutorial/robot-tutorial-qc/#robot-validate-profile","title":"ROBOT validate-profile","text":"<p>ROBOT validate-profile: Ensures that your ontology is a syntactically valid OWL ontology. This is the absolute minimum check - some \"violations\" to OWL 2 DL validity cause the reasoner to behave in unexpected and wrong ways!</p> <pre><code>robot validate-profile --profile DL -i example.owl\n</code></pre> <p>Thankfully, our test ontology is in valid OWL DL:</p> <pre><code>OWL 2 DL Profile Report: [Ontology and imports closure in profile]\n</code></pre> <p>This check is overlooked by a lot of OWL Ontology developers despite its importance to ensure both a predictable behaviour of the reasoner and of parsing tools. See here for an example where an ontology was not in OWL DL profile, causing various problems for parsing and computation: https://github.com/Orphanet/ORDO/issues/32.</p>"},{"location":"tutorial/robot-tutorial-qc/#robot-report","title":"ROBOT report","text":"<p>Let us generate a simple report:</p> <pre><code>robot report -i example.owl -o report.html\n</code></pre> <p>ROBOT report will do two things:</p> <ul> <li>It will print out the number of errors (violations) and an indication that the report failed:</li> </ul> <pre><code>Violations: 11\n-----------------\nERROR:      5\nWARN:       4\nINFO:       2\nERROR Report failed!\n</code></pre> <ul> <li>And it will provide you with a report file, report.html</li> </ul> <p>Let us look at the file in a browser (simply double-click on the html file the way you would open a PDF). Your report should look similar to this:</p> <p></p> <p>While there are other formats you can export your report to, HTML is a great format which not only offers useful colour coding, but also allows us to click on the related classes and properties and, more importantly, the checks to find our what they mean (for an overview of all ROBOT report checks see here).</p>"},{"location":"tutorial/robot-tutorial-qc/#exercise","title":"Exercise","text":"<p>We will leave it to the reader as an exercise to try and fix all the errors indicated by the report!</p>"},{"location":"tutorial/robot-tutorial-qc/#advanced-usage-of-robot-report","title":"Advanced usage of ROBOT report","text":""},{"location":"tutorial/robot-tutorial-qc/#customisation","title":"Customisation","text":"<p>While by far the most widely spread usage of ROBOT report is to check for OBO best practices, it is possible to customise the report by removing certain OBO ontology checks and adding custom ones.</p> <p>Lets first create a simple <code>profile.txt</code> in our directory and add the following lines:</p> <pre><code>WARN    annotation_whitespace\nERROR   missing_ontology_description\nERROR   missing_definition\nERROR   missing_ontology_license\nERROR   missing_ontology_title\nERROR   misused_obsolete_label\nERROR   multiple_labels\n</code></pre> <p>Now we tell ROBOT to run the command using our custom profile rather than the default ROBOT profile:</p> <pre><code>robot report -i example.owl --profile profile.txt -o report.html\n</code></pre> <p>The resulting report looks different:</p> <p></p> <p>In particular, some checks like <code>missing_superclass</code> which we did not care about for our use case are not shown at all anymore, and others, such as <code>missing_definition</code> are now considered <code>ERROR</code> (red) rather than <code>WARN</code> (warning, yellow) because for our use case, we have decided that definitions on terms are mandatory.</p>"},{"location":"tutorial/robot-tutorial-qc/#robot-verify","title":"ROBOT verify","text":"<p>ROBOT verify allows us to define QC checks for undesirable situation (we sometimes call this \"anti-pattern\") using the SPARQL query language. The idea is simple: we write a SPARQL query for the thing we do not want. For example, we can use SPARQL to look for classes with more than one label. Then, we feed this query to ROBOT verify. ROBOT verify than ensures that the query has no answers, i.e the thing we do not want actually does not happen:</p> <pre><code>PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT DISTINCT ?entity ?property ?value WHERE {\n  VALUES ?property { rdfs:label }\n  ?entity ?property ?value .\n  ?entity ?property ?value2 .\n  FILTER (?value != ?value2) .\n  FILTER NOT EXISTS { ?entity owl:deprecated true }\n  FILTER (!isBlank(?entity))\n}\nORDER BY ?entity\n</code></pre> <p>Let us safe this query now in our working directory as <code>bad_labels.sparql</code> and run the following:</p> <pre><code>robot verify -i example.owl --queries bad_labels.sparql\n</code></pre> <p>ROBOT will output this to tell us which terms have violations:</p> <pre><code>FAIL Rule bad_labels.sparql: 2 violation(s)\nentity,property,value\nhttp://purl.obolibrary.org/obo/OBI_0002986,http://www.w3.org/2000/01/rdf-schema#label,CT scan\nhttp://purl.obolibrary.org/obo/OBI_0002986,http://www.w3.org/2000/01/rdf-schema#label,computed tomography imaging assay\n</code></pre> <p>Now the cool thing with <code>verify</code> is that we can basically feed SPARQL SELECT queries in whatever shape or form we want. To make error messages more readable for curators, you can even encode a proper error message:</p> <pre><code>PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT DISTINCT ?error WHERE {\n  VALUES ?property { rdfs:label }\n  ?entity ?property ?value .\n  ?entity ?property ?value2 .\n  FILTER (?value != ?value2) .\n  FILTER NOT EXISTS { ?entity owl:deprecated true }\n  FILTER (!isBlank(?entity))\n  BIND(CONCAT(\"Entity \",STR(?entity),\" uses two different labels: (1) \",STR(?value),\" and (2) \",STR(?value2)) as ?error)\n}\nORDER BY ?entity\n</code></pre> <p>This time, when running the query, we get:</p> <pre><code>FAIL Rule bad_labels.sparql: 2 violation(s)\nerror\nEntity http://purl.obolibrary.org/obo/OBI_0002986 uses two different labels: (1) CT scan and (2) computed tomography imaging assay\nEntity http://purl.obolibrary.org/obo/OBI_0002986 uses two different labels: (1) computed tomography imaging assay and (2) CT scan\n</code></pre> <p>Which appears much more readable! You can tweak the output in whatever way you think is best. Two things about this:</p> <ul> <li>You cannot do this very well with <code>ROBOT report</code>: despite the ability to include custom queries, all queries must start with:</li> </ul> <pre><code>SELECT DISTINCT ?entity ?property ?value WHERE\n</code></pre> <p>This is one of the reasons we still like using ROBOT verify, despite the fact that ROBOT report can also be extended with custom checks.</p> <ul> <li>Another cool thing about <code>verify</code> is that you can add the <code>--output-dir results/</code> parameter to your query to get ROBOT to export the query results as TSV files. This can be useful if you have many QC queries and need to work with them independently of the checks.</li> <li>Note that ROBOT notices two errors despite there being only 1, technically speaking. This is because the WHERE clause in SPARQL which twice (one where label 1 is first, one where label 2 is first). You can be smart about it and get around it by sorting your results before binding them, but for most use cases this hack is hardly worth the effort.</li> </ul>"},{"location":"tutorial/robot-tutorial-qc/#robot-reason","title":"ROBOT reason","text":"<p>This is not an exhaustive tutorial for ROBOT reason (for more, see here). We only want to get across two checks that we feel absolutely every ontology developer should know about.</p>"},{"location":"tutorial/robot-tutorial-qc/#the-distinct-scope-assumption","title":"The \"distinct scope\" assumption","text":"<p>In most cases, we do not want to define the exact same concept twice. There are some exceptions, for example when we align ontologies such as CHEBI and GO which may have overlapping scope, but in 99.9% of the cases, having the reasoner infer that two classes are the same points to a mistake in the axiomatisation. Let us check that we do not have any such unintended equivalencies in our ontology:</p> <pre><code>robot reason -i example.owl --equivalent-classes-allowed none\n</code></pre> <p>ROBOT will note that:</p> <pre><code>ERROR No equivalent class axioms are allowed\nERROR Equivalence: &lt;http://purl.obolibrary.org/obo/TEST_0600047&gt; == &lt;http://purl.obolibrary.org/obo/OBI_0600047&gt;\n</code></pre> <p>Further investigation in Protege will reveal that TEST_0600047 and OBI_0600047 are subclasses of each other, which causes the reasoner to infer that they are equivalent.</p>"},{"location":"tutorial/setting-up-project-odk/","title":"Tutorial: How to get started with your own ODK-style repository","text":"<ol> <li>Preparation: Installing docker, installing ODK and setting memory. Follow the steps here.</li> <li>Creating your first ontology repository</li> </ol> <p>The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an <code>sh</code> file in the following there exists a corresponding <code>bat</code> file that can be run in the windows powershell, or CMD.</p>"},{"location":"tutorial/setting-up-project-odk/#prerequisites","title":"Prerequisites","text":"<p>You have:</p> <ul> <li>A Github account</li> <li>Completed the \"Preparation\" steps above</li> </ul>"},{"location":"tutorial/setting-up-project-odk/#video","title":"Video","text":"<p>A recording of a demo of creating a ODK-repo is available here</p>"},{"location":"tutorial/setting-up-project-odk/#your-first-repository","title":"Your first repository","text":"<ol> <li>Create temporary directory to get started</li> </ol> <p>On your machine, create a new folder somewhere:</p> <pre><code>cd ~\nmkdir odk_tutorial\ncd odk_tutorial\n</code></pre> <ol> <li>Download the seed-my-repo wrapper script</li> </ol> <p>Now download the seed-my-repo wrapper script from the ODK GitHub repository. A detailed explanation of how to do that can be found here. For simplicity, we just use wget here to download the seed-my-repo file, but you can do it manually:</p> <pre><code>wget https://raw.githubusercontent.com/INCATools/ontology-development-kit/master/seed-via-docker.sh\n</code></pre> <ol> <li>Download a basic config to start from and start building your own</li> </ol> <p>The last ingredient we need is an ODK config file. While you can, in theory, create an empty repo entirely without a config file (one will be generated for you), we recommend to just start right with one. You can find many examples of configs here. For the sake of this tutorial, we will start with a simple config:</p> <pre><code>id: cato\ntitle: \"Cat Anatomy Ontology\"\ngithub_org: obophenotype\ngit_main_branch: main\nrepo: cat_anatomy_ontology\nrelease_artefacts:\n  - base\n  - full\n  - simple\nprimary_release: full\nexport_formats:\n  - owl\n  - obo\n  - json\nimport_group:\n  products:\n    - id: ro\n    - id: pato\n    - id: omo\nrobot_java_args: \"-Xmx8G\"\nrobot_report:\n  use_labels: TRUE\n  fail_on: ERROR\n  custom_profile: TRUE\n  report_on:\n    - edit\n</code></pre> <p>Safe this config file as in your temporary directory, e.g. <code>~/odk_tutorial/cato-odk.yaml</code>.</p> <p>Most of your work managing your ODK in the future will involve editing this file. There are dozens of cool options that do magical things in there. For now, lets focus on the most essential:</p>"},{"location":"tutorial/setting-up-project-odk/#general-config","title":"General config:","text":"<pre><code>id: cato\ntitle: \"Cat Anatomy Ontology\"\n</code></pre> <p>The id is essential, as it will determine how files will be named, which default term IDs to assume, and many more. It should be a lowercase string which is, by convention at least 4 characters long - 5 is not unheard of. The <code>title</code> field is used to generate various default values in the repository, like the README and others. There are other fields, like <code>description</code>, but let's start minimal for now. A full list of elements can be found in this schema:</p> <p>https://github.com/INCATools/ontology-development-kit/blob/master/schema/project-schema.json</p>"},{"location":"tutorial/setting-up-project-odk/#git-config","title":"Git config:","text":"<pre><code>github_org: obophenotype\ngit_main_branch: main\nrepo: cat_anatomy_ontology\n</code></pre> <p>The <code>github_org</code> (the GitHub or GitLab organisation) and the <code>repo</code> (repository name) will be used for some basic config of the git repo. Enter your own <code>github_org</code> here rather than <code>obophenotype</code>. Your default <code>github_org</code> is your GitHub username. If you are not creating a new repo, but working on a repo that predates renaming the GitHub main branch from <code>master</code> to <code>main</code>, you may want to set the <code>git_main_branch</code> as well.</p>"},{"location":"tutorial/setting-up-project-odk/#pipeline-configuration","title":"Pipeline configuration","text":"<pre><code>release_artefacts:\n  - base\n  - full\n  - simple\nprimary_release: full\nexport_formats:\n  - owl\n  - obo\n  - json\n</code></pre> <p>With this configuration, we tell the ODK that we wish to automatically generate the base, full and simple release files for our ontology. We also say that we want the <code>primary_release</code> to be the <code>full</code> release (which is also the default). The primary release will be materialised as <code>cato.owl</code>, and is what most users of your ontology will interact with. More information and what these are can be found here. We always want to create a <code>base</code>, i.e. the release variant that contains all the axioms that belong to the ontology, and none of the imported ones, but we do not want to make it the <code>primary_release</code>, because it will be unclassified and missing a lot of the important inferences.</p> <p>We also configure export products: we always want to export to <code>OWL</code> (<code>owl</code>), but we can also chose to export to <code>OBO</code> (<code>obo</code>) format and <code>OBOGraphs JSON</code> (<code>json</code>).</p>"},{"location":"tutorial/setting-up-project-odk/#imports-config","title":"Imports config:","text":"<pre><code>import_group:\n  products:\n    - id: ro\n    - id: pato\n    - id: omo\n</code></pre> <p>This is a central part of the ODK, and the section of the config file you will interact with the most. Please see here for details. What we are asking the ODK here, in essence, to set us up for dynamically importing from the Relation Ontology (RO), the Phenotype And Trait Ontology (PATO) and the OBO Metadata Ontology (OMO).</p>"},{"location":"tutorial/setting-up-project-odk/#memory-management","title":"Memory management:","text":"<pre><code>robot_java_args: '-Xmx8G'\n</code></pre> <p>Here we say that we allow ROBOT to consume up to 8GB of memory. Make sure that your docker is set up to permit at least ~20% more memory than that, i.e. 9GB or 10GB, otherwise, some cryptic Docker errors may come up.</p>"},{"location":"tutorial/setting-up-project-odk/#robot-report","title":"ROBOT Report:","text":"<pre><code>robot_report:\n  use_labels: TRUE\n  fail_on: ERROR\n  report_on:\n    - edit\n</code></pre> <ul> <li><code>use_labels</code>: allows switching labels on and off in the ROBOT report</li> <li><code>fail_on</code>: the report will fail if there is an ERROR-level violation</li> <li><code>report_on</code>: specify which files to run the report over.</li> </ul> <p>With this configuration, we tell ODK we want to run a report to check the quality of the ontology. Check here the complete list of report queries.</p>"},{"location":"tutorial/setting-up-project-odk/#generate-the-repo","title":"Generate the repo","text":"<p>Run the following:</p> <pre><code>cd ~/odk_tutorial\nsh seed-via-docker.sh -c -C cato-odk.yaml\n</code></pre> <p>This will create a basic layout of your repo under <code>target/cato/*</code></p> <p>Note: after this run, you wont need <code>cato-odk.yaml</code> anymore as it will have been added to your ontology repo, which we will see later.</p> <p>By default, the <code>seed-via-docker.sh</code> script attempts to obtain your Git username and email from your local Git configuration (typically stored in <code>~/.gitconfig</code>. If for some reason you want your repository to be initialised with a different username and/or email, you may explicitly pass them to the script as follows:</p> <pre><code>ODK_GITNAME=Alice ODK_GITEMAIL=alice@example.org sh seed-via-docker.sh ...\n</code></pre>"},{"location":"tutorial/setting-up-project-odk/#publish-on-github","title":"Publish on GitHub","text":"<p>You can now move the <code>target/cato</code> directory to a more suitable location. For the sake of this tutorial we will move it to the Home directory.</p> <pre><code>mv target/cato ~/\n</code></pre>"},{"location":"tutorial/setting-up-project-odk/#using-github-desktop","title":"Using GitHub Desktop","text":"<p>If you use GitHub Desktop, you can now simply add this repo by selecting <code>File -&gt; Add local repository</code> and select the directory you moved the repo to (as an aside, you should really have a nice workspace directory like <code>~/git</code> or <code>~/ws</code> or some such to organise your projects).</p> <p>Then click <code>Publish the repository</code> on</p>"},{"location":"tutorial/setting-up-project-odk/#using-the-command-line","title":"Using the Command Line","text":"<p>Follow the instructions you see on the Terminal (they are printed after your seed-my-repo run).</p>"},{"location":"tutorial/setting-up-project-odk/#finish","title":"Finish!","text":"<p>Congratulations, you have successfully jump-started your very own ODK repository and can start developing.</p>"},{"location":"tutorial/setting-up-project-odk/#next-steps","title":"Next steps:","text":"<ol> <li>Start editing <code>~/cato/src/ontology/cato-edit.owl</code> using Protege.</li> <li>Run a release</li> </ol>"},{"location":"tutorial/sparql-llm/","title":"Generating SPARQL queries using Large Language Models (LLMs)","text":"<p>Last Updated: 14-Nov-2025</p> <p>This tutorial will teach you how to create SPARQL queries by prompting a large language model (LLM) via a chat interface. You may use whichever system you prefer, such as ChatGPT, Gemini, Claude, etc. The prompts in this tutorial have been tested with ChatGPT 4o, ChatGPT 5.x, and Gemini 2.5 Pro. However, while LLM responses are more consistent, they are non-deterministic (involve some randomness), so the same prompt may produce different answers.</p>"},{"location":"tutorial/sparql-llm/#contributors","title":"Contributors","text":"<ul> <li>Trish Whetzel</li> </ul>"},{"location":"tutorial/sparql-llm/#preparation","title":"Preparation","text":"<p>The resulting SPARQL queries can be tested using https://yasgui.triply.cc/# (endpoint <code>https://ubergraph.apps.renci.org/sparql</code>) or using the ROBOT query functionality. You are responsible for obtaining access to yasgui and/or installing ROBOT.</p> <p>The tutorial content can also be tested with SPARQLPrompt, a Chrome Extension for use with the yasgui site. An OpenAI API Key is needed for this tool.</p>"},{"location":"tutorial/sparql-llm/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Explain what SPARQL is and describe the types queries, e.g. SELECT, INSERT, DELETE, that can be used with ontologies.</li> <li>Identify and provide key prompt setup information to guide LLMs toward generating accurate SPARQL queries in chat interfaces.</li> <li>Create effective natural language prompts for large language models (via chat interfaces) to generate valid SPARQL queries.</li> <li>Design advanced prompts that incorporate ontology-specific features, such as axiom reification, source provenance, and IRI or synonym constraints, to guide LLMs in producing accurate and semantically rich queries.</li> <li>Interpret and debug the SPARQL queries produced by LLMs, recognizing things like CURIEs vs IRIs, missing filters, or extraneous results.</li> <li>Run queries on Ubergraph or similar endpoints, considering things like performance, excessive results, and correctness.</li> <li>Evaluate prompt engineering best practices to get consistent query outputs.</li> </ul>"},{"location":"tutorial/sparql-llm/#obo-academy-training-session","title":"OBO Academy Training Session","text":""},{"location":"tutorial/sparql-llm/#sparql-refresher","title":"SPARQL Refresher","text":""},{"location":"tutorial/sparql-llm/#what-is-sparql","title":"What is SPARQL?","text":"<ul> <li>SPARQL (pronounced 'sparkle') is a language to query ontologies</li> <li>A query can consist of:<ul> <li>Triple patterns (subject-predicate-object), e.g. an entity is an OWL class    <pre><code>?entity a owl:Class .\n</code></pre></li> <li>Conjunctions (multiple triple patterns that must match in order to return a result), e.g. Find all entities that are subclasses of MONDO:0000001 'disease' AND have a label. Both conditions must be true for the same entity.   <pre><code>?entity rdfs:subClassOf* MONDO:0000001 .\n?entity rdfs:label ?label .\n</code></pre></li> <li>Disjunctions (match either one triple pattern OR another), e.g. Find the label whether it's defined with either <code>rdfs:label</code> or <code>skos:prefLabel</code> <pre><code>{ ?entity rdfs:label ?label . }\nUNION\n{ ? entity skos:prefLabel ?label . }\n</code></pre></li> <li>Optional patterns (include extra information when available), e.g. Find all subclasses of MONDO:0000001 'disease'. If a definition exists, include it, but don\u2019t exclude entities without a definition.   <pre><code>?class rdfs:subClassOf* obo:MONDO_0000001 .\nOPTIONAL { ?class IAO:0000115 ?definition }\n</code></pre></li> </ul> </li> <li>SPARQL can be used to query class hierarchies, annotation properties, e.g. labels, definitions, database cross reference, and logical class definitions</li> </ul>"},{"location":"tutorial/sparql-llm/#sparql-query-operations","title":"SPARQL Query Operations","text":"<ul> <li>SPARQL operators<ul> <li>SELECT - retrieve specific data from an ontology, e.g. terms, labels, database cross references, hierarchy relationships, etc.</li> <li>INSERT - add information into the ontology</li> <li>DELETE - remove information from the ontology</li> </ul> </li> <li>SPARQL modifiers<ul> <li>ORDER BY - sorts results</li> <li>LIMIT - max rows</li> <li>OFFSET - skip rows</li> <li>DISTINCT - remove duplicate query results</li> <li>GROUP BY and HAVING - aggregate data</li> </ul> </li> <li>FILTER - restrict results based on expressions<ul> <li>Examples: Filter out obsolete terms, Filter to keep only terms with MONDO identifier</li> </ul> </li> </ul>"},{"location":"tutorial/sparql-llm/#sparql-query-structure","title":"SPARQL Query Structure","text":"<ul> <li> <p>Prefix declarations: Declares namespace abbreviations to shorten URIs (e.g., <code>owl:</code>, <code>rdfs:</code>)</p> </li> <li> <p>Select clause: Specifies the variables to be returned: <code>?entity</code> and <code>?label</code></p> </li> <li> <p>Where clause: Defines the pattern of RDF triples to match.</p> <ul> <li>The triple pattern <code>?entity a owl:Class</code> means match entities that are OWL classes.</li> <li>The triple pattern <code>?entity rdfs:label ?label</code> means match entities that have a <code>rdfs:label</code>.</li> </ul> </li> <li> <p>Filters:</p> <ul> <li>Limits results to MONDO classes:  <code>FILTER (STRSTARTS(STR(?entity), \"http://purl.obolibrary.org/obo/MONDO_\"))</code></li> <li>Excludes obsolete classes:  <code>FILTER NOT EXISTS { ?entity owl:deprecated true }</code></li> </ul> </li> <li> <p>Limit:</p> <ul> <li>Return at most 10 results: <code>LIMIT 10</code> </li> </ul> </li> </ul>"},{"location":"tutorial/sparql-llm/#detailed-sparql-information","title":"Detailed SPARQL information","text":"<ul> <li>For more details, review Basic SPARQL for OBO Engineers https://oboacademy.github.io/obook/tutorial/sparql/</li> </ul>"},{"location":"tutorial/sparql-llm/#tools-to-run-sparql-queries","title":"Tools to run SPARQL queries","text":"<ul> <li>ROBOT <ul> <li>A command line tool for working with OBO ontologies and has a query command https://robot.obolibrary.org/query</li> </ul> </li> <li>RENCI Ubergraph endpoint<ul> <li>Web based interface to query OBO ontologies https://yasgui.triply.cc/#</li> </ul> </li> </ul>"},{"location":"tutorial/sparql-llm/#sparql-use-cases","title":"SPARQL Use Cases","text":"<ul> <li>Create a custom report<ul> <li>How many disease classes are in Mondo? </li> <li>How many Mondo classes have a gene association?</li> <li>See Mondo stats for more statistics https://mondo.monarchinitiative.org/#stats</li> </ul> </li> <li>Ontology QC and modeling validation<ul> <li>Do all synonyms contain a database cross reference?</li> </ul> </li> <li>Insert and update an ontology<ul> <li>Many Mondo pipelines use SPARQL update queries, e.g. OMIM gene pipeline, to add content into the ontology</li> </ul> </li> </ul>"},{"location":"tutorial/sparql-llm/#what-are-large-language-models","title":"What are Large Language Models","text":"<ul> <li>Large Language Models (LLMs) are AI systems trained on huge amounts of text</li> <li>Understand and generate text and code<ul> <li>Examples: ChatGPT, Claude, Gemini</li> </ul> </li> <li>Interact using natural language prompts, not programming</li> <li>\u26a0\ufe0f LLMs can make mistakes or \u201challucinate\u201d information \u2014 always review and validate the results before using them in your work</li> </ul>"},{"location":"tutorial/sparql-llm/#why-large-language-models-llms-matter-for-querying-ontologies","title":"Why Large Language Models (LLMs) matter for querying ontologies","text":"Challenges for users: <ul> <li>Natural language is intuitive for humans, but not machine-readable</li> <li>SPARQL queries are precise, but hard to learn and time-consuming to write</li> </ul> How LLMs help: <ul> <li>Translate natural language questions into SPARQL queries (bridging the gap)</li> <li>Lower the barrier to writing SPARQL by drafting queries, saving time for learners and experts</li> </ul>"},{"location":"tutorial/sparql-llm/#prompting-llms-for-sparql-queries-via-chat-interfaces","title":"Prompting LLMs for SPARQL Queries via Chat Interfaces","text":"<ul> <li>Be specific and state the question cleary</li> <li>State what properties to return </li> <li>State modifiers and constraints</li> <li>Share an example query to extend</li> <li>Ask for explanations </li> <li>Review query, test, and iterate</li> </ul>"},{"location":"tutorial/sparql-llm/#prompt-setup-for-chat-based-llms","title":"Prompt Setup for Chat-Based LLMs","text":"<ul> <li>\u203c\ufe0f This setup is very important \u2014 it helps ensure the LLM responds correctly and consistently</li> <li>Text information to include at the start of your chat session to guide the LLM throughout your prompts </li> <li>This information will be remembered by the model throughout the chat session (limited by model, chat length, and chat settings)<ul> <li>ChatGPT - session memory is enabled by default</li> <li>Gemini -  iterative conversation history</li> <li>Claude - selective session memory</li> </ul> </li> <li>The information should be clear and specific to guide the LLM toward the desired output</li> <li>For best results, it is a good practice to set a persona, provide examples, and structure the information clearly</li> <li>\u26a1 Use the information in the Prompt Setup below before prompting the LLM to generate SPARQL queries. Customize as needed for your ontology of interest. </li> </ul> Prompt Setup <pre><code>Role: Act as an ontology engineer with expert knowledge of SPARQL and MONDO.\n\nEnvironment: Queries will be run in YASGUI against MONDO (OWL) and should also run via ROBOT.\n\nNamespace scope: By default, restrict results to MONDO classes:\n    FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n\nReasoning: Assume no entailment; use explicit patterns and property paths (e.g., rdfs:subClassOf*).\n\nObsoletes: Exclude classes with owl:deprecated true.\n\nLabels: Use `rdfs:label`.\n\nDefinition: Use `IAO:0000115`.\n\nPrefixes: Include only the PREFIX declarations actually used in the query (no extras).\n  These are the main prefixes we will need: \n    Core RDF/OWL\n      rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n      rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n      owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n      xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\n    MONDO\n      MONDO: &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\n    Common OBO namespaces\n      RO: &lt;http://purl.obolibrary.org/obo/RO_&gt;\n      IAO: &lt;http://purl.obolibrary.org/obo/IAO_&gt;\n      oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nAxiom-annotated data (synonyms, database cross references (also known as xrefs), provenance):\n  When querying properties that are commonly axiom-annotated (e.g., oboInOwl:hasExactSynonym),\n    1) Assert the base triple:\n       ?class oboInOwl:hasExactSynonym ?syn .\n    2) Tie the reified axiom back to that exact triple:\n       ?axiom a owl:Axiom ;\n              owl:annotatedSource ?class ;\n              owl:annotatedProperty oboInOwl:hasExactSynonym ;\n              owl:annotatedTarget ?syn .\n    3) Add any desired axiom annotations (e.g., oboInOwl:hasDbXref ?xref).\n\nGene associations in MONDO (e.g., RO:0004003 for germline mutations) are modeled as OWL existential restrictions:\n- Use `rdfs:subClassOf` pointing to a blank node that is a restriction (`a owl:Restriction`)\n- That restriction must contain:\n    - `owl:onProperty RO:0004003`\n    - `owl:someValuesFrom` the gene class (e.g., HGNC ID)\n- Provenance is annotated on the `rdfs:subClassOf` triple using `owl:Axiom` with:\n    - `owl:annotatedSource` = MONDO class\n    - `owl:annotatedProperty` = `rdfs:subClassOf`\n    - `owl:annotatedTarget` = restriction\n    - `oboInOwl:source` (or other provenance properties) for evidence\n\nMode switching:\n  * If the latest user message or system prompt contains the token `EXTENSION_MODE`, treat this as **extension mode**.\n\nOutput rules:\n  * In **extension mode** (for the Chrome extension):\n    - Return paste-ready SPARQL as plain text, with **no triple backticks** and no fenced code blocks.\n    - The query must contain only SPARQL, no natural-language text mixed in.\n    - If an explanation is explicitly requested, place it **after** the SPARQL query.\n    - In extension mode, every explanation line must begin with `# `.\n  * When **not** in extension mode (normal ChatGPT UI use):\n    - Return paste-ready SPARQL in a single fenced code block using triple backticks with language `sparql`.\n    - The code block must contain **only** the SPARQL query \u2014 no comments and no explanation inside it.\n    - Do NOT provide any explanation unless the user explicitly says \u201cinclude an explanation\u201d.\n    - If an explanation is explicitly requested, place it **after** the code block as plain text (and do **not** start explanation lines with `# `).\n  * Never include extra backticks outside a code block in non-extension mode.\n  * Use DISTINCT when appropriate (e.g., in COUNTs).\n  * If a list of results is requested, include the `?label` and convert the IRI to a CURIE.\n\nRequest format: \n  * I will provide prompts in plain English.\n\nDefaults (unless I override in the prompt): \n  * Consider all descendants (rdfs:subClassOf*), not just direct children.\n  * Filter out obsoletes as above.\n  * Keep results sorted using ORDER BY unless I request otherwise.\n</code></pre>"},{"location":"tutorial/sparql-llm/#llm-prompts","title":"LLM Prompts","text":""},{"location":"tutorial/sparql-llm/#counts-and-summaries","title":"Counts and Summaries","text":""},{"location":"tutorial/sparql-llm/#example-count-all-subclasses-of-disease-in-mondo","title":"Example - Count all subclasses of disease in Mondo","text":"<ul> <li> <p>Prompt: Write a SPARQL query that counts the number of classes in the MONDO ontology that are subclasses of MONDO:0000001 (disease).</p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query: COUNT</li> <li>Target class: all subclasses of MONDO:0000001 (disease)</li> <li>Ontology scope: MONDO</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\n\n# Get a count of all subclasses of disease excluding obsolete terms\n\nSELECT (COUNT(DISTINCT ?cls) AS ?count)\nWHERE {\n  ?cls rdfs:subClassOf* obo:MONDO_0000001 .\n  ?cls a owl:Class .\n}\n</code></pre>"},{"location":"tutorial/sparql-llm/#filtering-basics","title":"Filtering Basics","text":""},{"location":"tutorial/sparql-llm/#example-count-all-exact-synonyms-in-mondo-excluding-obsolete-classes","title":"Example - Count all exact synonyms in Mondo, excluding obsolete classes","text":"<ul> <li> <p>Prompt: Write a SPARQL query to count all exact synonyms (oboInOwl:hasExactSynonym) in MONDO, excluding obsolete Mondo classes.</p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query: COUNT</li> <li>Target class: all Mondo classes</li> <li>Filter: exclude obsolete terms</li> <li>Ontology scope: MONDO</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT (COUNT(DISTINCT ?syn) AS ?count_exact_synonyms)\nWHERE {\n  ?class a owl:Class ;\n         oboInOwl:hasExactSynonym ?syn .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n}\n</code></pre>"},{"location":"tutorial/sparql-llm/#labels-annotations","title":"Labels &amp; Annotations","text":""},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-curie-and-label-excluding-obsolete-terms","title":"Example - Get Mondo terms, CURIE and label, excluding obsolete terms","text":"<ul> <li> <p>Prompt: Write a SPARQL query to retrieve a MONDO classes with their human-readable labels (rdfs:label). Return the MONDO CURIE and the label, limited to 20 results. Exclude obsolete classes (owl:deprecated true).</p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query: SELECT - MONDO CURIE and label</li> <li>Target class: all Mondo classes</li> <li>Filter: exclude obsolete terms</li> <li>Ontology scope: MONDO</li> <li>Result size: LIMIT 20 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:  &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:   &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO: &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\n\nSELECT DISTINCT ?mondo_curie ?label\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nORDER BY ?mondo_curie\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#synonyms-cross-references","title":"Synonyms &amp; Cross-references","text":""},{"location":"tutorial/sparql-llm/#example-get-a-count-of-all-mondo-classes-with-an-exact-synonym-from-orphanet","title":"Example - Get a count of all MONDO classes with an exact synonym from Orphanet","text":"<p>Prompt:  Write a SPARQL query to count all exact synonyms (oboInOwl:hasExactSynonym) in non-obsolete MONDO classes where the synonym is annotated with a database cross reference (oboInOwl:hasDbXref) containing \"Orphanet\". Use the axiom annotation pattern to connect the synonym to its provenance.</p> <ul> <li>Prompt Breakdown:<ul> <li>Type of query: COUNT</li> <li>Target class: all non-obsolete Mondo classes</li> <li>Synonym constraint: class must have oboInOwl:hasExactSynonym</li> <li>Axiom constraint: the synonym must be annotated with a database cross reference and the value must contain \"Orphanet\"</li> <li>Filter: exclude obsolete classes</li> <li>Ontology scope: MONDO </li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT (COUNT(DISTINCT ?syn) AS ?count_orphanet_exact_synonyms)\nWHERE {\n  ?class a owl:Class .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  ?class oboInOwl:hasExactSynonym ?syn .\n\n  ?axiom a owl:Axiom ;\n         owl:annotatedSource ?class ;\n         owl:annotatedProperty oboInOwl:hasExactSynonym ;\n         owl:annotatedTarget ?syn ;\n         oboInOwl:hasDbXref ?xref .\n\n  FILTER CONTAINS(STR(?xref), \"Orphanet:\")\n}\n</code></pre>"},{"location":"tutorial/sparql-llm/#conjunctions-and","title":"Conjunctions (AND)","text":""},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-curie-label-synonym-excluding-obsolete-terms","title":"Example: Get Mondo terms, CURIE, label, synonym, excluding obsolete terms","text":"<p>Prompt:  Write a SPARQL query to select MONDO classes that have both a human-readable label (rdfs:label) and an exact synonym (oboInOwl:hasExactSynonym). Return the MONDO CURIE, label, and synonym. Restrict to MONDO classes, exclude obsolete terms, and limit to 20 results.</p> <ul> <li>Prompt Breakdown:<ul> <li>Type of query: SELECT - MONDO CURIE, label, and synonym</li> <li>Target class: all Mondo classes</li> <li>Annotation properties: require both <code>rdfs:label</code> AND <code>oboInOwl:hasExactSynonym</code></li> <li>Filter: exclude obsolete terms</li> <li>Ontology scope: MONDO</li> <li>Result size: LIMIT 20 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT DISTINCT ?mondo_curie ?label ?syn\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         oboInOwl:hasExactSynonym ?syn .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nORDER BY ?mondo_curie\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#disjunctions-or","title":"Disjunctions (OR)","text":"<p>Use UNION for alternative patterns</p>"},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-curie-label-synonym-and-synonym-type-for-terms-with-either-an-exact-synonym-or-a-related-synonym","title":"Example - Get Mondo terms, CURIE, label, synonym, and synonym type for terms with either an exact synonym or a related synonym","text":"<p>Prompt:  Write a SPARQL query to retrieve non-obsolete MONDO classes that have either an exact synonym (oboInOwl:hasExactSynonym) or a related synonym (oboInOwl:hasRelatedSynonym). Use UNION to combine the two patterns. Return the MONDO CURIE, label, the synonym and synonym type. Limit to 50 results.</p> <ul> <li>Prompt Breakdown<ul> <li>Type of query: SELECT - MONDO CURIE, label, and synonym</li> <li>Target class: all Mondo classes</li> <li>Annotation properties: match classes with an exact synonym OR related synonym</li> <li>Disjunction: use UNION to combine patterns</li> <li>Filter: exclude obsolete classes</li> <li>Ontology scope: MONDO</li> <li>Result size: LIMIT 50 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT DISTINCT ?mondo_curie ?label ?synonym ?synonym_type\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  {\n    # Exact synonyms\n    ?class oboInOwl:hasExactSynonym ?synonym .\n    BIND(\"EXACT\" AS ?synonym_type)\n  }\n  UNION\n  {\n    # Related synonyms\n    ?class oboInOwl:hasRelatedSynonym ?synonym .\n    BIND(\"RELATED\" AS ?synonym_type)\n  }\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nORDER BY ?mondo_curie ?synonym_type\nLIMIT 50\n</code></pre>"},{"location":"tutorial/sparql-llm/#optional-patterns","title":"Optional Patterns","text":""},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-and-the-definition-if-it-exists","title":"Example: Get Mondo terms and the definition if it exists","text":"<p>Prompt: Write a SPARQL query to retrieve non-obsolete MONDO classes with their human-readable label (rdfs:label). Also try to include a textual definition (IAO:0000115) if it exists. Use OPTIONAL so that classes without a definition are still returned. Return the MONDO CURIE, label, and definition. Limit to 20 results.</p> <ul> <li>Prompt Breakdown:<ul> <li>Type of query: SELECT \u2014 return MONDO CURIE, label, and optionally the definition</li> <li>Target class: all Mondo classes</li> <li>Annotations required: rdfs:label (mandatory)</li> <li>Annotations optional: IAO:0000115 (definition, may or may not be present)</li> <li>Filter: exclude obsolete classes</li> <li>Ontology scope: MONDO</li> <li>Output: MONDO CURIE, label, and definition</li> <li>Result size: LIMIT 20 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:  &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:   &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO: &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX IAO:   &lt;http://purl.obolibrary.org/obo/IAO_&gt;\n\nSELECT DISTINCT ?mondo_curie ?label ?definition\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  # Definition is optional \u2014 not all classes have one\n  OPTIONAL { ?class IAO:0000115 ?definition }\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nORDER BY ?mondo_curie\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#grouping-and-aggregation","title":"Grouping and Aggregation","text":"<p>Use GROUP_CONCAT, GROUP BY</p>"},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-and-the-count-of-exact-synonyms-for-each-term","title":"Example: Get Mondo terms and the count of exact synonyms for each term","text":"<p>Prompt: Write a SPARQL query to retrieve non-obsolete MONDO classes along with their exact synonyms. Use COUNT to calculate how many synonyms each class has, and GROUP_CONCAT to list them all in a single field. Return the MONDO CURIE, label, the count of synonyms, and the list of synonyms. Limit results to 20.</p> <ul> <li>Prompt Breakdown:<ul> <li>Type of query: SELECT with aggregation \u2014 return MONDO CURIE, label, count of synonyms, and list of the exact synonyms</li> <li>Target class: all Mondo classes</li> <li>Annotations required: rdfs:label (mandatory), oboInOwl:hasExactSynonym (to be counted)</li> <li>Filter: exclude obsolete classes</li> <li>Ontology scope: MONDO</li> <li>Aggregation: aggregate all exact synonym values for each Mondo class</li> <li>Result size: LIMIT 20 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT ?mondo_curie ?label \n       (COUNT(DISTINCT ?syn) AS ?synonym_count)\n       (GROUP_CONCAT(DISTINCT ?syn; separator=\", \") AS ?synonyms)\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         oboInOwl:hasExactSynonym ?syn .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nGROUP BY ?mondo_curie ?label\nORDER BY DESC(?synonym_count)\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#axiom-level-queries","title":"Axiom-level Queries","text":"<p>Reified axioms with owl:Axiom</p>"},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-exact-synonyms-and-their-provenance","title":"Example: Get Mondo terms, exact synonyms, and their provenance","text":"<p>Prompt: Write a SPARQL query to retrieve non-obsolete MONDO classes and their exact synonyms (oboInOwl:hasExactSynonym). For each synonym, also retrieve the list of database cross references (oboInOwl:hasDbXref) recorded on the synonym axiom. Use the axiom annotation pattern (owl:Axiom) to connect synonyms to their xrefs. Each synonym should appear only once, with all of its xrefs combined in a single column. Return the MONDO CURIE, label, synonym, and the list of xrefs. Limit results to 20.</p> <ul> <li>Prompt Breakdown:<ul> <li>Type of query: SELECT \u2014 return MONDO CURIE, label, synonym, and a combined list of xrefs</li> <li>Target class: all Mondo classes</li> <li>Annotation required: rdfs:label (mandatory for human-readable output)</li> <li>Synonyms: oboInOwl:hasExactSynonym (required)</li> <li>Axiom pattern: use owl:Axiom with owl:annotatedSource, owl:annotatedProperty, and owl:annotatedTarget to connect synonyms to oboInOwl:hasDbXref</li> <li>Aggregation: aggregate all xrefs for the synonym</li> <li>Filter: exclude obsolete classes</li> <li>Ontology scope: MONDO</li> <li>Result size: LIMIT 20 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT ?mondo_curie ?label ?synonym\n       (GROUP_CONCAT(DISTINCT ?xref; separator=\", \") AS ?xrefs)\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         oboInOwl:hasExactSynonym ?synonym .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  # Axiom pattern tying synonym to xref\n  ?axiom a owl:Axiom ;\n         owl:annotatedSource ?class ;\n         owl:annotatedProperty oboInOwl:hasExactSynonym ;\n         owl:annotatedTarget ?synonym ;\n         oboInOwl:hasDbXref ?xref .\n\n  # Convert class IRI into MONDO CURIE\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nGROUP BY ?mondo_curie ?label ?synonym\nORDER BY ?mondo_curie ?synonym\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#ontology-structure-queries","title":"Ontology Structure Queries","text":""},{"location":"tutorial/sparql-llm/#example-get-mondo-terms-that-have-a-gene-association","title":"Example: Get Mondo terms that have a gene association","text":"<p>Prompt: Write a SPARQL query to retrieve non-obsolete MONDO classes that have a gene association modeled as an rdfs:subClassOf restriction on the property RO:0004003 (\u201chas material basis in germline mutation in\u201d). Return the MONDO CURIE, label, and the gene identifier. Limit results to 20.</p> <ul> <li>Prompt Breakdown:<ul> <li>Type of query: SELECT \u2014 return MONDO CURIE, label, and gene identifier</li> <li>Target class: all Mondo classes</li> <li>Ontology structure: rdfs:subClassOf pointing to an owl:Restriction</li> <li>Restriction pattern:     <pre><code>owl:onProperty RO:0004003\nowl:someValuesFrom ?gene\n</code></pre></li> <li>Filter: exclude obsolete classes</li> <li>Ontology scope: MONDO</li> <li>Result size: LIMIT 20 for readability</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:  &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:   &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO: &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX RO:    &lt;http://purl.obolibrary.org/obo/RO_&gt;\n\nSELECT DISTINCT ?mondo_curie ?label ?gene\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         rdfs:subClassOf ?restriction .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  ?restriction a owl:Restriction ;\n               owl:onProperty RO:0004003 ;\n               owl:someValuesFrom ?gene .\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nORDER BY ?mondo_curie\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#prompting-best-practices","title":"Prompting Best Practices","text":"<ul> <li>Provide context for the prompts (see Prompt Setup)</li> <li>Be specific and state the question cleary<ul> <li>Include examples in the prompt</li> <li>Include prefixes and term IRIs/CURIEs in the prompt</li> <li>Use the real name of a property, 'has material basis in germline mutation in' vs. gene association</li> <li>Provide an OBO stanza or for more complicated queries the OWL class representation as needed</li> </ul> </li> <li>State what information to return <ul> <li>Select the CURIE, label, and definition</li> </ul> </li> <li>State modifiers and constraints<ul> <li>Limit to 10 results, sort by label</li> <li>Filter out obsolete terms</li> </ul> </li> <li>Share an example query to extend<ul> <li>Use a base query and ask LLM to extend the query</li> </ul> </li> <li>Ask for explanations of the query<ul> <li>Prompt for the query and also ask for a step-wise explanation of the query</li> </ul> </li> <li>Review query, test, and iterate<ul> <li>Test the query in your tool of interest</li> <li>If the query fails or returns incorrect information, share the error message and ask for a fix or clarify what\u2019s missing</li> <li>Some SPARQL constructs are not valid for ROBOT and the query needs to be modified</li> <li>If the LLM starts returning circular options ask it to reset to clear the current conversation context and then start again (remember to provide the Prompt Setup information again)</li> </ul> </li> </ul>"},{"location":"tutorial/sparql-llm/#pitfalls-and-limitations","title":"Pitfalls and Limitations","text":"<ul> <li>LLM hallucinations<ul> <li>queries might look plausible but be wrong or inefficient or not work with certain tools</li> </ul> </li> <li>Validate the query<ul> <li>Test the query using the tools mentioned earlier</li> </ul> </li> <li>Provide feedback to the LLM<ul> <li>That did not work, e.g. try again using the correct prefix for MONDO</li> </ul> </li> <li>Schema/ontology drift/new ontology<ul> <li>LLMs trained on old data may not match the current ontology or know of a new ontology   </li> </ul> </li> </ul>"},{"location":"tutorial/sparql-llm/#additional-llm-prompts-and-sparql-queries","title":"Additional LLM prompts and SPARQL Queries","text":""},{"location":"tutorial/sparql-llm/#example-get-all-mondo-classes-that-have-a-gene-association-and-the-provenance-for-the-gene-association","title":"Example - Get all Mondo classes that have a gene association and the provenance for the gene association","text":"<ul> <li> <p>Prompt: Write a SPARQL query to get all Mondo classes that have a gene association, e.g. RO:0004003 'has material basis in germline mutation in', and also return the source provenance for the gene association. Include the Mondo CURIE, Mondo label, gene identifier, and source provenance in the result. For testing, limit to 20 results.</p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query - select MONDO CURIE, label, gene identifier, source provenance</li> <li>Target class - any Mondo class that has a gene association</li> <li>Axiom constraint - the class must have a gene association (RO:0004003 'has material basis in germline mutation in')</li> <li>Ontology - MONDO</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX RO:       &lt;http://purl.obolibrary.org/obo/RO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT DISTINCT ?mondo_curie ?label ?gene_id ?provenance\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         rdfs:subClassOf ?restriction .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  # Restriction for RO:0004003 (has material basis in germline mutation in)\n  ?restriction a owl:Restriction ;\n               owl:onProperty RO:0004003 ;\n               owl:someValuesFrom ?gene .\n\n  # MONDO CURIE\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n\n  # Gene CURIE (assumes identifiers.org/hgnc/#### pattern)\n  BIND(REPLACE(STR(?gene), \"^.*/\", \"HGNC:\") AS ?gene_id)\n\n  # Provenance from the annotated subClassOf axiom\n  OPTIONAL {\n    ?axiom a owl:Axiom ;\n           owl:annotatedSource ?class ;\n           owl:annotatedProperty rdfs:subClassOf ;\n           owl:annotatedTarget ?restriction ;\n           oboInOwl:source ?provenance .\n  }\n}\nORDER BY ?mondo_curie ?gene_id\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#example-get-all-mondo-classes-that-have-a-gene-association-and-the-provenance-for-the-gene-association-aggregate-all-gene-association-sources","title":"Example - Get all Mondo classes that have a gene association and the provenance for the gene association, aggregate all gene association sources","text":"<ul> <li> <p>Prompt: Given the query above (Get all Mondo classes that have a gene association and the provenance for the gene association), collapse the multiple rows due to multiple sources for a gene association. The gene association provenance is represented using the oboInOwl:source annotation property.</p> </li> <li> <p>Prompt Breakdown: Here we are extending the query from a previous query prompt or you may paste in the working query from above and ask the LLM to modify the query. If you are using a tool like ChatGPT or Gemini it will have memory of the earlier query.</p> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX RO:       &lt;http://purl.obolibrary.org/obo/RO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT ?mondo_curie ?label ?gene_id\n       (GROUP_CONCAT(DISTINCT ?provenance; SEPARATOR=\" | \") AS ?sources)\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         rdfs:subClassOf ?restriction .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  # Restriction for RO:0004003 (has material basis in germline mutation in)\n  ?restriction a owl:Restriction ;\n               owl:onProperty RO:0004003 ;\n               owl:someValuesFrom ?gene .\n\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n\n  # Gene CURIE (assumes identifiers.org/hgnc/#### pattern)\n  BIND(REPLACE(STR(?gene), \"^.*/\", \"HGNC:\") AS ?gene_id)\n\n  # Provenance annotations on the restriction axiom\n  OPTIONAL {\n    ?axiom a owl:Axiom ;\n           owl:annotatedSource ?class ;\n           owl:annotatedProperty rdfs:subClassOf ;\n           owl:annotatedTarget ?restriction ;\n           oboInOwl:source ?provenance .\n  }\n}\nGROUP BY ?mondo_curie ?label ?gene_id\nORDER BY ?mondo_curie ?gene_id\nLIMIT 20\n</code></pre>"},{"location":"tutorial/sparql-llm/#example-get-all-mondo-classes-that-have-more-than-one-gene-association","title":"Example - Get all Mondo classes that have more than one gene association","text":"<ul> <li> <p>Prompt: Write a SPARQL query to get all Mondo classes that have more than one gene association, e.g. RO:0004003 'has material basis in germline mutation in', and the source provenance represented using oboInOWL:source. Include the Mondo CURIE, Mondo label, gene identifier, and source provenance in the result.</p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query - select MONDO CURIE, label, gene identifier, OMIM CURIE with HAVING</li> <li>Target class - any Mondo class that has more than one gene association</li> <li>Axiom constraint - the class must have a gene association (RO:0004003 'has material basis in germline mutation in')</li> <li>Filter - Must use RO:0004003 and have more than one gene association</li> <li>Ontology - MONDO</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT ?mondo_curie ?label ?gene (GROUP_CONCAT(DISTINCT ?omim_source; separator=\"|\") AS ?sources)\nWHERE {\n  {\n    SELECT ?mondo_class\n    WHERE {\n      ?mondo_class rdfs:subClassOf ?restriction .\n      ?restriction a owl:Restriction ;\n                   owl:onProperty obo:RO_0004003 ;\n                   owl:someValuesFrom ?gene .\n    }\n    GROUP BY ?mondo_class\n    HAVING (COUNT(DISTINCT ?gene) &gt; 1)\n  }\n\n  ?mondo_class rdfs:label ?label ;\n               rdfs:subClassOf ?restriction .\n\n  ?restriction a owl:Restriction ;\n               owl:onProperty obo:RO_0004003 ;\n               owl:someValuesFrom ?gene .\n\n  OPTIONAL {\n    ?axiom a owl:Axiom ;\n           owl:annotatedSource ?mondo_class ;\n           owl:annotatedProperty rdfs:subClassOf ;\n           owl:annotatedTarget ?restriction ;\n           oboInOwl:source ?omim_source .\n  }\n\n  BIND(REPLACE(STR(?mondo_class), \"http://purl.obolibrary.org/obo/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nGROUP BY ?mondo_curie ?label ?gene\nORDER BY ?mondo_curie ?gene\n</code></pre>"},{"location":"tutorial/sparql-llm/#example-confirm-that-obsolete-terms-have-a-label-that-starts-with-obsolete-do-not-have-any-subclassof-relationships","title":"Example - Confirm that obsolete terms have a label that starts with \u201cobsolete\u201d do not have any subClassOf relationships","text":"<ul> <li> <p>Prompt: Write a SPARQL query that checks for two quality control rules about obsolete Mondo classes: All classes marked with owl:deprecated true must have an rdfs:label that starts with the string \"obsolete \". Obsolete classes must not have any logical axioms, such as rdfs:subClassOf. For each violation, the query should return the class IRI, its label, and a description of which rule was violated. </p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query - select</li> <li>Target class - any owl:Class</li> <li>Filter - include only deprecated classes</li> <li>Rules - Class label must start with \u201cobsolete\u201d and logical axioms can not be on an obsolete class</li> <li>Ontology - MONDO</li> </ul> </li> </ul> <p>\u203c\ufe0f NOTE: This query times out on yasgui so let's break this down into two queries, one to find any obsolete class that does not have a label that starts with 'obsolete ' and and another query to find obsolete classes with logical axioms.</p> View SPARQL query - Check that obsolete classes have a label that starts with 'obsolete ' <pre><code>PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\n# Find obsolete classes where the label does not start with 'obsolete '\n\nSELECT ?cls ?label ?rule\nWHERE {\n  ?cls a owl:Class ;\n         owl:deprecated true ;\n         rdfs:label ?label .\n  FILTER STRSTARTS(STR(?cls), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER (!STRSTARTS(LCASE(STR(?label)), \"obsolete \"))\n  BIND(\"Label must start with 'obsolete '\" AS ?rule)\n}\nORDER BY ?cls\n</code></pre> <p></p> View SPARQL query - Check that obsolete classes do not have logical axioms <pre><code>PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\n# Check if any obsolete classes have a subClassOf axiom\n\nSELECT ?cls ?clsLabel ?parent WHERE {\n  ?cls a owl:Class ;\n       owl:deprecated true ;\n       rdfs:label ?clsLabel ;\n       rdfs:subClassOf ?parent .\n\n  FILTER STRSTARTS(STR(?cls), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER (?parent != owl:Thing)\n  FILTER (?parent != ?cls)\n}\nORDER BY ?cls\n</code></pre>"},{"location":"tutorial/sparql-llm/#example-find-all-classes-where-the-definition-does-not-have-any-provenance","title":"Example - Find all classes where the definition does not have any provenance","text":"<ul> <li> <p>Prompt: Write a SPARQL query to retrieve all MONDO classes with a textual definition (IAO:0000115). The query should only return results where it is not possible to find a corresponding database cross reference that provides provenance for that specific definition. Return the class CURIE, class label, and the definition.</p> </li> <li> <p>Prompt Breakdown:</p> <ul> <li>Type of query - select CURIE, label, definition</li> <li>Target class - any owl:Class</li> <li>Filter - include only classes where the definition does not have a database cross reference</li> <li>Ontology - MONDO</li> </ul> </li> </ul> View SPARQL query <pre><code>PREFIX rdfs:     &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl:      &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX MONDO:    &lt;http://purl.obolibrary.org/obo/MONDO_&gt;\nPREFIX IAO:      &lt;http://purl.obolibrary.org/obo/IAO_&gt;\nPREFIX oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\n\nSELECT DISTINCT ?mondo_curie ?label ?definition\nWHERE {\n  ?class a owl:Class ;\n         rdfs:label ?label ;\n         IAO:0000115 ?definition .\n  FILTER STRSTARTS(STR(?class), \"http://purl.obolibrary.org/obo/MONDO_\")\n  FILTER NOT EXISTS { ?class owl:deprecated true }\n\n  # Exclude any definition that has a provenance xref in an axiom annotation\n  FILTER NOT EXISTS {\n    ?axiom a owl:Axiom ;\n           owl:annotatedSource ?class ;\n           owl:annotatedProperty IAO:0000115 ;\n           owl:annotatedTarget ?definition ;\n           oboInOwl:hasDbXref ?xref .\n  }\n\n  # Convert class IRI to MONDO CURIE\n  BIND(REPLACE(STR(?class), \"^.*/MONDO_\", \"MONDO:\") AS ?mondo_curie)\n}\nORDER BY ?mondo_curie\n</code></pre> <p></p>"},{"location":"tutorial/sparql-report-odk/","title":"Generating SPARQL table reports with ODK","text":"<p>This tutorial will teach you how to create report tables using SPARQL and the ODK. Report tables are TSV files that can be viewed by programs such as Excel or Google Sheets.</p> <p>For a tutorial on how to generate reports independent of ODK please see here.</p>"},{"location":"tutorial/sparql-report-odk/#preparation","title":"Preparation","text":"<ul> <li>You are set up for executing ODK workflows</li> <li>We assume you have a modern ODK-based repository (ODK version &gt;= 1.2.32) set up. For a tutorial on creating a new ontology repo from scratch see here.</li> <li>Finish the ROBOT tutorial on queries</li> </ul>"},{"location":"tutorial/sparql-report-odk/#tutorial","title":"Tutorial","text":""},{"location":"tutorial/sparql-report-odk/#adding-a-configuration-to-the-odk-yaml-file","title":"Adding a configuration to the ODK YAML file:","text":"<pre><code>robot_report:\n  custom_sparql_exports:\n    - basic-report\n    - my-cat-report\n</code></pre> <p>This will tell the ODK that you no longer wish to generate the ODK default reports (synonyms, xrefs, etc), but instead:</p> <ol> <li>One of the custom reports (<code>basic-report</code>)</li> <li>and a new custom report, called <code>my-cat-report</code>.</li> </ol> <p>Now, we can apply these changes as usual:</p> <pre><code>sh run.sh make update_repo\n</code></pre>"},{"location":"tutorial/sparql-report-odk/#adding-the-actual-table-report","title":"Adding the actual table report","text":"<p>Similar to our ROBOT tutorial on queries, let us now add a simple table report for the terms and labels in our ontology. To do that, let us safe the following file in our <code>src/sparql</code> directory (standard ODK setup), i.e. <code>src/sparql/my-cat-report.sparql</code> (you must use the same name as the one you speciefied in your ODK yaml file above):</p> <pre><code>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\nSELECT ?term ?property ?value\nWHERE {\n  ?term a owl:Class ;\n  rdfs:label ?value .\n}\n</code></pre> <p>Now, let's generate our report (you have to be, as always, in <code>src/ontology/</code>):</p> <pre><code>sh run.sh make custom_reports\n</code></pre> <p>This will generate all custom reports you have configured in one go and save them in the <code>src/ontology/reports</code> directory. <code>reports/my-cat-report.tsv</code> looks probably something like this for you:</p> <pre><code>?term   ?property   ?value\n&lt;http://purl.obolibrary.org/obo/CATO_0000000&gt;       \"root node\"@en\n...\n</code></pre> <p>That is all there is. You can configure as many reports as you want, and they will all be generated with the <code>custom_reports</code> command above, or as part of your ontology releases.</p>"},{"location":"tutorial/sparql-report-robot/","title":"Generating SPARQL table reports with ROBOT","text":""},{"location":"tutorial/sparql-report-robot/#preparation","title":"Preparation","text":"<ul> <li>You should be able to run ROBOT.</li> </ul>"},{"location":"tutorial/sparql-report-robot/#overview","title":"Overview:","text":"<p>Creating table outputs from your ontology helps with many issues, for example during ontology curation (it is often easier to look at tables of related ontology terms rather than a hierarchy), for data aggregation (you want to know how many synonyms there are, and which) and simply to share \"a list of all terms with labels\". There are two major tools to help here:</p> <ul> <li>ROBOT export: Exporting standardised tables for typical use cases, like labels, definitions and similar. For details, please look at the documentation which should provide all the information for producing table reports.</li> <li>ROBOT query: Generating reports using SPARQL. This is the focus of the tutorial here.</li> </ul>"},{"location":"tutorial/sparql-report-robot/#download-test-ontology","title":"Download test ontology","text":"<p>Download <code>example.owl</code>, or get it via the command line:</p> <pre><code>curl https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/robot_tutorial_qc/example.owl &gt; example.owl\n</code></pre> <p>Let us ensure we are using the same ROBOT version:</p> <pre><code>robot --version\n</code></pre> <p>We see:</p> <pre><code>ROBOT version 1.8.3\n</code></pre>"},{"location":"tutorial/sparql-report-robot/#generating-a-simple-report","title":"Generating a simple report","text":"<p>Very frequently, we wish need to create summary tables (for a more detailed motivation see here).</p> <p>Here, lets generate a simple report table by specifying a query:</p> <pre><code>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT ?term ?property ?value\nWHERE {\n  ?term rdfs:label ?value .\n}\n</code></pre> <p>Let us safe the query as <code>labels.sparql</code> in our working directory.</p> <p>Let's now generate the report:</p> <pre><code>robot query -i example.owl --query labels.sparql labels.tsv\n</code></pre> <p>When looking at labels.tsv (in a text editor, or Excel, or whatever table editor you prefer), we notice that some properties are included in our list and decide to change that by restricting the results to classes:</p> <pre><code>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\nSELECT ?term ?property ?value\nWHERE {\n  ?term a owl:Class ;\n  rdfs:label ?value .\n}\n</code></pre> <p>Now, when running the <code>robot query</code> command again, we see only the terms we want.</p> <p>Note that you could have achieved all this with a simple ROBOT export command. However, there are many cool ways you can tweak your reports when you learn how to build them manually during SPARQL. Your only limit is essentially SPARQL itself, which gives you access too most things in your ontology, aside from perhaps complex logical axioms.</p>"},{"location":"tutorial/sparql/","title":"Basic SPARQL for OBO Engineers","text":"<p>In this tutorial we introduce SPARQL, with a particular spin on how we use it across OBO ontologies. Following this tutorial should give you a sense of how we use SPARQL across OBO, without going too much into technical details. You can find concrete tutorials on how to generate reports or QC checks with ROBOT and ODK towards the end of this page.</p>"},{"location":"tutorial/sparql/#preparation","title":"Preparation","text":"<ul> <li>Watch Linked Data Engineering: Querying RDF with SPARQL</li> </ul> <ul> <li>Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour)</li> </ul>"},{"location":"tutorial/sparql/#sparql-tools-for-obo-engineers","title":"SPARQL tools for OBO Engineers","text":"<ul> <li>RENCI Ubergraph Endpoint: Many key OBO ontologies are loaded here with lots of materialised inferences (docs).</li> <li>Ontobee SPARQL endpoint: Useful to run queries across all OBO Foundry ontologies.</li> <li>Yasgui: Yasgui is a simple and beautiful front-end for SPARQL endpoints which can be used not only to query, but also to share queries with others. For example this simple SPARQL query runs across the RENCI Ubergraph Endpoint.</li> <li>GTF: A UI that allows one to run SPARQL queries on TTL files on the web, or upload them. Looks like its based on Yasgui, as it shares the same share functionality.</li> <li>ROBOT query: ROBOT method to generate TSV reports from SPARQL queries, and applying data transformations (<code>--update</code>). ROBOT uses Jena internally to execute SPARQL queries.</li> <li>ROBOT verify: ROBOT method to run SPARQL QC queries. If the query returns a result, the QC test fails.</li> <li>ROBOT report: ROBOT report is a more powerful approach to running OBO QC queries. The default OBO report which ships with ROBOT can be customised by changing the error level, removing a test entirely and even extending the report to custom (SPARQL) checks. Robot report can generate beautiful HTML reports which are easy to read.</li> </ul>"},{"location":"tutorial/sparql/#sparql-in-the-obo-sphere","title":"SPARQL in the OBO-sphere","text":"<p>SPARQL has many uses in the OBO-sphere, but the following in particular:</p> <ol> <li>Quality control checking</li> <li>Creating summary tables for ontologies</li> <li>Sophisticated data transformations in ontology pipelines</li> </ol> <p>We will discuss each of these in the following and give examples. An informal discussion of SPARQL in OBO can be followed in video below.</p>"},{"location":"tutorial/sparql/#quality-control-checking","title":"Quality control checking","text":"<p>For us, ROBOT + SPARQL were a game changer for our quality control (QC) pipelines. This is how it works. First, we encode the error in the form of a SPARQL query (we sometimes call this \"anti-pattern\", i.e. an undesirable (anti-) representation). For example, the following check simply looks for entities that have more than one definition:</p> <pre><code>PREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\nSELECT DISTINCT ?entity ?property ?value WHERE {\n  VALUES ?property { obo:IAO_0000115\n                     obo:IAO_0000600 }\n  ?entity ?property ?value .\n  ?entity ?property ?value2 .\n  FILTER (?value != ?value2)\n  FILTER NOT EXISTS { ?entity owl:deprecated true }\n  FILTER (!isBlank(?entity))\n}\nORDER BY ?entity\n</code></pre> <p>This is a typical workflow. Think of an ontology editor working on an ontology. Often, that curator notices that the same problem happens repeatedly and tell us, the Ontology Pipeline Developer, that they would like a check to prevent the error. We then capture the erroneous situation as a SPARQL query. Then, we add it to our ontology repository, and execute it with ROBOT report or ROBOT verify (see above) in our CI pipelines, usually based on GitHub actions or Travis. Note that the Ontology Development Kit provides a built-in framework for such queries, building on ROBOT verify and report.</p>"},{"location":"tutorial/sparql/#creating-summary-tables-for-ontologies","title":"Creating summary tables for ontologies","text":"<p>Many times, we need to create tabular reports of our ontologies to share with stakeholders or to help with internal reviews, e.g.:</p> <ul> <li>create lists of ontology terms with their definitions and labels</li> <li>create summaries of ontologies, like aggregate statistics</li> </ul> <p>Sometimes using Yasgui, for example in conjunction with the RENCI Ubergraph Endpoint, is enough, but often, using ROBOT query is the better choice, especially if you want to make sure the right version of the ontology is used (Ubergraph occasionally is out of date).</p> <p>Using ROBOT in conjunction with a Workflows Automation system like Github actions helps with generating up-to-date reports. Here is an example of a GitHub action that generates a few reports with ROBOT and pushes them back to the repository.</p>"},{"location":"tutorial/sparql/#a-note-for-data-scientists","title":"A note for Data Scientists","text":"<p>In many cases we are asked how to best \"load an ontology\" into a python notebook or similar. Very often the answer is that it is best to first extract the content of the ontology into a table form, and then load it using a CSV reader like <code>pandas</code>. In this scenario, the workflow for interacting with ontologies is:</p> <ol> <li>Define the information you want in the form of a SPARQL query.</li> <li>Extract the the information as a TSV table using ROBOT query.</li> <li>Load the information into your notebook.</li> </ol> <p>If combined with for example a Makefile, you can always ensure that the report generation process is fully reproducible as well.</p>"},{"location":"tutorial/sparql/#sophisticated-data-transformations-in-ontology-pipelines","title":"Sophisticated data transformations in ontology pipelines","text":"<p>Lastly, we use ROBOT query to implement complex ontology transformation processes. For example the following complex query transforms related synonyms to exact synonyms if some complex condition is met:</p> <pre><code>prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix oboInOwl: &lt;http://www.geneontology.org/formats/oboInOwl#&gt;\nprefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nDELETE {\n  ?term oboInOwl:hasRelatedSynonym ?related .\n  ?relax a owl:Axiom ;\n       owl:annotatedSource ?term ;\n       owl:annotatedProperty oboInOwl:hasRelatedSynonym ;\n       owl:annotatedTarget ?related ;\n       oboInOwl:hasDbXref ?xref2 .\n}\n\nINSERT {\n  ?relax a owl:Axiom ;\n       owl:annotatedSource ?term ;\n       owl:annotatedProperty oboInOwl:hasExactSynonym ;\n       owl:annotatedTarget ?related ;\n       oboInOwl:hasDbXref ?xref2 .\n}\nWHERE\n{\n  {\n    ?term oboInOwl:hasRelatedSynonym ?related ;\n      oboInOwl:hasExactSynonym ?exact ;\n      a owl:Class .\n      ?exax a owl:Axiom ;\n           owl:annotatedSource ?term ;\n           owl:annotatedProperty oboInOwl:hasExactSynonym ;\n           owl:annotatedTarget ?exact ;\n           oboInOwl:hasDbXref ?xref1 .\n      ?relax a owl:Axiom ;\n           owl:annotatedSource ?term ;\n           owl:annotatedProperty oboInOwl:hasRelatedSynonym ;\n           owl:annotatedTarget ?related ;\n           oboInOwl:hasDbXref ?xref2 .\n\n    FILTER (str(?related)=str(?exact))\n    FILTER (isIRI(?term) &amp;&amp; regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\"))\n  }\n}\n</code></pre> <p>This can be a very useful tool for bulk editing the ontology, in particular where it is difficult or impossible to achieve the same using regular expressions or other forms of \"replacement\"-techniques. Here are some example queries we collected to do such mass operations in Mondo.</p>"},{"location":"tutorial/sparql/#related-tutorials","title":"Related tutorials","text":"<ul> <li>QC checks with ROBOT</li> <li>Generating SPARQL table reports with ROBOT</li> <li>Generating SPARQL table reports with ODK</li> </ul>"},{"location":"tutorial/sssom-toolkit/","title":"Introduction to the SSSOM Toolkit","text":"<ul> <li>A basic tutorial</li> <li>Documentation</li> </ul>"},{"location":"tutorial/sssom-tutorial/","title":"Practical introduction to SSSOM","text":""},{"location":"tutorial/sssom-tutorial/#tutorials-and-explainers","title":"Tutorials and Explainers","text":"<ul> <li>Introduction to mapping curation with SSSOM</li> <li>How to use mapping predicates?</li> <li>Guide to publishing 5-Star mappings</li> </ul>"},{"location":"tutorial/units/","title":"Units of Measurement","text":"<p>Units of measurement are among the most fundamental standards that make science, commerce, and much of our daily lives possible. Take a moment to think of some examples of units of measurement that you use in your work and daily life. Write down a few, and we'll come back to your list with some questions at the end.</p> <p>Despite their foundational status and familiarity, units of measurement can be difficult to use in information processing systems such as databases, programming languages, and the semantic web. Data integration of quantitative values is harder than it should be. In this article we'll consider some of the problems and solutions, and end with a short list of practical advice.</p>"},{"location":"tutorial/units/#writing-down-quantitative-values","title":"Writing Down Quantitative Values","text":"<p>When communicating with humans, we usually write down quantitative values as a number and unit, e.g. \"12m\", \"35 kg\". When dealing with computers, we need to be more pedantic. Do we want to represent our quantitative values as:</p> <ol> <li>a single combined value, e.g. the string <code>\"12m\"</code></li> <li>a pair of values, a number and a string or symbol, e.g. <code>[12, \"m\"]</code></li> <li>a larger complex, such as a number and a complex of symbols for the unit, e.g. <code>[35, k, g]</code></li> </ol> <p>The biggest advantage of separating out the numeric part is that we can do numeric operations such as greater-than-or-equal-to which would not be possible with option (1). We can also check the unit part to ensure that we're comparing identical units. Using the pair of a number and a string or symbol for the (combined) unit strikes a good balance.</p>"},{"location":"tutorial/units/#a-number-of-problems","title":"A Number of Problems","text":"<p>Even the number part of a quantitative value is not quite as simple as it seems. Mathematicians, computer hardware and software designers, and scientists all have different perspectives and techniques for working with numbers, and sometimes these do not line up.</p> <p>There's more than one kind of number. Most quantitative values can be represented by real numbers, but some just need integers. It took mathematicians a few thousand years to provide really clear rules for integers and real numbers.</p> <p>There's a \"countably infinite\" number of integers and an \"uncountably infinite\" number of real numbers, but our computers are finite. Our programs limit the numbers that they can represent to strict types that can be represented in binary, e.g. 32 bit signed integer, 64 bit unsigned integer, double precision floating point number. Computer scientists and hardware designers put a lot of work into the theory and practice of how computers crunch numbers. When moving data between programming languages, databases, or data formats, you sometimes do need to be careful about how numbers are converted from one format to another. When running calculations, the rules of specific numeric types and floating point arithmetic can ruin your day.</p> <p>Not only do mathematicians and programmers have different ideas about how numbers need to work, but scientists also have special rules about how numbers are used for measurements and for representing uncertainty. High-school science students are taught rules about significant digits. Publications and plots include uncertainty values and error bars. By default, most software systems do not account for these, and this can be particularly problematic when converting from one system of units to another.</p> <p>Some databases and software tools support numeric ranges, with closed or open bounds, and operators to make use of them. This provides another layer of power and complexity, which may help to handle quantitative values, or just confuse things further.</p> <p>Above we recommended using pairs of numbers and units to represent quantitative values, but that separation leaves us open to mistakes where we sort or search by the numeric part and forget to account for differences in units. Bugs in unit conversion have been known to crash very expensive spacecraft.</p> <p>Most of the time you don't need anything fancy from your numbers, but when you do, be careful and check your work.</p>"},{"location":"tutorial/units/#sorts-of-units","title":"Sorts of Units","text":"<p>The unit part of a quantitative value can be at least as complicated as the numeric part, with fewer standards to go by.</p> <p>While not exhaustive, let's narrow our discussion to three very broad, very rough sorts of units:</p> <ol> <li>\"metric\" (Syst\u00e8me International, SI) units</li> <li>\"historical\" units</li> <li>\"biomedical\" units</li> </ol> <p>The metric system is the dominant system of units across the globe for physical sciences, engineering, trade, and many other fields. It is well-defined and has wide scope, but not everything we want to measure has a metric unit.</p> <p>One other limitation, which is important for our purposes, is that there aren't any official URLs for metric units. This makes it a bit trickier than it should be to use the metric system with linked data, RDF, OWL, and the semantic web.</p> <p>Our \"historical\" group includes the various units that the metric system replaced, and others that fell out of favour. Some of these are only of interest for historical datasets, but I will (controversially?) include \"imperial\" units such as miles, feet, pounds, and ounces, which are still used today.</p> <p>Biomedicine often uses metric or historical units, but also has many other units for counting things, measuring rates (e.g. heart rate), and ratios (e.g. blood sugar, body mass index).</p> <p>Other fields may have units beyond these three groups, but this is already enough for us to chew on today.</p>"},{"location":"tutorial/units/#unit-codes","title":"Unit Codes","text":"<p>We're used to writing down units as short codes. These codes have their own rules that we need to keep in mind.</p> <p>Let's start by stipulating that there are some \"base units\", such as \"m\" for metre, \"s\" for second, foot, pound, \"nm\" for nautical mile, etc.</p> <p>We can often compose these units using multiplication or division, and \"m/s\" get metres per second, or foot pounds (of torque). We can compose units with themselves and then use exponent notation, e.g. \"m/s\u00b2\" for metres per second squared. Superscripts are often difficult to use with software or programming languages, so some systems do not bother with them: <code>m/s2</code>. Division can be expressed by negative exponents, so <code>m.s-2</code> can also express metres per second squared -- the <code>.</code> indicates multiplication, avoiding ambiguity with <code>ms</code> for milliseconds. Now we have two ways of writing the same composed unit: division and multiplication with negative exponents. Multiplication is associative, so in theory the order of the units can vary, allowing for more ways to write the same thing.</p> <p>Then there are the familiar prefixes for indicating powers of ten: \"m\" milli, \"k\" kilo, \"M\" mega, etc. These are an essential part of the metric system, but also used in other systems.</p> <p>These simple rules can generate an infinite number of possible units, and while there is no practical use for the vast majority of all the possible units, in practice it's hard to draw a reasonable boundary around the set of units that someone might want to use.</p> <p>The metric system has clear rules for base units, prefixes, and composition. When it comes to historical units, the Unified Code for Units of Measure (UCUM, https://ucum.org) combines metric and many historical units into a combined system that extends these rules. Like the metric system, in principle this allows for an unlimited number of possible units.</p> <p>What about the biomedical units we care about? UCUM provides a system for \"non-units\" indicated by curly brackets <code>{}</code> that allows arbitrary extension, e.g. <code>{cell}/ml</code> for cells per millilitre.</p> <p>UCUM has good coverage of metric and historical units, and the non-units provide a flexible way to include the \"biomedical\" group. The next two concerns are:</p> <ol> <li>the UCUM license https://ucum.org/license</li> <li>using UCUM on the semantic web</li> </ol> <p>For 2 we would like to have standard URLs that resolve to more information about units, but UCUM does not provide them. This would have to work for the unlimited number of complex units that the UCUM system can generate.</p>"},{"location":"tutorial/units/#unit-ontologies","title":"Unit Ontologies","text":"<p>Several ontologies have been developed to help use units of measurement with RDF, OWL, linked data, and the semantic web.</p> <ul> <li> <p>QUDT grew out of NASA to provide units for science and engineering. It's coverage of these is thorough, but it does not include historical or biological units</p> </li> <li> <p>OBOE Extensible Observation Ontology is a standard that grew out of the ecology domain, and includes units from all three groups</p> </li> <li> <p>OM, the Ontology for Units of Measure, includes all three sorts of units we are interested in</p> </li> <li> <p>UO, the Units of Measurement Ontology, was developed to support use cases in the OBO community, and has more focus on biomedical units than the others listed here</p> </li> </ul> <p>Some of these ontologies include UCUM codes, which will be relevant in a moment.</p> <p>Unfortunately there are some problems here:</p> <ol> <li>No one of these ontologies provides complete coverage of all the metric, historical, and biomedical units that we need</li> <li>All these projects model units of measurement in different ways</li> <li>The unlimited composition of complex units is a problem for any unit ontology build with OWL</li> </ol> <p>While QUDT has the widest support, it has the narrowest scope. OM and UO have wider scope but much narrower community support, with OBOE somewhere in the middle. UCUM has the widest scope and the widest support, but does not integrate with the semantic web.</p>"},{"location":"tutorial/units/#uom","title":"UoM","text":"<p>units-of-measurement.org</p> <p>The UoM project aims to provide URLs for UCUM codes that resolve to linked data web pages. These pages link to RDF/OWL representations, and to other unit ontologies when they are available. The bridge to these other ontologies is the annotations with UCUM codes that they provide. UoM implements a UCUM parser that can handle the unlimited combinations that UCUM allows -- although there are, of course, practical limitations to the system.</p> <p>For example, <code>m.s-2</code>:</p> <pre><code>@prefix NERC_P06: &lt;http://vocab.nerc.ac.uk/collection/P06/current/&gt; .\n@prefix OM: &lt;http://www.ontology-of-units-of-measure.org/resource/om-2/&gt; .\n@prefix QUDT: &lt;http://qudt.org/vocab/unit/&gt; .\n@prefix UO: &lt;http://purl.obolibrary.org/obo/UO_&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix unit: &lt;https://w3id.org/uom/&gt; .\n\nunit:SI_code a owl:AnnotationProperty ;\n    rdfs:label \"SI code\" .\n\nunit:UCUM_code a owl:AnnotationProperty ;\n    rdfs:label \"UCUM code\" .\n\nunit:m.s-2 a owl:NamedIndividual ;\n    rdfs:label \"metre per square second\"@en ;\n    skos:altLabel \"meter per square second\"@en ;\n    skos:definition \"A unit which is equal to 1 metre per 1 square second.\"@en ;\n    skos:exactMatch UO:0000077,\n        QUDT:M-PER-SEC2,\n        NERC_P06:MPS2,\n        OM:metrePerSecond-TimeSquared ;\n    unit:SI_code \"m s-2\" ;\n    unit:UCUM_code \"m.s-2\",\n        \"m/s2\" .\n</code></pre> <p>The first step to providing URLs for UCUM codes is to \"URL escape\" (i.e. \"percent encode\") any characters that are excluded from URLs or have special meanings, especially <code>/</code>.</p> <p>The second step is to pick a single normal form. UCUM allows multiple ways to write the same unit, such as \"m/s\" and \"m.s-1\". Most semantic web system compare URLs by string identity, so it is important to have a single normalized form. Since <code>/</code> is a special character for URLs, we chose to use exponents throughout, sorting from most positive to most negative exponent, and sorting alphabetically when exponents are the same.</p> <p>The third step is to provide labels for each unit. Since arbitrary compositions of units are possible, this is a little tricky. We currently support English labels, and would like to support more.</p> <p>The UoM system is still a work in progress. It likely contains some bugs, and UCUM non-units (<code>{}</code>) are not yet supported. If you would like to help finish this project, help would be appreciated!</p>"},{"location":"tutorial/units/#units-in-practice","title":"Units in Practice","text":"<p>Here's my practical advice:</p> <ol> <li> <p>If you only use metric units, stick to metric units, otherwise use UCUM</p> </li> <li> <p>If you need a UCUM non-unit, try to use an OBO ontology label</p> </li> <li> <p>Treat your quantitative values as the pair of a unit and a metric or UCUM code:</p> <ul> <li>an 2-tuple or array of length 2: <code>[12, \"m\"]</code></li> <li>two columns in a database</li> <li>two keys in a dictionary/object</li> <li>a subject with two triples in RDF</li> </ul> </li> <li> <p>If your units never change, consider building the unit part into the column name, dictionary key, RDF predicate, or OWL property, e.g. \"height in m\"</p> </li> <li> <p>Be careful with your numeric types</p> </li> <li> <p>If you need URLs for UCUM, use units-of-measurement.org</p> </li> <li> <p>Consider using LinkML and one of these patterns for 'How to model quantities and measurements'</p> </li> </ol>"},{"location":"tutorial/units/#are-we-there-yet","title":"Are We There Yet?","text":"<p>Look back at the list of units you wrote down at the beginning:</p> <ul> <li>Are they all metric?</li> <li>If not, are they all in UCUM?</li> <li>If not, could they fit in UCUM with non-units?</li> <li>If they use non-units, can those be found in OBO?</li> <li>If not, can they be added to OBO?</li> </ul>"}]}